{"notes":[{"tddate":null,"ddate":null,"tmdate":1515760541420,"tcdate":1515760541420,"number":11,"cdate":1515760541420,"id":"HkHiimLEG","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference/Paper1110/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/Authors"],"content":{"title":"Paper updated","comment":"We have updated the paper to address the concerns raised by the reviewers.\nIn particular we have included:\n - A detailed theoretical analysis of the MPO framework\n - An updated methods section that has a simpler derivation of the algorithm"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1515181811477,"tcdate":1515181811477,"number":7,"cdate":1515181811477,"id":"HyixP8aQz","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"rypb6tngM","signatures":["ICLR.cc/2018/Conference/Paper1110/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/Authors"],"content":{"title":"Thank you for your questions and insightful comments.","comment":"We thank you for your questions and insightful comments. \n\n> There are several approaches that already use the a combination of the KL-constraint with reverse KL on a non-\nparametric distribution and subsequently an M-projection to obtain again a parametric distribution, see HiREPS, non-parametric REPS [Hoof2017, JMLR] or AC-REPS [Wirth2016, AAAI]. \n> These algorithms do not use the inference-based view but the trust region justification. As in the non-parametric case, the asymptotic performance guarantees from the EM framework are gone, why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward?\n\nThank you for pointing out the additional related work. We will include it in the paper. Regarding the EM vs. trust-region question: The benefit of deriving the algorithm from the perspective of an EM-like coordinate ascent is that it motivates and provides a convenient means for theoretical analysis of the two-step procedure used in our approach. See the added a theoretical analysis that was added to the appendix of the paper.\n\n> It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1. \n> First, alpha changes every iteration of the algorithm while the objective assumes that alpha is constant. \n> This means that we change the objective all the time which is theoretically a bit weird. \n> Moreover, the presented algorithm also changes the prior all the time (in order to introduce the 2nd trust region) in the M-step. \n> Again, this changes the objective, so it is unclear to me what exactly is maximised in the end. \n> Would it not be cleaner to start with the average reward objective (no prior or alpha) and then introduce both trust regions\n> just out of the motivation that we need trust regions in policy search? Then the objective is clearly defined.    \n\nThe reviewers point is well taken. While we think the unconstrained (soft-regularized) is instructive and useful for theoretical analysis the hard-constrained version can indeed be understood as proposed by the reviewer and equally provides important insights. We will  clarify this in the paper and also include an experimental comparison between the soft and hard-regularized cases.\nRegarding your two concerns: For our theoretical guarantee (that we have now derived in the appendix) to hold we have to fix alpha. However, in practice it changes slowly during optimization and converges to a stable value. One can indeed think of the second trust-region as a simple regularizer that prevents overfitting/too large changes in the (sample-based) M-step (similar small changes in the policy are also required by our proof).\n\n- Regarding the additional experiments you asked for:\n\nWe agree and have carried out additional experiments that will be included in the final version, preliminary results are as follows:\n\n1) MPO without trust region in M-step:\nAlso works well for low-dimensional problems but is less robust for high-dimensional problems such as the humanoid.\n\n2) MPO without retrace algorithm for getting the Q-value\nIs significantly slower to reach the same level of performance in the majority of the control suite tasks (retrace + MPO is never worse in any of the control suite tasks).\n\n3) test different epsilons for E and M step\nThe algorithm seems to be robust to settings of epsilon - as long as it is set roughly to the right order of magnitude (10^-3 to 10^-2 for the E-step, 10^-4 to 10^-1 for the M-step). A very small epsilon will, of course, slow down convergence."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1515181591211,"tcdate":1515181591211,"number":6,"cdate":1515181591211,"id":"S1ymUU6Xz","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"H1y3N2alf","signatures":["ICLR.cc/2018/Conference/Paper1110/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/Authors"],"content":{"title":"We thank the reviewer for comments and thoughtful questions.","comment":"We thank the reviewer for comments and thoughtful questions. We reply to your main concerns in turn below.\n\n> When is it possible to normalize the non-parametric q(a|s) in equation (6)? It seems to me this will be challenging in most any situation where the action space is continuous. \n> Is this guaranteed to be Gaussian? If so, I don’t understand why.\n\nPlease see appendix, section C.2. In the parametric case the solution for q(a|s) is trivially normalized when we impose a parametric form that allows analytic evaluation of the normalization function (such as a Gaussian distribution). . \nFor the non-parametric case note that the normalizer is given by \nZ(s) = \\int \\pi_old(a|s) exp( Q(s,a)/eta) da,\ni.e. it is an expectation with respect to our old policy for which we can obtain a MC estimate: \\hat{Z}(s) = 1/N \\sum_i exp(Q(s,a_i)/eta)    with a_i \\sim \\pi_old( \\cdot | s).\nThus we can empirically normalize the density for those state-action samples that we use to estimate pi_new in the M-step.\n\n> In equations (5) and (10), a KL divergence regularizer is replaced by a “hard” constraint. \n> However, for optimization purposes, in C.3 the hard constraint is then replaced by a soft constraint (with Lagrange multipliers), which depend on values of epsilon. \n> Are these values of epsilon easy to pick in practice? If so, why are they easier to pick than e.g. the lambda value in eq (10)?\n\nThank you for pointing out that the reasoning behind this was not entirely easy to follow. We will improve the presentation in the paper. Indeed we found that choosing epsilon can be easier than choosing a multiplier for the KL regularizer. This is due to the fact that the scale of the rewards is unknown a-priori and hence the multiplier that trades of maximizing expected reward and minimizing KL can be expected to change for different RL environments. In contrast to this, when we put a hard constraint on the KL we can explicitly force the policy to stay \"epsilon-close\" to the last solution - independent of the reward scale. This allows for an easier transfer of hyperparameters across tasks."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1515181419987,"tcdate":1515181419987,"number":5,"cdate":1515181419987,"id":"HJEuS8amM","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"Hy4_ANE-f","signatures":["ICLR.cc/2018/Conference/Paper1110/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/Authors"],"content":{"title":"Thank you for your review, we have prepared an additional theoretical analysis and will update the paper","comment":"We appreciate the detailed comments and questions regarding the connection between our method and EM methods. We have addressed your main concern with an additional theoretical analysis of the algorithm, strengthening the paper.\n\n> 1. For parametric EM case, there is asymptotic convergence guarantee to local optima case; However, for nonparametric \n> EM case, there is no guarantee for that. This is the biggest concern I have for the theoretical justification of the paper.\n\nWe have derived a proof that gives a monotonic improvement  guarantee for the nonparametric variant of the algorithm under certain circumstances. We will include this proof in the paper. To summarize: Assuming Q can be represented and estimated, the \"partial\" E-step in combination with an appropriate gradient-based M-step leads to an improvement of the KL regularized objective and guarantees monotonic improvement of the overall procedure under certain circumstances. See also our response to the Anonymous question below.\n\n> 2. In section 4, it is said that Retrace algorithm from Munos et al. (2016) is used for policy evaluation. This is not true. \n> The Retrace algorithm, is per se, a value iteration algorithm. I think the author could say using the policy evaluation version of Retrace, \n> or use the truncated importance weights technique as used in Retrace algorithm, which is more accurate.\n\nWe will clarify that we are using the Retrace operator for policy evaluation only (This use case was indeed also analyzed in Munos et al. (2016)).\n\n> Besides, a minor point: Retrace algorithm is not off-policy stable with function approximation, as shown in several recent papers, such as \n> “Convergent Tree-Backup and Retrace with Function Approximation”. But this is a minor point if the author doesn’t emphasize too much about off-policy stability.\n\nWe agree that off-policy stability with function approximation is an important open problem that deserves additional attention but not one specific to this method (i.e. any existing DeepRL algorithm shares these concerns). We will add a short note.\n\n> 3. The shifting between the unconstrained multiplier formulation in Eq.9 to the constrained optimization formulation in Eq.10 should be clarified. \n> Usually, an in-depth analysis between the choice of \\lambda in multiplier formulation and the \\epsilon in the constraint should be discussed, which is necessary for further theoretical analysis. \n\nWe now have a detailed analysis of the unconstrained multiplier formulation (see comment above) of our algorithm. In practice we found that implementing updates according to both hard-constraints and using a fixed regularizer worked well for individual domains. Both \\lambda and \\epsilon can be found via a small hyperparameter search in this case. When applying the algorithm to many different domains  (with widely different reward scales) with the same set of hyperparameters we found it easier to use the hard-constrained version; which is why we placed a focus on it. We will include these experimental results in an updated version of the paper. We believe these observations are in-line with research on hard-constrained/KL-regularized on-policy learning algorithms such as PPO/TRPO (for which explicit connections between the two settings are also ). \n\n> 4. The experimental conclusions are conducted without sound evidence. For example, the author claims the method to be 'highly data efficient' compared with existing approaches, however, there is no strong evidence supporting this claim. \n\nWe believe that the large set of experiments we conducted in the experimental section gives evidence for this. Figure 4 e.g. clearly shows the improved data-efficiency MPO gives over our implementations of state-of-the-art RL algorithms for both on-policy (PPO) and off-policy learning (DDPG, policy gradient + Retrace). Further, when looking at the results for the parkour domain we observe an order of magnitude improvement over the reference experiment. We have started additional experiments for parkour with a full humanoid body - leading to similar speedups over PPO - which will be included in the final version and further solidify the claim on a more difficult benchmark."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1515642385319,"tcdate":1512488555764,"number":3,"cdate":1512488555764,"id":"Hy4_ANE-f","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Review","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference/Paper1110/AnonReviewer1"],"readers":["everyone"],"content":{"title":"some details to discuss","rating":"5: Marginally below acceptance threshold","review":"This paper studies new off-policy policy optimization algorithm using relative entropy objective and use EM algorithm to solve it. The general idea is not new, aka, formulating the MDP problem as a probabilistic inference problem. \n\nThere are some technical questions:\n1. For parametric EM case, there is asymptotic convergence guarantee to local optima case; However, for nonparametric EM case, there is no guarantee for that. This is the biggest concern I have for the theoretical justification of the paper.\n\n2. In section 4, it is said that Retrace algorithm from Munos et al. (2016) is used for policy evaluation. This is not true. The Retrace algorithm, is per se, a value iteration algorithm. I think the author could say using the policy evaluation version of Retrace, or use the truncated importance weights technique as used in Retrace algorithm, which is more accurate.\n\nBesides, a minor point: Retrace algorithm is not off-policy stable with function approximation, as shown in several recent papers, such as \n“Convergent Tree-Backup and Retrace with Function Approximation”. But this is a minor point if the author doesn’t emphasize too much about off-policy stability.\n\n3. The shifting between the unconstrained multiplier formulation in Eq.9 to the constrained optimization formulation in Eq.10 should be clarified. Usually, an in-depth analysis between the choice of \\lambda in multiplier formulation and the \\epsilon in the constraint should be discussed, which is necessary for further theoretical analysis. \n\n4. The experimental conclusions are conducted without sound evidence. For example, the author claims the method to be 'highly data efficient' compared with existing approaches, however, there is no strong evidence supporting this claim. \n\n\nOverall, although the motivation of this paper is interesting, I think there is still a lot of details to improve. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1515642385362,"tcdate":1512060070902,"number":2,"cdate":1512060070902,"id":"H1y3N2alf","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Review","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference/Paper1110/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting off-policy algorithms with nice results","rating":"6: Marginally above acceptance threshold","review":"This is an interesting policy-as-inference approach, presented in a reasonably clear and well-motivated way. I have a couple questions which somewhat echo questions of other commenters here. Unfortunately, I am not sufficiently familiar with the relevant recent policy learning literature to judge novelty. However, as best I am aware the empirical results presented here seem quite impressive for off-policy learning.\n\n- When is it possible to normalize the non-parametric q(a|s) in equation (6)? It seems to me this will be challenging in most any situation where the action space is continuous. Is this guaranteed to be Gaussian? If so, I don’t understand why.\n\n– In equations (5) and (10), a KL divergence regularizer is replaced by a “hard” constraint. However, for optimization purposes, in C.3 the hard constraint is then replaced by a soft constraint (with Lagrange multipliers), which depend on values of epsilon. Are these values of epsilon easy to pick in practice? If so, why are they easier to pick than e.g. the lambda value in eq (10)?\n\n","confidence":"1: The reviewer's evaluation is an educated guess"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1511984566837,"tcdate":1511984566837,"number":3,"cdate":1511984566837,"id":"HkJ6aF2xf","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference/Paper1110/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/AnonReviewer2"],"content":{"title":"Few comments...","comment":"\nI do have a few comments / corrections / questions about the paper:\n\n- There are several approaches that already use the a combination of the KL-constraint with reverse KL on a non-parametric distribution and subsequently an M-projection to obtain again a parametric distribution, see HiREPS, non-parametric REPS [Hoof2017, JMLR] or AC-REPS [Wirth2016, AAAI]. These algorithms do not use the inference-based view but the trust region justification. As in the non-parametric case, the asymptotic performance guarantees from the EM framework are gone, why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward?\n\n- It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1. First, alpha changes every iteration of the algorithm while the objective assumes that alpha is constant. This means that we change the objective all the time which is theoretically a bit weird. Moreover, the presented algorithm also changes the prior all the time (in order to introduce the 2nd trust region) in the M-step. Again, this changes the objective, so it is unclear to me what exactly is maximised in the end. Would it not be cleaner to start with the average reward objective (no prior or alpha) and then introduce both trust regions just out of the motivation that we need trust regions in policy search? Then the objective is clearly defined.    \n\n- I did not get whether the additional \"one-step KL regularisation\" is obtained from the lower bound or just added as additional regularisation? Could you explain?\n\n- The algorithm has now 2 KL constraints, for E and M step. Is the epsilon for both the same or can we achieve better performance by using different epsilons?\n\n- I think the following experiments would be very informative:\n\n   - MPO without trust region in M-step\n   \n   - MPO without retrace algorithm for getting the Q-value\n\n   - test different epsilons for E and M step\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1515642385402,"tcdate":1511984388978,"number":1,"cdate":1511984388978,"id":"rypb6tngM","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Review","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference/Paper1110/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper presents an interesting new algorithm for deep reinforcement learning which outperforms state of the art methods. ","rating":"7: Good paper, accept","review":"The paper presents a new algorithm for inference-based reinforcement learning for deep RL. The algorithm decomposes the policy update in two steps, an E and an M-step. In the E-step, the algorithm estimates a variational distribution q which is subsequentially used for the M-step to obtain a new policy. Two versions of the algorithm are presented, using a parametric or a non-parametric (sample-based) distribution for q. The algorithm is used in combination with the retrace algorithm to estimate the q-function, which is also needed in the policy update.\n\nThis is a well written paper presenting an interesting algorithm. The algorithm is similar to other inference-based RL algorithm, but is the first application of inference based RL to deep reinforcement learning. The results look very promising and define a new state of the art or deep reinforcement learning in continuous control, which is a very active topic right now. Hence, I think the paper should be accepted. \n\n\nI do have a few comments / corrections / questions about the paper:\n\n- There are several approaches that already use the a combination of the KL-constraint with reverse KL on a non-parametric distribution and subsequently an M-projection to obtain again a parametric distribution, see HiREPS, non-parametric REPS [Hoof2017, JMLR] or AC-REPS [Wirth2016, AAAI]. These algorithms do not use the inference-based view but the trust region justification. As in the non-parametric case, the asymptotic performance guarantees from the EM framework are gone, why is it beneficial to formulate it with EM instead of directly with a trust region of the expected reward?\n\n- It is not clear to me whether the algorithm really optimizes the original maximum a posteriori objective defined in Equation 1. First, alpha changes every iteration of the algorithm while the objective assumes that alpha is constant. This means that we change the objective all the time which is theoretically a bit weird. Moreover, the presented algorithm also changes the prior all the time (in order to introduce the 2nd trust region) in the M-step. Again, this changes the objective, so it is unclear to me what exactly is maximised in the end. Would it not be cleaner to start with the average reward objective (no prior or alpha) and then introduce both trust regions just out of the motivation that we need trust regions in policy search? Then the objective is clearly defined.    \n\n- I did not get whether the additional \"one-step KL regularisation\" is obtained from the lower bound or just added as additional regularisation? Could you explain?\n\n- The algorithm has now 2 KL constraints, for E and M step. Is the epsilon for both the same or can we achieve better performance by using different epsilons?\n\n- I think the following experiments would be very informative:\n\n   - MPO without trust region in M-step\n   \n   - MPO without retrace algorithm for getting the Q-value\n\n   - test different epsilons for E and M step\n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1511861375520,"tcdate":1511861375520,"number":2,"cdate":1511861375520,"id":"S1DYhi9gz","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"Hkk3DISC-","signatures":["ICLR.cc/2018/Conference/Paper1110/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/Authors"],"content":{"title":"Re: Clarifications ","comment":"Thank you for carefully reading of the paper and uncovering a few minor mistakes.\n\n> Firstly, I think it would be helpfull to formally define what $$q(\\rho)$$ is.  My current assumption is: $$q(\\rho) = p(s_0) \\prod_1^\\infty p(s_{t+1}|a_t, s_t) q(a_t|s_t)$$.\nYour assumption is correct. q(\\rho) is analogous to p(\\rho) (as described in the background section on MDPs). We will add this definition. \n\n>1. I think at the end of the line you should have $$+ \\log p(\\theta)$$ rather than $$+ p(\\theta)$$ (I believe this is a typo)\nCorrect, this is indeed a typo and will be fixed in the next revision of the paper.\n\n> 2. In the definition of the log-probabilities, the $$\\alpha$$ parameter appears only in the definition of 'p(O=1|\\rho)'. The way it appears is as a denominator in the log-probability. In line 4 of equation (1) it has suddenly appeared as a multiplier in front of the log-densities of $$\\pi(a|s_t)$$ and $$q(a|s_t)$$. This is possible if we factor out the $$\\alpha^{-1}$$ from the sum of the rewards, but then on that line, there should be a prefactor of $$\\alpha^{-1}$$ in front of the expectation over 'q' which seems missing. (I believe this is a typo as well).\n\nIn this step we indeed just multiplied with the (non-zero) \\alpha. We presume you meant that alpha is then, however, missing in front of the prior p(\\theta) here. You are correct and this will be also fixed in the next revision.\n\n> 3. In the resulting expectation, it is a bit unclear how did the discount factors $$\\gamma^t$$ have appeared as well as in front of the rewards also in front of the KL divergences? From the context provided I really failed to be able to account for this, and given that for the rest of the paper this form has been used more than once I was wondering if you could provide some clarification on the derivation of the equation as it is not obvious to at least some of the common readers of the paper.\n\nThank you for pointing out this inconsistency which has arisen due to some last minute changes in notation that we introduced when we unified the notation in the paper - switching from presenting the finite-horizon, undiscounted, setting to using the infinite-horizon formulation. As pointed out by previous work (e.g. Rawlik et al.)  there is a direct correspondence between learning / inference in an appropriately constructed graphical model (as suggested by the first line of Eq. 1) and the regularized control objective in the finite horizon, undiscounted case. The regularized RL objective still exists in the discounted, infinite horizon case (e.g. Rawlik et al. or see [1] for another construction), but an equivalent graphical model is harder to construct (and is not of the form currently presented in the paper; e.g. see [1]). We will fix this and clarify the relation in the revision\n\n[1] Probabilistic Inference for Solving Discrete and Continuous State Markov Decision Processes, Marc Toussaint, Amos Storkey, ICML 2004"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1511861301074,"tcdate":1511861301074,"number":1,"cdate":1511861301074,"id":"By6E3iqxz","invitation":"ICLR.cc/2018/Conference/-/Paper1110/Official_Comment","forum":"S1ANxQW0b","replyto":"HkX4eXIC-","signatures":["ICLR.cc/2018/Conference/Paper1110/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1110/Authors"],"content":{"title":"Thank you for spotting some minor inconsistencies","comment":"Thank you for your thorough read of the paper. \n\n> The derivation of \"one-step KL regularised objective\" is unclear to me and this seems to be related to a partial E-step. \n\nWe will clarify the relationship between the one-step objective and Eq. 1 in more detail in a revised version of the paper. We will also include a proof that the the specific \"partial\" update we use in the E-step leads to an improvement in Eq. (1) and guarantees monotonic improvement of the overall procedure.\n\nIn short, the relation between objective (1) and formula (4) is as follows:\ninstead of optimizing objective (1) directly in the E-step (which would entail running soft-Q-learning to convergence - e.g. Q-learning with additional KL terms of subsequent time-steps in a trajectory added to the rewards) we start from the \"unregularized\" Q-function (Eq. (3)) and expand it via the \"regularized\"  Bellman operator T Q(s,a) = E_a[Q(s,a)] + \\alpha KL(q || \\pi). We thus only consider the KL at a given state s in the E-step and not the \"full\" objective from (1). Nonetheless, as mentioned above we have now prepared a proof that this still leads to an improvement in (1).\n\n> (2) As far as I know, the previous works on variational RL maximize the marginal log-likelihood p(O=1|\\theta) (Toussaint (2009) and Rawlik (2012)), whereas you maximizes the unnormalized posterior p(O=1, \\theta) with the prior assumption on $\\theta$. I wonder if the prior assumption enhances the performance. \n\nCorrect. The prior p(\\theta) allows us to add regularization to the M-step of our procedure (enforcing a trust-region on the policy). We found this to be important when dealing with hihg-dimensional systems like the humanoid where the M-step could otherwise overfit (as the integral over action is only evaluated using 30 samples in our experiments).\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1509466155110,"tcdate":1509466155110,"number":2,"cdate":1509466155110,"id":"HkX4eXIC-","invitation":"ICLR.cc/2018/Conference/-/Paper1094/Public_Comment","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Comments","comment":"(1) Clarification of Equation 4\n\nThe derivation of \"one-step KL regularised objective\" is unclear to me and this seems to be related to a partial E-step. \n\nWould you explain this part in more detail?\n\n(2) As far as I know, the previous works on variational RL maximize the marginal log-likelihood p(O=1|\\theta) (Toussaint (2009) and Rawlik (2012)), whereas you maximizes the unnormalized posterior p(O=1, \\theta) with the prior assumption on $\\theta$. \nI wonder if the prior assumption enhances the performance. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1509414823044,"tcdate":1509414823044,"number":1,"cdate":1509414823044,"id":"Hkk3DISC-","invitation":"ICLR.cc/2018/Conference/-/Paper1094/Public_Comment","forum":"S1ANxQW0b","replyto":"S1ANxQW0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Clarification of Equation 1","comment":"These might be very obvious questions, but I failed to derive the last line (line 4) in equation (1) in the paper. \n\nFirstly, I think it would be helpfull to formally define what $$q(\\rho)$$ is. My current assumption is:\n$$q(\\rho) = p(s_0) \\prod_1^\\infty p(s_{t+1}|a_t, s_t) q(a_t|s_t)$$\nwhere the 'p' distributions are taken to be equal to the real environmental state transitions.\n\nNow, there are a few problems that I encountered when trying to derive equation (1):\n\n1. I think at the end of the line you should have $$+ \\log p(\\theta)$$ rather than $$+ p(\\theta)$$ (I believe this is a typo)\n\n2. In the definition of the log-probabilities, the $$\\alpha$$ parameter appears only in the definition of 'p(O=1|\\rho)'. The way it appears is as a denominator in the log-probability. In line 4 of equation (1) it has suddenly appeared as a multiplier in front of the log-densities of $$\\pi(a|s_t)$$ and $$q(a|s_t)$$. This is possible if we factor out the $$\\alpha^{-1}$$ from the sum of the rewards, but then on that line, there should be a prefactor of $$\\alpha^{-1}$$ in front of the expectation over 'q' which seems missing. (I believe this is a typo as well).\n\n3. In the resulting expectation, it is a bit unclear how did the discount factors $$\\gamma^t$$ have appeared as well as in front of the rewards also in front of the KL divergences? From the context provided I really failed to be able to account for this, and given that for the rest of the paper this form has been used more than once I was wondering if you could provide some clarification on the derivation of the equation as it is not obvious to at least some of the common readers of the paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]}},{"tddate":null,"ddate":null,"tmdate":1515752182809,"tcdate":1509138486475,"number":1110,"cdate":1510092359919,"id":"S1ANxQW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1ANxQW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Maximum a Posteriori Policy Optimisation","abstract":"We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.","pdf":"/pdf/dc655f3943b7cc3dcc9ba79bfaf1997c200420b1.pdf","paperhash":"anonymous|maximum_a_posteriori_policy_optimisation","_bibtex":"@article{\n  anonymous2018maximum,\n  title={Maximum a Posteriori Policy Optimisation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1ANxQW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1110/Authors"],"keywords":["Reinforcement Learning","Variational Inference","Control"]},"nonreaders":[],"replyCount":12,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}