{"notes":[{"tddate":null,"ddate":null,"tmdate":1512294804088,"tcdate":1512294804088,"number":3,"cdate":1512294804088,"id":"ryhcYB-bG","invitation":"ICLR.cc/2018/Conference/-/Paper390/Official_Review","forum":"BkN_r2lR-","replyto":"BkN_r2lR-","signatures":["ICLR.cc/2018/Conference/Paper390/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting direction but unconvincing experiments and uncompelling applications","rating":"4: Ok but not good enough - rejection","review":"This paper adds an interesting twist on top of recent unpaired image translation work. A domain-level translation function is jointly optimized with an instance-level matching objective. This yields the ability to extract corresponding image pairs out of two unpaired datasets, and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches. I think this is a promising direction, but the current paper has unconvincing results, and it’s not clear if the method is really solving an important problem yet.\n\nMy main criticism is with the experiments and results. The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets. Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches. This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches. It feels rather contrived to focus so much on the datasets with exact matches since, 1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly, 2) it’s hard to imagine datasets that have exact but unknown matches (I welcome the authors to put forward some such scenarios), 3) when exact matches exist, simpler methods may be sufficient, such as matching edges. There is no comparison to any such simple baselines.\n\nI think finding analogies that are not exact matches is much more compelling. Quantifying performance in this case may be hard, and the current paper only offers a few qualitative results. I’d like to see far more results, and some attempt at a metric. One option would be to run user studies where humans judge the quality of the matches. The results shown in Figure 2 don’t convince me, not just because they are qualitative and few, but also because I’m not sure I even agree that the proposed method is producing better results: for example, the DiscoGAN results have some artifacts but capture the texture better in row 3.\n\nI was also not convinced by the supervised second step in Section 4.3. Given that the first step achieves 97% alignment accuracy, it’s no surprised that running an off-the-shelf supervised method on top of this will match the performance of running on 100% correct data. In other words, this section does not really add much new information beyond what we could already infer given that the first stage alignment was so successful.\n\nWhat I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches. For example, the shoes and handbags dataset or even better, domain adaptation datasets like sim to real.\n\nI’d like to see more discussion of why the second stage supervised problem is beneficial. Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one-hot and T is simply training against a supervised objective (Equation 7)?\n\nMinor comments:\n1. In the intro, it would be useful to have a clear definition of “analogy” for the present context.\n2. Page 2: a link should be provided for the Putin example, as it is not actually in Zhu et al. 2017.\n3. Page 3: “Weakly Supervised Mapping” — I wouldn’t call this weakly supervised. Rather, I’d say it’s just another constraint / prior, similar to cycle-consistency, which was referred to under the “Unsupervised” section.\n4. Page 4 and throughout: It’s hard to follow which variables are being optimized over when. For example, in Eqn. 7, it would be clearer to write out the min over optimization variables.\n5. Page 6: The Maps dataset was introduced in Isola et al. 2017, not Zhu et al. 2017.\n6. Page 7: The following sentence is confusing and should be clarified: “This shows that the distribution matching is able to map source images that are semantically similar in the target domain.”\n7. Page 7: “This shows that a good initialization is important for this task.” — Isn’t this more than initialization? Rather, removing the distributional and cycle constraints changes the overall objective being optimized.\n8. In Figure 2, are the outputs the matched training images, or are they outputs of the translation function?\n9. Throughout the paper, some citations are missing enclosing parentheses.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Identifying Analogies Across Domains","abstract":"Identifying analogies across domains without supervision is a key task for artificial intelligence. Recent advances in cross domain image mapping have concentrated on translating images across domains. Although the progress made is impressive, the visual fidelity many times does not suffice for identifying the matching sample from the other domain. In this paper, we tackle this very task of finding exact analogies between datasets i.e. for every image from domain A find an analogous image in domain B. We present a matching-by-synthesis approach: AN-GAN, and show that it outperforms current techniques. We further show that the cross-domain mapping task can be broken into two parts: domain alignment and learning the mapping function. The tasks can be iteratively solved, and as the alignment is improved, the unsupervised translation function reaches quality comparable to full supervision. ","pdf":"/pdf/6b49a6efc8b64fafd498540f8bdf08762d2b1722.pdf","TL;DR":"Finding correspondences between domains by performing matching/mapping iterations","paperhash":"anonymous|identifying_analogies_across_domains","_bibtex":"@article{\n  anonymous2018identifying,\n  title={Identifying Analogies Across Domains},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkN_r2lR-}\n}","keywords":["unsupervised mapping","cross domain mapping"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper390/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222633844,"tcdate":1512079705086,"number":2,"cdate":1512079705086,"id":"HJ08-bCef","invitation":"ICLR.cc/2018/Conference/-/Paper390/Official_Review","forum":"BkN_r2lR-","replyto":"BkN_r2lR-","signatures":["ICLR.cc/2018/Conference/Paper390/AnonReviewer1"],"readers":["everyone"],"content":{"title":"The approach is interesting but the paper lacks clarity of presentation","rating":"5: Marginally below acceptance threshold","review":"The paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis. The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre-learned or not). The algorithm is tested on three datasets.\n\nWhile the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts), the paper lacks clarity and sufficient details.\n\nInstead of the longer intro and related work discussion, I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments. Also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level. It is hard to understand sufficiently well what the formalism means without more insight.\n\nAlso, the experiments need more details. For example, it is not clear what the numbers in Table 2 mean.\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Identifying Analogies Across Domains","abstract":"Identifying analogies across domains without supervision is a key task for artificial intelligence. Recent advances in cross domain image mapping have concentrated on translating images across domains. Although the progress made is impressive, the visual fidelity many times does not suffice for identifying the matching sample from the other domain. In this paper, we tackle this very task of finding exact analogies between datasets i.e. for every image from domain A find an analogous image in domain B. We present a matching-by-synthesis approach: AN-GAN, and show that it outperforms current techniques. We further show that the cross-domain mapping task can be broken into two parts: domain alignment and learning the mapping function. The tasks can be iteratively solved, and as the alignment is improved, the unsupervised translation function reaches quality comparable to full supervision. ","pdf":"/pdf/6b49a6efc8b64fafd498540f8bdf08762d2b1722.pdf","TL;DR":"Finding correspondences between domains by performing matching/mapping iterations","paperhash":"anonymous|identifying_analogies_across_domains","_bibtex":"@article{\n  anonymous2018identifying,\n  title={Identifying Analogies Across Domains},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkN_r2lR-}\n}","keywords":["unsupervised mapping","cross domain mapping"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper390/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1512222633885,"tcdate":1511913916642,"number":1,"cdate":1511913916642,"id":"SkHatuolz","invitation":"ICLR.cc/2018/Conference/-/Paper390/Official_Review","forum":"BkN_r2lR-","replyto":"BkN_r2lR-","signatures":["ICLR.cc/2018/Conference/Paper390/AnonReviewer3"],"readers":["everyone"],"content":{"title":"AN-GAN: match-aware translation of images across domains, new ideas for combining image matching and GANs","rating":"7: Good paper, accept","review":"This paper presents an image-to-image cross domain translation framework based on generative adversarial networks. The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved. The results show that the proposed method is superior for the task of exact correspondence identification and that AN-GAN rivals the performance of pix2pix with strong supervision.\n\n\nNegatives:\n1.) The task of exact correspondence identification seems contrived. It is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing.\n2.) The supervised vs unsupervised experiment on Facades->Labels (Table 3) is only one scenario where applying a supervised method on top of AN-GAN’s matches is better than an unsupervised method.  More transfer experiments of this kind would greatly benefit the paper and support the conclusion that “our self-supervised method performs similarly to the fully supervised method.” \n\nPositives:\n1.) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework\n2.) The paper shows promising results on applying a supervised method on top of AN-GAN’s matches.\n\nMinor comments:\n1. The paper sometimes uses L1 and sometimes L_1, it should be L_1 in all cases.\n2. DiscoGAN should have the Kim et al citation, right after the first time it is used. I had to look up DiscoGAN to realize it is just Kim et al.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Identifying Analogies Across Domains","abstract":"Identifying analogies across domains without supervision is a key task for artificial intelligence. Recent advances in cross domain image mapping have concentrated on translating images across domains. Although the progress made is impressive, the visual fidelity many times does not suffice for identifying the matching sample from the other domain. In this paper, we tackle this very task of finding exact analogies between datasets i.e. for every image from domain A find an analogous image in domain B. We present a matching-by-synthesis approach: AN-GAN, and show that it outperforms current techniques. We further show that the cross-domain mapping task can be broken into two parts: domain alignment and learning the mapping function. The tasks can be iteratively solved, and as the alignment is improved, the unsupervised translation function reaches quality comparable to full supervision. ","pdf":"/pdf/6b49a6efc8b64fafd498540f8bdf08762d2b1722.pdf","TL;DR":"Finding correspondences between domains by performing matching/mapping iterations","paperhash":"anonymous|identifying_analogies_across_domains","_bibtex":"@article{\n  anonymous2018identifying,\n  title={Identifying Analogies Across Domains},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkN_r2lR-}\n}","keywords":["unsupervised mapping","cross domain mapping"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper390/Authors"]}},{"tddate":null,"ddate":null,"tmdate":1509739329624,"tcdate":1509111148031,"number":390,"cdate":1509739326958,"id":"BkN_r2lR-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BkN_r2lR-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Identifying Analogies Across Domains","abstract":"Identifying analogies across domains without supervision is a key task for artificial intelligence. Recent advances in cross domain image mapping have concentrated on translating images across domains. Although the progress made is impressive, the visual fidelity many times does not suffice for identifying the matching sample from the other domain. In this paper, we tackle this very task of finding exact analogies between datasets i.e. for every image from domain A find an analogous image in domain B. We present a matching-by-synthesis approach: AN-GAN, and show that it outperforms current techniques. We further show that the cross-domain mapping task can be broken into two parts: domain alignment and learning the mapping function. The tasks can be iteratively solved, and as the alignment is improved, the unsupervised translation function reaches quality comparable to full supervision. ","pdf":"/pdf/6b49a6efc8b64fafd498540f8bdf08762d2b1722.pdf","TL;DR":"Finding correspondences between domains by performing matching/mapping iterations","paperhash":"anonymous|identifying_analogies_across_domains","_bibtex":"@article{\n  anonymous2018identifying,\n  title={Identifying Analogies Across Domains},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BkN_r2lR-}\n}","keywords":["unsupervised mapping","cross domain mapping"],"authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper390/Authors"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}