{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642409746,"tcdate":1512017774543,"number":3,"cdate":1512017774543,"id":"H18dJfpxM","invitation":"ICLR.cc/2018/Conference/-/Paper211/Official_Review","forum":"HyTrSegCb","replyto":"HyTrSegCb","signatures":["ICLR.cc/2018/Conference/Paper211/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Empirical results are convincing, contribution to representational learning is not much ","rating":"6: Marginally above acceptance threshold","review":"The key contributions of this paper are:\n(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a \"standard\" form and then into their correct morphological form,\n(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context,\n(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task, and\n(d) they demonstrate clear and substantial performance gains on a dialog question answer task.\n\nTheir analysis in Section 5.3 shows one clear advantage of this model in the context of long sequences. \n\nAs an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance. \n\nThe only drawback of the paper is that this does not advance representation learning per se though a nice application of current models.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving morphological agreement with Concorde","abstract":"Neural conversational models are widely used in applications like personal assistants and chat bots. These models seem to give better performance when operating on word level. However, for fusion languages like French, Russian and Polish vocabulary size sometimes become infeasible since most of the words have lots of word forms. We propose a neural network architecture for transforming normalized text into a grammatically correct one. Our model efficiently employs correspondence between normalized and target words and significantly outperforms character-level models while being 2x faster in training and 20\\% faster at evaluation. We also propose a new pipeline for building conversational models: first generate a normalized answer and then transform it into a grammatically correct one using our network. The proposed pipeline gives better performance than character-level conversational models according to assessor testing.","pdf":"/pdf/a526d821794832ca248475cb445c3438572d7867.pdf","TL;DR":"Proposed architecture to solve morphological agreement task","paperhash":"anonymous|achieving_morphological_agreement_with_concorde","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving morphological agreement with Concorde},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyTrSegCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper211/Authors"],"keywords":["NLP","morphology","seq2seq"]}},{"tddate":null,"ddate":null,"tmdate":1515642409786,"tcdate":1511840372134,"number":2,"cdate":1511840372134,"id":"By3d5LqlM","invitation":"ICLR.cc/2018/Conference/-/Paper211/Official_Review","forum":"HyTrSegCb","replyto":"HyTrSegCb","signatures":["ICLR.cc/2018/Conference/Paper211/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Borderline paper on morphological agreement","rating":"5: Marginally below acceptance threshold","review":"In this work, the authors propose a sequence-to-sequence architecture that learns a mapping from a normalized sentence to a grammatically correct sentence. The proposed technique is a simple modification to the standard encoder-decoder paradigm which makes it more efficient and better suited to this task. The authors evaluate their technique using three morphologically rich languages French, Polish and Russian and obtain promising results.\n\nThe morphological agreement task would be an interesting contribution of the paper, with wider potential. But one concern that I have is regarding the evaluation metrics used for it. Firstly, word accuracy rate doesn't seem appropriate, as it does not measure morphological agreement. Secondly, sentence accuracy (w.r.t. the sentences from which the normalized sentences are derived) is not indicative of morphological agreement: even \"wrong\" sentences in the output could be perfectly valid in terms of agreement. A grammatical error rate (fraction of grammatically wrong sentences produced) would probably be a better measure.\n\nAnother concern I have is regarding the quality of the baseline: Additional variants of the baseline models should be considered and the best one reported. Specifically, in the conversation task, have the authors considered switching the order of normalized answer and context in the input? Also, the word order of the normalized answer and/or context could be reversed (as is done in sequence-to-sequence translation models).\n\nAlso, many experimental details are missing from the draft:\n-- What are the sizes of the train/test sets derived from the OpenSubtitles database?\n-- Details of the validation sets used to tune the models.\n-- In Section 5.4, no details of the question-answer corpus are provided. How many pairs were extracted? How many were used for training and testing?\n-- In Section 5.4.1, how many assessors participated in the evaluation and how many questions were evaluated?\n-- In some of the tables (e.g. 6, 7, 8) which show example sentences from Polish, Russian and French, please provide some more information in the accompanying text on how to interpret these examples (since most readers may not be familiar with these languages).\n\nPros:\n-- Efficient model\n-- Proposed architecture is general enough to be useful for other sequence-to-sequence problems\n\nCons:\n-- Evaluation metrics for the morphological agreement task are unsatisfactory\n-- It would appear that the baselines could be improved further using standard techniques","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving morphological agreement with Concorde","abstract":"Neural conversational models are widely used in applications like personal assistants and chat bots. These models seem to give better performance when operating on word level. However, for fusion languages like French, Russian and Polish vocabulary size sometimes become infeasible since most of the words have lots of word forms. We propose a neural network architecture for transforming normalized text into a grammatically correct one. Our model efficiently employs correspondence between normalized and target words and significantly outperforms character-level models while being 2x faster in training and 20\\% faster at evaluation. We also propose a new pipeline for building conversational models: first generate a normalized answer and then transform it into a grammatically correct one using our network. The proposed pipeline gives better performance than character-level conversational models according to assessor testing.","pdf":"/pdf/a526d821794832ca248475cb445c3438572d7867.pdf","TL;DR":"Proposed architecture to solve morphological agreement task","paperhash":"anonymous|achieving_morphological_agreement_with_concorde","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving morphological agreement with Concorde},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyTrSegCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper211/Authors"],"keywords":["NLP","morphology","seq2seq"]}},{"tddate":null,"ddate":null,"tmdate":1515642409829,"tcdate":1511564575593,"number":1,"cdate":1511564575593,"id":"r1dQH78gM","invitation":"ICLR.cc/2018/Conference/-/Paper211/Official_Review","forum":"HyTrSegCb","replyto":"HyTrSegCb","signatures":["ICLR.cc/2018/Conference/Paper211/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Unreadable paper","rating":"2: Strong rejection","review":"The paper is a pain to read. Most of the citation styles are off (i.e., without parentheses). Most of the sentences are not grammatically correct. Most, if not all, of the determiners are missing. It is ironic that the paper is proposing a model to generate grammatically correct sentences, while most of the sentences in the paper are not grammatically correct.\n\nThe experimental numbers look skeptical. For example, 1/3 of the training results are worse than the test results in Table 1. It also happens a few times in Table 5. Either the models are not properly trained, or the models are heavily tuned on the test set.\n\nThe running times in Table 9 are also skeptical. Why are the Concorde models faster than unigrams and bigrams? Maybe this can be attributed to the difference in the size of the vocabulary, but why is the unigram model slower than the bigram model?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Achieving morphological agreement with Concorde","abstract":"Neural conversational models are widely used in applications like personal assistants and chat bots. These models seem to give better performance when operating on word level. However, for fusion languages like French, Russian and Polish vocabulary size sometimes become infeasible since most of the words have lots of word forms. We propose a neural network architecture for transforming normalized text into a grammatically correct one. Our model efficiently employs correspondence between normalized and target words and significantly outperforms character-level models while being 2x faster in training and 20\\% faster at evaluation. We also propose a new pipeline for building conversational models: first generate a normalized answer and then transform it into a grammatically correct one using our network. The proposed pipeline gives better performance than character-level conversational models according to assessor testing.","pdf":"/pdf/a526d821794832ca248475cb445c3438572d7867.pdf","TL;DR":"Proposed architecture to solve morphological agreement task","paperhash":"anonymous|achieving_morphological_agreement_with_concorde","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving morphological agreement with Concorde},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyTrSegCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper211/Authors"],"keywords":["NLP","morphology","seq2seq"]}},{"tddate":null,"ddate":null,"tmdate":1509739427207,"tcdate":1509061957133,"number":211,"cdate":1509739424554,"id":"HyTrSegCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyTrSegCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Achieving morphological agreement with Concorde","abstract":"Neural conversational models are widely used in applications like personal assistants and chat bots. These models seem to give better performance when operating on word level. However, for fusion languages like French, Russian and Polish vocabulary size sometimes become infeasible since most of the words have lots of word forms. We propose a neural network architecture for transforming normalized text into a grammatically correct one. Our model efficiently employs correspondence between normalized and target words and significantly outperforms character-level models while being 2x faster in training and 20\\% faster at evaluation. We also propose a new pipeline for building conversational models: first generate a normalized answer and then transform it into a grammatically correct one using our network. The proposed pipeline gives better performance than character-level conversational models according to assessor testing.","pdf":"/pdf/a526d821794832ca248475cb445c3438572d7867.pdf","TL;DR":"Proposed architecture to solve morphological agreement task","paperhash":"anonymous|achieving_morphological_agreement_with_concorde","_bibtex":"@article{\n  anonymous2018achieving,\n  title={Achieving morphological agreement with Concorde},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyTrSegCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper211/Authors"],"keywords":["NLP","morphology","seq2seq"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}