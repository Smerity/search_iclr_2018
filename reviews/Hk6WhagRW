{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222651628,"tcdate":1511989346958,"number":3,"cdate":1511989346958,"id":"BkoPxj3xz","invitation":"ICLR.cc/2018/Conference/-/Paper427/Official_Review","forum":"Hk6WhagRW","replyto":"Hk6WhagRW","signatures":["ICLR.cc/2018/Conference/Paper427/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A very interesting paper, it examines problems of how agents can use communication to maximise their rewards in a simple negotiation game. The good results of the paper are muddied by all the other data and aspects of the experiments that didn't work. ","rating":"5: Marginally below acceptance threshold","review":"The experimental setup is clear, although the length of the utterances and the number of symbols in them is not explicitly stated in the text (only the diagrams).\n\nExperiment 1 confirms that agents who seek only to maximise their own rewards fail to coordinate over a non-binding communication channel. The exposition of the experiments, however, is unclear.\nIn Fig 1, it is not clear what Agent 1 and Agent 2 are. Do they correspond to arbitrary labels or the turns that the agent takes in the game?\nWhy is Agent 1 the one who triumphs in the no-communication channel game? Is there any advantage to going first generally? Where are the tests of robustness on the curves demonstrated in Figure 2a?\nHas figure 2b been cherry picked? This should be demonstrated over many different negotiations with error bars.\nIn the discussion of the agents being unable to ground cheap talk, the symbolic nature of the linguistic channel clouds the fact that it is not the symbolic, ungrounded aspect but the non-binding nature of communication on this channel. This would be more clearly demonstrated and parsimonious by using a non-binding version of the proposal channel and saving the linguistic discussion for later.\n\nExperiment 2 shows that by making the agents prosocial, they are able to learn to communicate on the linguistic channel to achieve pretty much optimal rewards, a very nice result.\nThe agents are not able to reach the same levels of cooperation on the proposal channel, in fact performing worse than the no-communication baseline. Protocols could be designed that would allow the agents to communicate their utilities over this channel (within 4 turns), so the fact they don't suggests it is the learning procedure that is not able to find this optimum. Presenting this as a result about the superiority of communication over the linguistic channel is not well supported.\nWhy do they do worse with random termination than 10 turns in the proposal channel? 4 proposals should contain enough information to determine the utilities.\nWhy are the 10 turn games even included in this table? It seems that this was dismissed in the environment setup section, due to the first mover advantage.\nWhy do no-communication baselines change so much between random termination and 10 turns in the prosocial case?\nWhy do self-interested agents for 10 turns on the linguistic channel terminate early?\nTable 1 might be better represented using the median and quartiles, since the data is skewed.\n\nAnalysis of the communication, i.e. what is actually sent, is interesting and the division into speaker and listener suggests that this is a simple protocol that is easy for agents to learn.\n\nExperiment 3 aims to determine whether an agent is able to negotiate against a community of other agents with mixed levels of prosociality. It is shown that if the fixed agent is able to identify who they are playing against they can do better than not knowing, in the case where the fixed agent is self interested.\nThe pca plot of agent id embeddings related is good.\nBoth Figure 4 and Table 3 use Agent 1 and Agent 2 rather than Agent A and Agent B and is not clear whether this is a mistake or Agent 1 is different from Agent A.\nThe no-communication baseline is referred to in the text but the results are not shown in the table.\nThere are no estimates of the uncertainty of the results in table 3, how robust are these results to different initial conditions?\nThis section seems like a bit of an add-on to address criticisms that might arise about the initial experiment being only two agents.\n\nOverall, the paper has some nice results and an interesting ideas but could do with some tightening up of the results to make it really good.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1512222651674,"tcdate":1511819835572,"number":2,"cdate":1511819835572,"id":"Bk7S9ZclM","invitation":"ICLR.cc/2018/Conference/-/Paper427/Official_Review","forum":"Hk6WhagRW","replyto":"Hk6WhagRW","signatures":["ICLR.cc/2018/Conference/Paper427/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting","rating":"7: Good paper, accept","review":"This paper explores how agents can learn to communicate to solve a negotiation task. They explore several settings: grounded vs. ungrounded communication, and self-interested vs. prosocial agents. The main findings are that prosocial agents are able to learn to ground symbols using RL, but self-interested agents are not. The work is interesting and clearly described, and I think this is an interesting setting for studying emergent communication.\n\nMy only major comment is that I’m a bit skeptical about the claim that “self-interested agents cannot ground cheap talk to exchange meaningful information”. Given that the agents’ rewards would be improved if they were able to make agreements, and humans can use ‘cheap talk’ to negotiate, surely the inability to do so here shows a failure of the learning algorithm (rather than a general property of self-interested agents)?\n\nI am also concerned about the dangers posed by robots inventing their own language, perhap the authors should shut this down :-)\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1512222651716,"tcdate":1511790138350,"number":1,"cdate":1511790138350,"id":"SJGBLcYxG","invitation":"ICLR.cc/2018/Conference/-/Paper427/Official_Review","forum":"Hk6WhagRW","replyto":"Hk6WhagRW","signatures":["ICLR.cc/2018/Conference/Paper427/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review 'Emergent Communication through Negotiation'","rating":"6: Marginally above acceptance threshold","review":"The authors describe a variant of the negotiation game in which agents of different type, selfish or prosocial, and with different preferences. The central feature is the consideration of a secondary communication (linguistic) channel for the purpose of cheap talk, i.e. talk whose semantics are not laid out a priori. \n\nThe essential findings include that prosociality is a prerequisite for effective communication (i.e. formation of meaningful communication on the linguistic channel), and furthermore, that the secondary channel helps improve the negotiation outcomes.\n\nThe paper is well-structured and incrementally introduces the added features and includes staged evaluations for the individual additions, starting with the differentiation of agent characteristics, explored with combination of linguistic and proposal channel. Finally, agent societies are represented by injecting individuals' ID into the input representation.\n\nThe positive:\n- The authors attack the challenging task of given agents a means to develop communication patterns without apriori knowledge.\n- The paper presents the problem in a well-structured manner and sufficient clarity to retrace the essential contribution (minor points for improvement).\n- The quality of the text is very high and error-free.\n- The background and results are well-contextualised with relevant related work. \n\nThe problematic:\n- By the very nature of the employed learning mechanisms, the provided solution provides little insight into what the emerging communication is really about. In my view, the lack of interpretable semantics hardly warrants a reference to 'cheap talk'. As such the expectations set by the well-developed introduction and background sections are moderated over the course of the paper.\n- The goal of providing agents with richer communicative ability without providing prior grounding is challenging, since agents need to learn about communication partners at runtime. But it appears as of the main contribution of the paper can be reduced to the decomposition of the learnable feature space into two communication channels. The implicit relationship of linguistic channel on proposal channel input based on the time information (Page 4, top) provides agents with extended inputs, thus enabling a more nuanced learning based on the relationship of proposal and linguistic channel. As such the well-defined semantics of the proposal channel effectively act as the grounding for the linguistic channel. This, then, could have been equally achieved by providing agents with a richer input structure mediated by a single channel. From this perspective, the solution offers limited surprises. The improvement of accuracy in the context of agent societies based on provided ID follows the same pattern of extending the input features.\n- One of the motivating factors of using cheap talk is the exploitation of lying on the part of the agents. However, apart from this initial statement, this feature is not explicitly picked up. In combination with the previous point, the necessity/value of the additional communication channel is unclear.\n\nConcrete suggestions for improvement:\n\n- Providing exemplified communication traces would help the reader appreciate the complexity of the problem addressed by the paper.\n- Figure 3 is really hard to read/interpret. The same applies to Figure 4 (although less critical in this case).\n- Input parameters could have been made explicit in order to facilitate a more comprehensive understanding of technicalities (e.g. in appendix).\n- Emergent communication is effectively unidirectional, with one agent as listener. Have you observed other outcomes in your evaluation?\n\nIn summary, the paper presents an interesting approach to combine unsupervised learning with multiple communication channels to improve learning of preferences in a well-established negotiation game. The problem is addressed systematically and well-presented, but can leave the reader with the impression that the secondary channel, apart from decomposing the model, does not provide conceptual benefit over introducing a richer feature space that can be exploited by the learning mechanisms. Combined with the lack of specific cheap talk features, the use of actual cheap talk is rather abstract. Those aspects warrant justification.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1510580074955,"tcdate":1510580074955,"number":2,"cdate":1510580074955,"id":"ryQO17PyM","invitation":"ICLR.cc/2018/Conference/-/Paper427/Official_Comment","forum":"Hk6WhagRW","replyto":"r1Nwr02RW","signatures":["ICLR.cc/2018/Conference/Paper427/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper427/Authors"],"content":{"title":"Reply","comment":"Hi Hugh\n>- hold out set of 5 batches of 128 test games:\n>  - presumably this is disjoint from the training set?\n>  - what is then the algorithm for generating the training set, to ensure disjointness?\n\nWe generated the test games by fixing the seed of the RNG, generating 5 batches from our game environment generator, and then discarding any batches of games after this which overlapped with any of the test games.\n\n>- given that training seems to plateau for extended periods of time, eg see https://github.com/ASAPPinc/emergent_comms_negotiation/blob/master/images/20171104_192343gpu2_proposal_social_comms.png?raw=true up to first 100 thousand episodes, how did you decide when training was finished?\n\nWe basically just extended training until we saw no further improvements.\n\n>- you report the standard deviation across samples in a single batch of 128 games. To what extent did the fraction of reward vary across entire training runs?\n\nOur results were stable across training runs - we did extensive prototyping in a development environment and swept across random seeds, and the results were identical. We then moved onto the environment presented in the paper, and the results carried over. \n\n>Thoughts?  What things should I consider checking in order to improve on this?\n\nThe hyperparameter we found with the biggest impact on the final outcome was the strength of the entropy regularization, as this controls the tradeoff between exploration and exploitation: too low and our agents didn't explore enough to find the optimal negotiation policy, too high and our agents just didn't learn anything. We found a useful debugging technique to be to monitor the proportion of actions taken equal to the argmax action, to ensure that the agents are still exploring during periods when the reward has plateaued. We also change the test time policy from the training time policy - during test time, we just deterministically take the action with the highest probability.\n\n>- how were the entropy regularization weight hyperparameters determined?\n>   - using gridsearch?\n>   - what was the search space of the grid search?\n>   - was grid search run for both prosocial and not prosocial? or just for one particular set of settings?\n\nWe found the optimal hyperparameter values with grid search, using values in {1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2} for the termination and proposal policies, and {1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3} for the utterance policy. We ran this for all agent reward schemes, and the same hyperparameter values worked regardless of the agent reward scheme.\n\n> I wonder whether it might be useful to have similar curves for the prosocial agents, so we can easily see how the behavior of prosocial and not prosocial agents compares?\n\nWe have the training graphs for prosocial agents, but we did not include them in this revision due to space constraints. They mainly show the joint reward going fairly smoothly upwards as training goes on, and then plateauing at the end. We can add them to the next revision if necessary.\n\n> - disparity between number of symbols in utterance vocab (10), and the possible utilities (11)? (in an earlier reply you say there are 11 utterance tokens in fact?)\n\nThe item utilities range between 0 and 5 inclusive, not 0-10. The symbols the agents can communicate with range between 0-10. We also ran an experiment on how the bandwidth of the utterance channel affected prosocial agent negotiation success, and essentially i- disparity between number of symbols in utterance vocab (10), and the possible utilities (11)? (in an earlier reply you say there are 11 utterance tokens in fact?)t seems that as long as there is plenty of spare capacity in the utterance channel then the agents can learn to negotiate fine.\n\nHope this helps\nThe authors"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1509906811783,"tcdate":1509905755759,"number":5,"cdate":1509905755759,"id":"r1Nwr02RW","invitation":"ICLR.cc/2018/Conference/-/Paper427/Public_Comment","forum":"Hk6WhagRW","replyto":"Hk6WhagRW","signatures":["~Hugh_Nicholas_Perkins1"],"readers":["everyone"],"writers":["~Hugh_Nicholas_Perkins1"],"content":{"title":"Misc questions 2","comment":"Hi,\n\nTraining is gradually progressing, though I'm possibly missing some tricks currently.  Some questions on reproducibility:\n- hold out set of 5 batches of 128 test games:\n   - presumably this is disjoint from the training set?\n   - what is then the algorithm for generating the training set, to ensure disjointness?\n- given that training seems to plateau for extended periods of time, eg see https://github.com/ASAPPinc/emergent_comms_negotiation/blob/master/images/20171104_192343gpu2_proposal_social_comms.png?raw=true up to first 100 thousand episodes, how did you decide when training was finished?\n- you report the fraction of reward obtained in table 1.  Some questions on this:\n  - you report the standard deviation across samples in a single batch of 128 games. To what extent did the fraction of reward vary across entire training runs?\n  - you state that for prosocial, proposal channel only, the reward fraction was 0.92. However, in my own training run, the reward plateau's at ~0.81, after ~50,000 episodes of 128 games, and stays at ~0.81 for the remaining ~800,000 episodes, https://github.com/ASAPPinc/emergent_comms_negotiation/raw/master/images/20171104_144936_proposal_social_nocomms_b.png?raw=true  Thoughts?  What things should I consider checking in order to improve on this?\n\nSome other questions, not strictly necessary for reproducibility:\n- how were the entropy regularization weight hyperparameters determined?\n   - using gridsearch?\n   - what was the search space of the grid search?\n   - was grid search run for both prosocial and not prosocial? or just for one particular set of settings?\n- It looks to me like the main thesis of the paper is the behavior of selfish vs pro-social agents, is this an approximately fair impression? For example, the paper states 'self-interested agents cannot ground cheap talk. When using the linguistic channel, agents do not negotiate optimally. Instead, the agents randomly alternate taking all the items, which is borne out by the oscillations in Figure 2a.' I wonder whether it might be useful to have similar curves for the prosocial agents, so we can easily see how the behavior of prosocial and not prosocial agents compares?\n- why have length 6 for the comms channel? Presumably it mostly just needs to be able to represent the hidden utility, which is sufficient for the other agent to have complete world state information? (Edit: oh, I guess the additional 3 length is necessary in the 'no proposal' case, so that the agent can communicate its proposal?)\n- disparity between number of symbols in utterance vocab (10), and the possible utilities (11)? (in an earlier reply you say there are 11 utterance tokens in fact?)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1509721359718,"tcdate":1509721359718,"number":4,"cdate":1509721359718,"id":"HydzSWqCW","invitation":"ICLR.cc/2018/Conference/-/Paper427/Public_Comment","forum":"Hk6WhagRW","replyto":"HJ9FF6EAZ","signatures":["~Hugh_Nicholas_Perkins1"],"readers":["everyone"],"writers":["~Hugh_Nicholas_Perkins1"],"content":{"title":"Replication repo so far","comment":"Just for info, replication so far: https://github.com/ASAPPinc/emergent_comms_negotiation/blob/master/ecn.py\n\nExperiment log: https://github.com/ASAPPinc/emergent_comms_negotiation/blob/master/explog.md\n\n(stuck in a local minimum for now. digging a bit...)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1509548914896,"tcdate":1509548914896,"number":3,"cdate":1509548914896,"id":"SyodmvDC-","invitation":"ICLR.cc/2018/Conference/-/Paper427/Public_Comment","forum":"Hk6WhagRW","replyto":"SkM4N2SCW","signatures":["~Hugh_Nicholas_Perkins1"],"readers":["everyone"],"writers":["~Hugh_Nicholas_Perkins1"],"content":{"title":"Cite Entropy Regularization reference?","comment":"Opinion: might be worth including the reference for Entropy Regularization? I think it is 'Asynchronous methods for deep reinforcement learning', perhaps? (this itself cites Williams and Peng, but I think the Mnih et al reference is likely more useable?)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1509438506314,"tcdate":1509438506314,"number":2,"cdate":1509438506314,"id":"SkM4N2SCW","invitation":"ICLR.cc/2018/Conference/-/Paper427/Public_Comment","forum":"Hk6WhagRW","replyto":"HJ9FF6EAZ","signatures":["~Hugh_Nicholas_Perkins1"],"readers":["everyone"],"writers":["~Hugh_Nicholas_Perkins1"],"content":{"title":"Thanks!","comment":"Thanks! :)"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1510092426845,"tcdate":1509378434099,"number":1,"cdate":1509378434099,"id":"HJ9FF6EAZ","invitation":"ICLR.cc/2018/Conference/-/Paper427/Official_Comment","forum":"Hk6WhagRW","replyto":"SkW8mhN0-","signatures":["ICLR.cc/2018/Conference/Paper427/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper427/Authors"],"content":{"title":"Reply to misc comments 1","comment":"Hi Hugh\n\nGlad you liked the paper. Here's the answers:\n\n1) Yeah, it means that the number should be multiplied by 10 to get the number of training steps.\n2) We'll definitely consider this when revisions come around.\n3) Each step is a training round, consisting of a batch of 128 games.\n4) We didn't include training curves for prosocial agents - from memory, it took around 200k rounds to make use of the linguistic channel fully, and even more to use the proposal channel.\n5) This is just the hidden state - the cell state is initialised to zero.\n6) There are no termination tokens at all. There are 11 possible tokens, and each utterance is exactly of length 6.\n\nHope this helps the replication."},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1509372926531,"tcdate":1509372744683,"number":1,"cdate":1509372744683,"id":"SkW8mhN0-","invitation":"ICLR.cc/2018/Conference/-/Paper427/Public_Comment","forum":"Hk6WhagRW","replyto":"Hk6WhagRW","signatures":["~Hugh_Nicholas_Perkins1"],"readers":["everyone"],"writers":["~Hugh_Nicholas_Perkins1"],"content":{"title":"Misc comments 1","comment":"Nice paper :) So I'm trying to reproduce it :)\n\nSome very detailed questions, for trying to reproduce it:\n- in figure 2, what does 'x 10' mean? does it mean eg '10000' means '100000' steps?\n  - opinion: maybe putting eg 'x 100,000' or 'thousands', could be clearer, easier to read, remove some zeros from the graph?\n  - is a 'step' an episode (each episode comprising multiple turns/timesteps?), or is a step a timestep?\n- training curves for prosocial agents? how long does it take to learn to use the communications channel?\n- you refer to the 'hidden state of the lstm'. Does this include the cell? just the hidden state? If the latter, does this mean that the cell is initialized to zero at the start of each generated utterance? If the former, how are you handling the associated doubling of embedding size?\n- you say the vocabulary has 10 tokens. Does this include a termination token? or there are 11 possible tokens, 10 of which are part of the vocab, and one of which is termination?\n   - similarly, does the utterance length 6 include the termination token, or is the length 6 + termination token?\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]}},{"tddate":null,"ddate":null,"tmdate":1509739309443,"tcdate":1509116932665,"number":427,"cdate":1509739306786,"id":"Hk6WhagRW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hk6WhagRW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Emergent Communication through Negotiation","abstract":"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols - one grounded in the semantics of the game, and one which is a priori ungrounded.  We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded, cheap talk channel to do the same.  However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.","pdf":"/pdf/d4752dac94a120ec287cc0dc26064b0961b433c0.pdf","TL;DR":"We teach agents to negotiate using only reinforcement learning; selfish agents can do so, but only using a trustworthy communication channel, and prosocial agents can negotiate using cheap talk.","paperhash":"anonymous|emergent_communication_through_negotiation","_bibtex":"@article{\n  anonymous2018emergent,\n  title={Emergent Communication through Negotiation},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hk6WhagRW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper427/Authors"],"keywords":["multi-agent learning","reinforcement learning","game theory","emergent communication"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}