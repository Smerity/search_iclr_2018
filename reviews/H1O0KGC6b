{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222797511,"tcdate":1511980346180,"number":3,"cdate":1511980346180,"id":"H1zHTO2ef","invitation":"ICLR.cc/2018/Conference/-/Paper86/Official_Review","forum":"H1O0KGC6b","replyto":"H1O0KGC6b","signatures":["ICLR.cc/2018/Conference/Paper86/AnonReviewer3"],"readers":["everyone"],"content":{"title":"This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization.","rating":"4: Ok but not good enough - rejection","review":"This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization. However, the results do not convince this reviewer to switch to using 'post-training'.\n\nLearning features and then use a classifier such as a softmax or SVM is not new and were actually widely used 10 years ago. However, freezing the layers and continue to train the last layer is of a minor novelty. The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the \\lambda term which is added for post-training but not added for the baseline. c.f. Eq 3 and Eq 4.\nTherefore, it's unclear whether the gain in generalization is due to an additional \\lambda term or from the post-training training itself.\n\nA way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise.\n\nOther notes, \n\nRemark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments.\n\nFor table 1, please use decimal points instead of commas.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Post-training for Deep Learning","abstract":"One of the main challenges of deep learning methods is the choice of an appropriate training strategy. In particular, additional steps, such as unsupervised pre-training, have been shown to greatly improve the performances of deep structures. In this article, we propose an extra training step, called post-training, which only optimizes the last layer of the network. We show that this procedure can be analyzed in the context of kernel theory, with the first layers computing an embedding of the data and the last layer a statistical model to solve the task based on this embedding. This step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task. This idea is then tested on multiple architectures with various data sets, showing that it consistently provides a boost in performance.","pdf":"/pdf/b9824f93e0c7f4478b392454ed7f724007f44001.pdf","TL;DR":"We propose an additional training step, called post-training, which computes optimal weights for the last layer of the network.","paperhash":"anonymous|posttraining_for_deep_learning","_bibtex":"@article{\n  anonymous2018post-training,\n  title={Post-training for Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1O0KGC6b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper86/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222797550,"tcdate":1511619253466,"number":2,"cdate":1511619253466,"id":"ry63clwxz","invitation":"ICLR.cc/2018/Conference/-/Paper86/Official_Review","forum":"H1O0KGC6b","replyto":"H1O0KGC6b","signatures":["ICLR.cc/2018/Conference/Paper86/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review of 'Post-Training in Deep Learning'","rating":"3: Clear rejection","review":"This paper proposes to fine-tune the last layer while keeping the others fixed, after initial end-to-end training, viewing the last layer learning under the light of kernel theory (well actually it's just a linear model).\n\nSummary of evaluation\n\nThere is not much novelty in this idea (of optimizing carefully only the last layer as a post-training stage or treating the last layer as kernel machine in a post-processing step), which dates back at least a decade, so the only real contribution would be in the experiments. However the experimental setup is questionable as it does not look like the same care has been given to control overfitting with the 'regular training' method.\n\nMore details\n\nPrevious work on the same idea: at least a decade old, e.g., Huang and LeCun 2006. See a review of such work in 'Deep Learning using Linear Support Vector Machines' more recently.\n\nExperiments\n\nYou should also have a weight norm penalty in the end-to-end ('regular training') case and make sure it is appropriately and separately tuned (not necessarily the same value as for the post-training). Otherwise, the 'improvements' may simply be due to better regularization in one case vs the other, and the experimental curves suggest that interpretation is correct.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Post-training for Deep Learning","abstract":"One of the main challenges of deep learning methods is the choice of an appropriate training strategy. In particular, additional steps, such as unsupervised pre-training, have been shown to greatly improve the performances of deep structures. In this article, we propose an extra training step, called post-training, which only optimizes the last layer of the network. We show that this procedure can be analyzed in the context of kernel theory, with the first layers computing an embedding of the data and the last layer a statistical model to solve the task based on this embedding. This step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task. This idea is then tested on multiple architectures with various data sets, showing that it consistently provides a boost in performance.","pdf":"/pdf/b9824f93e0c7f4478b392454ed7f724007f44001.pdf","TL;DR":"We propose an additional training step, called post-training, which computes optimal weights for the last layer of the network.","paperhash":"anonymous|posttraining_for_deep_learning","_bibtex":"@article{\n  anonymous2018post-training,\n  title={Post-training for Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1O0KGC6b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper86/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222797591,"tcdate":1511127319468,"number":1,"cdate":1511127319468,"id":"HyyQKdklf","invitation":"ICLR.cc/2018/Conference/-/Paper86/Official_Review","forum":"H1O0KGC6b","replyto":"H1O0KGC6b","signatures":["ICLR.cc/2018/Conference/Paper86/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Overall, this is a nice idea, with experimental results showing its merits. Some of the empirical outcomes do not show that much improvement. However it is a step that could be used either way, even for a slight gain. It would have been more preferable to consider the future directions stated in the paper in this work.","rating":"5: Marginally below acceptance threshold","review":"Summary: \nBased on ideas within the context of kernel theory, the authors consider post-training of NNs as an extra training step, which only optimizes the last layer of the network.\nThis additional step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task (which is also reflected in the experiments).\n\nAccording to the authors, the contributions are the following:\n1. Post-training step: keeping the rest of the NN frozen (after training), the method trains the last layer in order to \"make sure\" that the representation learned is used in the most efficient way.\n2. Highlighting connections with kernel techniques and RKHS optimization (like kernel ridge regression).\n3. Experimental results.\n\nClarity:\nThe paper is well-written, the main ideas well-clarified. \n\nImportance:\nWhile the majority of papers nowadays focuses on the representation part (i.e., how we get to \\Phi_{L-1}(x)), this paper assumes this is given and proposes how to optimize the weights in the final step of the algorithm. This by itself is not enough to boost the performance universally (e.g., if \\Phi_{L-1} is not well-trained, the problem is deeper than training the last layer); however, it proposes an additional step that can be used in most NN architectures. From that front (i.e., proposing to do something different than simply training a NN), I find the paper interesting, that might attract some attention at the conference.\n\nOn the other hand, to my humble opinion, the experimental results do not show a significant gain in the performances of all networks (esp. Figure 3 and Table 1 are within the range of statistical error). In order to state something like this universally, either one needs to perform experiments with more than just MNIST/CIFAR datasets, or even more preferably, prove that the algorithm performs better.\n\nOriginality:\nIt would be great to have some more theory (if any) for the post-training step, or investigate more cases, rather than optimizing only the last layer.\n\nComments:\n1. I assume the authors focused in the last layer of the NN for simplicity, but is there a reason why one might want to focus only on the last layer? One reason is convexity in W of the problem (2). Any other? \n\n2. Have the authors considered (even in practice only) to include training of the last 2 layers of the NN? The authors state this question in the future direction, but it would make the paper more complete to consider it here.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Post-training for Deep Learning","abstract":"One of the main challenges of deep learning methods is the choice of an appropriate training strategy. In particular, additional steps, such as unsupervised pre-training, have been shown to greatly improve the performances of deep structures. In this article, we propose an extra training step, called post-training, which only optimizes the last layer of the network. We show that this procedure can be analyzed in the context of kernel theory, with the first layers computing an embedding of the data and the last layer a statistical model to solve the task based on this embedding. This step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task. This idea is then tested on multiple architectures with various data sets, showing that it consistently provides a boost in performance.","pdf":"/pdf/b9824f93e0c7f4478b392454ed7f724007f44001.pdf","TL;DR":"We propose an additional training step, called post-training, which computes optimal weights for the last layer of the network.","paperhash":"anonymous|posttraining_for_deep_learning","_bibtex":"@article{\n  anonymous2018post-training,\n  title={Post-training for Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1O0KGC6b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper86/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510932592186,"tcdate":1510931883303,"number":1,"cdate":1510931883303,"id":"r1QnTOhJf","invitation":"ICLR.cc/2018/Conference/-/Paper86/Public_Comment","forum":"H1O0KGC6b","replyto":"H1O0KGC6b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Alternative interpretation ","comment":"Dear authors, \n\nThank you to the authors for this interesting paper, which I enjoyed :)\n\nThe goal of the proposed method is to separate representation learning (done by the network in all but the final layer) and the task at hand (e.g., classification, which is performed by the final layer of the network). As such, the authors propose a final 'post training' step, which only updates the final layers of a network. One benefit of such an approach is that, given a fixed representation, learning the final layer weights is convex for many choice of the activation functions in the final layer. \n\nThe authors propose an interesting link with kernels. From reading the paper, it seemed to me that there was also a relationship between representation learning from the perspective of non-linear ICA, recently proposed by Hyvarinen & Morioka (2016), which I thought I would share with you.  \nIn their work, Hyvarinen & Morioka show that when a classification task is combined with a function approximator (eg a deep net) , the final representation learnt by the network (i.e., what the authors here refer to as $\\Phi_{L-1}(x)$) will be equal to the independent components which generated the data (roughly speaking). As a result, it may be possible/interesting to interpret post training as first learning the non-linear unmixing of independent components followed by post training which then performs classification on the original independent components. \n\nDisclaimer: I am not associated with the referenced paper, just wanted to provide an additional justification for the proposed method :)\n\nGood luck!\n\nReferences:\nHyvarinen & Morioka, \"Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA\", NIPS 2016"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Post-training for Deep Learning","abstract":"One of the main challenges of deep learning methods is the choice of an appropriate training strategy. In particular, additional steps, such as unsupervised pre-training, have been shown to greatly improve the performances of deep structures. In this article, we propose an extra training step, called post-training, which only optimizes the last layer of the network. We show that this procedure can be analyzed in the context of kernel theory, with the first layers computing an embedding of the data and the last layer a statistical model to solve the task based on this embedding. This step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task. This idea is then tested on multiple architectures with various data sets, showing that it consistently provides a boost in performance.","pdf":"/pdf/b9824f93e0c7f4478b392454ed7f724007f44001.pdf","TL;DR":"We propose an additional training step, called post-training, which computes optimal weights for the last layer of the network.","paperhash":"anonymous|posttraining_for_deep_learning","_bibtex":"@article{\n  anonymous2018post-training,\n  title={Post-training for Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1O0KGC6b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper86/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1509739495098,"tcdate":1508940240451,"number":86,"cdate":1509739492447,"id":"H1O0KGC6b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1O0KGC6b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Post-training for Deep Learning","abstract":"One of the main challenges of deep learning methods is the choice of an appropriate training strategy. In particular, additional steps, such as unsupervised pre-training, have been shown to greatly improve the performances of deep structures. In this article, we propose an extra training step, called post-training, which only optimizes the last layer of the network. We show that this procedure can be analyzed in the context of kernel theory, with the first layers computing an embedding of the data and the last layer a statistical model to solve the task based on this embedding. This step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task. This idea is then tested on multiple architectures with various data sets, showing that it consistently provides a boost in performance.","pdf":"/pdf/b9824f93e0c7f4478b392454ed7f724007f44001.pdf","TL;DR":"We propose an additional training step, called post-training, which computes optimal weights for the last layer of the network.","paperhash":"anonymous|posttraining_for_deep_learning","_bibtex":"@article{\n  anonymous2018post-training,\n  title={Post-training for Deep Learning},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1O0KGC6b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper86/Authors"],"keywords":[]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}