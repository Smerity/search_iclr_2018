{"notes":[{"tddate":null,"ddate":null,"tmdate":1512071416981,"tcdate":1512071416981,"number":2,"cdate":1512071416981,"id":"S1Wb-k0lz","invitation":"ICLR.cc/2018/Conference/-/Paper759/Public_Comment","forum":"S1p31z-Ab","replyto":"rkmWoaTlG","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Quick reply","comment":"That's entirely reasonable—it is somewhat ensemble-like. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512065786889,"tcdate":1512065786889,"number":1,"cdate":1512065786889,"id":"rkmWoaTlG","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Comment","forum":"S1p31z-Ab","replyto":"ryUFBhagM","signatures":["ICLR.cc/2018/Conference/Paper759/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper759/Authors"],"content":{"title":"Previous SNLI SotA","comment":"Glad you enjoyed the paper!  We are working on the code and should have an initial version to compute ELMo representations out in the next week or two.\n\nThanks for your comment about the previous SNLI single model SotA.  We'll change Table 1 to replace the reference to McCann 2017 with the ESIM + 300D Syntactic TreeLSTM (88.6) from Chen et al. 2017.  In our draft we had classified this model as an ensemble method (see Table 8 in Appendix) but will update in the next version."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512060285560,"tcdate":1512060285560,"number":1,"cdate":1512060285560,"id":"ryUFBhagM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Public_Comment","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Neat results! Minor comment.","comment":"Your claim about the SotA on SNLI isn't quite right. What to use depends on what you think about ensembles, but there's a leaderboard up here: https://nlp.stanford.edu/projects/snli/\n\nThe ESIM paper from 2016 reports slightly better numbers than the 2017 McCann paper you cite as SotA.\n\nOverall, though, these are exciting results, and I'd be interested in playing with the trained model(s) once they come out."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512222744634,"tcdate":1511857373429,"number":3,"cdate":1511857373429,"id":"rJByaq9eM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Review","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["ICLR.cc/2018/Conference/Paper759/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a method to learn contextualized word representations (ELMO) by pretraining a multilayer bidirectional LSTM language model and using representations from all levels of the LSTM in the input or output layer of a supervised task of interest. \nExperiments on various datasets (SNLI, SQuAD, SRL, Coref, NER, SST) show that the proposed method improve over baseline models.\nAblation analysis demonstrate that using all layers of ELMO is always better than just using only the final layer, and that representations learned by ELMO capture basic notions of word senses and part of speeches.\n\nThe paper is well written and I think learning contextualized word representations is an important topic.\nHowever, one thing that I am not sure about from experiments in the paper is whether the improvements come from an increase in model capacity and (unlabeled) data used to train the model, or whether there are more interesting things going on.\n- What makes the proposed approach different than just a deeper architecture for each of the considered tasks, where some parts of the network are trained using unlabeled data?\n- Is the pretraining with unlabeled data necessary, or can we just have this deep architecture and train everything with the available supervised data?\n- An ELMO enhanced model has more parameters than the baseline model for each task. What is the performance of the (non deep) baseline method with comparable number of parameters (bigger hidden size)?\n\nMore generally, it is not surprising that given sufficient training data, a deeper model (e.g., ELMO enhanced models) with multiple connections across layers will perform better than shallower models with fewer parameters.\nI would like to see more analysis and/or explanations on why the proposed method contributes more beyond this.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512222744682,"tcdate":1511812443891,"number":2,"cdate":1511812443891,"id":"HJED6k5lM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Review","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["ICLR.cc/2018/Conference/Paper759/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Promising results","rating":"6: Marginally above acceptance threshold","review":"This paper proposed a model called ‘deep contextualized’ to extract word embeddings for downstream applications. This model is simply a bi-directional language model (biLM) and the word embedding is a weighted combination of the output of the hidden layers (forward and backward). Furthermore like previous work, the authors proposed to pre-train the biLM with a large amount of data and then use the embeddings in combination with the context-independent word embedding in neural network models (in this paper, RNN) for final applications. Their results showed consistent improvements over the baseline and the previous best systems on several tasks.\n\nSome detailed comments:\n-\tI would like to see an overview figure which illustrates the biLM and its integration in downstream applications. Furthermore, it is also interesting to see the performance difference between with and without fine-tuning of the biLM.\n-\tIntrinsic evaluations are missing in this paper. Although we know that word similarity tasks are not the best way to evaluate word embeddings, it is always informative to report results on these standard tasks.\n-\tThe baseline results in table 1 and table 2 are not consistent. Is there something wrong?\n-\tDid the baseline systems use pretrained embeddings or randomly initialized ones? In my opinion, the right baseline systems should use pretrained word2vec or glove embeddings.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512222744722,"tcdate":1511757596315,"number":1,"cdate":1511757596315,"id":"rJmXPfYgG","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Review","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["ICLR.cc/2018/Conference/Paper759/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Learning contextual embeddings for tokens can improve accuracies in language tasks","rating":"6: Marginally above acceptance threshold","review":"The authors learn token embeddings that use surrounding context by concatenating representations obtained by training a bidirectional language model, very similar to Peters et al. 2017. They learn a distribution of weights for each layer of embeddings of the pre-trained bi-lm language model. These embeddings improve accuracies over a large range of tasks. \n\nThe technical contribution of the paper seems minimal on top of Peters et al.: Learning weights for every layer of embeddings, adding dropout, and adding a regularization term to the training. They have evaluated the efficacy of these embeddings on more tasks than the original paper that introduced them. As it stands, there is not enough novelty in this paper. Answering the questions from their future work would be interesting and would add more technical depth to the paper. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1509739118561,"tcdate":1509134261303,"number":759,"cdate":1509739115909,"id":"S1p31z-Ab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1p31z-Ab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]},"nonreaders":[],"replyCount":6,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}