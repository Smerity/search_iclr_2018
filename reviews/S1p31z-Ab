{"notes":[{"tddate":null,"ddate":null,"tmdate":1516586496688,"tcdate":1516586496688,"number":5,"cdate":1516586496688,"id":"HkFWLaMrM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Public_Comment","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Word level BiLM","comment":"Nice work! Would you happen to know how ELMo does with purely word-level inputs? In other words, if only a word-level BiLM was pre-trained? Do any of the baselines use character-level or subword features? Maybe there's not enough data in each task to train a character level model end-to-end and that's part of the perf increase seen here. It would be interesting to separate the performance increase from subword information + contextual information."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512674162546,"tcdate":1512674162546,"number":6,"cdate":1512674162546,"id":"r15_mfD-M","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Comment","forum":"S1p31z-Ab","replyto":"Syra67MWM","signatures":["ICLR.cc/2018/Conference/Paper759/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper759/Authors"],"content":{"title":"Implementation details","comment":"Thanks for your interest in our work!  To answer your questions:\n\n\"If I'm to concat x_k(1x512) with ELMo_k(1x512) to be used as new input, does it mean that I have to modify my model to take 1X1024 sized input?\"\nThe ELMo representations in our paper are 1024 dimensional -- 512 for both the forward and backward LSTMs.  If your existing input is size 512 then after adding ELMo the new input dimension will be 512 + 1024.\n\n\"By task model do you mean the biLM?\"\nIn our paper, the \"task model\" refers to the task specific supervised NLP model, e.g. the entailment model, or Q&A model, etc.  In your case it is the NMT system.  The biLM is the unsupervised pretrained component.\n\n\"In either case, could you elaborate more on \"including ELMo at the output of task RNN\"?\"\nIn our paper, the task RNN's input is GloVe (or other pretrained emeddings) concatenated with a layer of ELMo representation, [x_k; ELMo_k].  It outputs hidden states h_k for each token.  In some cases (e.g. SQuAD, SNLI) including another layer of ELMo representations at the output layer also helped.  This is accomplished by replacing h_k with [h_k; ELMo_k].  In these cases, ELMo is included twice in the task model, and each location uses a different set of learned scalar weights.  See Section 3.3 for more details.  \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512673454452,"tcdate":1512673454452,"number":5,"cdate":1512673454452,"id":"SkIhgfPbz","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Comment","forum":"S1p31z-Ab","replyto":"rJmXPfYgG","signatures":["ICLR.cc/2018/Conference/Paper759/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper759/Authors"],"content":{"title":"Response","comment":"We disagree about the lack of novelty from Peters et al. (2017).  The BiLM architecture is unified and improved, as compared to Peters et al., who used off-the-shelf independently trained left-to-right and right-to-left models. Furthermore, exposing the representations from all layers in the biLM to the task models via our scalar weighting function is a completely new idea. We show it is crucial to obtain the best performance, as demonstrated in Table 2.  Overall, our method works extremely well in practice, despite its technical simplicity. We think this fact should be considered a major strength of the work.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512673422681,"tcdate":1512673422681,"number":4,"cdate":1512673422681,"id":"r1PqgMwWM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Comment","forum":"S1p31z-Ab","replyto":"HJED6k5lM","signatures":["ICLR.cc/2018/Conference/Paper759/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper759/Authors"],"content":{"title":"Response","comment":"Our overall goal in this work is to improve performance on a wide range of NLP tasks, hence the primary focus on extrinsic evaluation.  However, we did provide intrinsic evaluations of the biLM’s contextual representations compared to those from a NMT system in Section 5.3.  As contextual representations are fundamentally different than traditional word embeddings, they require a different set of intrinsic evaluations.  Moreover, it’s likely that the WSD and POS evaluations in Section 5.3 are better correlated with extrinsic performance than the traditional intrinsic word embedding evaluations. This is an interesting area for future work.\n\nMinor points:\n1.  “The baseline results in table 1 and table 2 are not consistent. Is there something wrong?”  As mentioned in the captions, Table 1 evaluates on the test set, while Table 2 shows ablations on the development set.\n2.  “Did the baseline systems use pretrained embeddings or randomly initialized ones? In my opinion, the right baseline systems should use pretrained word2vec or glove embeddings.”  The baseline systems are state-of-the-art systems.  They all use pre-trained embeddings as described in Section 3.3 and the model details in the Appendix.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512673366563,"tcdate":1512673366563,"number":3,"cdate":1512673366563,"id":"SkJDgfv-G","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Comment","forum":"S1p31z-Ab","replyto":"rJByaq9eM","signatures":["ICLR.cc/2018/Conference/Paper759/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper759/Authors"],"content":{"title":"Response","comment":"The improvements in our paper are not just due to increasing model capacity.  The contextual ELMo representations are a fundamentally different type of word representation then GloVe and word2vec.  The biLM encodes information not available by simply increasing the hidden size of current supervised models and allowing supervised models to preferentially access the internal representations provides the best performance.\n\nWe can conclude this because:\n1.  Our baseline models are state-of-the-art systems that have been carefully tuned by their respective authors.  If solely increasing model capacity improved performance the original authors would have done so.\n2.  Adding ELMo improved performance significantly on every considered NLP task, generally with 10-20% relative error reduction.\n3.  In our ablation results (Table 2), the combination of all biLM layers is essential for the highest performance.\n4.  Pre-training with unlabeled data is necessary, as shown in previous work (Peters et al 2017 showed that training the language model at a large scale on a large unsupervised corpora was crucial for the NER task).\n\nCertainly, the unsupervised biLM objective allows us to train a high capacity model, similar to traditional word embeddings.  However, we note the pretrained component of the biLM used to compute ELMo representations has about 93 million parameters, whereas the 300 dimensional GloVe vectors have about 650 million parameters.  ELMo’s advantage is in the contextual nature of the representations and the manner in which we integrated it in the downstream task, not from increased capacity over GloVe.\n\nConceptually, our proposed approach is similar to a deep multitask architecture across tasks, where a piece of the architecture is shared and trained on unlabeled data (the ELMo piece).  However, that still leaves a huge space of possible architectures.  Concretely, one of our paper’s contributions is to propose a particular instantiation of this architecture that works extremely well in practice.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512673262206,"tcdate":1512673262206,"number":2,"cdate":1512673262206,"id":"rkUxeMwZz","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Comment","forum":"S1p31z-Ab","replyto":"B1KAJvNWM","signatures":["ICLR.cc/2018/Conference/Paper759/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper759/Authors"],"content":{"title":"Glad you enjoyed the paper!","comment":"Thanks for your comment and glad you enjoyed the paper!  We plan to release both tensorflow and pytorch implementations."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512497105508,"tcdate":1512497105508,"number":4,"cdate":1512497105508,"id":"B1KAJvNWM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Public_Comment","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Nice Contribution","comment":"I think this paper shows something that was being speculated about in the past years. Finally, someone proves this on a wide variety of tasks. The results are convincing in my view and the contribution is important, especially if the authors would manage to open source this for a variety of DL frameworks.\n\nGood work."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512353331695,"tcdate":1512353212595,"number":3,"cdate":1512353212595,"id":"Syra67MWM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Public_Comment","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Question from a novice","comment":"Hi, I'm a student conducting a deep learning project on NMT and I'm trying to implement your ELMo enhenced embeddings.\n\nI'm a bit confused about the dimension of the input. If I'm to concat x_k(1x512) with ELMo_k(1x512) to be used as new input, does it mean that I have to modify my model to take 1X1024 sized input?\n\nAnd I haven't clearly understood about appeding ELMo_k on the output side of the task model.\n1. By task model do you mean the biLM?\n2. Or the model that actually gets the task done? (in my case NMT)\nIn either case, could you elaborate more on \"including ELMo at the output of task RNN\"?\n\nThank you for the great paper."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512071416981,"tcdate":1512071416981,"number":2,"cdate":1512071416981,"id":"S1Wb-k0lz","invitation":"ICLR.cc/2018/Conference/-/Paper759/Public_Comment","forum":"S1p31z-Ab","replyto":"rkmWoaTlG","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Quick reply","comment":"That's entirely reasonable—it is somewhat ensemble-like. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512065786889,"tcdate":1512065786889,"number":1,"cdate":1512065786889,"id":"rkmWoaTlG","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Comment","forum":"S1p31z-Ab","replyto":"ryUFBhagM","signatures":["ICLR.cc/2018/Conference/Paper759/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper759/Authors"],"content":{"title":"Previous SNLI SotA","comment":"Glad you enjoyed the paper!  We are working on the code and should have an initial version to compute ELMo representations out in the next week or two.\n\nThanks for your comment about the previous SNLI single model SotA.  We'll change Table 1 to replace the reference to McCann 2017 with the ESIM + 300D Syntactic TreeLSTM (88.6) from Chen et al. 2017.  In our draft we had classified this model as an ensemble method (see Table 8 in Appendix) but will update in the next version."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1512060285560,"tcdate":1512060285560,"number":1,"cdate":1512060285560,"id":"ryUFBhagM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Public_Comment","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["~Samuel_R._Bowman1"],"readers":["everyone"],"writers":["~Samuel_R._Bowman1"],"content":{"title":"Neat results! Minor comment.","comment":"Your claim about the SotA on SNLI isn't quite right. What to use depends on what you think about ensembles, but there's a leaderboard up here: https://nlp.stanford.edu/projects/snli/\n\nThe ESIM paper from 2016 reports slightly better numbers than the 2017 McCann paper you cite as SotA.\n\nOverall, though, these are exciting results, and I'd be interested in playing with the trained model(s) once they come out."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1515642504074,"tcdate":1511857373429,"number":3,"cdate":1511857373429,"id":"rJByaq9eM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Review","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["ICLR.cc/2018/Conference/Paper759/AnonReviewer2"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"This paper proposes a method to learn contextualized word representations (ELMO) by pretraining a multilayer bidirectional LSTM language model and using representations from all levels of the LSTM in the input or output layer of a supervised task of interest. \nExperiments on various datasets (SNLI, SQuAD, SRL, Coref, NER, SST) show that the proposed method improve over baseline models.\nAblation analysis demonstrate that using all layers of ELMO is always better than just using only the final layer, and that representations learned by ELMO capture basic notions of word senses and part of speeches.\n\nThe paper is well written and I think learning contextualized word representations is an important topic.\nHowever, one thing that I am not sure about from experiments in the paper is whether the improvements come from an increase in model capacity and (unlabeled) data used to train the model, or whether there are more interesting things going on.\n- What makes the proposed approach different than just a deeper architecture for each of the considered tasks, where some parts of the network are trained using unlabeled data?\n- Is the pretraining with unlabeled data necessary, or can we just have this deep architecture and train everything with the available supervised data?\n- An ELMO enhanced model has more parameters than the baseline model for each task. What is the performance of the (non deep) baseline method with comparable number of parameters (bigger hidden size)?\n\nMore generally, it is not surprising that given sufficient training data, a deeper model (e.g., ELMO enhanced models) with multiple connections across layers will perform better than shallower models with fewer parameters.\nI would like to see more analysis and/or explanations on why the proposed method contributes more beyond this.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1515642504116,"tcdate":1511812443891,"number":2,"cdate":1511812443891,"id":"HJED6k5lM","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Review","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["ICLR.cc/2018/Conference/Paper759/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Promising results","rating":"6: Marginally above acceptance threshold","review":"This paper proposed a model called ‘deep contextualized’ to extract word embeddings for downstream applications. This model is simply a bi-directional language model (biLM) and the word embedding is a weighted combination of the output of the hidden layers (forward and backward). Furthermore like previous work, the authors proposed to pre-train the biLM with a large amount of data and then use the embeddings in combination with the context-independent word embedding in neural network models (in this paper, RNN) for final applications. Their results showed consistent improvements over the baseline and the previous best systems on several tasks.\n\nSome detailed comments:\n-\tI would like to see an overview figure which illustrates the biLM and its integration in downstream applications. Furthermore, it is also interesting to see the performance difference between with and without fine-tuning of the biLM.\n-\tIntrinsic evaluations are missing in this paper. Although we know that word similarity tasks are not the best way to evaluate word embeddings, it is always informative to report results on these standard tasks.\n-\tThe baseline results in table 1 and table 2 are not consistent. Is there something wrong?\n-\tDid the baseline systems use pretrained embeddings or randomly initialized ones? In my opinion, the right baseline systems should use pretrained word2vec or glove embeddings.\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1515642504156,"tcdate":1511757596315,"number":1,"cdate":1511757596315,"id":"rJmXPfYgG","invitation":"ICLR.cc/2018/Conference/-/Paper759/Official_Review","forum":"S1p31z-Ab","replyto":"S1p31z-Ab","signatures":["ICLR.cc/2018/Conference/Paper759/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Learning contextual embeddings for tokens can improve accuracies in language tasks","rating":"6: Marginally above acceptance threshold","review":"The authors learn token embeddings that use surrounding context by concatenating representations obtained by training a bidirectional language model, very similar to Peters et al. 2017. They learn a distribution of weights for each layer of embeddings of the pre-trained bi-lm language model. These embeddings improve accuracies over a large range of tasks. \n\nThe technical contribution of the paper seems minimal on top of Peters et al.: Learning weights for every layer of embeddings, adding dropout, and adding a regularization term to the training. They have evaluated the efficacy of these embeddings on more tasks than the original paper that introduced them. As it stands, there is not enough novelty in this paper. Answering the questions from their future work would be interesting and would add more technical depth to the paper. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]}},{"tddate":null,"ddate":null,"tmdate":1509739118561,"tcdate":1509134261303,"number":759,"cdate":1509739115909,"id":"S1p31z-Ab","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1p31z-Ab","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Deep contextualized word representations","abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).  Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.  We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.\n","pdf":"/pdf/590cfe03e44b84aea41c1961ad767027bc037dc4.pdf","TL;DR":"We introduce a new type of deep contextualized word representation that significantly improves the state of the art for a range of challenging NLP tasks.","paperhash":"anonymous|deep_contextualized_word_representations","_bibtex":"@article{\n  anonymous2018deep,\n  title={Deep contextualized word representations},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1p31z-Ab}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper759/Authors"],"keywords":["representation learning","contextualized word embeddings"]},"nonreaders":[],"replyCount":14,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}