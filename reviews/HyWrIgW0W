{"notes":[{"tddate":null,"ddate":null,"tmdate":1512459543407,"tcdate":1512459543407,"number":1,"cdate":1512459543407,"id":"r1yXa6QWM","invitation":"ICLR.cc/2018/Conference/-/Paper590/Official_Comment","forum":"HyWrIgW0W","replyto":"H1vZPmg-G","signatures":["ICLR.cc/2018/Conference/Paper590/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper590/Authors"],"content":{"title":"Re: A mathematician's perspective","comment":"Thank you for your comments. Our responses are enclosed below.\n\n1. “assumption 4 is not invariant w.r.t. a change in coordinates of the parameter space”, “in physics there is usually some kind of symmetry group”\n\nWe do not know of general results that indicate symmetries in SGD dynamics which would suggest the “right” metric space to perform our analysis. Indeed, our results indicate that such an analysis would be promising because this metric is expected to depend upon the architecture.\n\n1.1 “don't think the argument given backing up assumption 4 is very convincing”, “my opinion is that it is wrong (but don't think this disqualifies it from being assumed)”\n\nAssumption 4 is motivated by an argument that interprets the Fokker-Planck equation as a physical system in contact with the environment through energy exchange of the diffusion term. This assumption is sufficient to ensure that the second law of thermodynamics holds for such a system and is standard in the analysis of irreversible processes, see [1, 2, 3]. The second law may be violated when considering a few molecules of a gas, or analogously, a few trajectories of SGD, but our results always deal with the entire steady-state distribution.\n\n2. \"[Φ] is only a function of the architecture and the dataset is wildly misleading\", “They depend on both of Assumptions 4 and 16”, entirely reasonable to think that in the wild Φ(x) would depend on the learning rate”\n\nOne only needs assumption 4 to ensure that Phi does not depend on beta. The proof follows from (A4). Define Phi(x) = -beta^{-1} log rho^ss_beta(x) and J^ss_beta from Appendix D accordingly, we have used the subscript to emphasize the dependence on beta. (A4) implies that J^ss_1 is orthogonal to grad rho^ss_1. Decompose -grad f(x) again as\n             -grad f = J_1^ss/rho_1^ss - D grad (log rho^ss_1)\n                         = (rho_1^ss)^{-beta} ((rho_1^ss)^{beta-1} J_1^ss) - beta^{-1} D grad (log rho_1^ss)^beta.\nNow note that div((rho_1^ss)^{beta-1} J_1^ss) = 0 by assumption 4, this lets us identify J_beta^ss = J_1^ss (rho_1^ss)^{beta-1} and rho_beta^ss = (rho_1^ss)^{-beta}. The later gives the result that Phi does not depend on beta.\n\nTo conclude, under assumption 4, Phi does not depend on the learning rate or the batch-size, it is only a function of the architecture and the dataset. Also see #3 below, for isotropic noise, Phi(x) = f(x) without any assumptions.\n\n3. “The very first equation of the introduction is a tautology if Phi is defined as in equation (6)” / “feels like a sleight of hand that could hide the assumptions” / “The first part feels familiar”\n\nIndeed, the minimizer of (11) is of the form (6). However, the key point of Thm. 5 is instead that the Fokker-Planck equation reaches this minimizer *monotonically*. This is far from a sleight of hand, and if gradient noise is isotropic, in complete rigor, (11) with Phi = f becomes the celebrated Jordan-Kinderleher-Otto (JKO) functional [4]; we have steepest descent in the Wasserstein metric in this case, in addition to monotonic decrease. The JKO functional is one of the major results of the theory of optimal transportation in the 20th century, see Sec. 4.3 in [5]. The implicit definition of Phi in (6) is only used for Thm. 5. We give a completely explicit formula for Phi, in terms of f(x) and D(x), in Thm. 17 and (A13).\n\n4. “needs both the assumptions 4 and 16 to be prominent. To my mind neither of the assumptions are strictly correct, but that doesn't disqualify them from being made or stop the resulting models being taken seriously.“\n\nWe will make these assumptions prominent in the introduction. In our opinion, assumption 4 is mild and the low frequency modes of the FFT in Fig. 3a already validate it. Assumption 16 is less mild, but it is widely used by physicists and biologists (we provide references in the paper) to study real systems where it has been seen to hold.\n\n5. “does SGD undergo Brownian motion near a minimum”, “Is the evidence consistent with Brownian motion in a degenerate minimum with more complicated topology?”\n\nIrrespective of the topology, for isotropic noise, at low enough temperature, SGD performs Brownian motion near a minimum up to the first order. This can be seen from (3).\n\n[1] Prigogine, I. (1955). Thermodynamics of irreversible processes, volume 404. Thomas.\n[2] Qian, H. (2014). The zeroth law of thermodynamics and volume-preserving conservative system in equilibrium with stochastic damping. Physics Letters A, 378(7):609–616.\n[3] Frank, T. D. (2005). Nonlinear Fokker-Planck equations: fundamentals and applications. Springer Science & Business Media.\n[4] Jordan, R., Kinderlehrer, D., and Otto, F. (1997). Free energy and the Fokker-Planck equation. Physica D: Nonlinear Phenomena, 107(2-4):265–271.\n[5] Santambrogio, F. (2017). Euclidean, metric, and Wasserstein gradient flows: an overview. Bulletin of Mathematical Sciences, 7(1):87–154."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the inductive bias of stochastic gradient descent","abstract":"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such \"out-of-equilibrium\" behavior is a consequence of the fact that the gradient noise in SGD is highly non-isotropic; the covariance matrix of mini-batch gradients has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.","pdf":"/pdf/00ad468839c45257417d6432288fd0eea76978ea.pdf","TL;DR":"SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss","paperhash":"anonymous|on_the_inductive_bias_of_stochastic_gradient_descent","_bibtex":"@article{\n  anonymous2018on,\n  title={On the inductive bias of stochastic gradient descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyWrIgW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper590/Authors"],"keywords":["sgd","variational inference","gradient noise","out-of-equilibrium"]}},{"tddate":null,"ddate":null,"tmdate":1512296411337,"tcdate":1512220414906,"number":1,"cdate":1512220414906,"id":"H1vZPmg-G","invitation":"ICLR.cc/2018/Conference/-/Paper590/Public_Comment","forum":"HyWrIgW0W","replyto":"HyWrIgW0W","signatures":["~James_T_Griffin1"],"readers":["everyone"],"writers":["~James_T_Griffin1"],"content":{"title":"A mathematician's perspective","comment":"I really like this paper and have learnt a lot from reading it.  I think the basic ideas behind it are very important indeed and I don't know of anywhere else they are written down.  However I think it has some major issues.\n\nMost importantly I think the statements at the very start the introduction \"[Φ] is only a function of the architecture and the dataset\" and at the start of Section 3, \"The potential Φ(x) depends only on the full-gradient and the diffusion matrix, and will be made explicit in Section 5.\" are *wildly* misleading.  They depend on both of Assumptions 4 and 16, which even at the start of Section 3 have not been made yet.  I think's it's entirely reasonable to think that \"in the wild\" Φ(x) would depend on the learning rate, and the burden of proof to convince a reader otherwise should be very high.\n\nI am also confused by the use of the term \"full-gradient\".  In Lemma 14 formula for Φ involves U, but U depends on the Hessian of f.  So more than the gradient of f at x.\n\nThe very first equation of the introduction is a tautology if Phi is defined as in equation (6) and only has value if it has a given formula which is never actually given in the text and only alluded to (I don't count Lemma 14 as that applies to a quadratic form only).  There is nothing logically wrong about doing this and I personally find it quite entertaining, but it does feel like a sleight of hand that could hide the assumptions from an inattentive reader.\n\nThe second part of Theorem 5 is just an entropy maximisating theorem which is in every standard textbook (eg it's a corollary of Thm 12.1.1 from Elements of Infromation Theory 2nd Ed. by Cover and Thomas).  The first part feels familiar but I couldn't point you to a specific reference.\n\nConcerning Assumption 4... This assumption is not invariant w.r.t. a change in coordinates of the parameter space.  So it is reliant on the Euclidean metric, but why not any other metric, perhaps the Fisher metric?  In physics there is usually some kind of symmetry group on the underlying space pushing us to a metric, but there isn't here so I don't think the argument given backing up this assumption is very convincing.  In fact my opinion is that this assumption is wrong (though I don't think this disqualifies it from being assumed, it's interesting enough to see what happens given the assumption).\n\nFinally in Section 4, the experimental section about Brownian motion, I don't think the null hypothesis that SGD undergoes Brownian motion at a local minimum (which I assume is approximated by a quadratic form) is very strong.  Is the evidence consistent with Brownian motion in a degenerate minimum with more complicated topology?\n\nSo in summary I really like this paper, but it needs both the assumptions 4 and 16 to be prominent.  To my mind neither of the assumptions are strictly correct, but that doesn't disqualify them from being made or stop the resulting models being taken seriously."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the inductive bias of stochastic gradient descent","abstract":"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such \"out-of-equilibrium\" behavior is a consequence of the fact that the gradient noise in SGD is highly non-isotropic; the covariance matrix of mini-batch gradients has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.","pdf":"/pdf/00ad468839c45257417d6432288fd0eea76978ea.pdf","TL;DR":"SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss","paperhash":"anonymous|on_the_inductive_bias_of_stochastic_gradient_descent","_bibtex":"@article{\n  anonymous2018on,\n  title={On the inductive bias of stochastic gradient descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyWrIgW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper590/Authors"],"keywords":["sgd","variational inference","gradient noise","out-of-equilibrium"]}},{"tddate":null,"ddate":null,"tmdate":1512222694953,"tcdate":1511837330613,"number":3,"cdate":1511837330613,"id":"Bkic0BclM","invitation":"ICLR.cc/2018/Conference/-/Paper590/Official_Review","forum":"HyWrIgW0W","replyto":"HyWrIgW0W","signatures":["ICLR.cc/2018/Conference/Paper590/AnonReviewer1"],"readers":["everyone"],"content":{"title":"An interesting paper on analyzing the impact of gradient noise for SGD","rating":"6: Marginally above acceptance threshold","review":"This paper develop theory to study the impact of stochastic gradient noise for SGD, especially for deep neural network models. It is shown that when the gradient noise is isotropic normal, SGD converges to a distribution tilted by the original objective function. However, when the gradient noise is non isotropic normal, which is shown common in many models especially in deep neural network models, the behavior of SGD is intriguing, which will not converge to the tilted distribution by the original objective function, sometimes more interestingly, will converge to limit cycles around some critical points of the original objective function. The paper also provides some hints on why using SGD can get good generalization ability than gradient descend.\n\nI think the finding of this paper is interesting, and the technical details are correct. I still have the following comments.\n\nFirst, Assumption 4 seems a bit too abstract. It is not easy to see what the assumption means. It would be better if an example is given, which is verified to satisfy the assumption.\n\nAnother comment is related to the overall content of this paper. Thought the paper point out that SGD will have the out-of-equilibrium behavior when the gradient noise is non isotropic normal, it remains to show how far away this stationary distribution is from the original distribution defined by the objective function.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the inductive bias of stochastic gradient descent","abstract":"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such \"out-of-equilibrium\" behavior is a consequence of the fact that the gradient noise in SGD is highly non-isotropic; the covariance matrix of mini-batch gradients has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.","pdf":"/pdf/00ad468839c45257417d6432288fd0eea76978ea.pdf","TL;DR":"SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss","paperhash":"anonymous|on_the_inductive_bias_of_stochastic_gradient_descent","_bibtex":"@article{\n  anonymous2018on,\n  title={On the inductive bias of stochastic gradient descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyWrIgW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper590/Authors"],"keywords":["sgd","variational inference","gradient noise","out-of-equilibrium"]}},{"tddate":null,"ddate":null,"tmdate":1512222694993,"tcdate":1511807103509,"number":2,"cdate":1511807103509,"id":"B1PK_0tgf","invitation":"ICLR.cc/2018/Conference/-/Paper590/Official_Review","forum":"HyWrIgW0W","replyto":"HyWrIgW0W","signatures":["ICLR.cc/2018/Conference/Paper590/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Well written, but lacking in novelty.","rating":"5: Marginally below acceptance threshold","review":"The authors discuss the regularized objective function minimized by standard SGD in the context of neural nets, and provide a variational inference perspective using the Fokker-Planck equation. They note that the objective can be very different from the desired loss function if the SGD noise matrix is low rank, as evidenced in their experiments.\n\nOverall the paper is written quite well, and the authors do a good job of explaining their thesis. However I was unable to identify any real novelty in the theory: the Fokker-Planck equation has been widely used in analysis of stochastic noise in MCMC samplers in recent years, and this paper mostly rephrases those results. Also the fact that SGD theory only works for isotropic noise is well known, and that there is divergence from the true loss function in case of low rank noise is obvious. Thus I found most of section 3 to be a reformulation of known results, including Theorem 5 and its proof.\n\nSame goes for section 5; the symmetric- anti symmetric split is a common technique used in the stochastic MCMC literature over the last few years, and I did not find any new insight into those manipulations of the Fokker-Planck equation from this paper.\n\nThus I think that although this paper is written well, the theory is mostly recycled and the empirical results in Section 4 are known; thus it is below acceptance threshold due to lack of novelty.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the inductive bias of stochastic gradient descent","abstract":"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such \"out-of-equilibrium\" behavior is a consequence of the fact that the gradient noise in SGD is highly non-isotropic; the covariance matrix of mini-batch gradients has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.","pdf":"/pdf/00ad468839c45257417d6432288fd0eea76978ea.pdf","TL;DR":"SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss","paperhash":"anonymous|on_the_inductive_bias_of_stochastic_gradient_descent","_bibtex":"@article{\n  anonymous2018on,\n  title={On the inductive bias of stochastic gradient descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyWrIgW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper590/Authors"],"keywords":["sgd","variational inference","gradient noise","out-of-equilibrium"]}},{"tddate":null,"ddate":null,"tmdate":1512222695032,"tcdate":1511730439224,"number":1,"cdate":1511730439224,"id":"HkJG6iOlM","invitation":"ICLR.cc/2018/Conference/-/Paper590/Official_Review","forum":"HyWrIgW0W","replyto":"HyWrIgW0W","signatures":["ICLR.cc/2018/Conference/Paper590/AnonReviewer3"],"readers":["everyone"],"content":{"title":"A variational analysis of SGD as a non-equilibrium process.","rating":"8: Top 50% of accepted papers, clear accept","review":"The paper takes a closer look at the analysis of SGD as variational inference, first proposed by Duvenaud et al. 2016\nand Mandt et al. 2016. In particular, the authors point out that in general, SGD behaves quite differently from Langevin diffusion due to the multivariate nature of the Gaussian noise. As the authors show based on the Fokker-Planck equation of the underlying stochastic process, there exists a conservative current (a gradient of an underlying potential) and a non-conservative current (which might induce stationary persistent currents at long times). The non-conservative part leads to the fact that the dynamics of SGD\tmay show oscillations, and these oscillations may even prevent the algorithm from converging to the 'right' local optima. The theoretical analysis is carried-out very nicely, and the theory is supported by experiments on two-dimensional toy examples, and Fourier-spectra of the iterates of SGD.\n\nThis is a nice paper which I would like to see accepted. In particular I appreciate that the authors stress the importance\nof 'non-equilibrium physics' for understanding the SGD process. Also, the presentation is quite clear and the paper well written.\n\nThere are a few minor points which I would like to ask the authors to address:\n\n1. Why cite Kingma and Welling as a source for variational inference in\tsection 3.1? VI is a much older\tfield, and Kingma and Welling proposed a very special form of VI, namely amortized VI with inference networks. A better citation would be Jordan et\tal 1999.\n\n2. I'm not sure how much to trust the Fourier-spectra. In particular, perhaps the deviations from Brownian motion could also be due to the discrete\tnature of SGD (i.e. that the continuous-time formalism is only an approximation of a discrete process). Could you elaborate on this?\n\n3. Could you give the reader more details on how the uncertainty estimates on the Fourier transformations were obtained?\n\nThanks.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"On the inductive bias of stochastic gradient descent","abstract":"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such \"out-of-equilibrium\" behavior is a consequence of the fact that the gradient noise in SGD is highly non-isotropic; the covariance matrix of mini-batch gradients has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.","pdf":"/pdf/00ad468839c45257417d6432288fd0eea76978ea.pdf","TL;DR":"SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss","paperhash":"anonymous|on_the_inductive_bias_of_stochastic_gradient_descent","_bibtex":"@article{\n  anonymous2018on,\n  title={On the inductive bias of stochastic gradient descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyWrIgW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper590/Authors"],"keywords":["sgd","variational inference","gradient noise","out-of-equilibrium"]}},{"tddate":null,"ddate":null,"tmdate":1509739215234,"tcdate":1509127736955,"number":590,"cdate":1509739212574,"id":"HyWrIgW0W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HyWrIgW0W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"On the inductive bias of stochastic gradient descent","abstract":"Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such \"out-of-equilibrium\" behavior is a consequence of the fact that the gradient noise in SGD is highly non-isotropic; the covariance matrix of mini-batch gradients has a rank as small as 1% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.","pdf":"/pdf/00ad468839c45257417d6432288fd0eea76978ea.pdf","TL;DR":"SGD implicitly performs variational inference; gradient noise is highly non-isotropic, so SGD does not even converge to critical points of the original loss","paperhash":"anonymous|on_the_inductive_bias_of_stochastic_gradient_descent","_bibtex":"@article{\n  anonymous2018on,\n  title={On the inductive bias of stochastic gradient descent},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HyWrIgW0W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper590/Authors"],"keywords":["sgd","variational inference","gradient noise","out-of-equilibrium"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}