{"notes":[{"tddate":null,"ddate":null,"tmdate":1516413860375,"tcdate":1516351698864,"number":6,"cdate":1516351698864,"id":"rJiRgVkSz","invitation":"ICLR.cc/2018/Conference/-/Paper127/Official_Comment","forum":"Bym0cU1CZ","replyto":"Bym0cU1CZ","signatures":["ICLR.cc/2018/Conference/Paper127/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper127/Authors"],"content":{"title":"New updates","comment":"We updated the paper according to the comments about the existing work.\n\nSpecifically, in the new version, we made the following updates:\n\n1. We withdraw the claim of \"first work of using dialogue act in open domain dialogue generation\" and position our work as “we are the first who design dialogue acts to explain social interactions, control open domain response generation, and guide human-machine conversations.” (see the first paragraph of Related Work)\n\n2. We cite the ACL paper and clarify the difference from it at the end of the first paragraph of Related Work.\n\n3. We further emphasize our motivation on using RL in the last paragraph of Section 3.1 and the first paragraph of Section 3.2\n\n4. We correct some typos. For example: HERD-> HRED, VHERD->VHRED. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1516412983658,"tcdate":1515935387721,"number":5,"cdate":1515935387721,"id":"rJNoU0uEf","invitation":"ICLR.cc/2018/Conference/-/Paper127/Official_Comment","forum":"Bym0cU1CZ","replyto":"BkY9B2d4G","signatures":["ICLR.cc/2018/Conference/Paper127/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper127/Authors"],"content":{"title":"Difference lies in four aspects: goal,  dialogue acts,  model, and learning method","comment":"Thank you for leading us to the ACL paper. \n\nWhile overall the ACL paper is about how to model dialogues with latent variables using VAE techniques, which is very similar to the early work of Serban et al., https://arxiv.org/abs/1605.06069 (VHRED, this is also a baseline in our experiment), the paper does cover dialogue acts from an existing data set, the Switchboard Corpus, in the experiment part as an extra feature.\n\nWe would say that the ACL paper is about how to generate a reply for a static context. It is NOT a study of how dialogue acts can be used to interpret and control dynamic human-machine interactions as policies, NOR it is a study of how the generated responses will be affected by the dialogue acts, which are the major contributions of our work. \n\nHere, we would like to clarify the  originality (contributions) of our paper and the difference with the ACL paper.\n\nORIGINALITY:\n1. The first study of how to design dialogue acts to understand how human behave and engage in social chat, instead of using dialogue acts to ground semantics of utterances. The contribution lies in filling the gap between end-to-end open domain dialogue modeling and task-oriented dialogue modeling with task specific dialogue acts, which is not covered by the ACL paper. \n\n2. Insights from the analysis of human interactions. The contribution lies in discovery of the role of context switch for engagement in social chat with quantitative evidences, which is not covered by the ACL paper.\n\n3. The first work on modeling the policy in open domain dialogue management with dialogue acts  and learning the policy (i.e., the combination of dialogue acts in a conversation flow) for long-term conversation by reinforcement learning, which is not covered by the ACL paper.\n\n4. Empirical studies of how human-machine conversation is affected by the dialogue acts, from static generated text to dynamic interactions, which is not covered by the ACL paper. \n\n\nDIFFERENCE\n1. Goal.  The goal of the ACL paper is to study how to leverage VAE techniques to address the\"one-to-many\" problem in open domain dialogues. The major contribution of the ACL paper also lies in the VAE framework, as summarized by the authors in the last paragraph of Introduction of the paper. Our goal is to INTERPRET engagement in human-human social interactions with dialogue acts and thus ENHANCE human-machine engagement in open domain conversations by combining the dialogue acts.\n\n2. Dialogue acts.  Dialogue acts  in ACL paper  are extra features, therefore they come from an existing data set and follow a traditional scheme (42 acts).  Our dialogue acts, however, are designed to describe how human perform in order to keep their social interactions. Therefore, they highly connect to human behavior regarding to conversational contexts and are not covered by any of the existing data sets.  Dialogue acts in the ACL paper are used to measure how well the generated responses are and justify  that the learned latent representations are reasonable. Our dialogue acts, however, are used to interpret and control the flow of human-machine interactions in order to achieve long-term conversation. \n\n3. Model. The ACL paper deals with static response generation (i.e., given a fixed context, how to generate a proper response). Therefore, only the  dialogue act of the last turn (in training) and the predicted dialogue act of the response (in both training and test) are treated as extra features in latent representation learning and response decoding. There are no studies on how the dialogue acts are coordinated and thus affect the flow of conversation. While we deal with dynamic human-machine interactions, therefore, dialogue acts are used to model the policy of dialogue management and guide the flow of conversation. Coordination of dialogue acts across multiple turns is implicitly modeled in the policy network. Note that we also show results of static response generation. That is just to show that our model can give reasonable intermediate results in interactions.\n\n4.  Learning method. The ACL paper learns a dialogue model with VAE. While we treat dialogue acts as a kind of strategies and use reinforcement learning to enhance engagement in human-machine interactions. \n\nAlthough there exists significant difference, we would like to show our respect to this ACL work.  We upload a new version and make the following changes:\n1. We cite the paper and clarify the difference in Related Work\n2. We position our contributions on dialogue act design for interpretation and learning dialogue policies with the dialogue acts.  \n3. We withdraw the claim of “first work of using dialogue act in open domain dialogue generation”, and change it  to “we are the first who design dialogue acts to explain social interactions, control open domain response generation, and guide human-machine conversations.” (see the first paragraph of Related Work)\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515927100137,"tcdate":1515926928725,"number":1,"cdate":1515926928725,"id":"BkY9B2d4G","invitation":"ICLR.cc/2018/Conference/-/Paper127/Public_Comment","forum":"Bym0cU1CZ","replyto":"Bym0cU1CZ","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Similar idea from ACL'17","comment":"The idea in this paper looks very similar to the idea from <Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders> which was presented at ACL'17: https://arxiv.org/abs/1703.10960. Especially, the idea of using dialog act in open domain dialogue generation, which is the main contribution of this paper, was firstly introduced in https://arxiv.org/abs/1703.10960\n\nI'd like the authors to clarify how they differ and would like to ask the reviewers to read https://arxiv.org/abs/1703.10960 and see how this affects your judgment of the submission."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515052302671,"tcdate":1514106180429,"number":4,"cdate":1514106180429,"id":"BknHTJTfz","invitation":"ICLR.cc/2018/Conference/-/Paper127/Official_Comment","forum":"Bym0cU1CZ","replyto":"Bym0cU1CZ","signatures":["ICLR.cc/2018/Conference/Paper127/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper127/Authors"],"content":{"title":"A new version of the paper has been uploaded","comment":"We upload a new version of the paper in which we try our best to address the concerns from the reviewers. \nMajor revisions include:\n(1) We cited more work about dialogue acts in Section 2.1, and commented on the existing dialogue act corpus. \n(2) We changed Equation (4) in the previous version to Equation (4) and Equation (5). Now, the mechanism of dialogue generation becomes more clear and general. \n(3) We further justified the necessity of reinforcement learning at the end of Section 3.1 with the distribution of the data set. \n(4) To emphasize the effect of dialogue acts to generation, we moved Table 7 in  Appendix in the previous version to the main text. Now the table becomes Table 5. \n(5) We added Section 4.4 where we systematically analyze how the generated text is affected by the dialogue acts with some automatic metrics. \n(6) We broke up some long paragraphs for ease of reading. "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515048203149,"tcdate":1514104928083,"number":3,"cdate":1514104928083,"id":"SydD_y6fG","invitation":"ICLR.cc/2018/Conference/-/Paper127/Official_Comment","forum":"Bym0cU1CZ","replyto":"ry2CUfcxz","signatures":["ICLR.cc/2018/Conference/Paper127/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper127/Authors"],"content":{"title":"Rebuttal","comment":"Thank you for your valuable comments.\n\n1.\tWe replace the word \"scalability\" in Introduction with other words (e.g., \"scale to new domains\").     \n        \n2.\tWe follow your suggestions and cite related work about dialogue acts at the beginning of Section 2.1. We also mention a public DA corpora \"the Switchboard Corpus\" in the second paragraph of Section 2.1 and clarify that we build a new data set because no one has analyzed open domain dialogues with dialogue acts about conversational context before. \n\n3.\tWe modify Equation (4) in the previous version as Equation (4)+Equation (5). Now dialogue act selection in our model becomes more general and takes multiple strategies (top 1 and top K) as special cases. We can try dialogue generation with top K acts in our future work. \n\n4.\tWe break up some long paragraphs in Section 3.2 for ease of reading and cite (Li et al., 2016b) before the termination strategies, as some of them (e.g., regarding to repetitive turns) are inspired by the work. \n\n5.\tAlthough we use a different data set, the average number of turns of the simulated dialogues from RL-S2S in our work is very close to the number reported in (Li et al., 2016b). Our number is 4.36 (refer to the machine-machine column in Table 4(b)), while the number reported in (Li et al., 2016b) is 4.48. This might provide an additional evidence to the correctness of the implementation of the baseline model in the work.\n\n6.\tTable 5 in the previous version becomes Table 6 now. We describe the table right after it.  Basically, SL-DAGM and RL-DAGM share the same text generation but differs on how they select dialogue acts, as we only optimize the policy network with RL. The response given by RL-DAGM comes from CS.Q (clarified after the generated response in Table 6), while the response given by SL-DAGM comes from CS.S. Both are top dialogue acts under the corresponding policy networks. \n\n7.\tWe follow your suggestions and break up long paragraphs. \n\n8.\tThe examples given in Appendix are picked randomly.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1516414839314,"tcdate":1514101754882,"number":2,"cdate":1514101754882,"id":"SJ7bhA2fM","invitation":"ICLR.cc/2018/Conference/-/Paper127/Official_Comment","forum":"Bym0cU1CZ","replyto":"HklWe9qxz","signatures":["ICLR.cc/2018/Conference/Paper127/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper127/Authors"],"content":{"title":"Rebuttal","comment":"Thank you for your valuable comments.\n\n1.    Why we need reinforcement learning\n\nAs we have mentioned in the paper, open domain dialogue generation needs to be optimized for long-term engagement in practice.  Yes, in supervised learning, we have large scale of human dialogues tagged with dialogue acts, but that does not mean the algorithm can learn how to keep conversation going from the data, as more than 45% training dialogues are not longer than 5 turns (described in the last paragraph of Section 3.1 in the new version).  Supervised learning just learns a model by maximizing the likelihood of the observed data including the short dialogues. Dialogue acts are learned only according to the history, and no information of the future influence can flow in.  Then, without an additional objective (i.e., Equation (8) in Section 3.2) and mechanism (optimizing for future success), how can we (explicitly) guarantee that the model is optimized for long-term engagement? Therefore, supervised learning is to learn human language and reinforcement learning is to further optimize the combination of dialogue acts in order to achieve long-term conversation.\n\nModel optimization with reinforcement learning is also encouraged by the experimental results. In Table 5, response diversity is significantly improved by RL (see the difference between RL-DAGM and SL-DAGM on distinct-1 and distinct-2), and in Table 4(b), with RL, both the dialogues from machine-machine simulation and human-machine test become longer. Moreover, as we have analyzed in the last paragraph of Section 4.3, it is because RL can promote context switch in interactions that the model, after optimized with RL, can lead to better engagement. All the results well support our motivation to learning with RL.  \n\n2.     >>> the formulation in equation 4 seems to be problematic\n\nThanks for pointing out this problem. We have modified Equation (4) in the previous version as Equation (4)+Equation (5) in the new version. Now the procedure of generation becomes more clear. \n\n3.     >>>\"Simplify pr(ri|si,ai) as pr(ri|ai,ui−1,ui−2) since decoding natural language responses from long conversation history is challenging\" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something\n\nYes, from a model perspective, the simplification here just changes the encoder. However, what we mean here is that it is difficult for an RNN to memorize long conversation history, and thus encoding long history means either the response given by the decoder is irrelevant to the early history, or the response will be messed up. \n\n4.     >>>\"We train m(·, ·) with the 30 million crawled data through negative sampling.\" not sure I understand the connection between training $m(\\cdot, \\cdot)$ and the entire model\n\n $m(\\cdot, \\cdot)$ is pre-trained and used to estimate the reward function in Equation (9). This is the only connection between  $m(\\cdot, \\cdot)$ and the entire model. We have clarified this in the paragraph after Equation (9).\n\n5.    >>>the experiments are not convincing. At least, it should show the generation texts were affected about DAs in a systemic way. Only a single example in table 5 is not enough.\n\nThanks for your comments. We do three things to show how the generated texts are affected by dialogue acts:\n\n(1)\tWe move Table 7 in the previous version from Appendix to Section 4.2. Now the table is Table 5. In the table, one can see that with dialogue acts, the diversity of generated responses is significantly improved (corresponding to much larger distinct-1 and distinct-2).  In the following explanation (the third paragraph of Section 4.2), we claim that this is one benefit of dialogue acts, as search space now becomes act × language. \n\n(2)\tWe add Section 4.4 where we compare responses from different dialogue acts using some metrics. The conclusion is that responses generated from CS.* are longer, more informative, and contain more new words than responses generated from CM.*, and statements and answers are generally more informative than questions in both CS.* and CM.*.  Please refer to the new version of the paper to get more details.\n\n(3)\tIn the last paragraph of Section 4.3, we show that simulated dialogues without CS.* are much shorter than those with CS.* (SL: 4.78 v.s. 8.66, RL: 2.67 v.s., 8.18). The result indicates that if we remove CS.*, then the conversation engagement of our model may degrade to the baseline model. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1514101093337,"tcdate":1514101093337,"number":1,"cdate":1514101093337,"id":"r1aPFC2zz","invitation":"ICLR.cc/2018/Conference/-/Paper127/Official_Comment","forum":"Bym0cU1CZ","replyto":"ryiSW8nef","signatures":["ICLR.cc/2018/Conference/Paper127/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper127/Authors"],"content":{"title":"Rebuttal","comment":"Thank you for your comments"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642396231,"tcdate":1511969090771,"number":3,"cdate":1511969090771,"id":"ryiSW8nef","invitation":"ICLR.cc/2018/Conference/-/Paper127/Official_Review","forum":"Bym0cU1CZ","replyto":"Bym0cU1CZ","signatures":["ICLR.cc/2018/Conference/Paper127/AnonReviewer4"],"readers":["everyone"],"content":{"title":"review","rating":"7: Good paper, accept","review":"The paper describes a technique to incorporate dialog acts into neural conversational agents.  This is very interesting work.  Existing techniques for neural conversational agents essentially mimic the data in large corpora of message-response pairs and therefore do not use any notion of dialog act.  A very important type of dialog act is \"switching topic\", often done to ensure that the conversation will continue.  The paper describes a classifier that predicts the dialog act of the next utterance.  The next utterance is then generated based on this dialog act.  The paper also describes how to increase the relevance of responses and the length of conversations by self reinforcement learning.  This is also very interesting.  The empirical evaluation demonstrates the effectiveness of the approach.  The paper is also well written.  I do not have any suggestion for improvement.  This is good work that should be published.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642396274,"tcdate":1511854072422,"number":2,"cdate":1511854072422,"id":"HklWe9qxz","invitation":"ICLR.cc/2018/Conference/-/Paper127/Official_Review","forum":"Bym0cU1CZ","replyto":"Bym0cU1CZ","signatures":["ICLR.cc/2018/Conference/Paper127/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting topic discussed, but the paper is not good enough","rating":"4: Ok but not good enough - rejection","review":"The topic discussed in this paper is interesting. Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation. It is interesting to see how DAs are used for conversational modeling, however this paper is difficult for me to follow. For example:\n\n1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning. Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning.\n2) the formulation in equation 4 seems to be problematic\n3) \"simplify pr(ri|si,ai) as pr(ri|ai,ui−1,ui−2) since decoding natural language responses from long conversation history is challenging\" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something?\n4) about section 3.2, again I didn't get whether the model needs RL for training.\n5) \"We train m(·, ·) with the 30 million crawled data through negative sampling.\" not sure I understand the connection between training $m(\\cdot, \\cdot)$ and the entire model.\n6) the experiments are not convincing. At least, it should show the generation texts were affected about DAs in a systemic way. Only a single example in table 5 is not enough.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642396313,"tcdate":1511823059870,"number":1,"cdate":1511823059870,"id":"ry2CUfcxz","invitation":"ICLR.cc/2018/Conference/-/Paper127/Official_Review","forum":"Bym0cU1CZ","replyto":"Bym0cU1CZ","signatures":["ICLR.cc/2018/Conference/Paper127/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Strong results from a simple idea","rating":"7: Good paper, accept","review":"The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues. In their evaluations, this approach, and one that additionally uses policy gradient RL with discourse-level objectives to fine-tune the dialogue act predictions, outperform past models for human-scored response quality and conversation engagement.\nWhile this is a fairly straightforward idea with a long history, the authors claim to be the first to use dialogue act prediction for open-domain (rather than task-driven) dialogue. If that claim to originality is not contested, and the authors provide additional assurances to confirm the correctness of the implementations used for baseline models, this article fills an important gap in open-domain dialogue research and suggests a fruitful future for structured prediction in deep learning-based dialogue systems.\n\nSome points:\n1. The introduction uses \"scalability\" throughout to mean something closer to \"ability to generalize.\" Consider revising the wording here.\n2. The dialogue act tag set used in the paper is not original to Ivanovic (2005) but derives, with modifications, from the tag set constructed for the DAMSL project (Jurafsky et al., 1997; Stolcke et al., 2000). It's probably worth citing some of this early work that pioneered the use of dialogue acts in NLP, since they discuss motivations for building DA corpora.\n3. In Section 2.1, the authors don't explicitly mention existing DA-annotated corpora or discuss specifically why they are not sufficient (is there e.g. a dataset that would be ideal for the purposes of this paper except that it isn't large enough?)\n3. The authors appear to consider only one option (selecting the top predicted dialogue act, then conditioning the response generator on this DA) among many for inference-time search over the joint DA-response space. A more comprehensive search strategy (e.g. selecting the top K dialogue acts, then evaluating several responses for each DA) might lead to higher response diversity.\n4. The description of the RL approach in Section 3.2 was fairly terse and included a number of ad-hoc choices. If these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited. Examples (perhaps in the appendix) might also be helpful for the reader to understand that the chosen termination conditions or relevance metrics are reasonable.\n5. The comparison against previous work is missing some assurances I'd like to see. While directly citing the codebases you used or built off of is fantastic, it's also important to give the reader confidence that the implementations you're comparing to are the same as those used in the original papers, such as by mentioning that you can replicate or confirm quantitative results from the papers you're comparing to. Without that there could always be the chance that something is missing from the implementation of e.g. RL-S2S that you're using for comparison.\n6. Table 5 is not described in the main text, so it isn't clear what the different potential outputs of e.g. the RL-DAGM system result from (my guess: conditioning the response generation on the top 3 predicted dialogue acts?)\n7. A simple way to improve the paper's clarity for readers would be to break up some of the very long paragraphs, especially in later sections. It's fine if that pushes the paper somewhat over the 8th page.\n8. A consistent focus on human evaluation, as found in this paper, is probably the right approach for contemporary dialogue research.\n9. The examples provided in the appendix are great. It would be helpful to have confirmation that they were selected randomly (rather than cherry-picked).","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1516350969602,"tcdate":1509022411541,"number":127,"cdate":1509739467787,"id":"Bym0cU1CZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Bym0cU1CZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts","abstract":"Conventional methods model open domain dialogue generation as a black box through end-to-end learning from large scale conversation data. In this work, we make the first step to open the black box by introducing dialogue acts into open domain dialogue generation. The dialogue acts are generally designed and reveal how people engage in social chat. Inspired by analysis on real data, we propose jointly modeling dialogue act selection and response generation, and perform learning with human-human conversations tagged with a dialogue act classifier and a reinforcement approach to further optimizing the model for long-term conversation. With the dialogue acts, we not only achieve significant improvement over state-of-the-art methods on response quality for given contexts and long-term conversation in both machine-machine simulation and human-machine conversation, but also are capable of explaining why such achievements can be made.","pdf":"/pdf/6220a353803ed439143f9c6c1ba5d2f2a4631f90.pdf","TL;DR":"open domain dialogue generation with dialogue acts","paperhash":"anonymous|towards_interpretable_chitchat_open_domain_dialogue_generation_with_dialogue_acts","_bibtex":"@article{\n  anonymous2018towards,\n  title={Towards Interpretable Chit-chat: Open Domain Dialogue Generation with Dialogue Acts},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Bym0cU1CZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper127/Authors"],"keywords":["dialogue generation","dialogue acts","open domain conversation","supervised learning","reinforcement learning"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}