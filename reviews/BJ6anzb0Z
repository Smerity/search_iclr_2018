{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222829660,"tcdate":1511812706471,"number":3,"cdate":1511812706471,"id":"HJcw0y5eM","invitation":"ICLR.cc/2018/Conference/-/Paper978/Official_Review","forum":"BJ6anzb0Z","replyto":"BJ6anzb0Z","signatures":["ICLR.cc/2018/Conference/Paper978/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Interesting paper on sentiment analysis combining image and text ","rating":"5: Marginally below acceptance threshold","review":"The paper presents a multi-modal CNN model for sentiment analysis that combines images and text.  The model is trained on a new dataset collected from Tumblr.\n\nPositive aspects:\n+ Emphasis in model interpretability and its connection to psychological findings in emotions\n+ The idea of using Tumblr data seems interesting, allowing to work with a large set of emotion categories, instead of considering just the binary task positive vs. negative. \n\nWeaknesses:\n- A deeper analysis of previous work on the combination of image and text for sentiment analysis (both datasets and methods) and its relation with the presented work is necessary. \n- The proposed method is not compared with other methods that combine text and image for sentiment analysis.\n-  The study is limited to just one dataset.\n\nThe paper presents interesting ideas and findings in an important challenging area. The main novelties of the paper are: (1) the use of Tumblr data, (2) the proposed CNN architecture, combining images and text (using word embedding. \n\nI missed a \"related work section\", where authors clearly mention previous works on similar datasets. Some related works are mentioned in the paper, but those are spread in different sections. It's hard to get a clear overview of the previous research: datasets, methods and contextualization of the proposed approach in relation with previous work. I think authors should cite Sentibanks. Also, at some point authors should compare their proposal with previous work. \n\nMore comments:\n\n- Some figures could be more complete: to see more examples in Fig 1, 2, 3 would help to understand better the dataset and the challenges. \n- In table 4, for example, it would be nice to see the performance on the different emotion categories.\n- It would be interesting to see qualitative visual results on recognitions.\n\nI like this work, but I think authors should improve the aspects I mention for its publication.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multimodal Sentiment Analysis To Explore the Structure of Emotions","abstract":"We propose a novel approach to multimodal sentiment analysis using deep neural\nnetworks combining visual recognition and natural language processing. Our\ngoal is different than the standard sentiment analysis goal of predicting whether\na sentence expresses positive or negative sentiment; instead, we aim to infer the\nlatent emotional state of the user. Thus, we focus on predicting the emotion word\ntags attached by users to their Tumblr posts, treating these as “self-reported emotions.”\nWe demonstrate that our multimodal model combining both text and image\nfeatures outperforms separate models based solely on either images or text. Our\nmodel’s results are interpretable, automatically yielding sensible word lists associated\nwith emotions. We explore the structure of emotions implied by our model\nand compare it to what has been posited in the psychology literature, and validate\nour model on a set of images that have been used in psychology studies. Finally,\nour work also provides a useful tool for the growing academic study of images—\nboth photographs and memes—on social networks.","pdf":"/pdf/c00807a22d7e64009951a086257ff4f8b0f763be.pdf","paperhash":"anonymous|multimodal_sentiment_analysis_to_explore_the_structure_of_emotions","_bibtex":"@article{\n  anonymous2018multimodal,\n  title={Multimodal Sentiment Analysis To Explore the Structure of Emotions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ6anzb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper978/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222830488,"tcdate":1511801572341,"number":2,"cdate":1511801572341,"id":"BJ2J7pFgf","invitation":"ICLR.cc/2018/Conference/-/Paper978/Official_Review","forum":"BJ6anzb0Z","replyto":"BJ6anzb0Z","signatures":["ICLR.cc/2018/Conference/Paper978/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Hashtag classification of Tumblr Posts ","rating":"4: Ok but not good enough - rejection","review":"This paper presents a method for classifying Tumblr posts with associated images according to associated single emotion word hashtags.  The method relies on sentiment pre-processing from GloVe and image pre-processing from Inception.  \n \nMy strongest criticism for this paper is against the claim that Tumblr post represent self-reported emotions and that this method sheds new insight on emotion representation and my secondary criticism is a lack of novelty in the method, which seems to be simply a combination of previously published sentiment analysis module and previously published image analysis module, fused in an output layer. \n\nThe authors claim that the hashtags represent self-reported emotions, but this is not true in the way that psychologists query participants regarding emotion words in psychology studies.  Instead these are emotion words that a person chooses to broadcast along with an associated announcement.  As the authors point out, hashtags and words may be used sarcastically or in different ways from what is understood in emotion theory.  It is quite common for everyday people to use emotion words this way e.g. using #love to express strong approval rather than an actual feeling of love.   \n\nIn their analysis the authors claim:\n“The 15 emotions retained were those with high relative frequencies on Tumblr among the PANAS-X scale (Watson & Clark, 1999)”.\nHowever five of the words the authors retain: bored, annoyed, love, optimistic, and pensive are not in fact found in the PANAS-X scale:\n\nReference: The PANAS-X Scale: https://wiki.aalto.fi/download/attachments/50102838/PANAS-X-scale_spec.pdf Also the longer version that the authors cited: \nhttps://www2.psychology.uiowa.edu/faculty/clark/panas-x.pdf\n\nIt should also be noted that the PANAS (Positive and Negative Affect Scale) scale and the PANAS-X (the “X” is for eXtended) scale are questionnaires used to elicit from participants feelings of positive and negative affect, they are not collections of \"core\" emotion words, but rather words that are colloquially attached to either positive or negative sentiment.  For example PANAS-X includes words like:“strong” ,“active”, “healthy”, “sleepy” which are not considered emotion words by psychology.  \n\nIf the authors stated goal is \"different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment\" they should be aware that this is exactly what PANAS is designed to do - not to infer the latent emotional state of a person, except to the extent that their affect is positive or negative.\n\n\nThe work of representing emotions had been an field in psychology for over a hundred years and it is still continuing.  https://en.wikipedia.org/wiki/Contrasting_and_categorization_of_emotions.\n\nOne of the most popular theories of emotion is the theory that there exist “basic” emotions: Anger, Disgust, Fear, Happiness (enjoyment), Sadness and Surprise (Paul Ekman, cited by the authors).  These are short duration sates lasting only seconds.  They are also fairly specific, for example “surprise” is sudden reaction to something unexpected, which is it exactly the same as seeing a flower on your car and expressing “what a nice surprise.”  The surprise would be the initial reaction of “what’s that on my car?  Is it dangerous?” but after identifying the object as non-threatening, the emotion of “surprise” would likely pass and be replaced with appreciation.  \n\nThe Circumplex Model of Emotions (Posner et al 2005) the authors refer to actually stands in opposition to the theories of Ekman.  From the cited paper by Posner et al : \n\"The circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological systems. This model stands in contrast to theories of basic emotions, which posit that a discrete and independent neural system subserves every emotion.\"\nFrom my reading of this paper, it is clear to me that the authors do not have a clear understanding of the current state of psychology’s view of emotion representation and this work would not likely contribute to a new understanding of the latent structure of peoples’ emotions.\n\nIn the PCA result, it is not \"clear\" that the first axis represents valence, as \"sad\" has a slight positive on this scale and \"sad\" is one of the emotions most clearly associated with negative valence.\n\nWith respect to the rest of the paper, the level of novelty and impact is \"ok, but not good enough.\"  This analysis does not seem very different from Twitter analysis, because although Tumblr posts are allowed to be longer than Twitter posts, the authors truncate the posts to 50 characters.  Additionally, the images do not seem to add very much to the classification.  The authors algorithm also seems to be essentially a combination of two other, previously published algorithms.\n\nFor me the novelty of this paper was in its application to the realm of emotion theory, but I do not feel there is a contribution here.  This paper is more about classifying Tumblr posts according to emotion word hashtags than a paper that generates a new insights into emotion representation or that can infer latent emotional state. \n\n\n\n\n\n\n\n\n\n\n\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multimodal Sentiment Analysis To Explore the Structure of Emotions","abstract":"We propose a novel approach to multimodal sentiment analysis using deep neural\nnetworks combining visual recognition and natural language processing. Our\ngoal is different than the standard sentiment analysis goal of predicting whether\na sentence expresses positive or negative sentiment; instead, we aim to infer the\nlatent emotional state of the user. Thus, we focus on predicting the emotion word\ntags attached by users to their Tumblr posts, treating these as “self-reported emotions.”\nWe demonstrate that our multimodal model combining both text and image\nfeatures outperforms separate models based solely on either images or text. Our\nmodel’s results are interpretable, automatically yielding sensible word lists associated\nwith emotions. We explore the structure of emotions implied by our model\nand compare it to what has been posited in the psychology literature, and validate\nour model on a set of images that have been used in psychology studies. Finally,\nour work also provides a useful tool for the growing academic study of images—\nboth photographs and memes—on social networks.","pdf":"/pdf/c00807a22d7e64009951a086257ff4f8b0f763be.pdf","paperhash":"anonymous|multimodal_sentiment_analysis_to_explore_the_structure_of_emotions","_bibtex":"@article{\n  anonymous2018multimodal,\n  title={Multimodal Sentiment Analysis To Explore the Structure of Emotions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ6anzb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper978/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1512222830529,"tcdate":1511557814275,"number":1,"cdate":1511557814275,"id":"rJA29bLxf","invitation":"ICLR.cc/2018/Conference/-/Paper978/Official_Review","forum":"BJ6anzb0Z","replyto":"BJ6anzb0Z","signatures":["ICLR.cc/2018/Conference/Paper978/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The authors present a study that aims at inferring the \"emotional\" tags provided by Thumblr users starting from images and texts in the captions. ","rating":"6: Marginally above acceptance threshold","review":"\nThe authors present a study that aims at inferring the \"emotional\" tags provided by Thumblr users starting from images and texts in the captions. For text processing the authors use a standard LSTM taking as input GLOVE vectors of words in a sentence. For visual information, authors use a pretrained CNN (with fine tuning). A fully connected layer is used to fuse the multimodal information. Experimental results are reported in a self generated data set. \n\nThe contribution from the RL perspective is limited, in the sense that the authors simply applied standard models to predict a bunch of labels (in this case, emotion labels). It is interesting the \"psychological\" analysis that the authors present in Section 6. Still, I think the contribution in that part is a: sentiment-psychologically inspired analysis of the Thumbrl data set. \n\nI think the author's statement on that this study leads to a more plausible psychological model of emotion is not well founded (they also mention to learn to recognize the latent emotional state). Whereas it is true that psychological studies rely on self - filled questionnaires, comparing a questionnaire (produced by expert psychologist) to the tags provided by users in a social network is to ambitious. (in some parts the authors make explicit this is an approximation, this should be stressed in every part of the paper)\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Multimodal Sentiment Analysis To Explore the Structure of Emotions","abstract":"We propose a novel approach to multimodal sentiment analysis using deep neural\nnetworks combining visual recognition and natural language processing. Our\ngoal is different than the standard sentiment analysis goal of predicting whether\na sentence expresses positive or negative sentiment; instead, we aim to infer the\nlatent emotional state of the user. Thus, we focus on predicting the emotion word\ntags attached by users to their Tumblr posts, treating these as “self-reported emotions.”\nWe demonstrate that our multimodal model combining both text and image\nfeatures outperforms separate models based solely on either images or text. Our\nmodel’s results are interpretable, automatically yielding sensible word lists associated\nwith emotions. We explore the structure of emotions implied by our model\nand compare it to what has been posited in the psychology literature, and validate\nour model on a set of images that have been used in psychology studies. Finally,\nour work also provides a useful tool for the growing academic study of images—\nboth photographs and memes—on social networks.","pdf":"/pdf/c00807a22d7e64009951a086257ff4f8b0f763be.pdf","paperhash":"anonymous|multimodal_sentiment_analysis_to_explore_the_structure_of_emotions","_bibtex":"@article{\n  anonymous2018multimodal,\n  title={Multimodal Sentiment Analysis To Explore the Structure of Emotions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ6anzb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper978/Authors"],"keywords":[]}},{"tddate":null,"ddate":null,"tmdate":1510092383210,"tcdate":1509137609410,"number":978,"cdate":1510092361010,"id":"BJ6anzb0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJ6anzb0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Multimodal Sentiment Analysis To Explore the Structure of Emotions","abstract":"We propose a novel approach to multimodal sentiment analysis using deep neural\nnetworks combining visual recognition and natural language processing. Our\ngoal is different than the standard sentiment analysis goal of predicting whether\na sentence expresses positive or negative sentiment; instead, we aim to infer the\nlatent emotional state of the user. Thus, we focus on predicting the emotion word\ntags attached by users to their Tumblr posts, treating these as “self-reported emotions.”\nWe demonstrate that our multimodal model combining both text and image\nfeatures outperforms separate models based solely on either images or text. Our\nmodel’s results are interpretable, automatically yielding sensible word lists associated\nwith emotions. We explore the structure of emotions implied by our model\nand compare it to what has been posited in the psychology literature, and validate\nour model on a set of images that have been used in psychology studies. Finally,\nour work also provides a useful tool for the growing academic study of images—\nboth photographs and memes—on social networks.","pdf":"/pdf/c00807a22d7e64009951a086257ff4f8b0f763be.pdf","paperhash":"anonymous|multimodal_sentiment_analysis_to_explore_the_structure_of_emotions","_bibtex":"@article{\n  anonymous2018multimodal,\n  title={Multimodal Sentiment Analysis To Explore the Structure of Emotions},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ6anzb0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper978/Authors"],"keywords":[]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}