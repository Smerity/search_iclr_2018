{"notes":[{"tddate":null,"ddate":null,"tmdate":1515642385182,"tcdate":1513756137346,"number":3,"cdate":1513756137346,"id":"ry-xL5Dfz","invitation":"ICLR.cc/2018/Conference/-/Paper111/Official_Review","forum":"SJa1Nk10b","replyto":"SJa1Nk10b","signatures":["ICLR.cc/2018/Conference/Paper111/AnonReviewer2"],"readers":["everyone"],"content":{"title":"well written paper but lack of justification and comparison","rating":"5: Marginally below acceptance threshold","review":"This paper aims to endow neural networks the ability to produce anytime prediction. The authors propose several heuristics to reweight and oscillate the loss to improve the anytime performance. In addition, they propose to use a sequence of exponentially deepening anytime neural networks to reduce the performance gap for early classifiers. The proposed approaches are validated on two image classification datasets.\nPros:\n- The paper is well written and easy to follow. \n- It addresses an interesting problem with reasonable approaches. \nCons:\n- The loss reweighting and oscillating schemes appear to be just heuristics. It is not clear what the scientific contributions are.  \n- I do not fully agree with the explanation given for the “alternating weights”. If the joint loss leads to zero gradient for some weights, then why would you consider it problematic?\n- There are few baselines compared in the result section. In addition, the proposed method underperforms the MSDNet (Huang et al., 2017) on ILSVRC2012.\n- The EANN is similar to the method used by Adaptive Networks (Bolukbasi et al., 2017), and the baseline “Ensemble of ResNets (varying depth)” in the MSDNet paper. \n-  Could you show the error bar In Figure 2(a)? Usually an error difference less than 0.5% on CIFAR-100 is not considered as significant. \n- I’m not convinced that AANN really works significantly better than ANN according to the results in Table 1(a). It seems that ANN still outperform AANN in many cases.\n- I would suggest to show the results in Table 1(b) with a figure.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy","abstract":"Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. \nIn this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically,  near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.","pdf":"/pdf/2eeafeb86c4766cddd20be49d2da3c46fa358276.pdf","TL;DR":"We propose methods to train auxiliary predictors of neural networks to output competitive anytime predictions. We assemble such anytime networks to be near-optimal at any budget.","paperhash":"anonymous|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy","_bibtex":"@article{\n  anonymous2018anytime,\n  title={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa1Nk10b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper111/Authors"],"keywords":["anytime","neural network","adaptive prediction","budgeted prediction"]}},{"tddate":null,"ddate":null,"tmdate":1515642385223,"tcdate":1511904199879,"number":2,"cdate":1511904199879,"id":"rJeC7UixM","invitation":"ICLR.cc/2018/Conference/-/Paper111/Official_Review","forum":"SJa1Nk10b","replyto":"SJa1Nk10b","signatures":["ICLR.cc/2018/Conference/Paper111/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Similar to Prior Cascade Work, Unclear Weighing Schemes, and Lack of Experimental Comparison","rating":"5: Marginally below acceptance threshold","review":"1. Paper Summary\n\nThis paper adds a separate network at every layer of a residual network that performs classification. They minimize the loss of every classifier using two proposed weighting schemes. They also ensemble this model.\n\n\n2. High level paper\n\nThe organization of this paper is a bit confusing. Two weighing schemes are introduced in Section 3.1, then the ensemble model is described in Section 3.2, then the weighing schemes are justified in Section 4.1.\nOverall this method is essentially an cascade where each cascade classifier is a residual block. Every input is passed through as many stages as possible until the budget is reached. While this model is likely quite useful in industrial settings, I don't think the model itself is wholly original.\nThe authors have done extensive experiments evaluating their method in different settings. I would have liked to see a comparison with at least one other anytime method. I think it is slightly unfair to say that you are comparing with Xie & Tu, 2015 and Huang et al., 2017 just because they use the CONSTANT weighing schemes.\n\n\n3. High level technical\n\nI have a few concerns:\n- Why does AANN+LINEAR nearly match the accuracy of EANN+SIEVE near 3e9 FLOPS in Figure 4b but EANN+LINEAR does not in Figure 4a? Shouldn't EANN+LINEAR be strictly better than AANN+LINEAR?\n- Why do the authors choose these specific weighing schemes? Section 4.1 is devoted to explaining this but it is still unclear to me. They talk about there being correlation between the predictors near the end of the model so they don't want to distribute weight near the final predictors but this general observation doesn't obviously lead to these weighing schemes, they still seem a bit adhoc.\n\nA few other comments:\n- Figure 3b seems to contain strictly less information than Figure 4a, I would remove Figure 3b and draw lines showing the speedup you get for one or two accuracy levels.\n\nQuestions:\n- Section 3.1: \"Such an ideal θ* does not exist in general and often does not exist in practice.\" Why is this the case? \n- Section 3.1: \" In particular, spreading weights evenly as in (Lee et al., 2015) keeps all i away from their possible respective minimum\" Why is this true?\n- Section 3.1: \"Since we will evaluate near depth b3L/4e, and it\nis the center of L/2 low-weight layers, we increase it weight by 1/8.\" I am completely lost here, why do you do this?\n\n\n4. Review summary\n\nUltimately because the model itself resembles previous cascade models, the selected weighings have little justification, and there isn't a comparison with another anytime method, I think this paper isn't yet ready for acceptance at ICLR.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy","abstract":"Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. \nIn this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically,  near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.","pdf":"/pdf/2eeafeb86c4766cddd20be49d2da3c46fa358276.pdf","TL;DR":"We propose methods to train auxiliary predictors of neural networks to output competitive anytime predictions. We assemble such anytime networks to be near-optimal at any budget.","paperhash":"anonymous|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy","_bibtex":"@article{\n  anonymous2018anytime,\n  title={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa1Nk10b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper111/Authors"],"keywords":["anytime","neural network","adaptive prediction","budgeted prediction"]}},{"tddate":null,"ddate":null,"tmdate":1515642385261,"tcdate":1511782698094,"number":1,"cdate":1511782698094,"id":"HJMEt_FeM","invitation":"ICLR.cc/2018/Conference/-/Paper111/Official_Review","forum":"SJa1Nk10b","replyto":"SJa1Nk10b","signatures":["ICLR.cc/2018/Conference/Paper111/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Anytime neural network","rating":"7: Good paper, accept","review":"This paper proposes an anytime neural network, which can predict anytime while training. To achieve that, the model includes auxiliary predictions which can make early predictions. Specifically, the paper presents a loss weighting scheme that considers high correlation among nearby predictions, an oscillating loss weighting scheme for further improvement, and an ensemble of anytime neural networks. In the experiments, test error of the proposed model was shown to be comparable to the optimal one at each time budget. \n\nIt is an interesting idea to add auxiliary predictions to enable early predictions and the experimental results look promising as they are close to optimal at each time budget. \n\n1. In Section 3.2, there are some discussions on the parallel computations of EANN. The parallel training is not clear to me and it would be great to have more explanation on this with examples.  \n\n2. It seems that EANN is not scalable because the depth is increasing exponentially. For example, given 10 machines, the model with the largest depth would have 2^10 layers, which is difficult to train. It would be great to discuss this issue.\n\n3. In the experiments, it would be great to add a few alternatives to be compared for anytime predictions. \n\n\n\n\n\n\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy","abstract":"Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. \nIn this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically,  near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.","pdf":"/pdf/2eeafeb86c4766cddd20be49d2da3c46fa358276.pdf","TL;DR":"We propose methods to train auxiliary predictors of neural networks to output competitive anytime predictions. We assemble such anytime networks to be near-optimal at any budget.","paperhash":"anonymous|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy","_bibtex":"@article{\n  anonymous2018anytime,\n  title={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa1Nk10b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper111/Authors"],"keywords":["anytime","neural network","adaptive prediction","budgeted prediction"]}},{"tddate":null,"ddate":null,"tmdate":1514789784591,"tcdate":1508991972780,"number":111,"cdate":1509739475443,"id":"SJa1Nk10b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"SJa1Nk10b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy","abstract":"Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. \nIn this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically,  near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.","pdf":"/pdf/2eeafeb86c4766cddd20be49d2da3c46fa358276.pdf","TL;DR":"We propose methods to train auxiliary predictors of neural networks to output competitive anytime predictions. We assemble such anytime networks to be near-optimal at any budget.","paperhash":"anonymous|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy","_bibtex":"@article{\n  anonymous2018anytime,\n  title={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=SJa1Nk10b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper111/Authors"],"keywords":["anytime","neural network","adaptive prediction","budgeted prediction"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}