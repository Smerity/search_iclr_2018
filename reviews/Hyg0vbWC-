{"notes":[{"tddate":null,"ddate":null,"tmdate":1514778587736,"tcdate":1514778587736,"number":8,"cdate":1514778587736,"id":"SyEJe4v7M","invitation":"ICLR.cc/2018/Conference/-/Paper689/Official_Comment","forum":"Hyg0vbWC-","replyto":"SJ3RoxLzz","signatures":["ICLR.cc/2018/Conference/Paper689/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper689/AnonReviewer1"],"content":{"title":"Thanks for your detailed response","comment":"Thanks for your detailed response and follow-up work!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi-\ndocument summarization of source documents. We use extractive summarization\nto coarsely identify salient information and a neural abstractive model to generate\nthe article. For the abstractive model, we introduce a decoder-only architecture\nthat can scalably attend to very long sequences, much longer than typical encoder-\ndecoder architectures used in sequence transduction. We show that this model can\ngenerate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\narticles. When given reference documents, we show it can extract relevant factual\ninformation as reflected in perplexity, ROUGE scores and human evaluations.","pdf":"/pdf/a4fd36d38153ffbfd6c52e3e8764d088d1fb6c59.pdf","TL;DR":"We generate Wikipedia articles abstractively conditioned on source document text.","paperhash":"anonymous|generating_wikipedia_by_summarizing_long_sequences","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Wikipedia by Summarizing Long Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyg0vbWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper689/Authors"],"keywords":["abstractive summarization","Transformer","long sequences","natural language processing","sequence transduction","Wikipedia","extractive summarization"]}},{"tddate":null,"ddate":null,"tmdate":1513651396872,"tcdate":1513651396872,"number":5,"cdate":1513651396872,"id":"Hy663x8ff","invitation":"ICLR.cc/2018/Conference/-/Paper689/Official_Comment","forum":"Hyg0vbWC-","replyto":"r129mGrxf","signatures":["ICLR.cc/2018/Conference/Paper689/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper689/Authors"],"content":{"title":"We agree evaluation needed improvement and believe we have addressed your feedback","comment":"“It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches. Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact.”\n\n- In addition to our proposed neural architectures, we compare to very strong existing baselines, seq2seq with attention, which gets state-of-the-art on the Gigaword summarization task as well as the Transformer encoder-decoder, which gets state-of-the-art on translation, a related task from which most summarization techniques arise. We show our models significantly outperform those competitive abstractive methods.\n\n\n“Furthermore, I really expected to see a comparison with Sauper & Barzilay (2009)'s non-neural extractive approach of Wikipedia article generation, which could certainly strengthen the technical merit of the paper.”\n\n- This is a fair point and we added a section comparing with Sauper & Barzilay in Experiments.\n\n\n“More importantly, it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content. For example, Figure 5-7 show variable sizes of the generated outputs. With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference.”\n\n- The models are not constrained to output a certain length. Instead we generate until an end-of-sequence token is encountered. There is a length-penalty, alpha, that we tune based on performance of the validation set. In our case, the ROUGE F1 evaluation is fair because it is the harmonic mean of ROUGE-Recall (favors long summaries) and ROUGE-Precision (favors short summaries). As a result, longer output is penalized if it is not useful and related to the target. We tried to clarify this in the Experiments section.\n\n\n“It would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures.”\n\n- We don’t claim for this task that T-D does better than T-ED for short input lengths. However, we hope Figure 3 makes it clear that the T-ED architecture begins to fail and is no longer competitive for longer inputs. In particular, our architecture improvements allow us to consider much larger input lengths, which results in significantly higher ROUGE and and human evaluation scores.\n\n\n“Authors claim that the proposed model can generate \"fluent, coherent\" output, however, no evaluation has been conducted to justify this claim. The human evaluation only compares two alternative models for preference, which is not enough to support this claim. I would suggest to carry out a DUC-style user evaluation (http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt) methodology to really show that the proposed method works well for abstractive summarization.”\n\n- This is a good point and we followed your suggestion and added a DUC-style human evaluation of linguistic quality. We hope we make it clear that the best abstractive model proposed is significanlty much more fluent/coherent than the best extractive method we tried and another baseline abstractive method (seq2seq). The quality scores are also high in the absolute sense.\n\n\n“Does Figure 8 show an example input after the extractive stage or before? Please clarify.”\n\n- We clarified in the paper (now Figure 10) that it is the output of the extractive stage, before the abstractive stage.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi-\ndocument summarization of source documents. We use extractive summarization\nto coarsely identify salient information and a neural abstractive model to generate\nthe article. For the abstractive model, we introduce a decoder-only architecture\nthat can scalably attend to very long sequences, much longer than typical encoder-\ndecoder architectures used in sequence transduction. We show that this model can\ngenerate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\narticles. When given reference documents, we show it can extract relevant factual\ninformation as reflected in perplexity, ROUGE scores and human evaluations.","pdf":"/pdf/a4fd36d38153ffbfd6c52e3e8764d088d1fb6c59.pdf","TL;DR":"We generate Wikipedia articles abstractively conditioned on source document text.","paperhash":"anonymous|generating_wikipedia_by_summarizing_long_sequences","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Wikipedia by Summarizing Long Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyg0vbWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper689/Authors"],"keywords":["abstractive summarization","Transformer","long sequences","natural language processing","sequence transduction","Wikipedia","extractive summarization"]}},{"tddate":null,"ddate":null,"tmdate":1513651156196,"tcdate":1513651156196,"number":4,"cdate":1513651156196,"id":"SJ3RoxLzz","invitation":"ICLR.cc/2018/Conference/-/Paper689/Official_Comment","forum":"Hyg0vbWC-","replyto":"BJGaExqgz","signatures":["ICLR.cc/2018/Conference/Paper689/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper689/Authors"],"content":{"title":"Hopefully the technical contribution is a lot clearer after significantly augmented evaluation","comment":"Thank you for the detailed review with actionable feedback. We found common feedback from the three reviewers to augment the evaluation section of the paper and believe we have significantly improved it. In particular, please see responses below in-line as well as rebuttals for other reviews and the summary of changes above.\n\n\n“In the task setup the information retrieval-based extractive stage is crucial to performance, but this contribution might be less important to the ICLR community.”\n\n- We added additional analysis to demonstrate that the extractive stage while important, is far from sufficient to produce good wikipedia articles. We show that ROUGE and human evaluation are greatly improved by the abstractive stage.\n\n\n“It willl also be hard to reproduce without significant computational resources, even if the URLs of the dataset are made available.”\n\n- We will be providing a script for generating the dataset from the CommonCrawl dataset (which is freely available for download). It will run locally instead of downloading over the Internet, and so will be relatively much faster.\n\n\n“Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage.”\n\n- We added analysis of the incremental performance of the abstractive model over the extractive output in terms of ROUGE and human evaluation (DUC-style linguistic quality evaluation). We believe we make a strong case that the abstractive model is doing something highly non-trivial and significant. \n\n\n“In some of the examples the system output seems to be significantly shorter than the reference, so it would be helpful to quantify this, as well how much the quality degrades when the model is forced to generate outputs of a given minimum length.”\n- We clarified in the paper that the models are not constrained to output a certain length. Instead we generate until an end-of-sequence token is encountered. There is a length-penalty, alpha, that we tune based on performance of the validation set.\n\n\n“So while the performance of the overall system is impressive, it is hard to judge the significance of the technical contribution made by the paper.”\n- In addition to the proposed task/dataset, we believe the technical significance is demonstrating how very long text-to-text sequence transduction tasks can be done. Previous related work in translation or summarization focused on much shorter sequences. We had to introduce new model architectures to solve this new problem and believe it would be of great interest to the ICLR community. We hope our added evaluations make this claim more convincing.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi-\ndocument summarization of source documents. We use extractive summarization\nto coarsely identify salient information and a neural abstractive model to generate\nthe article. For the abstractive model, we introduce a decoder-only architecture\nthat can scalably attend to very long sequences, much longer than typical encoder-\ndecoder architectures used in sequence transduction. We show that this model can\ngenerate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\narticles. When given reference documents, we show it can extract relevant factual\ninformation as reflected in perplexity, ROUGE scores and human evaluations.","pdf":"/pdf/a4fd36d38153ffbfd6c52e3e8764d088d1fb6c59.pdf","TL;DR":"We generate Wikipedia articles abstractively conditioned on source document text.","paperhash":"anonymous|generating_wikipedia_by_summarizing_long_sequences","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Wikipedia by Summarizing Long Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyg0vbWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper689/Authors"],"keywords":["abstractive summarization","Transformer","long sequences","natural language processing","sequence transduction","Wikipedia","extractive summarization"]}},{"tddate":null,"ddate":null,"tmdate":1513650884301,"tcdate":1513650884301,"number":3,"cdate":1513650884301,"id":"S1npqlIMf","invitation":"ICLR.cc/2018/Conference/-/Paper689/Official_Comment","forum":"Hyg0vbWC-","replyto":"H1VuTvqgG","signatures":["ICLR.cc/2018/Conference/Paper689/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper689/Authors"],"content":{"title":"We significantly augmented the evaluation section and tried addressing all your feedback","comment":"Thank you for the detailed review with actionable feedback. We found common feedback from the three reviewers to augment the evaluation section of the paper and believe we have significantly improved it. In particular, please see responses below in-line where we address all of your feedback. \n\n“This paper is quite original and clearly written. The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles. “\n\n- In addition to the task setup, we believe we’ve demonstrated how to do very long (much longer than previously attempted) text-to-text sequence transduction and introduced a new model architecture to do it. We believe this is of great interest to the ICLR community.\n\n“Currently, only neural abstractive methods are compared. I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic. Do redundancy cues which work for multi-document news summarization still work for this task?”\n\n- We implemented SumBasic and TextRank (along with tf-idf) to evaluate extractive methods on their own and evaluated them on this task. I believe we show convincingly in the results (e.g. extractive bar-plot) that the abstractive stage indeed adds a lot to the extractive output in terms of ROUGE and human evaluation of linguistic quality and that redundancy cues are not enough.\n\n“I would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are. Does higher extractiveness correspond to higher or lower system ROUGE scores? This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help.”\n\n- In Section 2.1 we computed the proportion of unigrams/words in the output co-occurring in the input for our task and for the Gigaword and CNN/DailyMail datasets and showed that by this measure WikiSum is much less extractive. In particular, the presence of wiki-clones in the input would give a score of 100%, whereas we report 59.2%.\n\n“A further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not.”\n\n- We added a comparison in the paper with Sauper & Barzilay on two Wiki categories. It turns out we do worse on Diseases compared to Actors. We think this is because we use a single model for all categories and the training data is heavily biased toward people.\n\n“I have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection. In this case, the problem could become less interesting, as no real analysis is required to do well here.”\n- We hope our added analysis in Section 2.1 mentioned above should address this concern.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi-\ndocument summarization of source documents. We use extractive summarization\nto coarsely identify salient information and a neural abstractive model to generate\nthe article. For the abstractive model, we introduce a decoder-only architecture\nthat can scalably attend to very long sequences, much longer than typical encoder-\ndecoder architectures used in sequence transduction. We show that this model can\ngenerate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\narticles. When given reference documents, we show it can extract relevant factual\ninformation as reflected in perplexity, ROUGE scores and human evaluations.","pdf":"/pdf/a4fd36d38153ffbfd6c52e3e8764d088d1fb6c59.pdf","TL;DR":"We generate Wikipedia articles abstractively conditioned on source document text.","paperhash":"anonymous|generating_wikipedia_by_summarizing_long_sequences","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Wikipedia by Summarizing Long Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyg0vbWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper689/Authors"],"keywords":["abstractive summarization","Transformer","long sequences","natural language processing","sequence transduction","Wikipedia","extractive summarization"]}},{"tddate":null,"ddate":null,"tmdate":1513717408278,"tcdate":1513650669925,"number":2,"cdate":1513650669925,"id":"SkUg9xLfz","invitation":"ICLR.cc/2018/Conference/-/Paper689/Official_Comment","forum":"Hyg0vbWC-","replyto":"Hyg0vbWC-","signatures":["ICLR.cc/2018/Conference/Paper689/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper689/Authors"],"content":{"title":"Summary of changes to revision 12/18/2017","comment":"- We found common, actionable feedback from the three reviewers to augment the evaluation section of the paper and believe we have significantly improved it.\n    - We added results from a DUC-style linguistic quality human evaluation, showing our model significantly outperforms pure extractive methods we tried as well as an abstractive baseline.\n    - We quantified the performance of extractive-only methods on our proposed task, adding results of two more well-cited methods, SumBasic and TextRank (in addition to tf-idf). We show that the abstractive stage indeed adds significant lift to ROUGE and human evaluation performance.\n    - We added a section with a comparison with Sauper and Barzilay.\n\n- We quantify the extractiveness of the dataset in section 2.1. Although we mentioned we had a Wiki clone-detection algorithm, we didn’t quantify the results. In Table 1, we show that the proportion of unigrams/words in the output co-occurring in the input is much lower than in other summarization datasets at 59.2%. \n\n- Some other clarifications made in paper\n    - Modified caption for Figure 8 to make it clear it is the output of the extractive stage and what the abstractive model uses as input for article generation.\n    - We note output length is not constrained in abstractive models. There’s a length penalty hyper-parameter, \\alpha, that is tuned on the validation set. For ROUGE-L scores, we report the more appropriate F1 flavor, which is the harmonic   \nmean of ROUGE-Recall (favors long summaries) and ROUGE-Precision (favors short summaries).\nelaborated on justification for T-D architecture for monolingual text-to-text problems\n\n- On feasibility of reproducibility: We will be providing a script that generates the data locally from a local copy of the CommonCrawl dataset, which can be downloaded from http://commoncrawl.org/.  Because the script will run locally it will be significantly faster than downloading webpages from the Internet.\n\nOverall we believe the paper is much stronger after the suggestions from the reviewers. Please re-consider scores after this revision. Thank you!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi-\ndocument summarization of source documents. We use extractive summarization\nto coarsely identify salient information and a neural abstractive model to generate\nthe article. For the abstractive model, we introduce a decoder-only architecture\nthat can scalably attend to very long sequences, much longer than typical encoder-\ndecoder architectures used in sequence transduction. We show that this model can\ngenerate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\narticles. When given reference documents, we show it can extract relevant factual\ninformation as reflected in perplexity, ROUGE scores and human evaluations.","pdf":"/pdf/a4fd36d38153ffbfd6c52e3e8764d088d1fb6c59.pdf","TL;DR":"We generate Wikipedia articles abstractively conditioned on source document text.","paperhash":"anonymous|generating_wikipedia_by_summarizing_long_sequences","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Wikipedia by Summarizing Long Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyg0vbWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper689/Authors"],"keywords":["abstractive summarization","Transformer","long sequences","natural language processing","sequence transduction","Wikipedia","extractive summarization"]}},{"tddate":null,"ddate":null,"tmdate":1515642492450,"tcdate":1511845227925,"number":3,"cdate":1511845227925,"id":"H1VuTvqgG","invitation":"ICLR.cc/2018/Conference/-/Paper689/Official_Review","forum":"Hyg0vbWC-","replyto":"Hyg0vbWC-","signatures":["ICLR.cc/2018/Conference/Paper689/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Interesting task. Would appreciate more analysis.","rating":"7: Good paper, accept","review":"The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem. Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated. Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer. A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences. The outputs are evaluated by ROUGE-L and test perplexity. There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences.\n\nThis paper is quite original and clearly written. The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles. The main weakness is that I would have liked to see more analysis and comparisons in the evaluation.\n\nEvaluation:\nCurrently, only neural abstractive methods are compared. I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic. Do redundancy cues which work for multi-document news summarization still work for this task?\n\nExtractiveness analysis:\nI would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are. Does higher extractiveness correspond to higher or lower system ROUGE scores? This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help. \n\nA further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not. \n\nI have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection. In this case, the problem could become less interesting, as no real analysis is required to do well here.\n\nOverall, I quite like this line of work, but I think the paper would be a lot stronger and more convincing with some additional work.\n\n----\nAfter reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper. This is a very nice contribution.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi-\ndocument summarization of source documents. We use extractive summarization\nto coarsely identify salient information and a neural abstractive model to generate\nthe article. For the abstractive model, we introduce a decoder-only architecture\nthat can scalably attend to very long sequences, much longer than typical encoder-\ndecoder architectures used in sequence transduction. We show that this model can\ngenerate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\narticles. When given reference documents, we show it can extract relevant factual\ninformation as reflected in perplexity, ROUGE scores and human evaluations.","pdf":"/pdf/a4fd36d38153ffbfd6c52e3e8764d088d1fb6c59.pdf","TL;DR":"We generate Wikipedia articles abstractively conditioned on source document text.","paperhash":"anonymous|generating_wikipedia_by_summarizing_long_sequences","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Wikipedia by Summarizing Long Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyg0vbWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper689/Authors"],"keywords":["abstractive summarization","Transformer","long sequences","natural language processing","sequence transduction","Wikipedia","extractive summarization"]}},{"tddate":null,"ddate":null,"tmdate":1515642492492,"tcdate":1511814329672,"number":2,"cdate":1511814329672,"id":"BJGaExqgz","invitation":"ICLR.cc/2018/Conference/-/Paper689/Official_Review","forum":"Hyg0vbWC-","replyto":"Hyg0vbWC-","signatures":["ICLR.cc/2018/Conference/Paper689/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Impressive application, but hard to judge technical contribution","rating":"8: Top 50% of accepted papers, clear accept","review":"This paper proposes an approach to generating the first section of Wikipedia articles (and potentially entire articles). \nFirst relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a TD-IDF-based ranking. Then abstractive summarization is performed using a modification of Transformer networks (Vasvani et al 2017). A mixture of experts layer further improves performance. \nThe proposed transformer decoder defines a distribution over both the input and output sequences using the same self-attention-based network. On its own this modification improves perplexity (on longer sequences) but not the Rouge score; however the architecture enables memory-compressed attention which is more scalable to long input sequences. It is claimed that the transformer decoder makes optimization easier but no complete explanation or justification of this is given. Computing self-attention and softmaxes over entire input sequences will significantly increase the computational cost of training.\n\nIn the task setup the information retrieval-based extractive stage is crucial to performance, but this contribution might be less important to the ICLR community. It willl also be hard to reproduce without significant computational resources, even if the URLs of the dataset are made available. The training data is significantly larger than the CNN/DailyMail single-document summarization dataset.\n\nThe paper presents strong quantitative results and qualitative examples. Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage.\nIn some of the examples the system output seems to be significantly shorter than the reference, so it would be helpful to quantify this, as well how much the quality degrades when the model is forced to generate outputs of a given minimum length. While the proposed approach is more scalable, it is hard to judge the extend of this.\n\nSo while the performance of the overall system is impressive, it is hard to judge the significance of the technical contribution made by the paper.\n\n---\nThe additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper, and I would like to see the paper accepted. \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi-\ndocument summarization of source documents. We use extractive summarization\nto coarsely identify salient information and a neural abstractive model to generate\nthe article. For the abstractive model, we introduce a decoder-only architecture\nthat can scalably attend to very long sequences, much longer than typical encoder-\ndecoder architectures used in sequence transduction. We show that this model can\ngenerate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\narticles. When given reference documents, we show it can extract relevant factual\ninformation as reflected in perplexity, ROUGE scores and human evaluations.","pdf":"/pdf/a4fd36d38153ffbfd6c52e3e8764d088d1fb6c59.pdf","TL;DR":"We generate Wikipedia articles abstractively conditioned on source document text.","paperhash":"anonymous|generating_wikipedia_by_summarizing_long_sequences","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Wikipedia by Summarizing Long Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyg0vbWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper689/Authors"],"keywords":["abstractive summarization","Transformer","long sequences","natural language processing","sequence transduction","Wikipedia","extractive summarization"]}},{"tddate":null,"ddate":null,"tmdate":1515642492532,"tcdate":1511494547744,"number":1,"cdate":1511494547744,"id":"r129mGrxf","invitation":"ICLR.cc/2018/Conference/-/Paper689/Official_Review","forum":"Hyg0vbWC-","replyto":"Hyg0vbWC-","signatures":["ICLR.cc/2018/Conference/Paper689/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting, but evaluation could have been stronger","rating":"7: Good paper, accept","review":"This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page. The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention.   \n\nIn general, the paper is well-written and the main ideas are clear. However, my main concern is the evaluation. It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches. Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact. Furthermore, I really expected to see a comparison with Sauper & Barzilay (2009)'s non-neural extractive approach of Wikipedia article generation, which could certainly strengthen the technical merit of the paper.\n\nMore importantly, it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content. For example, Figure 5-7 show variable sizes of the generated outputs. With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference. \n\nIt would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures. Did you run any statistical significance test on the evaluation results? \n\nAuthors claim that the proposed model can generate \"fluent, coherent\" output, however, no evaluation has been conducted to justify this claim. The human evaluation only compares two alternative models for preference, which is not enough to support this claim. I would suggest to carry out a DUC-style user evaluation (http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt) methodology to really show that the proposed method works well for abstractive summarization.\n\nDoes Figure 8 show an example input after the extractive stage or before? Please clarify.\n\n---------------\nI have updated my scores as authors clarified most of my concerns.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi-\ndocument summarization of source documents. We use extractive summarization\nto coarsely identify salient information and a neural abstractive model to generate\nthe article. For the abstractive model, we introduce a decoder-only architecture\nthat can scalably attend to very long sequences, much longer than typical encoder-\ndecoder architectures used in sequence transduction. We show that this model can\ngenerate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\narticles. When given reference documents, we show it can extract relevant factual\ninformation as reflected in perplexity, ROUGE scores and human evaluations.","pdf":"/pdf/a4fd36d38153ffbfd6c52e3e8764d088d1fb6c59.pdf","TL;DR":"We generate Wikipedia articles abstractively conditioned on source document text.","paperhash":"anonymous|generating_wikipedia_by_summarizing_long_sequences","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Wikipedia by Summarizing Long Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyg0vbWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper689/Authors"],"keywords":["abstractive summarization","Transformer","long sequences","natural language processing","sequence transduction","Wikipedia","extractive summarization"]}},{"tddate":null,"ddate":null,"tmdate":1513650280315,"tcdate":1509132231929,"number":689,"cdate":1509739155618,"id":"Hyg0vbWC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"Hyg0vbWC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Generating Wikipedia by Summarizing Long Sequences","abstract":"We show that generating English Wikipedia articles can be approached as a multi-\ndocument summarization of source documents. We use extractive summarization\nto coarsely identify salient information and a neural abstractive model to generate\nthe article. For the abstractive model, we introduce a decoder-only architecture\nthat can scalably attend to very long sequences, much longer than typical encoder-\ndecoder architectures used in sequence transduction. We show that this model can\ngenerate fluent, coherent multi-sentence paragraphs and even whole Wikipedia\narticles. When given reference documents, we show it can extract relevant factual\ninformation as reflected in perplexity, ROUGE scores and human evaluations.","pdf":"/pdf/a4fd36d38153ffbfd6c52e3e8764d088d1fb6c59.pdf","TL;DR":"We generate Wikipedia articles abstractively conditioned on source document text.","paperhash":"anonymous|generating_wikipedia_by_summarizing_long_sequences","_bibtex":"@article{\n  anonymous2018generating,\n  title={Generating Wikipedia by Summarizing Long Sequences},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=Hyg0vbWC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper689/Authors"],"keywords":["abstractive summarization","Transformer","long sequences","natural language processing","sequence transduction","Wikipedia","extractive summarization"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}