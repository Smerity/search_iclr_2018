{"notes":[{"tddate":null,"ddate":null,"tmdate":1512363844445,"tcdate":1512363844445,"number":3,"cdate":1512363844445,"id":"Sy3BPIMbM","invitation":"ICLR.cc/2018/Conference/-/Paper748/Official_Review","forum":"H1bM1fZCW","replyto":"H1bM1fZCW","signatures":["ICLR.cc/2018/Conference/Paper748/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Lacking in clarity; experiments not convincing","rating":"4: Ok but not good enough - rejection","review":"The paper addresses an important problem in multitask learning. But its current form has several serious issues. \n\nAlthough I get the high-level goal of the paper, I find Sec. 3.1, which describes the technical approach, nearly incomprehensible. There are many things unclear. For example:\n\n-  it starts with talking about multiple tasks, and then immediately talks about a \"filter F\", without defining what the kind of network is being addressed. \n\n- Also it is not clear what L_grad is. It looks like a loss, but Equation 2 seems to define it to be the difference between the gradient norm of a task and the average over all tasks. It is not clear how it is used. In particular, it is not clear how it is used to \"update the task weights\"\n\n- Equation 2 seems sloppy. “j” appears as a free index on the right side, but it doesn’t appear on the left side. \n\nAs a result, I am unable to understand how the method works exactly, and unable to judge its quality and originality.\n\nThe toy experiment is not convincing. \n\n- the evaluation metric is the sum of the relative losses, that is, the sum of the original losses weighted by the inverse of the initial loss of each task. This is different from the sum of the original losses, which seems to be the one used to train the “equal weight” baseline. A more fair baseline is to directly use the evaluation metric as the training loss. \n- the curves seem to have not converged.\n\nThe experiments on NYUv2 involves non-standard settings, without a good justification. So it is not clear if the proposed method can make a real difference on state of the art systems. \n\nAnd the reason that the proposed method outperforms the equal weight baseline seems to be that the method prevents overfitting on some tasks (e.g. depth). However, the method works by normalizing the norms of the gradients, which does not necessarily prevent overfitting — it can in fact magnify gradients of certain tasks and cause over-training and over-fitting. So the performance gain is likely dataset dependent, and what happens on NYU depth can be a fluke and does not necessarily generalize to other datasets. ","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks","abstract":"Deep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the multitask loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that direct gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.","pdf":"/pdf/03d7c9a7a9b6d5286dbe512ac7ce190a7fd3ba87.pdf","TL;DR":"We show how you can boost performance in a multitask network by tuning an adaptive multitask loss function that is learned through directly balancing network gradients.","paperhash":"anonymous|gradnorm_gradient_normalization_for_adaptive_loss_balancing_in_deep_multitask_networks","_bibtex":"@article{\n  anonymous2018gradnorm:,\n  title={GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1bM1fZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper748/Authors"],"keywords":["Multitask learning","computer vision","multitask loss function"]}},{"tddate":null,"ddate":null,"tmdate":1511937320968,"tcdate":1511937238586,"number":1,"cdate":1511937238586,"id":"H1kyHRslM","invitation":"ICLR.cc/2018/Conference/-/Paper748/Official_Comment","forum":"H1bM1fZCW","replyto":"rkHMM4KeM","signatures":["ICLR.cc/2018/Conference/Paper748/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper748/Authors"],"content":{"title":"RE: The name of method","comment":"Hi there,\n\nThanks very much for the comment. You're absolutely right - GradNorm at an implementation level amounts to dynamically finding the right weights w_i(t) which then goes into a weighted average of each individual task loss. This is implemented via the equations in the manuscript. However, the core reason these equations are meaningful is because they set a common scale for our backpropped gradients by *normalizing* gradients to two additional pieces of data: (1) the average gradient norms amongst different tasks, and (2) the relative training rate of tasks. The influence of the latter is controlled by the asymmetry parameter alpha, as described in the manuscript. That's why we are normalizing our gradients: we discovered a meaningful common scale for these gradients which tells us how they relate to each other and we use these relationships to our advantage during training.\n\nThanks also for pointing out the other paper - I think it would be more well-defined to just refer to our method as GradNorm, which we also used to draw the analogy to BatchNorm. Normalizing gradients can mean many different things (depending on the objective, the scale/data you normalize to, etc.), and our proposed method focuses on one way to do this in the context of multitask learning. But depending on the application there can certainly be other methods where the term \"gradient normalization\" would also apply. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks","abstract":"Deep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the multitask loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that direct gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.","pdf":"/pdf/03d7c9a7a9b6d5286dbe512ac7ce190a7fd3ba87.pdf","TL;DR":"We show how you can boost performance in a multitask network by tuning an adaptive multitask loss function that is learned through directly balancing network gradients.","paperhash":"anonymous|gradnorm_gradient_normalization_for_adaptive_loss_balancing_in_deep_multitask_networks","_bibtex":"@article{\n  anonymous2018gradnorm:,\n  title={GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1bM1fZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper748/Authors"],"keywords":["Multitask learning","computer vision","multitask loss function"]}},{"tddate":null,"ddate":null,"tmdate":1512222743197,"tcdate":1511804065702,"number":2,"cdate":1511804065702,"id":"Bycjn6tef","invitation":"ICLR.cc/2018/Conference/-/Paper748/Official_Review","forum":"H1bM1fZCW","replyto":"H1bM1fZCW","signatures":["ICLR.cc/2018/Conference/Paper748/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Technique seems interesting and useful. But, the exposition of the technique is poor and several important details are missing.","rating":"4: Ok but not good enough - rejection","review":"Paper summary:\nExisting works on multi-task neural networks typically use hand-tuned weights for weighing losses across different tasks. This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks. Experiments on two different network indicate that the proposed scheme is better than using hand-tuned weights for multi-task neural networks.\n\n\nPaper Strengths:\n- The proposed technique seems simple yet effective for multi-task learning.\n- Experiments on two different network architectures showcasing the generality of the proposed method.\n\n\nMajor Weaknesses:\n- The main weakness of this work is the unclear exposition of the proposed technique. Entire technique is explained in a short section-3.1 with many important details missing. There is no clear basis for the main equations 1 and 2. How does equation-2 follow from equation-1? Where is the expectation coming from? What exactly does ‘F’ refer to? There is dependency of ‘F’ on only one of sides in equations 1 and 2? More importantly, how does the gradient normalization relate to loss weight update? It is very difficult to decipher these details from the short descriptions given in the paper.\n- Also, several details are missing in toy experiments. What is the task here? What are input and output distributions and what is the relation between input and output? Are they just random noises? If so, is the network learning to overfit to the data as there is no relationship between input and output? \n\n\nMinor Weaknesses:\n- There are no training time comparisons between the proposed technique and the standard fixed loss learning.\n- Authors claim that they operate directly on the gradients inside the network. But, as far as I understood, the authors only update loss weights in this paper. Did authors also experiment with gradient normalization in the intermediate CNN layers?\n- No comparison with state-of-the-art techniques on the experimented tasks and datasets.\n\n\nClarifications:\n- See the above mentioned issues with the exposition of the technique.\n- In the experiments, why are the input images downsampled to 320x320?\n- What does it mean by ‘unofficial dataset’ (page-4). Any references here?\n- Why is 'task normalized' test-time loss as good measure for comparison between models in the toy example (Section 4)? The loss ratios depend on initial loss, which is not important for the final performance of the system.\n\n\nSuggestions:\n- I strongly suggest the authors to clearly explain the proposed technique to get this into a publishable state. \n- The term ’GradNorm’ seem to be not defined anywhere in the paper.\n\n\nReview Summary:\nDespite promising results, the proposed technique is quite unclear from the paper. With its poor exposition of the technique, it is difficult to recommend this paper for publication.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks","abstract":"Deep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the multitask loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that direct gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.","pdf":"/pdf/03d7c9a7a9b6d5286dbe512ac7ce190a7fd3ba87.pdf","TL;DR":"We show how you can boost performance in a multitask network by tuning an adaptive multitask loss function that is learned through directly balancing network gradients.","paperhash":"anonymous|gradnorm_gradient_normalization_for_adaptive_loss_balancing_in_deep_multitask_networks","_bibtex":"@article{\n  anonymous2018gradnorm:,\n  title={GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1bM1fZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper748/Authors"],"keywords":["Multitask learning","computer vision","multitask loss function"]}},{"tddate":null,"ddate":null,"tmdate":1511764494697,"tcdate":1511764494697,"number":1,"cdate":1511764494697,"id":"rkHMM4KeM","invitation":"ICLR.cc/2018/Conference/-/Paper748/Public_Comment","forum":"H1bM1fZCW","replyto":"H1bM1fZCW","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"The name of method","comment":"Hi,\n\nI am a bit confused why the method is called gradient NORMALIZATION? From my understanding, it is essentially dynamic weighted average according to eqn (1)(2). Am I correct?\n\nIn fact, the name \"gradient normalization\" was proposed earlier in the following paper: \nhttps://arxiv.org/pdf/1707.04822.pdf\n\nIt might be good to clarify this to avoid any confusion.\n\nBest."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks","abstract":"Deep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the multitask loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that direct gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.","pdf":"/pdf/03d7c9a7a9b6d5286dbe512ac7ce190a7fd3ba87.pdf","TL;DR":"We show how you can boost performance in a multitask network by tuning an adaptive multitask loss function that is learned through directly balancing network gradients.","paperhash":"anonymous|gradnorm_gradient_normalization_for_adaptive_loss_balancing_in_deep_multitask_networks","_bibtex":"@article{\n  anonymous2018gradnorm:,\n  title={GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1bM1fZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper748/Authors"],"keywords":["Multitask learning","computer vision","multitask loss function"]}},{"tddate":null,"ddate":null,"tmdate":1512222743239,"tcdate":1511736069639,"number":1,"cdate":1511736069639,"id":"H10ZQaugz","invitation":"ICLR.cc/2018/Conference/-/Paper748/Official_Review","forum":"H1bM1fZCW","replyto":"H1bM1fZCW","signatures":["ICLR.cc/2018/Conference/Paper748/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Normalizing gradients for efficient multi-task network learning ","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a method to train deep multi-task networks using gradient normalization. The key idea is to enforce the gradients from multi tasks balanced so that no tasks are ignored in the training. The authors also demonstrated that the technique can improve test errors over single task learning and uncertainty weighting on a large real-world dataset.\n\nIt is an interesting paper with a novel approach to multi-task learning. To improve the paper, it would be helpful to evaluate the method under various settings. My detailed comments are below.\n\n1. Multi-task learning can have various settings. For example, we may have multiple groups of tasks, where tasks are correlated within groups but tasks in different groups are not much correlated. Also, tasks may have hierarchical correlation structures. These patterns often appear in biological datasets. I am wondering how a variety of multi-task settings can be handled by the proposed approach. It would be helpful to discuss the conditions where we can benefit from the proposed method.\n\n2. One intuitive approach to task balancing would be to weight each task objective based on the variance of each task.  It would be helpful to add a few simple and intuitive baselines in the experiments. \n\n3. In Section 4, it would be great to have more in-depth simulations (e.g., multi-task learning in various settings). Also, in the bottom right panel in Figure 2, GrandNorm and equal weighting decrease test errors effectively even after 15000 steps but uncertainty weighting seems to reach a plateau. Discussions on this would be useful.\n\n4. It would be useful to discuss the implementation of the method as well. \n\n\n\n\n\n\n\n\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks","abstract":"Deep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the multitask loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that direct gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.","pdf":"/pdf/03d7c9a7a9b6d5286dbe512ac7ce190a7fd3ba87.pdf","TL;DR":"We show how you can boost performance in a multitask network by tuning an adaptive multitask loss function that is learned through directly balancing network gradients.","paperhash":"anonymous|gradnorm_gradient_normalization_for_adaptive_loss_balancing_in_deep_multitask_networks","_bibtex":"@article{\n  anonymous2018gradnorm:,\n  title={GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1bM1fZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper748/Authors"],"keywords":["Multitask learning","computer vision","multitask loss function"]}},{"tddate":null,"ddate":null,"tmdate":1509739125276,"tcdate":1509134088860,"number":748,"cdate":1509739122613,"id":"H1bM1fZCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1bM1fZCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks","abstract":"Deep multitask networks, in which one neural network produces multiple predictive outputs, are more scalable and often better regularized than their single-task counterparts. Such advantages can potentially lead to gains in both speed and performance, but multitask networks are also difficult to train without finding the right balance between tasks. We present a novel gradient normalization (GradNorm) technique which automatically balances the multitask loss function by directly tuning the gradients to equalize task training rates. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting over single networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process which incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we hope to demonstrate that direct gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.","pdf":"/pdf/03d7c9a7a9b6d5286dbe512ac7ce190a7fd3ba87.pdf","TL;DR":"We show how you can boost performance in a multitask network by tuning an adaptive multitask loss function that is learned through directly balancing network gradients.","paperhash":"anonymous|gradnorm_gradient_normalization_for_adaptive_loss_balancing_in_deep_multitask_networks","_bibtex":"@article{\n  anonymous2018gradnorm:,\n  title={GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1bM1fZCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper748/Authors"],"keywords":["Multitask learning","computer vision","multitask loss function"]},"nonreaders":[],"replyCount":5,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}