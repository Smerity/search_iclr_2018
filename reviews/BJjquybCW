{"notes":[{"tddate":null,"ddate":null,"tmdate":1515186899255,"tcdate":1515186899255,"number":4,"cdate":1515186899255,"id":"rJoC5PTQM","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Comment","forum":"BJjquybCW","replyto":"HJ0nIZkfM","signatures":["ICLR.cc/2018/Conference/Paper493/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper493/Authors"],"content":{"title":"Author's replies","comment":"We thank reviewer 4 for the detailed comments.\n\n\"They present a collection of assumptions, lemmas, and theorems. They have no choice but to have assumptions, because they want to abstract away the \"data\" part of the analysis while still being able to use certain properties about the rank of the features at certain layers.\"\n\nYes, the reviewer is right, we did not want to make assumptions on the distribution of the training data\nas these assumptions are very difficult to check. Instead our assumptions can all be easily checked for a given training set and CNN architecture.\n\n\"Most of my doubts about this paper come from the feeling that equivalent results could be obtained with a more elegant argument about perturbation theory, instead of something like the proof of Lemma A1. That being said, it's easy to voice such concerns, and I'm willing to believe that there might not exist a simple way to derive the same results with an approach more along the line of \"whatever your data, pick whatever small epsilon, and you can always have the desired properties by perturbing your data by that small epsilon in a random direction\". Have the authors tried this ?\"\n\nWe don't know but we can prove Lemma A1 for any given dataset (fulfilling the stated assumptions). However, we use a perturbation argument to show that our assumptions on the training data are always fulfilled for an arbitrarily small perturbation of the data (similar to what the reviewer suggests).\n\n\"I'm not sure if the authors were the first to present this approach of analyzing the effects of convolutions from a \"patch perspective\", but I think this is a clever approach. It simplifies the statement of some of their results. I also like the idea of factoring the argument along the concept of some critical \"wide layer\".\n\nGood review of the literature.\"\n\nUp to the best of our knowledge we have not seen that this patch argument has been used before. It is a very convenient tool to analyze even much more general CNN architectures than the ones currently used.\n\n\"I wished the paper was easier to read. Some of the concepts could have been illustrated to give the reader some way to visualize the intuitive notions. For example, maybe it would have been interesting to plot the rank of features a every layer for LeNet+MNIST ?\"\n\nWe would be very grateful for pointers where we could improve the readability of the paper. We have added a plot for the architecture of Figure 1, where we vary the number of filters T_1 and plot the rank of the feature at the first convolutional layer. As shown by Theorem 3.5 we get full rank for T_1>=89 which implies n_1>=N for the first convolutional layer. In this case the rank of F_1 is 60000 and training error is zero and the loss is minimized almost up to single precision.  We think that this illustrates nicely the result of Theorem 3.5\n\n\" \"This paper is one of the first ones, which studies CNNs.\"\nThis sentence is strange to read, but I can understand what the authors mean.\"\n\nWe agree: please check the new uploaded version, where we have changed it to:\nThis paper is one of the first ones, which theoretically analyzes deep CNNs\n\n\"\"This is true even if the bottom layers (from input to the wide layer) and chosen randomly with probability one.\"\nThere's a certain meaning to \"with probability one\" when it comes to measure theory. The authors are using it correctly in the rest of the paper, but in this sentence I think they simply mean that something holds if \"all\" the bottom layers have random features.\"\n\nWe agree that this can be misunderstood. What we prove is that it holds for almost any weight configuration for the layers from input to the wide layer with respect to the Lebesgue measure (up to a set of measure zero). As in practice the weights are often initialized using e.g. a Gaussian distribution, we wanted to highlight that our result holds with probability 1. In order to clarify this we have added a footnote \"are choosen randomly (\"with respect to any probability measure which has a density with respect to the Lebesgue measure\"). Thus it holds for any probability measure on the weight space which has a density function. We have changed the uploaded manuscript in that way.\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1513849761020,"tcdate":1513849761020,"number":3,"cdate":1513849761020,"id":"ByYsQbYGz","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Comment","forum":"BJjquybCW","replyto":"S136E0hZf","signatures":["ICLR.cc/2018/Conference/Paper493/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper493/Authors"],"content":{"title":"Author's replies","comment":"\"I like the presentation and writing of this paper. However, I find it uneasy to fully evaluate the merit of this paper, mainly because the \"wide\"-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected.\"\n\nPlease note Table 1, where we have listed several state-of-the-art CNN networks, which have such a wide layer (more hidden units than the number of training points) in the case of ImageNet. These are VGG, Inception V3 and Inception V4. Thus we don't see why this wide layer assumption is \"artificial\" if CNNs which had large practical success fulfill this condition.\n\n\"The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy. This is not surprising.\"\n\nWe think that our finding that practical CNNs such as VGG/Inception produce linearly independent features at the wide layer for ImageNet for almost any weight configuration up to the wide layer is an interesting finding which fosters the understanding of these CNNs. While the fact that whether the result is surprising or not is rather a matter of personal taste, what we find more relevant and important is if this result can help to advance the theoretical understanding of practical networks using rigorous math, which it does.\n\n\"It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error.\"\n\nSuch results are currently only available for coarse approximations of neural networks where it is not clear how and if they apply to neural networks used in practice. Meanwhile, our results hold exactly for the architectures used in practice."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1515642456435,"tcdate":1513195189900,"number":4,"cdate":1513195189900,"id":"HJ0nIZkfM","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Review","forum":"BJjquybCW","replyto":"BJjquybCW","signatures":["ICLR.cc/2018/Conference/Paper493/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Interesting direction.","rating":"6: Marginally above acceptance threshold","review":"This paper presents an analysis of convolutional neural networks from the perspective of how the rank of the features is affected by the kinds of layers found in the most popular networks. Their analysis leads to the formulation of a certain theorem about the global minima with respect to parameters in the latter portion of the network.\n\nThe authors ask important questions, but I am not sure that they obtain important answers. On the plus side, I'm glad that people are trying to further our understanding our neural networks, and I think that their investigation is worthy of being published.\n\nThey present a collection of assumptions, lemmas, and theorems. They have no choice but to have assumptions, because they want to abstract away the \"data\" part of the analysis while still being able to use certain properties about the rank of the features at certain layers.\n\nMost of my doubts about this paper come from the feeling that equivalent results could be obtained with a more elegant argument about perturbation theory, instead of something like the proof of Lemma A1. That being said, it's easy to voice such concerns, and I'm willing to believe that there might not exist a simple way to derive the same results with an approach more along the line of \"whatever your data, pick whatever small epsilon, and you can always have the desired properties by perturbing your data by that small epsilon in a random direction\". Have the authors tried this ?\n\nI'm not sure if the authors were the first to present this approach of analyzing the effects of convolutions from a \"patch perspective\", but I think this is a clever approach. It simplifies the statement of some of their results. I also like the idea of factoring the argument along the concept of some critical \"wide layer\".\n\nGood review of the literature.\n\nI wished the paper was easier to read. Some of the concepts could have been illustrated to give the reader some way to visualize the intuitive notions. For example, maybe it would have been interesting to plot the rank of features a every layer for LeNet+MNIST ?\n\nAt the end of the day, if a friend asked me to summarize the paper, I would tell them :\n\n\"Features are basically full rank. Then they use a square loss and end up with an over-parametrized system, so they can achieve loss zero (i.e. global minimum) with a multitude of parameters values.\"\n\n\nNitpicking :\n\n\"This paper is one of the first ones, which studies CNNs.\"\nThis sentence is strange to read, but I can understand what the authors mean.\n\n\"This is true even if the bottom layers (from input to the wide layer) and chosen randomly with probability one.\"\nThere's a certain meaning to \"with probability one\" when it comes to measure theory. The authors are using it correctly in the rest of the paper, but in this sentence I think they simply mean that something holds if \"all\" the bottom layers have random features.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1515642456472,"tcdate":1513051331965,"number":3,"cdate":1513051331965,"id":"S136E0hZf","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Review","forum":"BJjquybCW","replyto":"BJjquybCW","signatures":["ICLR.cc/2018/Conference/Paper493/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The loss surface and expressivity of deep convolutional neural networks","rating":"5: Marginally below acceptance threshold","review":"This paper analyzes the loss function and properties of CNNs with one \"wide\" layer, i.e., a layer with number of neurons greater than the train sample size. Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums. I like the presentation and writing of this paper. However, I find it uneasy to fully evaluate the merit of this paper, mainly because the \"wide\"-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected. The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy. This is not surprising. It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error. ","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1512666806332,"tcdate":1512666806332,"number":2,"cdate":1512666806332,"id":"SyA2UxD-z","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Comment","forum":"BJjquybCW","replyto":"rkvS6-9gG","signatures":["ICLR.cc/2018/Conference/Paper493/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper493/Authors"],"content":{"title":"Author's replies","comment":"Thanks a lot for your reviews. We are happy to answer any additional questions you might have regarding our work."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1512666675853,"tcdate":1512666675853,"number":1,"cdate":1512666675853,"id":"SJsVLlv-f","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Comment","forum":"BJjquybCW","replyto":"BkIW6fYxz","signatures":["ICLR.cc/2018/Conference/Paper493/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper493/Authors"],"content":{"title":"Author's replies","comment":"We do not agree with the assessment of novelty and contribution of reviewer 3. Up to our knowledge only the paper of Cohen and Shashua (ICML 2016) analyzes general CNN architectures. As CNN architectures are obviously very important in practice, we think that a better theoretical understanding is urgently needed. Our paper contains two main results. First we show that CNNs used in practice produce linearly independent features (for ImageNet with VGG or Inception architecture) with probability 1 (Theorem 3.5) at the wide layer (first layer in VGG and Inception). We think that this is a very helpful result to understand how and why current CNNs work also with respect to the recent debate around generalization properties of state of the art networks (Zhang et al, 2017). Second, we give necessary and sufficient conditions for global optima under squared loss (Theorem 4.4) and show that all critical points in S_k are globally optimal under the conditions of Theorem 4.5. We think that this is a significant contribution to the theoretical understanding of CNN architectures. In particular, we would like to emphasize that all our results are applicable to the real problem of interest without any simplifying assumptions.\n\nWe agree in general with the reviewer that it might be nice to have even stronger results e.g. convergence of gradient descent/SGD to the global optimum. But given that the current state of the art in this regard is limited to one hidden layer together with additional distributional assumptions and does not cover deep CNNs used in practice (multiple filters, overlapping patches, deep architecture), we think that the reviewer demands too much. Even papers which consider just deep linear models have been appreciated in the community and get very good reviews at ICLR 2018.\n\nSpecific answers:\n\"Intuitively, (1) is an easy result. Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent.\"\n\nThere are a lot of mathematical results which are intuitive but that does not mean that they are easy to prove.\n\n\"The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small.\"\n\nWe agree that this result would be interesting, but one has to start somewhere (see general comment above).\n\n\"Result (3) has severe limitations, because: (a) there can be infinitely many critical point not in S_k that are spurious local minima; (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure; (c) inside S_k, Theorem 4.4 doesn't exclude the solutions with exponentially small gradients, but whose loss function values are bounded away above zero. If an optimization algorithm falls onto these solutions, it will be hard to escape.\"\n\n(a) Yes, but then these critical points not in S_k (the complement of S_k has measure zero) must have either low rank weight matrices in the layers above the wide layer or the features are not linearly independent at the wide layer. We don't see any reason in the properties of the loss which would enforce low rank in the weight matrices of a CNN. Moreover, it seems unlikely that a critical point with a low rank matrix is a suboptimal local minimum as this would imply that all possible full rank perturbations have larger/equal objective (we don't care if the complement of S_k potentially contains additional global minima). Even for simpler models like two layer linear networks, it has been shown by (Baldi and Hornik, 1989) that all the critical points with low rank weight matrices have to be saddle points and thus cannot be suboptimal local minima. See also other parallel submissions at ICLR 2018 for similar results and indications for deep linear models (e.g. Theorem 2.1, 2.2 in https://openreview.net/pdf?id=BJk7Gf-CZ, and Theorem 5 in https://openreview.net/pdf?id=ByxLBMZCb).\nMoreover, a similar argument applies to the case where one has critical point such that the features are not linearly independent at the wide layer. As any neighborhood of such a critical point contains points which have linearly independent features at the wide layer (and thus it is easy to achieve zero loss), it is again unlikely that this critical point is a suboptimal local minimum.\nIn summary, if there are any critical points in the complement of S_k, then it is very unlikely that these are suboptimal local minima but they are rather also global minima, saddle points or local maxima.\n\n(b/c) We agree that these are certainly interesting questions but the same comment applies as above. Moreover, we see no reason why critical points with low rank weight matrices should be attractors.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1515642456516,"tcdate":1511820607119,"number":2,"cdate":1511820607119,"id":"rkvS6-9gG","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Review","forum":"BJjquybCW","replyto":"BJjquybCW","signatures":["ICLR.cc/2018/Conference/Paper493/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Review","rating":"7: Good paper, accept","review":"This paper analyzes the expressiveness and loss surface of deep CNN. I think the paper is clearly written, and has some interesting insights.","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1515642456555,"tcdate":1511759101582,"number":1,"cdate":1511759101582,"id":"BkIW6fYxz","invitation":"ICLR.cc/2018/Conference/-/Paper493/Official_Review","forum":"BJjquybCW","replyto":"BJjquybCW","signatures":["ICLR.cc/2018/Conference/Paper493/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review of \"The loss surface and expressivity of deep convolutional neural networks\"","rating":"4: Ok but not good enough - rejection","review":"This paper presents several theoretical results on the loss functions of CNNs and fully-connected neural networks. I summarize the results as follows:\n\n(1) Under certain assumptions, if the network contains a \"wide“ hidden layer, such that the layer width is larger than the number of training examples, then (with random weights) this layer almost surely extracts linearly independent features for the training examples.\n\n(2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data.\n\n(3) Under similar assumptions and within a restricted parameter set S_k, all critical points are the global minimum. These solutions achieve zero squared-loss.\n\nI would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1). Intuitively, (1) is an easy result. Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent. The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small.\n\nResult (3) has severe limitations, because: (a) there can be infinitely many critical point not in S_k that are spurious local minima; (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure; (c) inside S_k, Theorem 4.4 doesn't exclude the solutions with exponentially small gradients, but whose loss function values are bounded away above zero. If an optimization algorithm falls onto these solutions, it will be hard to escape.\n\nOverall, the paper presents several incremental improvement over existing theories. However, the novelty and the technical contribution are not sufficient for securing an acceptance.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]}},{"tddate":null,"ddate":null,"tmdate":1515196289989,"tcdate":1509124242758,"number":493,"cdate":1509739270159,"id":"BJjquybCW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"BJjquybCW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"The loss surface and expressivity of deep convolutional neural networks","abstract":"We analyze the expressiveness and loss surface of practical deep convolutional\nneural networks (CNNs) with shared weights and max pooling layers. We show\nthat such CNNs produce linearly independent features at a “wide” layer which\nhas more neurons than the number of training samples. This condition holds e.g.\nfor the VGG network. Furthermore, we provide for such wide CNNs necessary\nand sufficient conditions for global minima with zero training error. For the case\nwhere the wide layer is followed by a fully connected layer we show that almost\nevery critical point of the empirical loss is a global minimum with zero training\nerror. Our analysis suggests that both depth and width are very important in deep\nlearning. While depth brings more representational power and allows the network\nto learn high level features, width smoothes the optimization landscape of the\nloss function in the sense that a sufficiently wide network has a well-behaved loss\nsurface with almost no bad local minima.","pdf":"/pdf/ddb1bab801069a4ff4667997411dcff6ed0a059d.pdf","paperhash":"anonymous|the_loss_surface_and_expressivity_of_deep_convolutional_neural_networks","_bibtex":"@article{\n  anonymous2018the,\n  title={The loss surface and expressivity of deep convolutional neural networks},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=BJjquybCW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper493/Authors"],"keywords":["convolutional neural networks","loss surface","expressivity","critical point","global minima","linear separability"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}