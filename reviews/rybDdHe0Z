{"notes":[{"tddate":null,"ddate":null,"tmdate":1515535962704,"tcdate":1515535962704,"number":5,"cdate":1515535962704,"id":"SyQDCnMNz","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Comment","forum":"rybDdHe0Z","replyto":"H1S6vPofG","signatures":["ICLR.cc/2018/Conference/Paper259/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper259/AnonReviewer3"],"content":{"title":"Thank you for the clarifications","comment":"I will response to all relevant comments here.\n\nw.r.t. Subject B\n- Here I did not change my opinion. \nThe hyper-parameters are optimized on the data that is used for evaluation. This is a basic machine learning error. Therefore the best thing would be to exclude the data from subject B since there is some doubt about the validity of those results.\n\nw.r.t. the LDA model\n- Thanks for the clarification. This approach is more complex than I expected.\n A different solution to having to train different models of different length could be to view it as a convolutional approach with max or mean pooling to obtain the actual output. Given that it is very easy to implement this, I would encourage the authors to try this.\nThis approach would also have the advantage that more data is available to train the entire model.\nAlso, this would enable the authors to test the affine transformation in combination with the linear model. \nIf the LSTM still performs better, this would make the paper a lot stronger. \n\nw.r.t. model analysis\n- Without a colour map next to the weights I cannot understand the plotted matrix. \n- The conclusion that zero weights do not contribute to the output is only valid if they are actually zero and not really small. The whole point of the Haufe paper is that a small weight might be important and processing information, while a large weight might be there primarily due to noise cancelling.\n- Since there is no information about whether the weights or zero or just small, I cannot conclude anything from the provided data.\n- That being said, I am not convinced that the paper needs Fig 5A. Making fig 5b larger would be more informative. \n\nw.r.t. language model init.\nIt is still not clear to me how exactly the model is pre-trained. The notion of language model does not exist here and this might be confusing me. Is it just an auto-encoder trained to predict the data one time-step ahead?\n\nw.r.t. relation to other TL approaches.\nI understand that the difference between EEG and ECOG is that ECOG is typically placed on different parts of the brain. What I fail to grasp is why an affine transformation makes sense here. Is there an intuition about how this can compensate for electrode placement?\n\nFinally, another suggestion. \nIf transfer learning works well, it would make sense to jointly train on all subjects. Where each subject has its own affine transformation but where the model after the transformation is shared across subjects."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/e290626ef2812f50350a9d116003061a77b432a7.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1514006772684,"tcdate":1514006647176,"number":4,"cdate":1514006647176,"id":"HkJYOvsGG","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Comment","forum":"rybDdHe0Z","replyto":"HJ_bsmPxG","signatures":["ICLR.cc/2018/Conference/Paper259/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper259/Authors"],"content":{"title":"Response to Reviewer3","comment":"Thank you for your detailed remarks - we hope to address all of your concerns below and in the updated manuscript. \n\n> “I would argue that subject B should not be considered for evaluation since its data is heavily used for hyper-parameter optimization and the results obtained on this subject are at risk of being biased.”\nIn an attempt to limit the amount of subject-specific tuning performed for the LSTMs, we limit the hyper-parameter tuning to only Subject B. Regarding the inclusion of Subject B, we limit the amount of fine tuning that is done for on the model to limit the potential bias when evaluating Subject B. Furthermore, we demonstrate that other subjects, using Subject B’s hyperparameters, perform comparably. While tuning hyperparameters will likely improve performance for each subject, that would require setting aside data for hyperparameter tuning in the already data constrained situation. Clarifying the transfer learning results, the variance can be reported with standard error of mean (SE) values where, for all reported accuracies, the SE is at most 0.02. \n\n> “it is not clear that an LSTM model is an improvement”\nAs we state in the manuscript, we demonstrate the LSTM model achieves performance comparable to the other models. The primary advantage to utilizing LSTMs resides in the ability to learn the mapping for an affine transformation (please refer to our response to Reviewer2). \n\n> “In the BCI community there are many approaches that use transfer learning with linear models. I think that it would be interesting how linear model transfer learning would fare in this task.”\nIn exploring existing techniques for transfer learning in neural data, there are limited approaches that can be applied to ECoG data and none found in existing literature. Typical techniques utilize EEG data which has a significant amount of spatial averaging allowing for more direct mapping between subjects. In ECoG, the array placements are unique for each subject have greater spatial resolution that exploits underlying neural structures (i.e. using only sensorimotor cortex electrodes), but makes it increasingly difficult to directly map between subjects. Regarding the models, specifically, we may be able to adapt them for transfer learning, however, they would be limited by either the need for a hand-tuned affine transform, or would not represent time series.Specifically, exploring the transfer learning capabilities of the other proposed models, it is important to consider the key advantage of LSTMs is that backpropagation allows for learning the affine transform. Modification of the other models would require hand tuning a mapping layer, or constructing an ensemble of models to leverage a learned mapping to make TL possible. While it is possible to extend LDA, a key goal was to move away from models that operate only on fixed time contexts such as LDA, to a time-series model.\n\n> “issue that might inflate the results is the fact that the data is shuffled randomly”\nRegarding the random shuffle, the experimental setup does not require analysis of contiguous data folds. While neural activity changes over time, the duration of the experiment is on the order of tens of minutes, not hours, which should limit the amount of variability present. Furthermore, between each trial, there is a refractory period that allows the baseline dynamics to be achieved. \n\n> “Accuracy above chance level half a second before movement onset”\nThis is possible because cue processing activity was used to obtain the classification result. As mentioned in the manuscript, the trials are segmented based on the cue. Hence, accuracy above chance is achieved by integrating over 300 ms of neural activity beginning from the display of the cue. Hotson et al. 2016 also show that it is possible to decode prior to movement onset."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/e290626ef2812f50350a9d116003061a77b432a7.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1514006820379,"tcdate":1514006460771,"number":3,"cdate":1514006460771,"id":"H1S6vPofG","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Comment","forum":"rybDdHe0Z","replyto":"HJ_bsmPxG","signatures":["ICLR.cc/2018/Conference/Paper259/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper259/Authors"],"content":{"title":"Response to Reviewer3 continued","comment":"> “For the LDA baseline, how is the varying sequence length treated?”; “The claim that LDA works for structured experimental tasks but not in naturalistic scenarios and will not generalize when electrode count and trial duration increases is a statement that might be true. However, it is never empirically verified. Therefore this statement should not be in the paper.”\nFor each of the possible sequence lengths an LDA model is trained, thus, a family of LDA models is constructed and the model that is picked depends on the length of the sequence. This is the reasoning behind the claim that LDA would not generalize when trial duration increases, as it would be infeasible to construct an LDA model for every possible sequence length. \n\n> “How are the 1 and the 2 state HMM used w.r.t. the 5 classes?”\nThere is a single HMM per class; we have updated the manuscript to make this clear.\n\n> “What is the random and language model initialization scheme?”\nWith respect to the LSTM initialization schemes: by random we mean initializing weights of the network randomly, say using Xavier initialization (Glorot and Bengio 2010); language model initialization is from the Dai and Le paper. We have updated to make this clear in the manuscript.\n\n> Model analysis\nThank you for the insightful reference; reviewing the statements of Haufe et al and our results, we think our interpretations are still valid, but need to be reworded in the manuscript to prevent misinterpretation. While we are utilizing a backward model, we are not hoping to make conclusions about the learned weights in the affine mapping specifically concerning the underlying brain processes. We are addressing the fact that the LSTM model input features are electrodes with specific locations on Subject 1, and when we include known non-informative electrodes, the learned representation excludes them. I.e. we know from the task design and neurophysiology that the occipital region in motor movement tasks should have little activation. Therefore, we would expect the affine mapping to learn zero weights for the signals from occipital region and we do see that in our model. We believe that this interpretation does not violate the points from Haufe et al.\n\n> References\nOur apologies for the missing references. We have included the citations for the Ledoit-Wolf lemma and the prior transfer learning work in EEG (Morioka et al.). In the discussion section we refer to usage of auto-encoders as an unsupervised feature extractor, this is in contrast to our current features which are based on neurophysiology, and hence the reference to Le et al 2011."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/e290626ef2812f50350a9d116003061a77b432a7.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1514004412634,"tcdate":1514004412634,"number":2,"cdate":1514004412634,"id":"BkrakDsfz","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Comment","forum":"rybDdHe0Z","replyto":"HJmBCpKeG","signatures":["ICLR.cc/2018/Conference/Paper259/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper259/Authors"],"content":{"title":"Response to Reviewer1","comment":"Thank you for your remarks - we hope to address all of your concerns below and in the updated manuscript. \n\n> “The data includes only few samples per class. The validation procedure to obtain the model accuray is a bit iffy. Currently, I do not see evidence for a stable training procedure in the ms.”\nThe availability of invasive neural recordings in humans is quite limited. Therefore, we selected a validation procedure that demonstrates the robustness of the model training across multiple runs and partitions of data. We show in a the various groupings, a consistent performance is achieved. Furthermore, we did not finetune the parameters for all subjects, rather, a single set of hyperparameters for the model was selected after being coarsely trained on a single subject. \n\n> “Comparison to a k-NN classifier using embedded data to gauge the problem difficulty”\nRegarding the comparison to a k-NN classifier (presumably on the 2-d embedding), the t-SNE embedding is obtained on the training data after the network was optimized on it. As such, the embedding looks separable because it was optimized on this data. Because t-SNE is nonparametric, the test data cannot be projected onto the same embedding space to facilitate the k-NN experiment. We use the t-SNE embedding to show that the learned parameters of the network separate classes for both subjects well and the embedding for the same class from the two subjects cluster together even though the network weights were not explicitly optimized for it.\n\n> “Also, the paper does not really decide whether it is a neuroscience contribution or an ML one.”\nThe primary contributions we hope to make clear through the paper are not restricted to neuroscience or machine learning due to the nature of the experimentation and analysis. We hope to demonstrate the benefits and disadvantages of existing approaches, applications of new models that have not been applied to neural data, and to propose improvements to the model demonstrating benefits in performance with potential for understanding what is being learned rooted in neuroscience principles."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/e290626ef2812f50350a9d116003061a77b432a7.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1514004326410,"tcdate":1514004326410,"number":1,"cdate":1514004326410,"id":"H1AvJwszG","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Comment","forum":"rybDdHe0Z","replyto":"S1D3Hb9eM","signatures":["ICLR.cc/2018/Conference/Paper259/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper259/Authors"],"content":{"title":"Response to Reviewer2","comment":"Thank you for your remarks - we hope to answer all of your concerns below or in the updated manuscript. \n\n> “the feature space is unclear (number of variables etc)”\nBriefly clarifying the feature space, we use high frequency band (70 - 150 Hz) power extracted from the electrodes in the sensorimotor region. The number of features (equivalent to the number of electrodes as a single average power is utilized) vary based on the subject but are between 6 and 8. \n\n> “Unclear if prior odds are uniform. Total amount of data unclear.”\nFor the models, the classes are balanced and the number of samples per classes vary based on the subject, but are between 27 and 29. Thus the total number of samples are between 135 and 145 and a uniform prior is utilized.\n \n> “A simplistic but interesting transfer scheme is proposed amounting to an affine transform of features(??) - the complexity of this transform is unclear.”\nRegarding the affine transform, it transforms the feature space from the new target subject to the feature space of the original subject. As the transform is affine, the number of parameters of the transform is e_2*(e_1+1) where e_1, e_2 are the number of electrodes in the original subject and the transferring subject, respectively.\n \n> “The decoding performance of the LSTMs does not convincingly exceed that of the simple baselines.”; “I fail to see how it can be attractive to obtain similar performance with a model of 100x (?) the complexity”\nWith respect to the decoding performance of the LSTM’s, we establish the model as an approach that provides comparable results even while having significantly more parameters to learn and being data constrained. An advantage to utilizing such a technique is due to the scalability of the model (as seen in speech recognition literature Graves et al. 2013). The key advantage to the LSTM model, however, is the demonstrated superior performance utilizing the TL architecture which is only possible due to the network structure and the ability to back propagate errors. Furthermore, preventing training of the LSTM and only training the affine mapping provides comparable performance to existing techniques that could have applications where subject specific training data is limited.\n\n> “When analyzing the transfer mechanism only the LSTMs are investigated and it remains unclear how well trans works.”\nExploring the transfer learning capabilities of the other proposed models, it is important to consider the key advantage of LSTMs is that backpropagation allows for learning the affine transform. Modification of the other models would require hand tuning a mapping layer, or constructing an ensemble of models to leverage a learned mapping to make TL possible. While it is possible to extend LDA, a key goal was to move away from models that operate only on fixed time contexts such as LDA, to a time-series model. We discuss reasoning for this in the manuscript.\n\n > “There is an interesting visualization (t-SNE) of the latent representations. But very limited discussion of what we learn from it, or how such visualization could be used to provide neuroscience insights.”\nThe t-SNE representation is meant to demonstrate that a meaningful mapping between the two subjects exists rather than some arbitrary mapping that gives good results (i.e. the results are interpretable). It is likely a representation of an underlying physiological basis rooted in the structure of the sensorimotor cortex. However, a more detailed examination with more subjects is necessary to be able to concretely say anything about the underlying physiology."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/e290626ef2812f50350a9d116003061a77b432a7.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1515642421223,"tcdate":1511818671426,"number":3,"cdate":1511818671426,"id":"S1D3Hb9eM","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Review","forum":"rybDdHe0Z","replyto":"rybDdHe0Z","signatures":["ICLR.cc/2018/Conference/Paper259/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Application of LSTM to decoding of neural signals, limited novelty, inconclusive","rating":"3: Clear rejection","review":"This work addresses brain state  decoding (intent to move) based on intra-cranial \"electrocorticography (ECoG) grids\". ECoG signals are generally of much higher quality than more conventional EEG signals acquired on the skalp, hence it appears meaningful to invest significant effort to decode.  \nPreprocessing is only descibed in a few lines in Section 2.1, and the the feature space is unclear (number of variables etc)\n\nLinear discriminants, \"1-state and 2-state\" hidden markov models, and LSTMs are considered for classification (5 classes, unclear if prior odds are uniform). Data involves multiple subjects (4 selected from a larger pool). Total amount of data unclear. \"A validation set is not used due to the limited data size.\"  The LSTM setup and training follows conventional wisdom.\n\"The model used for our analyses was constructed with 100 hidden units with no performance gain identified using larger or stacked networks.\"\nA simplistic but interesting  transfer scheme is proposed amounting to an affine transform of features(??) - the complexity of this transform is unclear.\n\nWhile limited novelty is found in the methodology/engineering - novelty being mainly related to the affine transfer mechanism, results are disappointing.  \nThe decoding performance of the LSTMs does not convincingly exceed that of the simple baselines. \n\nWhen analyzing the transfer mechanism only the LSTMs are investigated and it remains unclear how well trans works.\n\nThere is an interesting visualization (t-SNE) of the latent representations. But very limited discussion of what we learn from it, or how such visualization could  be used to provide neuroscience insights.\n\nIn the discussion we find the claim: \"In this work, we have shown that LSTMs can model the variation within a neural sequence and are a good alternative to state-of-the-art decoders.\"  I fail to see how it can be attractive to obtain similar performance with a model of 100x (?) the complexity\n\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/e290626ef2812f50350a9d116003061a77b432a7.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1515642421261,"tcdate":1511804475119,"number":2,"cdate":1511804475119,"id":"HJmBCpKeG","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Review","forum":"rybDdHe0Z","replyto":"rybDdHe0Z","signatures":["ICLR.cc/2018/Conference/Paper259/AnonReviewer1"],"readers":["everyone"],"content":{"title":"LSTMs for ECoG","rating":"6: Marginally above acceptance threshold","review":"The ms applies an LSTM on ECoG data and studies tranfer between subjects etc. \n\nThe data includes only few samples per class. The validation procedure to obtain the model accuray is a bit iffy. \nThe ms says: The test data contains 'at least 2 samples per class'. Data of the type analysed is highly dependend, so it is not unclear whether this validation procedure will not provide overoptimistic results. Currently, I do not see evidence for a stable training procedure in the ms. I would be curious also to see a comparison to a k-NN classifier using embedded data to gauge the problem difficulty. \nAlso, the paper does not really decide whether it is a neuroscience contribution or an ML one. If it were a neuroscience contribution, then it would be important to analyse and understand the LSTM representation and to put it into a biological context fig 5B is a first step in this direction. \nIf it where a ML contribution, then there should be a comprehensive analysis that indeed the proposed architecture using the 2 steps is actually doing the right thing, i.e. that the method converges to the truth if more and more data is available. \nThere is also some initial experiments in fig 3A. Currently, I find the paper somewhat unsatisfactory and thus preliminary. ","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/e290626ef2812f50350a9d116003061a77b432a7.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1515642421303,"tcdate":1511631616458,"number":1,"cdate":1511631616458,"id":"HJ_bsmPxG","invitation":"ICLR.cc/2018/Conference/-/Paper259/Official_Review","forum":"rybDdHe0Z","replyto":"rybDdHe0Z","signatures":["ICLR.cc/2018/Conference/Paper259/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Difficult problem, some aspects are unclear, evaluation could be improved","rating":"4: Ok but not good enough - rejection","review":"The paper describes an approach to use LSTM’s for finger classification based on ECOG. and a transfer learning extension of which two variations exists. From the presented results, the LSTM model is not an improvement over a basic linear model. The transfer learning models performs better than subject specific models on a subset of the subjects. Overall, I think the problem Is interesting but the technical description and the evaluation can be improved. I am not confident in the analysis of the model. Additionally, the citations are not always correct and some related work is not referenced at all. For the reasons above, I am not willing to recommend the paper for acceptance at his point.\n\nThe paper tackles a problem that is challenging and interesting. Unfortunately, the dataset size is limited. \nThis is common for brain data and makes evaluation much more difficult.\n The paper states that all hyper-parameters were optimized on 75% of subject B data.\nThe actual model training was done using cross-validation. \nSo far this approach seems more or less correct but in this case I would argue that subject B should not be considered for evaluation since its data is heavily used for hyper-parameter optimization and the results obtained on this subject are at risk of being biased.\nOmitting subject B from the analysis, each non-transfer learning method  performs best on one of the remaining subjects.\nTherefore it is not clear that an LSTM model is an improvement. \nFor transfer learning (ignoring B again) only C and D are improved but it is unclear what the variance is.\nIn the BCI community there are many approaches that use transfer learning with linear models. I think that it would be interesting how linear model transfer learning would fare in this task. \n\nA second issue that might inflate the results is the fact that the data is shuffled randomly. While this is common practice for most machine learning tasks, it is dangerous when working with brain data due to changes in the signal over time. As a result, selecting random samples might inflate the accuracy compared to having a proper train and test set that are separated in time. Ideally the cross-validation should be done using contiguous folds. \n\nI am not quite sure whether it should be possible to have an accuracy above chance level half a second before movement onset? How long does motor preparation take? I am not familiar with this specific subject, but a quick search gave me a reaction time for sprinters of .15 seconds. Is it possible that cue processing activity was used to obtain the classification result? Please discuss this effect because I am do not understand why it should be possible to get above chance level accuracy half a second before movement onset. \n\nThere are also several technical aspects that are not clear to me. I am confident that I am unable to re-implement the proposed method and their baseline given the information provided.\n\nLDA baseline:\n—————————\nFor the LDA baseline, how is the varying sequence length treated? \nLedoit wolf analytic  regularization is used, but it isn not referenced. If you use that method, cite the paper. \nThe claim that LDA works for structured experimental tasks but not in naturalistic scenarios and will not generalize when electrode count and trial duration increases is a statement that might be true. However, it is never empirically verified.  Therefore this statement should not be in the paper. \n\nHMM baseline\n—————————\nHow are the 1 and the 2 state HMM used w.r.t. the 5 classes? It is unclear to me how they are used exactly. Is there a single HMM per class? Please be specific. \n\nLSTM Model\n—————\nWhat is the random and language model initialization scheme? I can only find the sequence auto-encoder in the Dai and Le paper. \n\n\nModel analysis\n——————————-\nIt is widely accepted in the neuroimaging community that linear weight vectors should not be interpreted directly. It is actually impossible to do this.  Therefore this section should be completely re-done. Please read the following paper on this subject.\nHaufe, Stefan, et al. \"On the interpretation of weight vectors of linear models in multivariate neuroimaging.\" Neuroimage 87 (2014): 96-110.\n\nReferences\n———— \nLedoit wolf regularization is used but not cited. Fix this.\nThere is no citation for the random/language model initialization of the LSTM model. I have no clue how to do this without proper citation.\nLe at al (2011) are referenced for auto-encoders. This is definitely not the right citation. \nRumelhart, Hinton, & Williams, 1986a; Bourlard & Kamp, 1988; Hinton & Zemel, 1994 and Bengio, Lamblin, Popovici, & Larochelle, 2007; Ranzato, Poultney, Chopra, & LeCun, 2007 are probably all more relevant.\nPlease cite the relevant work on affine transformations for transfer learning especially the work by morioka et al who also learn an input transferm.\nMorioka, Hiroshi, et al. \"Learning a common dictionary for subject-transfer decoding with resting calibration.\" NeuroImage 111 (2015): 167-178.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/e290626ef2812f50350a9d116003061a77b432a7.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]}},{"tddate":null,"ddate":null,"tmdate":1514003884727,"tcdate":1509083224816,"number":259,"cdate":1509739396560,"id":"rybDdHe0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"rybDdHe0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Sequence Transfer Learning for Neural Decoding","abstract":"A fundamental challenge in designing brain-computer interfaces (BCIs) is decoding behavior from time-varying neural oscillations. In typical applications, decoders are constructed for individual subjects and with limited data leading to restrictions on the types of models that can be utilized. Currently, the best performing decoders are typically linear models capable of utilizing rigid timing constraints with limited training data. Here we demonstrate the use of Long Short-Term Memory (LSTM) networks to take advantage of the temporal information present in sequential neural data collected from subjects implanted with electrocorticographic (ECoG) electrode arrays performing a finger flexion task. Our constructed models are capable of achieving accuracies that are comparable to existing techniques while also being robust to variation in sample data size. Moreover, we utilize the LSTM networks and an affine transformation layer to construct a novel architecture for transfer learning. We demonstrate that in scenarios where only the affine transform is learned for a new subject, it is possible to achieve results comparable to existing state-of-the-art techniques. The notable advantage is the increased stability of the model during training on novel subjects. Relaxing the constraint of only training the affine transformation, we establish our model as capable of exceeding performance of current models across all training data sizes. Overall, this work demonstrates that LSTMs are a versatile model that can accurately capture temporal patterns in neural data and can provide a foundation for transfer learning in neural decoding.","pdf":"/pdf/e290626ef2812f50350a9d116003061a77b432a7.pdf","paperhash":"anonymous|sequence_transfer_learning_for_neural_decoding","_bibtex":"@article{\n  anonymous2018sequence,\n  title={Sequence Transfer Learning for Neural Decoding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=rybDdHe0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper259/Authors"],"keywords":["Transfer Learning","Applications","Neural decoding"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}