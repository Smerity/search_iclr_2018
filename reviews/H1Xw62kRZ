{"notes":[{"tddate":null,"ddate":null,"tmdate":1514388137971,"tcdate":1514388137971,"number":4,"cdate":1514388137971,"id":"SyzhcVbXf","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Comment","forum":"H1Xw62kRZ","replyto":"Sy8bJjuMM","signatures":["ICLR.cc/2018/Conference/Paper175/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper175/Authors"],"content":{"title":"Reply to Commenter","comment":"We thank the commenter for their interest in our paper.\n\nMore details about the generation of the dataset are available in a previous paper that was making use of the Karel Dataset [1]. We have already released the karel dataset (link omitted because of double-blind constraints) and also plan on releasing the code used to run the experiments. What follows are the answers to the specific questions:\n\n1, 2, 3 -> The input grids are generated by randomly sampling for each cell whether the cell contains an obstacle / a marker / several markers. The agent’s position is also selected at random.\nA program is then sampled with a maximum nesting depth (nested loops or conditionals) of 4 and a maximum number of tokens that is set to be 20. We execute the programs on the inputs grids to generate the output grids. If the program hasn’t halted before performing 200 actions, if there is a collision with an obstacle or if the program doesn’t do anything when run on the sampled grid (i.e. the input grid is unchanged), we discard the program and sample a new one.\n\n4 -> The 52 tokens are: <s> (start of sequence), not, DEF, run, REPEAT, WHILE, IF,  IFELSE,  ELSE, markersPresent, noMarkersPresent, leftIsClear, rightIsClear, frontIsClear, move, turnLeft, turnRight, pickMarker, putMarker, m(, m) (open and close parens for a function), c(, c) (open and close parens for a conditional),  r(, r) (open and close parens for a repeat instruction), w(, w) (open and close parens for a while conditional), i(,i) (open and close parens for a if statement conditional), e(, e) (open and close parens for an else clause) + 20 scalar values from 0 to 19.\n\n5 -> The batch size used for the supervised setting was 128. For the RL type experiments, a batch was composed of 16 samples. We used 100 rollouts per samples for the Reinforce method and a beam size of 64 for methods based on the beam search.\n\n6-> We have added more details in the paper regarding how the beam search is performed.\n\n[1] Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli. Neural Program Meta-Induction. In NIPS, 2017\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/a794f8835f430e2f9800c6864641ffca572c70d3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1514387885928,"tcdate":1514387885928,"number":3,"cdate":1514387885928,"id":"S1UnKVWXf","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Comment","forum":"H1Xw62kRZ","replyto":"HkcxQ4Rxf","signatures":["ICLR.cc/2018/Conference/Paper175/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper175/Authors"],"content":{"title":"Reply to Reviewer 1","comment":"We thank the reviewer for the comments on the part of the paper that need clarification. We incorporated his feedback in the new version. Here are answers to the raised questions:\n\n-> Figure 2 is too small\nThe size of Figure 2 was increased to make it full page width.\n\n-> “The syntaxLSTM can be trained without supervision in the form of known valid programs”\nWhile the syntaxLSTM can be trained without access to known valid programs when RL-based training is employed (because gradient will flow to it through the softmax defining the probability of each token), we point out in the experiments section that we weren’t successful in training models using only RL training. As a result, it would not be accurate to claim that it can be trained without any valid programs as supervision. \n\n-> Is the Karel DSL the full Karel language?\nThe exact description of our DSL is in Appendix B. It doesn’t exactly match the full Karel language, as most notably there is no possibility to define subroutines. We made this clearer in the paper.\n\n-> What was the beam width used?\nAll of our experiments used a beam width of 64. We didn’t study the effect of this hyperparameter and chose it as the maximum width we could afford based on available GPU memory. In the limit, using an extremely large beam size would be equivalent to computing the complete sum for the expected reward but this is not feasible for any applications where program would be longer than a few tokens.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/a794f8835f430e2f9800c6864641ffca572c70d3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1514387808589,"tcdate":1514387808589,"number":2,"cdate":1514387808589,"id":"ByKvFVW7f","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Comment","forum":"H1Xw62kRZ","replyto":"H1JSNUjeG","signatures":["ICLR.cc/2018/Conference/Paper175/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper175/Authors"],"content":{"title":"Reply to Reviewer 3","comment":"We thank the reviewer for the comments on the paper, and for pointing out the missing related work.\n\nIt is difficult to perform an exact comparison between the two papers as they are solving two different problems. The model developed in [1] (Devlin et al, 2017) performs program induction: i.e. it produces the output world on a new input world where the desired program semantics are encoded in the network itself. On the other hand, in our case, we perform program synthesis, i.e. generate a program in the Karel DSL that performs the desired transformation from input to output. \n \nUsing the terminology of Devlin et al., what we describe is closest to the meta-induction approach: strong cross task knowledge sharing but no task specific learning. Overall, our MLE baseline architecture will correspond to the Devlin et al. meta-induction architecture if the decoder was trained to generate program tokens instead of output worlds. This precision was added to the paper\n\n-> No real-world task such as FlashFill?\nThe FlashFill DSL considered in previous neural program synthesis work such as RobustFill is essentially a functional language comprising of compositions of a sequence of functions. In this work, we wanted to increase the complexity of the DSL one step further to better understand what neural architectures are more appropriate for learning programs with such complexity. Concretely, the Karel DSL consists of complex control-flow such as nested loops and conditionals, which are not present in the FlashFill DSL. The difference of performance of meta-induction on FlashFill (~70% from  Figure 7 of [2]) vs. KarelDSL (~40% from Figure 4 of [1]) points towards Karel being a more complex dataset.\n\n Learning Karel programs can also be considered close to a real-world task as this language is used to teach introductory programming to Stanford students, and the program synthesis models can be used to help students if they are having difficulty in writing correct programs.\n\n\n[1] Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli. Neural Program Meta-Induction. In NIPS, 2017\n[2] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy I/O. In ICML, 2017 \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/a794f8835f430e2f9800c6864641ffca572c70d3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1514387706136,"tcdate":1514387706136,"number":1,"cdate":1514387706136,"id":"r1GbKVb7G","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Comment","forum":"H1Xw62kRZ","replyto":"Hk4_Jw9xG","signatures":["ICLR.cc/2018/Conference/Paper175/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper175/Authors"],"content":{"title":"Reply to Reviewer 2","comment":"We thank the reviewer for the detailed comments on how to improve the exposition of our paper, which we included in the revised version.\n\n- Figure 2’s size was increased to make the model clearer.\n\n- Reinforce vs. Monte Carlo estimate of the expected reward:\nEquation 8 indeed describes how we estimate the expected reward, we also added the form of the estimator of the expected gradient for a sample i to make what me meant clearer\n\nBeam search and Approximate probabilities\nEquation 10 indeed had a typo. The i in “i \\in 1..C” should have been a “r”, making the product a product of the probability of the C programs sampled from the approximate probability distribution. At a search depth of d, the heuristic used to prune candidate paths is the probability of the prefix (which you can think of as the product of equation (5) but limited to the first d terms). \n\nWe arrive at a search depth d with a set of S candidates. For each of these S candidates, we obtain the probability of the next token using the softmax. Combining the probability of this token with the product of the whole path that comes before it, we obtain the probability for S * (nb_possible_token) possible paths. We only keep the S best ones (possibly removing the ones that have reached a termination symbol) and repeat the step at the depth d+1.\nWe end up at the end with a set of S samples which are going to be used as the basis for our approximate distribution. We have added more description of the process to make it clearer.\n\nWhen syntax checking is available, whether in its learned form or not, it is implicitly included as its contribution is introduced just before the softmax (see Figure 2 if you can zoom in). A token judged non-syntactically correct would have a probability of zero, so the probability of the path containing it would be zero and would therefore not be included into the promising paths going to the next stage.\n\n\n- Is there value in learning syntax?\nIt might be possible to have access to a large amount of programs in a language without having access to a syntax checker, such as for example if we have downloaded a large amount of programs from a code repository. Moreover, it might be useful even for common languages: Note that what we require is a bit different to a traditional syntax checker: answering the question “is this program syntactically correct”, which any compiler would give; as opposed to what we have in equation 13 which corresponds to “Do these first t tokens contain no syntax error and may therefore be a valid prefix to a program”. The syntax checker we need has to return a decision even for non-complete program, therefore it would require some work to transform current compilers to return such answers.\nFinally, as shown in our experiments, using a learned syntax checker might perform better than using a formal one, as it can capture what represents an “idiomatic” program vs. a technically correct one.\n\n\n\f\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/a794f8835f430e2f9800c6864641ffca572c70d3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1513909560318,"tcdate":1513823998272,"number":1,"cdate":1513823998272,"id":"Sy8bJjuMM","invitation":"ICLR.cc/2018/Conference/-/Paper175/Public_Comment","forum":"H1Xw62kRZ","replyto":"H1Xw62kRZ","signatures":["~Taehoon_Kim1"],"readers":["everyone"],"writers":["~Taehoon_Kim1"],"content":{"title":"Questions about reproducibility of the paper","comment":"Dear authors, \n\nThanks for your interesting paper that I enjoyed a lot. It is great to read new approaches in the field of program synthesis and its promising results. I believe the contributions of the paper are clear but the experimental details are not sufficient to reproduce the results. Below is the list that I found missing in the paper:\n\n1. Sampling method for input/output grid world (ex. # of markers, # of obstacles, Code blocks like repeat(19) { repeat (15) { ... }} or repeat(17) { turnRight } might work as noises)\n2. Sampling method for Karel program (ex. max # of tokens, max depth of program)\n3. How to deal with corner cases like program with endless loop\n4. 52 tokens for Karel DSL\n5. Batch size\n6. Detailed on beam search\n\nBecause the sampling methods of world and program are critical to set the difficulty of the problem, I think the authors could discuss in more details about it to extend the suggested methods. Can the author offer some details on this?\n\nThe current attempt to reproduce the Karel dataset can be found https://github.com/carpedm20/karel and https://github.com/carpedm20/program-synthesis-rl-tensorflow."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/a794f8835f430e2f9800c6864641ffca572c70d3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1515642404107,"tcdate":1512092401565,"number":3,"cdate":1512092401565,"id":"HkcxQ4Rxf","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Review","forum":"H1Xw62kRZ","replyto":"H1Xw62kRZ","signatures":["ICLR.cc/2018/Conference/Paper175/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good paper, accept","rating":"7: Good paper, accept","review":"This is a nice paper. It makes novel contributions to neural program synthesis by (a) using RL to tune neural program synthesizers such that they can generate a wider variety of correct programs and (b) using a syntax checker (or a learned approximation thereof) to prevent the synthesizer from outputting any syntactically-invalid programs, thus pruning the search space. In experiments, the proposed method synthesizes correct Karel programs (non-trivial programs involving loops and conditionals) more frequently than synthesizers trained using only maximum likelihood supervised training.\n\nI have a few minor questions and requests for clarification, but overall the paper presents strong results and, I believe, should be accepted.\n\n\nSpecific comments/questions follow:\n\n\nFigure 2 is too small. It would be much more helpful (and easier to read) if it were enlarged to take the full page width.\n\nPage 7: \"In the supervised setting...\" This suggests that the syntaxLSTM can be trained without supervision in the form of known valid programs, a possibility which might not have occurred to me without this little aside. If that is indeed the case, that's a surprising and interesting result that deserves having more attention called to it (I appreciated the analysis in the results section to this effect, but you could call attention to this sooner, here on page 7).\n\nIs the \"Karel DSL\" in your experiments the full Karel language, or a subset designed for the paper?\n\nFor the versions of the model that use beam search, what beam width was used? Do the results reported in e.g. Table 1 change as a function of beam width, and if so, how? \n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/a794f8835f430e2f9800c6864641ffca572c70d3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1515642404146,"tcdate":1511904311368,"number":2,"cdate":1511904311368,"id":"H1JSNUjeG","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Review","forum":"H1Xw62kRZ","replyto":"H1Xw62kRZ","signatures":["ICLR.cc/2018/Conference/Paper175/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"6: Marginally above acceptance threshold","review":"The paper presents a reinforcement learning-based approach for program synthesis. The proposed approach claims two advantages over a baseline maximum likelihood estimation-based approach. MLE-based methods penalize syntactically different but semantically equivalent programs. Further, typical program synthesis approaches don't explicitly learn to produce correct syntax. The proposed approach uses a syntax-checker to limit the next-token distribution to syntactically-valid tokens.\n\nThe approach, and its constituent contributions, i.e. of using RL for program synthesis, and limiting to syntactically valid programs, are novel. Although both the contributions are fairly obvious, there is of course merit in empirically validating these ideas.\n\nThe paper presents comparisons with baseline methods. The improvements over the baseline methods is small but substantial, and enough experimental details are provided to reproduce the results.  However, there is no comparison with other approaches in the literature. The authors claim to improve the state-of-the-art, but fail to mention and compare with the state-of-the-art, such as [1]. I do find it hard to trust papers which do not compare with results from other papers.\n\nPros:\n1. Well-written paper, with clear contributions.\n2. Good empirical evaluation with ablations.\n\nCons:\n1. No SOTA comparison.\n2. Only one task / No real-world task, such as Excel Flashfill.\n\n[1]: \"Neural Program Meta-Induction\", Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/a794f8835f430e2f9800c6864641ffca572c70d3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1515642404191,"tcdate":1511841643706,"number":1,"cdate":1511841643706,"id":"Hk4_Jw9xG","invitation":"ICLR.cc/2018/Conference/-/Paper175/Official_Review","forum":"H1Xw62kRZ","replyto":"H1Xw62kRZ","signatures":["ICLR.cc/2018/Conference/Paper175/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good paper, could be more clearly written.","rating":"5: Marginally below acceptance threshold","review":"The authors consider the task of program synthesis in the Karel DSL. Their innovations are to use reinforcement learning to guide sequential generation of tokes towards a high reward output, incorporate syntax checking into the synthesis procedure to prune syntactically invalid programs. Finally they learn a model that predicts correctness of syntax in absence of a syntax checker. \n\nWhile the results in this paper look good, I found many aspects of the exposition difficult to follow. In section 4, the authors define objectives, but do not clearly describe how these objectives are optimized, instead relying on the read to infer from context how REINFORCE and beam search are applied. I was not able to understand whether syntactic corrected is enforce by way of the reward introduced in section 4, or by way of the conditioning introduced in section 5.1. Discussion of the experimental results coould similarly be clearer. The best method very clearly depends on the taks and the amount of available data, but I found it difficult to extract an intuition for which method works best in which setting and why. \n\nOn the whole this seems like a promising paper. That said, I think the authors would need to convincingly address issues of clarity in order for this to appear. \n\nSpecific comments \n\n- Figure 2 is too small \n\n- Equation 8 is confusing in that it defines a Monte Carlo estimate of the expected reward, rather than an estimator of the gradient of the expected reward (which is what REINFORCE is). \n\n- It is not clear the how beam search is carried out. In equation (10) there appear to be two problems. The first is that the index i appears twice (once in i=1..N and once in i \\in 1..C), the second is that λ_r refers to an index that does not appear. More generally, beam search is normally an algorithm where at each search depth, the set of candidate paths is pruned according to some heuristic. What is the heuristic here? Is syntax checking used at each step of token generation, or something along these lines? \n \n- What is the value of the learned syntax in section 5.2? Presumaly we need a large corpus of syntax-checked training examples to learn this model, which means that, in practice, we still need to have a syntax-checker available, do we not?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/a794f8835f430e2f9800c6864641ffca572c70d3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]}},{"tddate":null,"ddate":null,"tmdate":1514387398482,"tcdate":1509047643423,"number":175,"cdate":1509739441903,"id":"H1Xw62kRZ","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1Xw62kRZ","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis","abstract":"Program synthesis is the task of automatically generating a program consistent with\na specification. Recent years have seen proposal of a number of neural approaches\nfor program synthesis, many of which adopt a sequence generation paradigm similar\nto neural machine translation, in which sequence-to-sequence models are trained to\nmaximize the likelihood of known reference programs. While achieving impressive\nresults, this strategy has two key limitations. First, it ignores Program Aliasing: the\nfact that many different programs may satisfy a given specification (especially with\nincomplete specifications such as a few input-output examples). By maximizing\nthe likelihood of only a single reference program, it penalizes many semantically\ncorrect programs, which can adversely affect the synthesizer performance. Second,\nthis strategy overlooks the fact that programs have a strict syntax that can be\nefficiently checked. To address the first limitation, we perform reinforcement\nlearning on top of a supervised model with an objective that explicitly maximizes\nthe likelihood of generating semantically correct programs. For addressing the\nsecond limitation, we introduce a training procedure that directly maximizes the\nprobability of generating syntactically correct programs that fulfill the specification.\nWe show that our contributions lead to improved accuracy of the models, especially\nin cases where the training data is limited.","pdf":"/pdf/a794f8835f430e2f9800c6864641ffca572c70d3.pdf","TL;DR":"Using the DSL grammar and reinforcement learning to improve synthesis of programs with complex control flow.","paperhash":"anonymous|leveraging_grammar_and_reinforcement_learning_for_neural_program_synthesis","_bibtex":"@article{\n  anonymous2018leveraging,\n  title={Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1Xw62kRZ}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper175/Authors"],"keywords":["Program Synthesis","Reinforcement Learning","Language Model"]},"nonreaders":[],"replyCount":8,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}