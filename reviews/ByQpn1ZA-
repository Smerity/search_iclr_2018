{"notes":[{"tddate":null,"ddate":null,"tmdate":1515814998400,"tcdate":1515814998400,"number":12,"cdate":1515814998400,"id":"B1RLg-DNz","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Comment","forum":"ByQpn1ZA-","replyto":"H1oisTzMf","signatures":["ICLR.cc/2018/Conference/Paper521/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper521/Authors"],"content":{"title":"Practical Implications and Theoretical Context","comment":"Thanks again for your review!  Before the rebuttal process concludes, do you have any outstanding questions regarding our revision? We ensure this includes specific practical suggestions for GAN-training to guide the community.  In regards to your point about theoretical results, we hope our paper serves to encourage future theoretical research compatible with our observed empirical results. We believe this paper tests a prevailing theoretical understanding of GAN training as directly as possible and that these observations may help validate or invalidate later theoretical models."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1515712790029,"tcdate":1515712790029,"number":11,"cdate":1515712790029,"id":"BJAGbdrVf","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Comment","forum":"ByQpn1ZA-","replyto":"SkiI_Bixz","signatures":["ICLR.cc/2018/Conference/Paper521/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper521/Authors"],"content":{"title":"Changes in metrics for the synthetic experiments - using Frechet distance","comment":"All the figures for synthetic experiments are now updated to use the Frechet distance between Gaussians, instead of l2 distance. Thank you for your suggestion!"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1513442337696,"tcdate":1513442337696,"number":6,"cdate":1513442337696,"id":"HyqX2aMGG","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Comment","forum":"ByQpn1ZA-","replyto":"SkqpgbLgM","signatures":["ICLR.cc/2018/Conference/Paper521/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper521/Authors"],"content":{"title":"RE: Well-written experimental study; light on theory; poses new questions and aims to answer some","comment":"Thanks for the detailed and thorough review! \n\nWe have now updated the paper with a practical considerations section as well as updated the conclusion to reflect some of your take aways, such as:\n\n- GAN training remains difficult and good results are not guaranteed;\n- Gradient penalties work in all settings, but why is not completely clear;\n- NS-GANs + GPs seems to be best sample-generating combination, and faster than WGAN-GP.\n- Some of the used metrics can detect mode collapse. \n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1513442211301,"tcdate":1513442211301,"number":5,"cdate":1513442211301,"id":"H1oisTzMf","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Comment","forum":"ByQpn1ZA-","replyto":"Sks895Fxz","signatures":["ICLR.cc/2018/Conference/Paper521/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper521/Authors"],"content":{"title":"Re: A great first study","comment":"Thank you for your review. We hope to have addressed most of your concerns below:\n\n* We don't believe that the paper is premature for the following reasons:\n       - Gradient penalties are helpful to stabilize GAN training, regardless of the cost function. This is also supported by \n          another paper (https://arxiv.org/pdf/1705.07215v4.pdf). \n       - Gradient penalties are a cost effective way to improve the performance of a GAN. Compared to Wasserstein \n         GAN, in which one needs to do 5 discriminator updates per generator update, DRAGAN-NS and GAN-GP still do 1 \n         discriminator update per generator update.\n      - Using multiple metrics can provide a better overview of how an algorithm is performing, as opposed to just using \n        inception score.\n    We will include an additional section in the paper that includes this discussion.\n\n* Our empirical approach to the paper is not a disregard for the importance of theory, but rather a push for an encompassing theory which is inline with the experimental results in our paper. We prove empirically that the exported regularization techniques work outside their proposed scopes, thus showing that a different theoretical justification is needed.  In addition, we show that a theoretical view of GAN training as divergence minimization is incompatible with empirical results.  Specifically, the NS-GAN through GAN training can converge on data distributions that gradient updates on the underlying equilibrium divergence would not.  We wish to encourage the research community to continue to explore theories compatible with these observations.\n\n* Please also see the takeaways of AnonReviewer3: “As a purely empirical study, it poses more new and open questions on GAN optimization than it is able to answer; providing theoretical answers is deferred to future studies. This is not necessarily a bad thing, since the extensive experiments (both \"toy\" and \"real\") are well-designed, convincing and comprehensible.\"\n\n* To make the paper easier to read, we will move more results to the appendix.\n\n* Regarding the theory of gradient penalties, this is something we do not have a handle on currently. We show here that gradient penalties work better independently of the theoretical justification they were introduced with. Perhaps a future avenue of work would be to see if these gradient penalties are related to work which tries to analyze and stabilize GANs by looking at the properties of the Jacobian of the vector field associated with the game (see https://arxiv.org/pdf/1705.10461.pdf, https://arxiv.org/abs/1706.04156)\n\n* To clarify when NS-GAN will not work, we will perform experiments which change the number of updates in the discriminator, and see how that affects performance of model. We note however that for the toy data experiments (Section 4) we performed 5 discriminator updates per generator update.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1513441914156,"tcdate":1513441914156,"number":4,"cdate":1513441914156,"id":"HkfYcazGz","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Comment","forum":"ByQpn1ZA-","replyto":"SkiI_Bixz","signatures":["ICLR.cc/2018/Conference/Paper521/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper521/Authors"],"content":{"title":"Re: Good point","comment":"Thank you for the review, your comments made the paper more accessible and improves our experiment evaluations on toy data.\n\n* We will replace the l2 distance between the covariance matrices with the Frechet Distance between two Gaussians as used in Heusel et al. (2017) and update our figures accordingly. \n* We will clarify the statement regarding the KL, together with the difference between step and iteration.\n* We will update section 2.3 to ensure that it is more accessible to a wider audience.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1515642460542,"tcdate":1511901266797,"number":3,"cdate":1511901266797,"id":"SkiI_Bixz","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Review","forum":"ByQpn1ZA-","replyto":"ByQpn1ZA-","signatures":["ICLR.cc/2018/Conference/Paper521/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Good point","rating":"7: Good paper, accept","review":"This paper answers recent critiques about ``standard GAN'' that were recently formulated to motivate variants based on other losses, in particular using ideas from optimal transport.  It makes main points\n1) ``standard GAN'' is an ill-defined term that may refer to two different learning criteria, with different properties\n2) though the non-saturating variant (see Eq. 3) of ``standard GAN'' may converge towards a minimum of the Jensen-Shannon divergence, it does not mean that the minimization process follows gradients of the Jensen-Shannon divergence (and conversely, following gradient paths of the Jensen-Shannon divergence may not converge towards a minimum, but this was rather the point of the previous critiques about ``standard GAN''). \n3) the penalization strategies introduced for ``non-standard GAN'' with specific motivations, may also apply successfully to the ``standard GAN'', improving robustness, thereby helping to set hyperparameters.\nNote that item 2) is relevant in many other setups in the deep learning framework and is often overlooked.\n\nOverall, I believe that the paper provides enough material to substantiate these claims, even if the message could be better delivered. In particular, the writing is sometimes ambiguous (e.g. in Section 2.3, the reader who did not follow the recent developments on the subject on arXiv will have difficulties to rebuild the cross-references between authors, acronyms and formulae). The answers to the critiques referenced in the \n paper are convincing, though I must admit that I don't know how crucial it is to answer these critics, since it is difficult to assess wether they reached or will reach a large audience.\n\nDetails:\n- p. 4 please do not qualify KL as a distance metric \n- Section 4.3: \"Every GAN variant was trained for 200000 iterations, and 5 discriminator updates were done for each generator update\" is ambiguous: what is exactly meant by \"iteration\" (and sometimes step elsewhere)? \n- Section 4.3: the performance measure is not relevant regarding distributions. The l2 distance is somewhat OK for means, but it makes little sense for covariance matrices. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1515642460586,"tcdate":1511791187161,"number":2,"cdate":1511791187161,"id":"Sks895Fxz","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Review","forum":"ByQpn1ZA-","replyto":"ByQpn1ZA-","signatures":["ICLR.cc/2018/Conference/Paper521/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A great first study","rating":"4: Ok but not good enough - rejection","review":"Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches. The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments. \n\nClarity: The paper is well-written and clear. The authors could be more concise when reporting results. I would suggest keeping the main results in the main body and move extended results to an appendix.\n\nOriginality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs. More specifically, the provide empirical evidence that they can fit problems where Jensen-Shannon divergence fails. They also show experimentally that penalized gradients stabilize the learning process.\n\nSignificance: The problems the authors consider is worth exploring further. The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally. However, publishing this  work is in my opinion premature for the following reasons:\n\n- The authors do not provide further evidence of why non-saturating GANs perform better or under which mathematical conditions (non-saturating) GANs will be able to handle cases where distribution manifolds do not overlap;\n- The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory;\n- The authors do not provide practical recommendations how to set-up GANs and not that these findings did not lead to a bullet-proof recipe to train them.\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1515642460630,"tcdate":1511555266318,"number":1,"cdate":1511555266318,"id":"SkqpgbLgM","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Review","forum":"ByQpn1ZA-","replyto":"ByQpn1ZA-","signatures":["ICLR.cc/2018/Conference/Paper521/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Well-written experimental study; light on theory; poses new questions and aims to answer some","rating":"7: Good paper, accept","review":"The submission describes an empirical study regarding the training performance\nof GANs; more specifically, it aims to present empirical evidence that the\ntheory of divergence minimization is more a tool to understand the outcome of\ntraining (i.e. Nash equillibrium) than a necessary condition to be enforce\nduring training itself.\n\nThe work focuses on studying \"non-saturating\" GANs, using the modified generator\nobjective function proposed by Goodfellow et al. in their seminal GAN paper, and\naims to show increased capabilities of this variant, compared to the \"standard\"\nminimax formulation. Since most theory around divergence minimization is based\non the unmodified loss function for generator G, the experiments carried out in\nthe submission might yield somewhat surprising results compared the theory.\n\nIf I may summarize the key takeaways from Sections 5.4 and 6, they are:\n- GAN training remains difficult and good results are not guaranteed (2nd bullet\n  point);\n- Gradient penalties work in all settings, but why is not completely clear;\n- NS-GANs + GPs seems to be best sample-generating combination, and faster than\n  WGAN-GP.\n- Some of the used metrics can detect mode collapse.\n\nThe submission's (counter-)claims are served by example (cf. Figure 2, or Figure\n3 description, last sentence), and mostly relate to statements made in the WGAN\npaper (Arjovsky et al., 2017).\n\nAs a purely empirical study, it poses more new and open questions on GAN\noptimization than it is able to answer; providing theoretical answers is\ndeferred to future studies. This is not necessarily a bad thing, since the\nextensive experiments (both \"toy\" and \"real\") are well-designed, convincing and\ncomprehensible. Novel combinations of GAN formulations (non-saturating with\ngradient penalties) are evaluated to disentangle the effects of formulation\nchanges.\n\nOverall, this work is providing useful experimental insights, clearly motivating\nfurther study.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1510536796843,"tcdate":1510536796843,"number":3,"cdate":1510536796843,"id":"BySwLOUkM","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Comment","forum":"ByQpn1ZA-","replyto":"BkUrF7rJG","signatures":["ICLR.cc/2018/Conference/Paper521/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper521/Authors"],"content":{"title":"Applying DRAGAN Gradient Penalty in Your Example","comment":"Thanks for the clarification.  \n\nAnd yes, but just to again reiterate, we are not suggesting that you apply the DRAGAN gradient penalty inside the 1D uniform [-1, 1] region.  Enforcing the gradient norm to be 1 inside here would fail as you described.  You should have no issue fitting this training distribution if you only apply the DRAGAN gradient penalty on the *boundaries* of the data distribution, i.e. -1 + delta^i and +1 + delta^j."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1510451606240,"tcdate":1510451518516,"number":3,"cdate":1510451518516,"id":"BkUrF7rJG","invitation":"ICLR.cc/2018/Conference/-/Paper521/Public_Comment","forum":"ByQpn1ZA-","replyto":"ryDZ5ySJz","signatures":["~Leon_Boellmann1"],"readers":["everyone"],"writers":["~Leon_Boellmann1"],"content":{"title":"Observation","comment":" Dear authors, \n I was using the code shared on the github link in the DRAGAN paper (https://arxiv.org/pdf/1705.07215v1.pdf). Sorry, there was a typo in my last message. Both the discriminator and the generator have 2 layers. The conventional GAN and WGAN works even without regularization. \n\n I strongly agree with the authors that it is very interesting to investigate something that empirically works but is not fully known in theory. I think that is why deep learning is so attractive to so many people including myself. On the other hand, I also think we should investigate something that makes sense. Decades ago, we already established the \"universal approximation\" theorem for neural networks. We know that it can fundamentally fits any continuous function on a compact set. If we know it cannot fit certain functions, say a purely linear network, we would not even start training it to fit high-dimensional complicated functions. \n\nI think my argument is based on this philosophy. We know that the DRAGAN regularization is actually wrong for the original GAN in some cases, because gradient norm should be 0 at the optimal point (for example the uniform [-1,1] example). It may have good results for some applications in training images, however, for some applications (say, autonomous driving) we cannot take any risk in applying an algorithm that does not work in come corner cases. In this case, I would devote more time to investigate other methods that are fundamentally correct, for example DRAGAN regularization on WGAN. \n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1510435326720,"tcdate":1510435326720,"number":2,"cdate":1510435326720,"id":"ryDZ5ySJz","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Comment","forum":"ByQpn1ZA-","replyto":"H1N7QxeJM","signatures":["ICLR.cc/2018/Conference/Paper521/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper521/Authors"],"content":{"title":"DRAGAN Regularization Observations","comment":"Thanks for the comment, Leon.\n\nAs a quick validation of your architectures and your training code, did you first confirm that you were able to fit the [-1,1] uniform distribution with standard GAN or with improved WGAN?  Also, when you say that your discriminator has one layer, I’m assuming you mean it has one hidden layer and is capable of producing non-linear decision boundaries? An affine discriminator would be insufficient.\n\nTo your point about the theoretical correctness of the penalty, the v1 DRAGAN paper (https://arxiv.org/pdf/1705.07215v1.pdf) “How to Train Your DRAGAN” first introduces this regularization penalty onto the *original* GAN discriminator objective (defined as the minimax GAN variant in our paper) as seen in Algorithm 1.  However, this paper actually had an error that their noisy data was not even centered on the original data manifold!  Despite this bug, DRAGAN still succeeded in producing better samples. \n\nThe regularization is not necessarily 'fundamentally wrong'. Instead, it is very counterintuitive that it works, given our current level of theoretical understanding. That means that the empirical results showing that it works are more interesting. Empirical results are mostly useful to science when they are surprising. If we experimented with a method that theory predicts should work and it worked, we would not have learned anything. Our results are surprising because we experimented with a method that the theory does not predict should work and yet it worked. This suggests that the theory is at best incomplete and needs to be revised.\n\nFor your particular experimental issue, the regularization should be applied in a region *around* the real-data manifold, not over the entire real-data region.  If data manifold is 1D, you should not be applying the DRAGAN penalty throughout the entire region of [-1,1], only at the boundaries.  In higher dimensions, these perturbations will almost always be off-manifold. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1510188497230,"tcdate":1510188497230,"number":1,"cdate":1510188497230,"id":"HytAS7W1z","invitation":"ICLR.cc/2018/Conference/-/Paper521/Official_Comment","forum":"ByQpn1ZA-","replyto":"HksiG0DA-","signatures":["ICLR.cc/2018/Conference/Paper521/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper521/Authors"],"content":{"title":"Relating Findings to Earlier Results of Arjovsky et al. (2017)","comment":"Thanks for you comments, Xu.  In response,\n\n1.  Lemma 1 does indeed show that g(z) will be a set of measure 0 in X for dim(Z) < dim(X), however, this does not necessarily imply that it’s impossible to generate samples matching the data manifold.  The authors are simply stating that it is plausible that the manifold that the data lies on and the manifold of points produced by the generator are disjoint in X.  This would imply a perfect discriminator may exist between the manifolds.  Further, if one tried to bring these manifolds together by minimizing a JS-divergence, the gradients would be meaningless. This motivated the authors' later development of a softer distance measure and the Wasserstein GAN.  \n2.  Theorem 2.6 assumes that the noise of D and the gradient of D are decorrelated, which may be too strong of an assumption.  The authors acknowledge this and then show empirical gradient norms while training DCGAN, which grow with training iterations.  However, in practice, one typically does not train the Discriminator for so many iterations and thus one may avoid the extreme variance cautioned with this theorem.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1510109980286,"tcdate":1510109980286,"number":2,"cdate":1510109980286,"id":"H1N7QxeJM","invitation":"ICLR.cc/2018/Conference/-/Paper521/Public_Comment","forum":"ByQpn1ZA-","replyto":"ByQpn1ZA-","signatures":["~Leon_Boellmann1"],"readers":["everyone"],"writers":["~Leon_Boellmann1"],"content":{"title":"Correctness of regularization","comment":"Dear authors, \n\nWe did a simple exercise is to generate a [-1,1] uniform distribution from a Gaussian distribution using GAN with DRAGAN regularization. However, it does not work. What we observed is that D(x) converges to a function with a hump and therefore all the generated samples are concentrated on a small region, instead of uniform distribution. We adopt a sample code from github. The generator has 2 layers and the discriminator has 1 layer.  The lambda is 10.\n\nThe reason is that the regularization term pushes the function to have some slope at the data support, which results in the hump shape. Therefore, the generated samples are mostly concentrated in the region with large D(x).\n\nWe see that some regularizations make sense mathematically:\n- The gradient norm penalty makes sense for WGAN, because the authors in the paper show that \"The optimal critic has unit gradient norm almost everywhere under Pr and Pg\". \n- The application of DRAGAN regularization to WGAN also makes sense because it shows in the paper that there is minor difference between the unit norm argument and the actual application of WGAN-GP, therefore it only applies the gradient penalty in the neighborhood of the data samples. The same holds for the paper of \"On the regularization of Wasserstein GANs\". \n- The regularization term in the paper of \"Stabilizing Training of Generative Adversarial Networks through Regularization\" makes sense because by Taylor expansion, the noise perturbation at the input approximately adds a regularization term at the objective function. And the training with noise is justified theoretically in the paper of \"Towards principled methods for training generative adversarial networks\". \n\nHowever, the application of DRAGAN regularization to the original GAN needs justification. For the original GAN, the optimal D(x) is 1/2 on the data support and hence its gradient is zero. The DRAGAN regularization, however, pushes the gradient norm to 1, which makes the training converge to a wrong value. If we know the regularization is fundamentally and mathematically wrong, why do we investigate its performance?\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1512607802554,"tcdate":1509577378906,"number":1,"cdate":1509577378906,"id":"HksiG0DA-","invitation":"ICLR.cc/2018/Conference/-/Paper521/Public_Comment","forum":"ByQpn1ZA-","replyto":"ByQpn1ZA-","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"How to relate the finding of the paper with the results in [Arjovsky 2017]","comment":" In the paper of \"TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE ADVERSARIAL NETWORKS\" by Arjovsky, they show two results:\n 1. Lemma 1 shows that if the dimension of Z is less than the dimension of X, then g(Z) will be a set of measure 0 in X. This implies that it is almost impossible to generate samples that are similar to true data samples. \n 2. Theorem 2.6 shows that with the non-saturating loss function for the generator, the gradient is of generator has infinite expectation and variance. It implies that using non-saturating loss function is not stable. \n\n In the paper, the authors show that the non-saturating GAN can learn a high dimensional distribution even though the noise is 1-D. This finding seems to be not aligned with the arguments in [Arjovsky 2017]. I would appreciate if the authors could give more intuitive ideas to explain the relation between the experiment results and the theoretical arguments in Arjovsky 2017. Thanks!"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1515712507597,"tcdate":1509125306889,"number":521,"cdate":1510092369116,"id":"ByQpn1ZA-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByQpn1ZA-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step","abstract":"Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players’ parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.","pdf":"/pdf/3e3acfdb49a852a93a0d5671f523e0c141ff519f.pdf","TL;DR":"We find evidence that divergence minimization may not be an accurate characterization of GAN training.","paperhash":"anonymous|many_paths_to_equilibrium_gans_do_not_need_to_decrease_a_divergence_at_every_step","_bibtex":"@article{\n  anonymous2018many,\n  title={Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByQpn1ZA-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper521/Authors"],"keywords":["Deep learning","GAN"]},"nonreaders":[],"replyCount":14,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}