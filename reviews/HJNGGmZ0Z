{"notes":[{"tddate":null,"ddate":null,"tmdate":1513973079926,"tcdate":1513973079926,"number":8,"cdate":1513973079926,"id":"H1xPSyofM","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Comment","forum":"HJNGGmZ0Z","replyto":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference/Paper1140/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1140/Authors"],"content":{"title":"Updates","comment":"We have updated the paper with these salient changes: \n\n* re-written introduction\n* updated results with SPICE\n* updated sections 4.3, 4.4 and 4.5 with more support to claims \n* re-written conclusion "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1513934137551,"tcdate":1513933218185,"number":7,"cdate":1513933218185,"id":"rkqoKB9fz","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Comment","forum":"HJNGGmZ0Z","replyto":"BybWlKXgz","signatures":["ICLR.cc/2018/Conference/Paper1140/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1140/Authors"],"content":{"title":"on specific comments","comment":"> Main claim of the paper Devlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly. Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier. Metrics for evaluation\n\nOnce again, we use the object representations as a tool for our investigation. Our aim is not to improve on the task.\n\n>  - Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions. The COCO leaderboard also uses this metric as one of its evaluation metrics. If the authors are evaluating on the test set and reporting numbel rs, then it is odd that they `skipped' reporting SPICE numbers.\n\nWe have answered this before. We note however that our observations are also consistent with the numbers on the SPICE metric. \n\n>  Choice of Datasets - If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important. Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO. These datasets should show whether using more general features (YOLO-9k) helps. The authors should evaluate on these datasets to make their findings stronger and more valuable.\n\nSIND represents a very different type of data where sentences compose a narrative. Different kinds of models are needed and these are evaluated using different metrics. Visual Genome, on the other hand, is a subset of MSCOCO with different kind of annotations (object specific captions). We are interested in investigating the CNN-LSTM model in this paper, and while it may be applied to a different domain of the same task (e.g. image captioning on Flickr30k), it is not clear how this can be applied directly to a different set of tasks.\n\n\n> Minor comments - Figure 1 is hard to read on paper. Please improve it. - Figure 2 is hard to read even on screen. It is really interesting, so improving the quality of this figure will really help.\n\nWe have enlarged the Figure 1.\n\nWe initially planned to add the full, high-resolution versions of Figure 2 in the appendix. Unfortunately each t-SNE visualisation was around 18MB -- which will increase the file size to over 100MB if we were to add all images (3 pairs before-after projection). We have added an anonymised external link in the updated version of the paper. The images can now be found here: https://github.com/anonymousiclr/HJNGGmZ0Z\n "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1513933192607,"tcdate":1513933192607,"number":6,"cdate":1513933192607,"id":"HybctS5zf","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Comment","forum":"HJNGGmZ0Z","replyto":"BybWlKXgz","signatures":["ICLR.cc/2018/Conference/Paper1140/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1140/Authors"],"content":{"title":"on specific comments","comment":"> - The introduction to the paper could be made clearer\n\nWe have updated the introduction to make it clearer.\n\n> the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later.\n\nIn our analysis we observed that in all cases, i.e., using any type of representation, there is only a small subset (20-30%) of the captions that are unique. This was mentioned in section 4.5 of our original submission of the paper. We have further clarified this section in the updated version.\n\n\n> The introduction also states that the authors will propose ways to improve image captioning. This is never discussed.\n\nWe do not promise to do that, but rather state that findings could help improve image captioning systems.\n\n>  Captioning Model and Table 1 - The authors use greedy (argmax) decoding which is known to result in repetitive captions. In fact, Vinyals et al. note this very point in their paper. I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting.\n>  In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning\n\nThis was purposefully done for determinism. We wanted to understand the best 'choice of words' by the model given a particular representation. \n\n\n> The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22.  Similar trend holds for other metrics like BLEU-4.\n\nOur model should be compared with the Neuraltalk model as it has the same settings. Other similar models (like Vinyals et al 2015) use ensembles and other engineering tricks that we are not interested in. \n\n> - The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding. Experimental Setup and Training Details - How was the model optimized? No training details are provided. Did you use dropout? Were hyperparameters fixed for training across different feature sizes of VGG19 and ResNet-152? What is the variance in the numbers for Table 1?\n\nOur settings are: \nLSTM with 128 dimensional word embeddings and 256 dimensional hidden representations\nDropout over LSTM of 0.8\nAdam for optimization. \nLearning rate = 4e-4\nWe’ll add the variance figures to an improved version of the paper.\n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1513932789759,"tcdate":1513932789759,"number":5,"cdate":1513932789759,"id":"SkCgOrcGG","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Comment","forum":"HJNGGmZ0Z","replyto":"r1fNl7Cgz","signatures":["ICLR.cc/2018/Conference/Paper1140/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1140/Authors"],"content":{"title":"on specific comments","comment":"> The paper lacks novelty, just reports some results without proper analysis or insights.\n> Main weakness of the paper:\n> - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results)\n\nWe stress that our evaluations are with respect to the model proposed by Karpathy et al 2015. Our goal is not to 'beat or break' systems but to understand the 'whys' and 'hows'.\n\n>  - According to \"SPICE: Semantic Propositional Image Caption Evaluation\" current metrics used in image captioning don't correlate with human judgement.\n\nWe are not claiming explicitly that any of the metrics has good correlation with human judgements. As we mentioned before our focus on CIDEr is because a) the official evaluation script from MSCOCO  contains only CIDEr, Meteor, BLEU and ROUGE, b) CIDEr is a metric that was officially developed for the task of image captioning, c) CIDEr is the official metric for MSCOCO, d) papers by Liu et al 2017, Kilickaya et al. 2017 and Vedantam et al, 2015 (with the human correlation experiments over Flickr8k dataset) still state the importance of CIDEr as a metric for image captioning. We further note that we observe a similar trend as we found in CIDEr, so all our observations are still valid. \n\n\n>  - Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016). Therefore correlation of the image features with the captions is weaker that it could be.\n\nWhile it is true that fine-tuning could have been helpful to bump performance, our paper deals with an exploration of representational properties. Vinyals et al. 2016 has shown that fine-tuning gives only a minor 1-point improvement for BLEU. This is also using an ensemble of models. We again state that our experiments are about understanding image captioning models.\n\n> - To provide a fair comparison, authors should compare their results with other paper results. - Tables 2 and 3 are missing the original baselines.\n\nWe will add the results from the comparable papers, even though our focus is not comparisons or to show performance improvements over other models. However, we do not understand what the reviewer means by “original baselines”. Could you please clarify?\n\n> The evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference.\n\nPlease see answer above regarding metrics. In addition, our focus is not to improve the performance of the system, but to interpret the 'how' and 'why' of the system. To this end, we have made significant progress.\n\n>  - Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3.\n\nWe have not chosen Vinyals et al. 2016 since it uses ensembles and other clever engineering tricks. This would make it hard to answer the questions we ask in this paper -- namely, the contribution of image representation. Our results are comparable to those in Karpathy et al, 2015. We will add this into the table.\n\n> - The claim \"The bag of objects model clusters these group the best\" is not supported by any evidence or metric.\n\nWe believe that the reviewer has misunderstood the sentence. This sentence explains the observations in Figure 1 (more specifically Figure 1a). The figure shows that the bag of objects representation forms better clusters. It shows the cosine distances between each group for the bag of objects representation. We see from the figure that the bag of objects representations clusters these groups best. For example, the average image representation of “dog” correlates with images containing “dog” as a pair like “dog+person” and “dog+toilet”. We are aware that this is true for our given example, however we expect this to extrapolate over other examples in the dataset. \n\n\n\n> One interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task.\n\nWe will do it as a future work, even though this does not allow us to answer our questions posed in this paper.  \n\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1513932533461,"tcdate":1513932500810,"number":4,"cdate":1513932500810,"id":"Hk60UB5GM","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Comment","forum":"HJNGGmZ0Z","replyto":"By5_q5y-z","signatures":["ICLR.cc/2018/Conference/Paper1140/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1140/Authors"],"content":{"title":"on comments","comment":"> Overall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings.\n> The main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact.\n\nOur claim is in the novel 'insights' into end-to-end model of image captioning models. Our empirical evaluations with multiple representations, visualizations and out of domain experiments reveal new and important insights that should be of interest to the community.\n\nWe kindly ask clarification from the reviewer regarding what is meant by 'novelty in experimental methodology'. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1513932409643,"tcdate":1513932409643,"number":3,"cdate":1513932409643,"id":"rJGFUBqfM","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Comment","forum":"HJNGGmZ0Z","replyto":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference/Paper1140/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper1140/Authors"],"content":{"title":"Rebuttal","comment":"We thank the reviewers for the comments. \n\nOur submission is based on the simple end-to-end model as proposed by Karpathy et al 2015. We use this model because its simplicity makes it easier to focus on the image component. We are interested in the interpretability of the image-captioning system rather than the performance on the task. In addition, more advanced models can be considered similar variants of Karpathy et al 2015. We do not claim novelty with respect to the captioning model. Instead, our submission presents novel insights into the image captioning task which we are confident that it should be of interest to the community. Also as our submission involves work on understanding the representational contributions, we consider our work highly relevant to the conference on learning representations. Our main contributions are:\n\n1) We show that the image-conditioned language model implicitly learns and exploits a joint image representation and language semantic space instead of actually understanding images (sections 4.2, 4.3, 4.4). \n\n2) Our experiments with factorized and compressed image embeddings (section 4.1) reveals that the models do not benefit from the full representational space. We observe that the performance of the model trained with a 2048 dimensional representation is nearly identical to the performance of the model trained with a compressed 80-dimensional representation virtually resulting in ‘no information loss’. \n\n3) The experiments with pseudorandom representations (section 3.2) reveal that the end-to-end models learn to separate structure from noisy representations in the framework and exploit it to produce near ideal performance, i.e., the performance with structured representations versus the performance with noisy representations is similar. \n\n\nThe reviewers also raised concern regarding the absence of SPICE as a metric for evaluation. We focus on CIDEr because: a) the metrics in the official evaluation script from MSCOCO contains support for only CIDEr, Meteor, BLEU and ROUGE; b) CIDEr is a metric that was officially developed for the task of image captioning, and is supposed to be the official metric for MSCOCO; c) papers by Liu et al 2017, Kilickaya et al. 2017 and Vedantam et al, 2015 (with the human correlation experiments over Flickr8k dataset) still state the importance of CIDEr as a metric for image captioning. However, we will provide the results on SPICE in the revised version. We also note that a similar trend is observed with SPICE. \n\n* Liu et al. (ICCV 2017) Improved Image Captioning via Policy Gradient Optimization of SPIDEr\n* Kilickaya et al. (EACL 2017) Re-evaluating Automatic Metrics for Image Captioning\n* Vedantam et al. (CVPR 2015) CIDEr: Consensus-based Image Description Evaluation\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1513398895534,"tcdate":1513398895534,"number":1,"cdate":1513398895534,"id":"S1P_G7Mfz","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Public_Comment","forum":"HJNGGmZ0Z","replyto":"HJNGGmZ0Z","signatures":["~Abhisek_Konar1"],"readers":["everyone"],"writers":["~Abhisek_Konar1"],"content":{"title":"Report on reproducibility of the paper","comment":"In this report, the ﬁndings of this paper submitted to the ICLR 2018 Conference were attempted to be replicated. In the process of replication, two major components were identiﬁed. The ﬁrst breakdown included building the baseline model. The second subsection contained the core of the research, which was to answer the key questions and address which image transformations affected the accuracy of neural image captioning systems. Following the steps outlined in the paper as closely as possible, we were able to build a very similar baseline model, and perform three of the ﬁve image transformations that were speciﬁed. \nThe base line model used in the paper is a combination of the approaches of Karpathy [1] and Vinyals [2]. We were able to closely replicate that model by breaking down it into 3 subsets, a combination of an image model and a language model with a CNN used as an encoder of the images, and an LSTM for the language model as mentioned in the paper. \nFor the image transformations, we were able to successfully reproduce three out of the ﬁve: penultimate layer extraction, class prediction vector, and object-class word embeddings. For the penultimate layer extraction, we implemented the pretrained VGG19 and ResNet152 models. The VGG19 uses very small convolutional ﬁlters and uses very deep weight layers of up to 19. The ResNet152 model, as implemented by He et al.[9], uses 8 times deeper nets than VGG19. Both the models were implemented via the Keras distribution with a TensorFlow backend. \nThe class prediction vector transformation involved investigating more complex image representations, where the vector elements are now estimated posterior probabilities of the possible object categories. To obtain these posterior distribution vectors, the pre-trained network ResNet152 was again used to retrieve a 1000 dimensional posterior vector. \nThe last transformation we were able to replicate was the object-class word embeddings. This procedure is carried out over the entire 1000 dimensional output of the Softmax layer of pre-trained model ResNet152 where all the procured word2vec representations are ﬁnally averaged. This averaged vector acts as the image representation for the image model. \nThe evaluation metric used for the score calculation nltk based corpus BLEU introduced by Papineni et al. [12]. Using a beam size of 1, as done by the authors, a steady rise was observed in the corpus BLEU score for all three representations. Penultimate layer and softmax implementations outperformed the word2vec image representation which had BLEU scores ranging between 0.7540 and 0.4646 from BLEU-2 to BLEU4. For both penultimate and softmax image representations, ResNet152 performed better than VGG19 with BLEU scores ranging from 0.5598 to 0.9216 for softmax and 0.5889 to 0.8937 for penultimate with BLEU varying from 4 to 1. It was only marginally better than VGG19’s BLEU 4-1 scores ranging between 0.5346 and 0.9158 for softmax and 0.5962 and 0.8524 for penultimate. \nOne caveat of this report was that it was not feasible to train the model on the MSCOCO dataset as the paper. This was due to computational restrictions, as training a model on the Flickr8K dataset, which is much smaller than the MSCOCO dataset, took a K80 equipped server approximately 2 days for a small batch size. Due to the inability to use the MSCOCO, we experienced two drawbacks during the replication; The ﬁrst included hindering our ability to implement the 4th and 5th image transformations, and the second was fact that we were not able to reproduce an exact copy of the works presented by the authors. Although we used a different dataset, we still noticed similar trends in the ones obtained by the tests carried out in the MSCOCO dataset. For example, both our tests and the original authors’ test both had the ResNet152 pre-trained network slightly outperforming the VGG19 network in the different image transformations.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1515642389104,"tcdate":1512184433607,"number":3,"cdate":1512184433607,"id":"By5_q5y-z","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Review","forum":"HJNGGmZ0Z","replyto":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference/Paper1140/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Clear rejection.","rating":"4: Ok but not good enough - rejection","review":"This paper is an experimental paper. It investigates what sort of image representations are good for image captioning systems. \n\nOverall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings.\n\nThe main issue of the paper is the lack of novelty. Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact. Unfortunately, I do not see any novel concept in the experimental setup.\n\nI recomend this paper for a workshop presentation.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1515642389148,"tcdate":1512087594283,"number":2,"cdate":1512087594283,"id":"r1fNl7Cgz","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Review","forum":"HJNGGmZ0Z","replyto":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference/Paper1140/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Not clear contributions, lack of comparisons with other methods and weak results.","rating":"4: Ok but not good enough - rejection","review":"The paper claims that image captioning systems work so well, while most recent state of the art papers show that they produce 50% errors, so far from perfect.\n\nThe paper lacks novelty, just reports some results without proper analysis or insights.\n\nMain weakness of the paper:\n - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results)\n - According to \"SPICE: Semantic Propositional Image Caption Evaluation\" current metrics used in image captioning don't correlate with human judgement.\n- Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016). Therefore correlation of the image features with the captions is weaker that it could be.\n- The experiments reported in Table1 are way below state-of-the-art results, there a tons of previous work with much better results, see https://competitions.codalab.org/competitions/3221#results\n - To provide a fair comparison authors, should compare their results with other paper results.\n - Tables 2 and 3 are missing the original baselines.\nThe evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference.\n- Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3. \n- The claim \"The bag of objects model clusters these group the best\" is not supported by any evidence or metric.\n\nOne interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task.\n\n\nTypos:\n - synsest-level -> synsets-level","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1515642389189,"tcdate":1511391224608,"number":1,"cdate":1511391224608,"id":"BybWlKXgz","invitation":"ICLR.cc/2018/Conference/-/Paper1140/Official_Review","forum":"HJNGGmZ0Z","replyto":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference/Paper1140/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Needs work","rating":"4: Ok but not good enough - rejection","review":"This paper analyzes the effect of image features on image captioning. The authors propose to use a model similar to that of Vinyals et al., 2015 and change the image features it is conditioned on. The MSCOCO captioning and Flickr30K datasets are used for evaluation.\n\nIntroduction\n- The introduction to the paper could be made clearer - the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later.\n- The introduction also states that the authors will propose ways to improve image captioning. This is never discussed.\n\nCaptioning Model and Table 1\n- The authors use greedy (argmax) decoding which is known to result in repetitive captions. In fact, Vinyals et al. note this very point in their paper. I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting. In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning - Table 1 shows meteor scores within 0.19 - 0.22 for all methods.\n- Another effect (possibly due to greedy decoding + choice of model), is that the numbers in Table 1 are rather low compared to the COCO leaderboard. The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22. Similar trend holds for other metrics like BLEU-4.\n- The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding.\n\nExperimental Setup and Training Details\n- How was the model optimized? No training details are provided. Did you use dropout? Were hyperparamters fixed for training across different feature sizes of VGG19 and ResNet-152? What is the variance in the numbers for Table 1?\n\nMain claim of the paper\nDevlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly. Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier.\n\nMetrics for evaluation\n- Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions. The COCO leaderboard also uses this metric as one of its evaluation metrics. If the authors are evaluating on the test set and reporting numbers, then it is odd that they `skipped' reporting SPICE numbers.\n\nChoice of Datasets\n- If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important. Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO. These datasets should show whether using more general features (YOLO-9k) helps.\nThe authors should evaluate on these datasets to make their findings stronger and more valuable.\n\nMinor comments\n- Figure 1 is hard to read on paper. Please improve it.\n- Figure 2 is hard to read even on screen. It is really interesting, so improving the quality of this figure will really help.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]}},{"tddate":null,"ddate":null,"tmdate":1513938088973,"tcdate":1509138956347,"number":1140,"cdate":1510092359527,"id":"HJNGGmZ0Z","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"HJNGGmZ0Z","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"What is image captioning made of?","abstract":"We hypothesize that end-to-end neural image captioning systems work seemingly well because they exploit and learn ‘distributional similarity’ in a multimodal feature space, by mapping a test image to similar training images in this space and generating a caption from the same space. To validate our hypothesis, we focus on the ‘image’ side of image captioning, and vary the input image representation but keep the RNN text generation model of a CNN-RNN constant. We propose a sparse bag-of-objects vector as an interpretable representation to investigate our distributional similarity hypothesis. We found that image captioning models (i) are capable of separating structure from noisy input representations; (ii) experience virtually no significant performance loss when a high dimensional representation is compressed to a lower dimensional space; (iii) cluster images with similar visual and linguistic information together; (iv) are heavily reliant on test sets with a similar distribution as the training set; (v) repeatedly generate the same captions by matching images and ‘retrieving’ a caption in the joint visual-textual space. Our experiments all point to one fact: that our distributional similarity hypothesis holds. We conclude that, regardless of the image representation, image captioning systems seem to match images and generate captions in a learned joint image-text semantic subspace.\n","pdf":"/pdf/3de772b9d6ac6ce3b96255f77fb15d6827d67321.pdf","TL;DR":"This paper presents an empirical analysis on the role of different types of image representations and probes the properties of these representations for the task of image captioning.","paperhash":"anonymous|what_is_image_captioning_made_of","_bibtex":"@article{\n  anonymous2018what,\n  title={What are image captions made of?},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=HJNGGmZ0Z}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1140/Authors"],"keywords":["image captioning","representation learning","interpretability","rnn","multimodal","vision to language"]},"nonreaders":[],"replyCount":10,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}