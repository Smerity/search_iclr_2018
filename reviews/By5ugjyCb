{"notes":[{"tddate":null,"ddate":null,"tmdate":1515189013592,"tcdate":1515188359713,"number":9,"cdate":1515188359713,"id":"SJgql_6Xf","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Comment","forum":"By5ugjyCb","replyto":"BkgW-ZteG","signatures":["ICLR.cc/2018/Conference/Paper154/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper154/Authors"],"content":{"title":"Significant portion of new materials are added to support novelty and demonstrate superiority of PACT. Please share your thoughts.","comment":"We thank Reviewer2 for time and effort for reviewing our paper.\n\nWe have updated our draft with the following significant changes in contents (all the changes are colored in blue):\n\n1) We added accuracy comparison between PACT and DoReFa-Net (state-of-the-art in quantized neural network) for ResNet-50 (in Table 1) to demonstrate superiority of PACT. Notice that PACT outperforms DoReFa-Net with > 5% higher accuracy. This confirms that PACT enables no accuracy degradation for the quantized ResNet-50, which was not achievable by previous state-of-the-art activation quantization schemes. Also note that this superior accuracy is achieved without ANY tuning in hyper-parameters; the same hyper-parameters are used in both baseline and the PACT experiments and the networks are trained from scratch. This indicates that Deep Learning practitioners can simply replace ReLU with PACT to achieve robust accuracy when quantizing activation of their neural networks. This claim is supported by our extensive experimental results in Section 5 and Appendix E.\n\n2) We added two sections in the Appendix to explain why PACT is superior than both ReLU and Clipping activation function for quantized neural networks. In Appendix A, we show theoretical analysis that PACT is as expressive as ReLU when it is used as an activation function. Furthermore,  we explain in Appendix B that PACT finds a balancing point between clipping and quantization errors to minimize their impact to classification accuracy. This analysis demonstrates novelty of PACT as a superior activation function for quantized neural network.\n\n3) We reflect comments from Reviewer 1 in the draft. We merged the contribution statements in Section 1 (Introduction), and clarified experimental settings to highlight that the baseline networks use ReLU as described in the references, and the PACT experiments use the identical hyper-parameters as the baseline, except that the activation function is replaced from ReLU to PACT. \n\nPlease read the updated draft and share your thoughts. Especially, we are curious about in which aspect the reviewer thinks that our paper lacks novelty. Any detail comments would be very appreciated for improving our paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1515189046205,"tcdate":1515188315925,"number":8,"cdate":1515188315925,"id":"r1VDedaXz","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Comment","forum":"By5ugjyCb","replyto":"B1wlzrslf","signatures":["ICLR.cc/2018/Conference/Paper154/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper154/Authors"],"content":{"title":"Significant portion of new materials are added to support novelty and demonstrate superiority of PACT. Please share your thoughts.","comment":"We thank Reviewer1 for time and effort for reviewing our paper.\n\nWe have updated our draft with the following significant changes in contents (all the changes are colored in blue):\n\n1) We reflect your comments in the draft. We merged the contribution statements in Section 1 (Introduction), and clarified experimental settings to highlight that the baseline networks use ReLU as described in the references, and the PACT experiments use the identical hyper-parameters as the baseline, except that the activation function is replaced from ReLU to PACT. \n\n2) We added accuracy comparison between PACT and DoReFa-Net (state-of-the-art in quantized neural network) for ResNet-50 (in Table 1) to demonstrate superiority of PACT. Notice that PACT outperforms DoReFa-Net with > 5% higher accuracy. This confirms that PACT enables no accuracy degradation for the quantized ResNet-50, which was not achievable by previous state-of-the-art activation quantization schemes. Also note that this superior accuracy is achieved without ANY tuning in hyper-parameters; the same hyper-parameters are used in both baseline and the PACT experiments and the networks are trained from scratch. This indicates that Deep Learning practitioners can simply replace ReLU with PACT to achieve robust accuracy when quantizing activation of their neural networks. This claim is supported by our extensive experimental results in Section 5 and Appendix E.\n\n3) We added two sections in the Appendix to explain why PACT is superior than both ReLU and Clipping activation function for quantized neural networks. In Appendix A, we show theoretical analysis that PACT is as expressive as ReLU when it is used as an activation function. Furthermore,  we explain in Appendix B that PACT finds a balancing point between clipping and quantization errors to minimize their impact to classification accuracy. This analysis demonstrates novelty of PACT as a superior activation function for quantized neural network.\n\nPlease read the updated draft and share your thoughts. Any comments would be very appreciated for improving our paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1515189076037,"tcdate":1515188230776,"number":7,"cdate":1515188230776,"id":"HJ1GluT7z","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Comment","forum":"By5ugjyCb","replyto":"S1-ToUJWz","signatures":["ICLR.cc/2018/Conference/Paper154/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper154/Authors"],"content":{"title":"Significant portion of new materials are added to support novelty and demonstrate superiority of PACT. Please share your thoughts. ","comment":"We thank Reviewer3 for time and effort for reviewing our paper.\n\nWe have updated our draft with the following significant changes in contents (all the changes are colored in blue):\n\n1) We added accuracy comparison between PACT and DoReFa-Net (state-of-the-art in quantized neural network) for ResNet-50 (in Table 1) to demonstrate superiority of PACT. Notice that PACT outperforms DoReFa-Net with > 5% higher accuracy. This confirms that PACT enables no accuracy degradation for the quantized ResNet-50, which was not achievable by previous state-of-the-art activation quantization schemes. Also note that this superior accuracy is achieved without ANY tuning in hyper-parameters; the same hyper-parameters are used in both baseline and the PACT experiments and the networks are trained from scratch. This indicates that Deep Learning practitioners can simply replace ReLU with PACT to achieve robust accuracy when quantizing activation of their neural networks. This claim is supported by our extensive experimental results in Section 5 and Appendix E.\n\n2) We added two sections in the Appendix to explain why PACT is superior than both ReLU and Clipping activation function for quantized neural networks. In Appendix A, we show theoretical analysis that PACT is as expressive as ReLU when it is used as an activation function. Furthermore,  we explain in Appendix B that PACT finds a balancing point between clipping and quantization errors to minimize their impact to classification accuracy. This analysis demonstrates novelty of PACT as a superior activation function for quantized neural network.\n\n3) We reflect comments from Reviewer 1 in the draft. We merged the contribution statements in Section 1 (Introduction), and clarified experimental settings to highlight that the baseline networks use ReLU as described in the references, and the PACT experiments use the identical hyper-parameters as the baseline, except that the activation function is replaced from ReLU to PACT. \n\nPlease read the updated draft and share your thoughts. Especially, we are curious about in which aspect the reviewer thinks that our paper lacks novelty. Any detail comments would be very appreciated for improving our paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1513077074024,"tcdate":1513077074024,"number":6,"cdate":1513077074024,"id":"S1cLKVpZG","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Comment","forum":"By5ugjyCb","replyto":"BkgW-ZteG","signatures":["ICLR.cc/2018/Conference/Paper154/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper154/Authors"],"content":{"title":"PACT UNIVERSALLY outperforms ReLU based activation quantization schemes","comment":"Thank you for your review and showing interest to our work. To answer your question on the novelty of PACT, we put a detail response in the first comment above. \n\nAnd here's a brief summary: We claim that PACT is a new activation function that is best suitable for activation quantization. We claim that (1) PACT demonstrates (for the first time) no-accuracy-degraded 4-bit quantization (both weight and activation) for challenging ResNet-50 for ImageNet dataset, and (2) PACT UNIVERSALLY outperforms ReLU based activation quantization schemes for all the CNN models we tested.\n\nTo better explain why PACT outperforms ReLU based activation quantization schemes, we newly added Appendix A and B for deeper analysis of PACT. We showed that (1) PACT is as expressive as ReLU, and (2) PACT balances clipping and quantization errors when activation is quantized. \n\nAlso, please note that all the robust accuracies we achieved with PACT do NOT require any modification in the original hyper-parameters and network structures the baselines use.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1513076864848,"tcdate":1513076566442,"number":5,"cdate":1513076566442,"id":"Bk08vE6ZM","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Comment","forum":"By5ugjyCb","replyto":"S1-ToUJWz","signatures":["ICLR.cc/2018/Conference/Paper154/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper154/Authors"],"content":{"title":"PACT UNIVERSALLY outperforms ReLU based activation quantization schemes","comment":"Thank you for your review and showing interest to our work. To answer your question on the novelty of PACT, we put a detail response in the first comment above. \n\nAnd here's a brief summary: We claim that PACT is a new activation function that is best suitable for activation quantization. We claim that (1) PACT demonstrates (for the first time) no-accuracy-degraded 4-bit quantization (both weight and activation) for challenging ResNet-50 for ImageNet dataset, and (2) PACT UNIVERSALLY outperforms ReLU based activation quantization schemes for all the CNN models we tested.\n\nTo better explain why PACT outperforms ReLU based activation quantization schemes, we newly added Appendix A and B for deeper analysis of PACT. We showed that (1) PACT is as expressive as ReLU, and (2) PACT balances clipping and quantization errors when activation is quantized. \n\nAlso, please note that all the robust accuracies we achieved with PACT do NOT require any modification in the original hyper-parameters and network structures the baselines use.\n\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1513075706607,"tcdate":1513075706607,"number":4,"cdate":1513075706607,"id":"HkQb4VabM","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Comment","forum":"By5ugjyCb","replyto":"B1wlzrslf","signatures":["ICLR.cc/2018/Conference/Paper154/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper154/Authors"],"content":{"title":"Experiments on PACT show a clear trend that it outperforms ReLU based activation quantization schemes","comment":"Thank you for the detail comments. Here are our answers:\n\nQ1. Over-claim that PACT’s accuracy degradation is much less than others?\nA1. There are two aspects to consider for PACT's accuracy degradation. \nFirst, PACT outperforms (in terms of accuracy degradation) for all the bit-width configuration we compared, demonstrating superior robustness of PACT compared to the other quantization schemes. This clear trend can be seen in Tables 3-8, where the bold numbers indicate the one with lowest accuracy degradation for each column. We added Appendix A and B to analyze why PACT can outperform ReLU based activation quantization schemes.\n\nSecond, PACT's accuracy degradation is much lower for the challenging activation quantization (e.g., quantizing activation of binary/ternary weight networks) for ResNet-50. For example, as shown in Table 7, accuracy degradations for HWGQ and FGQ are 11.4% and 6.7%, respectively, whereas PACT's accuracy degradations are 9.1% and 2.4% for the same bit-precision. This gap in accuracy degradation becomes even larger when PACT is compared to the LPBN technique. In case of 3-bit activation with full-precision weight, LPBN's accuracy degradation is 19.9%, whereas PACT's accuracy degradation is only 1.4%. \n\n\nQ2. Baseline uses Clipping activation function?\nA2. No, our full-precision baselines use the same activation function (i.e., ReLU) as the network structure is proposed in the original paper. Tables 3-8 show that the accuracies for our full-precision baselines are comparable to the full-precision reference of the other work we compared. We will clarify this more in Section 5 and Appendix D.\n\n\nQ3. Do not separate contribution for “Why PACT works” with “PACT”\nA3. Thanks for the suggestion. We will merge the first two contributions to one. Furthermore, we now include enhanced analysis on PACT in Appendix A and B to provide deeper understanding about why PACT outperforms previous ReLU based activation quantization schemes.\n\n   "},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1513074588630,"tcdate":1513074385886,"number":3,"cdate":1513074385886,"id":"By90AXpZM","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Comment","forum":"By5ugjyCb","replyto":"rJ1FCma-z","signatures":["ICLR.cc/2018/Conference/Paper154/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper154/Authors"],"content":{"title":"(Cont'd) PACT needs NO change in hyper-parameters from baseline","comment":"Furthermore, we want to emphasize that PACT's robust accuracy is achieved WITHOUT any changes to the original model, except that ReLU is replaced with PACT. In other words, we used the same hyper-parameters (learning rate schedules, weight initialization, mini batch size, optimizers (ADAM or SGD with momentum), etc.) as well as the original models and network structures in all of our experiments. Furthermore, all of the training was done from scratch, showing that this work does not require any pre-trained weights for good initialization, or any warm-start or larger number of training epochs.    \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1513074294552,"tcdate":1513074294552,"number":1,"cdate":1513074294552,"id":"rJ1FCma-z","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Comment","forum":"By5ugjyCb","replyto":"By5ugjyCb","signatures":["ICLR.cc/2018/Conference/Paper154/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper154/Authors"],"content":{"title":"Response to the question about novelty of PACT","comment":"The authors thank reviewers for their contribution to improving this paper.\n\nWe'd like to highlight the following 3 novel aspects of our PACT paper and we're hoping this communicates to the reviewers the significance of our work:\n\n(A) NO loss of accuracy during quantization:  Over the past 3 years, there has been a tremendous amount of work focused on quantization (binarization / ternarization etc.) of neural networks. Most of these publications focused on applying these techniques to simpler networks (based on the CIFAR10, SVHN and MNIST datasets) where they reported little loss of accuracy. However, in cases when the same exact techniques were applied to larger models (based on the ImageNet dataset), significant loss of accuracy has been reported - leading us to conclude that all previous quantization techniques ONLY work when there is significant redundancy in the model and do not scale well to state of the art networks.  \n\nAs Table 7 shows, our work is the first paper that shows state-of-the-art accuracy (<0.5% Top-1 accuracy degradation and slight IMPROVEMENT in Top-5 accuracy) using 4-bit quantizations for both weights and activations for ResNet-50 for ImageNet dataset. Furthermore, our work shows robust accuracy for ternary and binary weight network (with 4 and 2-bit activation, respectively), when all previous techniques showed significantly (2.3 ~ 4.2% Top-1) more accuracy degradation. \n\nThis is critically important since a significant number of models in Medical Imaging [1], Automotive [2] and other domains are based on transfer learning applied to ResNet like models (based on ImageNet) - and preserving accuracy is extremely critical in these domains due to its direct impact on safety and human life. Allowing 4-bit quantizations to work with the same level of accuracy, as discussed in depth in this paper (Section 6), also allows 2X improvement in Inference/Watt throughput in co./mparison to state-of-the-art 8-bit models - which is critical for power-constrained mobile, IoT and even Cloud hardware devices.\n[1] Litjens, Geert, et al. \"A survey on deep learning in medical image analysis.\" arXiv preprint arXiv:1702.05747 (2017).\n[2] Adam Grzywaczewski, “Training AI for Self-Driving Vehicles: the Challenge of Scale,” in https://devblogs.nvidia.com/parallelforall/training-self-driving-vehicles-challenge-scale/\n\n\n(B) Second, PACT shows superior performance, quantified in terms of accuracy degradation, for all the bit-configurations and networks tested in comparison to seven state-of-the-art quantization publications. From Tables 3-8, we can observe that the accuracy degradation (averaged over all the bit-configurations) of the compared publications for AlexNet, ResNet18, and ResNet50 are 6.1%, 5.1% and 8.1% (Top-1), respectively. In contrast, PACT's accuracy degradation (averaged across all the bit-configurations) is -0.2% (i.e., achieving slightly better accuracy than reference), 3.1% and 2.7%. (Tables 3-8 also highlight in bold which scheme achieves the lowest accuracy degradation for each bit-configuration.) \n\nThis showcases the reliability of PACT for quantization. Please note that large degradation in accuracy nullifies the use of large scale DNNs - rendering previous techniques largely unusable in most scenarios. For example, ResNet-18 takes 3.6B Flops to achieve 72.12% Top-1 accuracy, whereas ResNet-50 takes 7.6B Flops to achieve 77.15% Top-1 accuracy. Thus it's better to use ResNet-18 if accuracy degradation is >3% for ResNet-50.    \n\n\n(C) Third, PACT has a unique characteristic that balances clipping and quantization errors when quantizing activation. We newly added a deeper analysis on why PACT outperforms other activation functions for activation quantization in Appendix A and B. In Appendix A, we showed the expressivity of PACT that it can be trained via SGD to tune the clipping levels properly in order to produce the output that the same network with ReLU would produce. This tuning capability was validated with the CIFAR10-ResNet20 experiment shown in Fig. 7 that PACT based networks could converge to almost identical training curves in comparison to the ReLU based network. \n\nIn Appendix B, we further explained why PACT provides robustness to activation quantization. We first observed that when activation is quantized, there is a trade-off between clipping and quantization errors depending on the clipping level (Fig. 8a). We demonstrated that PACT auto-tunes the clipping level during training to achieve optimal accuracy under activation quantization (Fig. 8b). Since PACT does not require sweeping to obtain the right clipping level, this is a very computationally feasible way.     \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1515642399769,"tcdate":1512168376891,"number":3,"cdate":1512168376891,"id":"S1-ToUJWz","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Review","forum":"By5ugjyCb","replyto":"By5ugjyCb","signatures":["ICLR.cc/2018/Conference/Paper154/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Review","rating":"5: Marginally below acceptance threshold","review":"This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network. The idea is interesting and novel that PACT has not been applied to compressing networks in the past. The results from this paper is also promising that it showed convincing compression results. \n\nThe experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks. Results look promising too.\n\nOverall the paper is a descent one, but with limited novelty. I am a weak reject","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1516157974233,"tcdate":1511899630648,"number":2,"cdate":1511899630648,"id":"B1wlzrslf","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Review","forum":"By5ugjyCb","replyto":"By5ugjyCb","signatures":["ICLR.cc/2018/Conference/Paper154/AnonReviewer1"],"readers":["everyone"],"content":{"title":"This paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations.","rating":"5: Marginally below acceptance threshold","review":"The authors have addressed my concerns, and clarified a misunderstanding of the baseline that I had, which I appreciate. I do think that it is a solid contribution with thorough experiments. I still keep my original rating of the paper because the method presented is heavily based on previous works, which limits the novelty of the paper. It uses previously proposed clipping activation function for quantization of neural networks, adding a learnable parameter to this function. \n_______________\nORIGINAL REVIEW:\n\nThis paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations. It shows empirically that even though the clipping activation function obtains a larger training error for full-precision model, it maintains the same error when applying quantization, whereas training with quantized ReLu activation function does not work in practice because it is unbounded.\n\nThe experiments are thorough, and report results on many datasets, showing that PACT can reduce down to 4 bits of quantization of weights and activation with a slight loss in accuracy compared to the full-precision model. \nRelated to that, it seams a bit an over claim to state that the accuracy decrease of quantizing the DNN with PACT in comparison with previous quantization methods is much less because the decrease is smaller or equal than 1%, when competing methods accuracy decrease compared to the full-precision model is more than 1%. Also, it is unfair to compare to the full-precision model using clipping, because ReLu activation function in full-precision is the standard and gives much better results, and this should be the reference accuracy. Also, previous methods take as reference the model with ReLu activation function, so it could be that in absolute value the accuracy performance of competing methods is actually higher than when using PACT for quantizing DNN.\n\nOTHER COMMENTS:\n\n- the list of contributions is a bit strange. It seams that the true contribution is number 1 on the list, which is to introduce the parameter \\alpha in the activation function that is learned with back-propagation, which reduces the quantization error with respect to using ReLu as activation function. To provide an analysis of why it works and quantitative results, is part of the same contribution I would say.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1515642399845,"tcdate":1511751927638,"number":1,"cdate":1511751927638,"id":"BkgW-ZteG","invitation":"ICLR.cc/2018/Conference/-/Paper154/Official_Review","forum":"By5ugjyCb","replyto":"By5ugjyCb","signatures":["ICLR.cc/2018/Conference/Paper154/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Is the idea strong enough?","rating":"5: Marginally below acceptance threshold","review":"The parameterized clipping activation (PACT) idea is very clear: extend clipping activation by learning the clipping parameter. Then,  PACT is combined with quantizing the activations. \n\nThe proposed technique sounds. The performance improvement is expected and validated by experiments. \n\nBut I am not sure if the novelty is strong enough for an ICLR paper. \n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]}},{"tddate":null,"ddate":null,"tmdate":1515189535304,"tcdate":1509040242005,"number":154,"cdate":1509739454019,"id":"By5ugjyCb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"By5ugjyCb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"PACT: Parameterized Clipping Activation for Quantized Neural Networks","abstract":"Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. To address this cost, a number of quantization schemeshave been proposed - but most of these techniques focused on quantizing weights, which are relatively smaller in size compared to activations. This paper proposes a novel quantization scheme for activations during training - that enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation.  This technique, PArameterized Clipping acTi-vation (PACT), uses an activation clipping parameter α that is optimized duringtraining to find the right quantization scale. PACT allows quantizing activations toarbitrary bit precisions, while achieving much better accuracy relative to publishedstate-of-the-art quantization schemes. We show, for the first time, that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. We also show that exploiting these reduced-precision computational units in hardware can enable a super-linear improvement in inferencing performance dueto a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories.","pdf":"/pdf/72b81ce72cb94ac746c4d2fc028a8176c37f0b10.pdf","TL;DR":"A new way of quantizing activation of Deep Neural Network via parameterized clipping which optimizes the quantization scale via stochastic gradient descent.","paperhash":"anonymous|pact_parameterized_clipping_activation_for_quantized_neural_networks","_bibtex":"@article{\n  anonymous2018pact:,\n  title={PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=By5ugjyCb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper154/Authors"],"keywords":["deep learning","quantized deep neural network","activation quantization"]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}