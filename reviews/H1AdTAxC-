{"notes":[{"tddate":null,"ddate":null,"tmdate":1515190115439,"tcdate":1515190115439,"number":8,"cdate":1515190115439,"id":"HysPwOpXG","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Comment","forum":"H1AdTAxC-","replyto":"rkpgbxh7z","signatures":["ICLR.cc/2018/Conference/Paper467/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper467/Authors"],"content":{"title":"Thank you for your response","comment":"Thank you for your response. The motivation in our explanation can also be extended into more complex problem, e.g. multiple variables (or many pixels in image generation) and for the case that  the performance is not perfectly optimized. Since the hamming loss is used to measure the distance, when the performance not perfectly optimized, the number of 'correct' variables / pixels used in hamming loss can serve as an intermediate target. Hence, focusing more on the variables / pixels that are 'incorrect' rather than the one that are 'correct' may speed up the training progress. \n\nIn terms of the empirical study of the algorithm,  we recognize the need for more extensive experiments, as this is a newly developed GAN architecture for discrete distributions. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1515167829532,"tcdate":1515167829532,"number":7,"cdate":1515167829532,"id":"SkaUlXpmz","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Comment","forum":"H1AdTAxC-","replyto":"H1AdTAxC-","signatures":["ICLR.cc/2018/Conference/Paper467/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper467/Authors"],"content":{"title":"Uploaded revised paper","comment":"- We have elaborated on our experiment with MNIST handwritten digits, showing 100 generated samples. The generated samples exhibit plenty of diversity. Thus, the MNIST result is one non-trivial and non-synthetic experiment, beyond our first constructed example.  \n\n- We recognize the need for more extensive experiments, as this is a newly developed GAN architecture for discrete distributions. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1515090164709,"tcdate":1515090164709,"number":6,"cdate":1515090164709,"id":"rkpgbxh7z","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Comment","forum":"H1AdTAxC-","replyto":"r1f38juzG","signatures":["ICLR.cc/2018/Conference/Paper467/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper467/AnonReviewer3"],"content":{"title":"Follow up","comment":"Thanks for your responses.  I am afraid my view has not changed.\n\nWith regards to the motivation, I am still far from convinced, though as I said before, I am not trying to argue that I think the target definitely won't work, just that the provided motivation does little to convince me it will.  The arguments you are making are assuming that we will perfectly optimize the target which is effectively never the case in the neural net literature - one is only carrying out local optimization and even then rarely gets near that local optimum.  Thus good targets need to give good performance when not perfectly optimized, so unstable solutions like you are targetting here are often poor choices.  This is not to mention all the other considerations about generalization, multiple data points etc.\n\nIn practice, it is often prohibitively difficult to make convincing and concrete arguments in such problems of choosing optimization targets because there are so many factors at play.  As such, substantial supporting empirical evidence is necessary for the work to be acceptable and, unfortunately, this is clearly lacking for this paper.  The problem here is not only that the experimental evaluation is far from the required quality and coverage, but that the evaluation that is there is not very promising.  Though this is hopefully not the case, there is a very serious chance that the approach simply does not work and so improving the experiments may not just be a case of spending more time to add more experiments and further engineering the algorithm etc, but it is possible that the idea will turn out to be a complete dead-end.  The regularization idea here is interesting but is somewhat tangential to the paper's main contribution and the problem it is addressing is already a very active area of research."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1513827248617,"tcdate":1513825961757,"number":5,"cdate":1513825961757,"id":"r1f38juzG","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Comment","forum":"H1AdTAxC-","replyto":"SkNXYkbkz","signatures":["ICLR.cc/2018/Conference/Paper467/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper467/Authors"],"content":{"title":"Preliminary Explanations for AnonReviewer3 [1/3]","comment":"We thank the reviewer for the thorough and detailed review. We now address the main concerns raised.\n\n=== Is the Discrete Metric Always Better for Training the Generative Model? ===\nThe reviewer states that it may not be the case, by providing an illustrative example of training sample [0, 0, 1; 0, 1, 0] and two generated samples:\n(1) [0.333, 0.333, 0.334; 0.333, 0.334, 0.333]\n(3) [0, 0, 1; 0.334, 0.333, 0.333]\nThe reviewer’s argument is that even though (1) has a better final solution than (3), in training a generative model (3) is preferable to (1) since (1) is very close to the boundary of the final solution prediction while the first part of (3) is optimized all the way to the edge ([0, 0, 1]).\n\nWe still propose that a discrete metric is better for training generative models for discrete distributions. The reason is that for discrete cases, we do not need to train all the way to the edges (i.e. all close to [0, 0, 1]). Producing solutions that are sufficiently above the boundary is the important aspect to address. For an illustration, consider a single training binary example [0, 1]. Assume that our generator currently produces 10 samples of the following composition: \n- 5 of them are [0.4, 0.6], and \n- the other 5 are [0.6, 0.4]. \nRather than encouraging to push all the samples towards [0, 1], we propose that focusing on the last 5 samples to switch from [0.6, 0.4] to [0.4, 0.6] is a better training objective. More precisely, for the purpose of generating discrete samples, [0.4, 0.6] is good enough due to the rounding process when generating samples. Rather than trying to push samples towards the edge (e.g. from [0.4, 0.6] to [0, 1]), focusing to move as many samples as possible sufficiently above the boundary will produce better generated samples. To encourage this ‘sufficiently above the boundary’ behaviour, we utilize the scaled softmax function which discourages the ‘close to uniform’ predictions. \n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1513827268680,"tcdate":1513825890955,"number":4,"cdate":1513825890955,"id":"HyjvLi_MM","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Comment","forum":"H1AdTAxC-","replyto":"SkNXYkbkz","signatures":["ICLR.cc/2018/Conference/Paper467/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper467/Authors"],"content":{"title":"Preliminary Explanations for AnonReviewer3 [2/3]","comment":"=== Concern about the Experiment ===\nWe agree with the reviewer that the experiments in our paper can be improved. We are actively improving our experiments during this rebuttal period. The binarized MNIST example shows that our theoretical approach for optimization is also practical beyond synthetic examples. However, the optimization involves several different challenges in the training process. This is also true for training standard continuous Wasserstein GANs. \n\nRegarding the concerns for our current experiments, we would like to clarify our motivation in the experimental design and results.\n\n:: Experiment design\nOne main purpose of our experiments is to compare the performance of a WGAN that is based on a discrete metric (our model), with the standard continuous WGAN, for the task of modelling discrete distributions. A popular application of generative modelling for discrete distributions is NLP tasks. However, evaluating the performance of generative models for NLP tasks fairly and objectively is difficult. Unlike the domain of image generation in which we can evaluate the performance of a GAN model by visually looking to the generated images, displaying generated text (like in (Subramanian,et.al; 2017)) does not provide a meaningful comparison, especially if the performance of two models are relatively similar. Evaluating the performance with a proxy measure like BLEU is not ideal either. \n\nAccordingly, we focus on constructing an experiment that captures the complexity of generating discrete distributions, but still allows a simple strategy to fairly and objectively evaluate its performance. For this reason, our paper focuses first on a generalized ‘tic-tac-toe’ board-game problem, which satisfies our requirements for performance benchmarks. However, we also provide binarized MNIST as an experiment, which is not a synthetic example. During this rebuttal period, we are also evaluating other experiments to highlight the advantages of our Discrete WGAN model. \n\n:: Memorization issue\nWe agree with the reviewer that it is necessary to demonstrate that the GAN is doing something different to just memorizing previous examples. As mentioned in our paper, we also tracked ‘percentage of new samples’ generated as a measure of evaluation (i.e, the percentage of generated samples that do not appear in the training data). Our results indicate that DWGAN does not just memorize training data. For example, in a 5-by-5 board with 8 players (Figure 5a), at iteration 500, the ‘percentage of new samples’ values indicates that 100% of generated samples do not appear in the training data. Due to limitations for space in our manuscript, we did not show the full figure. We will try to incorporate this result in our paper in the main text, or in the appendix in our revised version.\n\n:: Mode collapse issue\nWe acknowledge that the standard formulation of our DWGAN has a mode-collapse issue (producing a single output) if we run too many iterations. However, as described in Figure 5a, before the mode-collapse happens, the network produces good generated samples. For examples in iteration 500, the ‘average of maximum player’s gain’ measure indicates that the sample produced is high quality. The percentage of unique samples close to 100% indicates the diversity of the generated samples. The mode-collapse issue started to occur at around iteration 520, at which point the percentage of unique samples dropped. \n\nOur analysis suggests that this behavior is caused by the fact that the network optimizes the function difference, which tends to give advantage if the outputs of the critic are not diverse. We overcome the issue by adding a norm penalty to the critic network. By adding the norm penalty, the network can produce 96% valid samples while maintaining the diversity of the samples (at a level of 50 different types of unique samples out of 100 samples). Beyond this norm penalty which addresses the mode collapse problem, and partially resolves it, we are considering a more in-depth analysis of this issue, and advanced techniques to handle this issue. The mode collapse problem is also visible when training other well-known GANs and WGANs published in the literature, and we hope to leverage state-of-the-art discoveries to address this problem.  \n\n:: Presentation\nWe will improve the presentation of our results in our revised paper (e.g. more generated samples for MNIST, plots for the ‘percentage of new samples’ measure, the number of iterations shown in plots, etc.)"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1513827289676,"tcdate":1513825817216,"number":3,"cdate":1513825817216,"id":"S1W7IiOzz","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Comment","forum":"H1AdTAxC-","replyto":"SkNXYkbkz","signatures":["ICLR.cc/2018/Conference/Paper467/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper467/Authors"],"content":{"title":"Preliminary Explanations for AnonReviewer3 [3/3]","comment":"=== Other concerns ===\n\n:: We will add a note in the appendices that our explanation of the dual LP for the discrete Wasserstein distance readily follows from known concepts in the literature. As advised by the reviewer, the dual formulation is not a new result, and the derivation of optimal transport is available in several texts, including the reviewer’s recommended citation: Lawrence C Evans. Partial differential equations and Monge-Kantorovich mass transfer. Current developments in mathematics, 1997(1):65–126, 1997. However, our novelty is the network architecture to enforce the dual constraints, and the resulting optimization framework.  \n\n:: We will fix and clarify our notation issue (y, x, [0,1], and {0,1}) to make the presentation clearer. \n\n:: The critic takes the real valued generator’s softmax output y’ (so that the gradient with respect to the generator parameters can be computed) and also uses the rounded sample x’ for ensuring the inequality constraints hold. This setup provides flexibility to the critic to set the output of the critic network based on the continuous value of the softmax output— to within the valid continuous values of critic output allowed by the inequality constraint.\n\n:: The purpose of the scaling constant k for the softmax function is to discourage the ‘close to uniform’ output. We suggest to set parameter k based on the number of classes, i.e. a larger k value for a larger number of classes.\n\n:: Explanation for the “explained in the sequel” statement. \nThe purpose of this statement was to point the reader to the supplementary text, in which we elaborate on the solution of the linear program. Specifically, in Example 1 of the supplementary text, to compute the discrete Wasserstein distance between two probability distributions, we solve an LP and associated dual LP. In this example, and more generally, the optimal dual vector variables obey a constraint which was not enforced in the original optimization, but which holds as an optimality condition for the solution. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1513827200825,"tcdate":1513825299165,"number":2,"cdate":1513825299165,"id":"HkiMNjdGf","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Comment","forum":"H1AdTAxC-","replyto":"Bk3c6RKgf","signatures":["ICLR.cc/2018/Conference/Paper467/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper467/Authors"],"content":{"title":"Preliminary Explanations for AnonReviewer1","comment":"We thank the reviewer for the useful review. We now address the main concerns raised.\n\n=== Concern about the Experiment ===\nWe agree with the reviewer that the experiments in our paper can be improved. We are actively improving our experiments during this rebuttal period.\n\nOne main purpose of our experiments is to compare the performance of a WGAN that is based on a discrete metric (our model), with the standard continuous WGAN, for the task of modelling discrete distributions. A popular application of generative modelling for discrete distributions is NLP tasks. However, evaluating the performance of generative models for NLP tasks fairly and objectively is difficult. Unlike the domain of image generation in which we can evaluate the performance of a GAN model by visually looking to the generated images, displaying generated text (like in (Subramanian,et.al; 2017)) does not provide a meaningful comparison, especially if the performance of two models are relatively similar. Evaluating the performance with a proxy measure like BLEU is not ideal either. \n\nAccordingly, we focus on constructing an experiment that captures the complexity of generating discrete distributions, but still allows a simple strategy to fairly and objectively evaluate its performance. For this reason, our paper focuses first on a generalized ‘tic-tac-toe’ board-game problem, which satisfies our requirements for performance benchmarks. However, we also provide a binarized MNIST experiment, which is not a synthetic example. Indeed it is a non-trivial optimization for discrete distributions with high degrees of freedom. During this rebuttal period, we are also evaluating other experiments to highlight the advantages of our Discrete WGAN model. \n\n:: Mode collapse issue\nWe acknowledge that the standard formulation of our DWGAN has a mode-collapse issue (producing a single output) if we run too many iterations. However, as described in Figure 5a, before the mode-collapse happens, the network produces good generated samples. For examples in iteration 500, the ‘average of maximum player’s gain’ measure indicates that the sample produced is high quality. The percentage of unique samples close to 100% indicates the diversity of the generated samples. The mode-collapse issue started to occur at around iteration 520, at which point the percentage of unique samples dropped. \n\nOur analysis suggests that this behavior is caused by the fact that the network optimizes the function difference, which tends to give advantage if the outputs of the critic are not diverse. We overcome the issue by adding a norm penalty to the critic network. By adding the norm penalty, the network can produce 96% valid samples while maintaining the diversity of the samples (at a level of 50 different types of unique samples out of 100 samples). Beyond this norm penalty which addresses the mode collapse problem, and partially resolves it, we are considering a more in-depth analysis of this issue, and advanced techniques to handle this issue. The mode collapse problem is also visible when training other well-known GANs and WGANs published in the literature, and we hope to leverage state-of-the-art discoveries to address this problem. \n\n=== Optimal critic ===\nThe job of the critic network is to maximize the expected value of h. An optimal critic will assign a big positive value that is consistent with the constraints to a pair of samples (x,x’) where x is taken from the training data and x’ is the generated sample, and a negative value if the inputs are flipped. The generator’s job is to confuse the critic such that the critic can only assign values close to zero to h(x,x’) and h(x’,x). We are working towards a more theoretical analysis of DWGAN convergence. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1513827223943,"tcdate":1513825197835,"number":1,"cdate":1513825197835,"id":"SyU3QiOGf","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Comment","forum":"H1AdTAxC-","replyto":"S1u6hGhgM","signatures":["ICLR.cc/2018/Conference/Paper467/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper467/Authors"],"content":{"title":"Preliminary Explanations for AnonReviewer2","comment":"We thank the reviewer for the useful review. We now address the main concerns raised.\n\n=== Use Hamming distance (a Discrete metric) instead of Euclidean distance ===\nWe note that the discrete metric may be chosen to be any discrete distortion metric. The Hamming distance is one type of discrete metric. This is important because discrete metrics can behave quite differently from Euclidean distance, and therein the problem is non-trivial. \n\n=== Approximation ===\nWe would like to clarify that our approximations of the dual formulation of the discrete wasserstein distance is similar to the approximation used in the standard continuous WGAN. The standard WGAN approximates the 1-Lipschitz function of the dual continuous Wasserstein distance by weight-clipping the critic networks (or gradient penalty in a subsequent Improved WGAN paper). In our formulation, we approximate it by creating a network architecture that automatically satisfies the inequality constraint in the dual form of the discrete wasserstein distance.\n\n== Architecture design ==\nThe purpose of flipping arguments is to speedup the training process. The original dual formulation of Wasserstein distance has the constraint f(x_i) + g(x_j) <= d(x_i, x_j). It has been shown in the literature that the optimal solution of maximization of dual formulations satisfies g(x) = -f(x). By directly using the structure of this optimal solution known from theory, we aim to speedup the training process of finding the approximation of the maximum of the dual function h. To summarize, we do not need to include this particular speedup in training, but the optimal solution has this property, and it may be enforced directly. Example 1 in our Supplementary Text provides a basic example of solving the LP and dual LP, and the resulting structure of the dual vectors at the optimum. \n\nWe note that in our architecture h(x, x’) is the summation of n x m filtered tanh output (n x m is the size of inputs), i.e. h(x,x’) = sum_{j} (h_j(x, x’)). In this architecture, our assumption is that the function h and f is decomposable into a summation over n x m functions where each models one input variable (and interaction with other variables via the intermediate layers). In this setting, using the property h(x, x') = -h(x', x) to model h(x, x') = f(x) - f(x') is reasonable. Again, we note, as in Example 1 in the Supplementary Text, that this optimal condition need not be enforced, but will be true at the optimum. \n\nThe idea of a neural network approximating a function has several precedences in the literature, and is supported by mathematical arguments in the field of deep learning. We note that our framework works for a binarized MNIST dataset, which is non-trivial, but may need further architectural changes for improved performance results.  \n\n=== Rounding ===\nThe critic still takes the original softmax output as the input. The rounding process is used to compute the ‘filter’ to make sure the output of the critic satisfies the inequality constraints of the dual Wasserstein formulation. Since this filter can be treated as constant multiplications to the part of the functions, the overall formulation is still differentiable.\n\n=== Experiment design ===\nOne main purpose of our experiments is to compare the performance of a WGAN that is based on a discrete metric (our model), with the standard continuous WGAN, for the task of modelling discrete distributions. A popular application of generative modelling for discrete distributions is NLP tasks. However, evaluating the performance of generative models for NLP tasks fairly and objectively is difficult. Unlike the domain of image generation in which we can evaluate the performance of a GAN model by visually looking to the generated images, displaying generated text (like in (Subramanian,et.al; 2017)) does not provide a meaningful comparison, especially if the performance of two models are relatively similar. Evaluating the performance with a proxy measure like BLEU is not ideal either. \n\nAccordingly, we focus on constructing an experiment that captures the complexity of generating discrete distributions, but still allows a simple strategy to fairly and objectively evaluate its performance. For this reason, our paper focuses first on a generalized ‘tic-tac-toe’ board-game problem, which satisfies our requirements for performance benchmarks. \n\nHowever, we also provide a binarized MNIST experiment, which is not a synthetic example. During this rebuttal period, we are also evaluating other experiments to highlight the advantages of our Discrete WGAN model.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1516097687789,"tcdate":1511955648431,"number":3,"cdate":1511955648431,"id":"S1u6hGhgM","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Review","forum":"H1AdTAxC-","replyto":"H1AdTAxC-","signatures":["ICLR.cc/2018/Conference/Paper467/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Theory doesn't match practical algorithm","rating":"3: Clear rejection","review":"Summary of paper: This paper learns parameters of a generative model by minimizing the Wasserstein distance between the real generative model and the generative model that is being learned, as has been proposed by Arjovsky et al. (2017). The difference is that this paper uses the Hamming distance instead of the Euclidean distance. This makes the optimization difficult because the condition for the critic function to be K-Lipschitz can no longer be enforced by weight clipping. The paper proposes some sort of approximation to enforce this condition.\n\n---\n\nIt is a straightforward idea: just use Hamming instead of Euclidean distance. I'm a bit concerned with the many approximations that have to made to the algorithm to work in practice. In particular:\n- It is not clear how h(x, x') relates to f(x) - f(x') in the practical algorithm.\n- The thing about switching arguments to ensure that h(x, x') = -h(x', x) is OK but but whereas h(x, x') = f(x) - f(x') implies h(x, x') = -h(x', x), the opposite is not true.\n- The rounding of of the generator output is also a bit weird: Does this not make the generator not differentiable? How is that dealt with?\n- Does having to convert to one-hot preclude this from working in high-dimensional spaces?\nIt seems that the practical algorithm goes too far from the proposed theory. This reduces the value of the theory as an explanation of why the practical algorithm works. The practical algorithm has only been tested on a toy experiment which makes judgement of the practical algorithm alone also difficult.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1515642453019,"tcdate":1511808403827,"number":2,"cdate":1511808403827,"id":"Bk3c6RKgf","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Review","forum":"H1AdTAxC-","replyto":"H1AdTAxC-","signatures":["ICLR.cc/2018/Conference/Paper467/AnonReviewer1"],"readers":["everyone"],"content":{"title":"cool idea but experiments are rather toy","rating":"5: Marginally below acceptance threshold","review":"Generative adversarial networks (GANs) are state-of-art when it comes to image generation. Many existing works extend GANs to divergences other than the original Jensen-Shannon divergence but also propose ideas for stabilizing training. One such extension is the Wasserstein GAN. This paper extends the Wasserstein GAN to discrete data. This is an important line of work since many observed data are discrete (text is an obvious example).\n\nPros:\n--The paper is very well written. Each choice leading to the final architecture and learning algorithm is well motivated\n--I liked the justification (empirical) of why existing extensions of GANs to discrete data are not optimal. \n--I appreciated the transparency regarding the shortcomings of the proposed approach. We need more of this!\n--I found the synthetic experiment very insightful.\n\nCons:\n--The experiments are very very limited. Given this is work extending GANs to discrete data I was expecting an application to text data. \n--I would have liked to see a theoretical analysis of convergence. What is the optimal discriminator like?\n\nI will improve my rating if I see results on a \"real\" experiment such as text generation.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1515642453057,"tcdate":1510172956115,"number":1,"cdate":1510172956115,"id":"SkNXYkbkz","invitation":"ICLR.cc/2018/Conference/-/Paper467/Official_Review","forum":"H1AdTAxC-","replyto":"H1AdTAxC-","signatures":["ICLR.cc/2018/Conference/Paper467/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting idea but experiments a long way off what is required","rating":"4: Ok but not good enough - rejection","review":"This paper introduces an algorithm for learning Wasserstein GANs for discrete distributions. Taking the dual form of the discrete Wasserstein distance introduced by [Evans 1997] (which produces a constrained optimization problem) and using this as a basis of a GAN training algorithm. The key algorithmic distinction from conventional GAN approaches is that the critic takes pairs of real and simulated datapoints as inputs and returns a measure of which it thinks is the real datapoint, namely f(x_r)–f(g(z)) where x_r is the data point and g(z) the generator output, rather than the critic corresponding to f directly.  An architecture is proposed for this framework that guarantees that the constraints from the formulation of [Evans 1997] as satisfied.\n\nThe topic of this paper is timely and of clear interest to the ICLR community. The underlying idea is interesting and the architecture seems appropriate for the chosen target. Further, the paper is relatively clear and easy to follow. However, the experimental evaluation is not sufficiently strenuous and produces very underwhelming results. Relatedly, I think the motivation behind using the discrete Wasserstein distance, though seemingly reasonable, needs more careful consideration. Unfortunately, the serious shortfalls in the experiments mean that the paper, in my opinion, falls noticeably below the acceptance threshold for ICLR.  Having said that, my stance might change substantially if more impressive experimental results can be obtained; without this, I am unconvinced the method actually behaves as intended. Regretfully, this is probably beyond the scope of a revision during the rebuttal period though as it will most likely require significant algorithmic changes.\n\n%%%% Shortcomings with Experiments %%%%%\n\nPut simply, I do not think that the experiments demonstrate that the approach works and actually suggest the opposite. The so-called “mode collapse” issue is effectively a sugar-coated way of saying that the method has learned to return a single output rather than learning a generative model. This is a huge issue and needs sorting before the paper can be seriously considered for publication. The attempted at a fix at the end of the paper might be a step in the right direction but is not evaluated sufficiently and the preliminary results provided are not particularly promising.\n\nGoing past this issue, there are still a lot of problems with the experimental evaluation. More numerous and more difficult problems need to be considered. For example, an NLP problem would fit well with the motivation for the approach given in the intro. It is also necessary to demonstrate that the GAN is doing something different to just memorizing previous examples. For example, this is a problem where one can actually reasonably compare to just sampling from the data by checking that more unseen correct samples are generated then incorrect samples. \n\nIt is worrying that the convergence plots show a single line that ostensibly comes from the “best result”. This is not a reasonable scientific comparison method and should be replaced by mean (or median) performance with uncertainty estimates (i.e. +/- some numbers of standard deviations or quantiles). \n\nThe small number of samples from the MNIST problem really doesn’t convey anything meaningful – even if this looked exceptional, a small number of unqualified samples is hardly a serious evaluation metric.\n\nMore iterations for Figure 4a or needed to see if the WGAN keeps improving beyond on the DWGAN, noting that this has not converged at 100%. \n\nAll four methods should be added to Figure 5.\n\n%%%% Is the Discrete Metric Always Better for Training the Generative Model? %%%%%\n\nThough I think it is probably true that the discrete metric should more beneficial from the point of the view of the critic, I am not completely convinced it is always beneficial from the point of view of training the generator, which is at the end of the day what really matters. Note that I am not trying to argue that discrete metric is worse, just that you haven’t done enough to convince me that it is always better. If indeed it is always better, please tear apart my following argument, which will hopefully provide stimulation for improving the motivation of the metric in the paper. If it is not always better, the paper should be updated to outline some of the potential pathologies and the cases were you expect the approach to work well and when you do not.\n\nTo demonstrate my argument, consider the training sample [0, 0, 1; 0, 1, 0] and the following four example generated outputs\n\n(1) [0.333, 0.333, 0.334; \n      0.333, 0.334, 0.333]\n(2) [0, 0, 1; \n       0, 1, 0]\n(3) [0, 0, 1;\n       0.334, 0.333, 0.333]\n(4) [0.333, 0.334, 0.333; \n       0.334, 0.333, 0.333]\n\ngiving respective discrete distances to of 0, 0, 1, and 2; and continuous distances of 1.330668, 0, 0.667334, and 1.334668. Now imagine we are training a GAN with the one example datapoint [0, 0, 1; 0, 1, 0]. Even though (1) will lead to the target sample [0, 0, 1; 0, 1, 0] after passing through the argmax function and (3) will not, (1) is also very close to (4) which has the maximum possible discrete distance. Consequently, the generator for (1) is most likely not at a stable optimum and very close in the space of neural net weights to some very poor generators. During training, we would thus like to guide our network towards generating (2), as this is a stable solution, particularly given we are using stochastic optimization methods. From this perspective, then (3) is perhaps a better generated output than (1), a fact conveyed by the continuous metric but not the discrete metric. In other words, even though (1) is arguably a better final solution than (3), from the perspective of effective training, it may be favorable to use a target function that prefers (3) to (1) to better guide the training to a stable solution.\n\nOnce we consider the fact that our aim is not to replicate a single datapoint but learn a generator that in some way interpolates between datapoints, it becomes even less clear if the discrete metric is better. For example, small changes to the input z for (1) are likely to lead to generating samples that are very different to anything seen in the training data (which is, in this case, a single point).\n\nThough I do not think this argument undermines the suggested approach, I do think it highlights why the supplied motivation for the approach is insufficient and needs more explicitly linking back to the training procedure. At the very least, I think the above argument shows why it is not immediately clear cut that the suggested approach will perform better and so needs backing up with strong empirical evidence, which unfortunately the paper does not currently have, and/or a more convincing argument for why the discrete metric is better.\n\n%%%% Other points %%%% \n\n- Tables 1, 2, and 3 are not worth spending more than a few lines on, let alone nearly a page. They should be substantially compressed or preferably just cut (particularly Tables 2 and 3). The point they convey is obvious and actually somewhat tangential to the key questions.\n\n- It worries me that the method learns an h that explicitly takes y as input, not x. This is a notational issue in the exposition, but also, more importantly, raises questions about whether the original linear programming problem is actually being solved because the inputs from the generative method are not discrete variables.\n\n- It should be made clear that the Appendices are directly following the derivation of Evans 1997, rather than being a new derivation.\n\n- At times the paper is little sloppy at making distinguishing between y ∈ [0, 1] and y ∈ {0, 1}. This makes it a bit confusing at times what is output from the generator, particularly when you talk about it being one-hot encoded. As I understand it, the output is always the former, while data lives in the latter but this is not always clear. I think you should also make a bigger deal of this in terms of the problem with previous methods that ignore that the critic might be able to distinguish based on the fact that the training and generated data points have an explicitly different type (discrete and continuous respectively).\n\n- On the fourth line of the abstract, both instances of GAN should be GANs.\n\n- What do you mean “explained in the sequel”?\n\n- Does k need to be the same for each variable?\n\n%%%% References %%%%\n\nLawrence C Evans. Partial differential equations and Monge-Kantorovich mass transfer. Current developments in mathematics, 1997(1):65–126, 1997.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]}},{"tddate":null,"ddate":null,"tmdate":1515167113016,"tcdate":1509121398495,"number":467,"cdate":1509739283466,"id":"H1AdTAxC-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1AdTAxC-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Discrete Wasserstein Generative Adversarial Networks (DWGAN)","abstract":"Generating complex discrete distributions remains as one of the challenging problems in machine learning. Existing techniques for generating complex distributions with high degrees of freedom depend on standard generative models like Generative Adversarial Networks (GAN), Wasserstein GAN, and associated variations. Such models are based on an optimization involving the distance between two continuous distributions. We introduce a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions. We derive a novel training algorithm and corresponding network architecture based on the formulation. Experimental results are provided for both synthetic discrete data, and real discretized data from MNIST handwritten digits.","pdf":"/pdf/5a3a390a5909253d65019df258a48b21b08163c0.pdf","TL;DR":"We propose a Discrete Wasserstein GAN (DWGAN) model which is based on a dual formulation of the Wasserstein distance between two discrete distributions.","paperhash":"anonymous|discrete_wasserstein_generative_adversarial_networks_dwgan","_bibtex":"@article{\n  anonymous2018discrete,\n  title={Discrete Wasserstein Generative Adversarial Networks (DWGAN)},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1AdTAxC-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper467/Authors"],"keywords":["GAN","wasserstein distance","discrete probability distribution"]},"nonreaders":[],"replyCount":11,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}