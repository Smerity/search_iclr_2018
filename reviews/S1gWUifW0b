{"notes":[{"tddate":null,"ddate":null,"tmdate":1515176698878,"tcdate":1515176698878,"number":5,"cdate":1515176698878,"id":"HJQZmSTmM","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Comment","forum":"S1gWUifW0b","replyto":"B1FWJbcMf","signatures":["ICLR.cc/2018/Conference/Paper933/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper933/Authors"],"content":{"title":"Reply","comment":"Thank for you taking the time to reimplement this work.\n\nWe have looked briefly at your code and read your writeup. \n\nWe didn't try any experiments with very few parallel environments as you mention in your writeup, it is useful to know that this destabilizes the process. As mentioned in the appendix we use 32 environments for all experiments.\n\nSome minor things we noticed were that you use an embedding size of 288 instead of 512, your initialization for the weights of the next state prediction layers is very small, we used a residual predictor ie predicting next_state - prev_state, and we do not use end of episode signal (we zero all the dones), finally we used the features pre-relu for the next state predictor so that features are not constrained to be always positive (making the predictor spend extra capacity modeling this).\n\nIt is encouraging to see that even with these different choices you are still able to get similar results."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515112826258,"tcdate":1515112826258,"number":4,"cdate":1515112826258,"id":"ByzYFB2mM","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Comment","forum":"S1gWUifW0b","replyto":"SyelTYIgM","signatures":["ICLR.cc/2018/Conference/Paper933/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper933/Authors"],"content":{"title":"Reply","comment":"Thank you for your comments, in addition to this comment please see our general rebuttal.\n\n“Originality\nThe proposed Curiosity by Bootstrapping Features can be viewed as a simplified version of Pathak et al., 2017. But no significant advantage of CBF is demonstrated.”\n\nAs discussed in points 2 and 3, showing no significant difference between CBF and IDF+joint training is a surprising finding and an important part of our contribution because it sheds light on what aspects of the method are actually contributing to its success.\n\n“The average return could be a better metric. And some plots require standard errors. ”\n\nWe have mentioned in our response to Reviewer2 why we feel that the max return is more appropriate for pure exploration experiments, but we are happy to add this information in an appendix. We will also add standard errors.\n\n“Cons:\n- No effective new method is demonstrated.”\n\nWe contend that previous work had shown promising results, but had not shown the method to be truly effective. Please see points 1 and 4 for more on this. We also show that the previous work was not working for the reasons claimed (see points 2 and 3) which is useful by itself. \n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515112789011,"tcdate":1515112789011,"number":3,"cdate":1515112789011,"id":"H1a8KH3mf","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Comment","forum":"S1gWUifW0b","replyto":"rJMoToYlz","signatures":["ICLR.cc/2018/Conference/Paper933/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper933/Authors"],"content":{"title":"Reply","comment":"Thank you for your comments, in addition to this comment please see our general rebuttal.\n\n“Overall I think the novelty is too limited for acceptance. The main point of the authors (heterogeneous results\nover different auxilirary learning methods),  is not suprising at all, and to be expected. The method the authors introduce\nis just a submodule of already published results[1].”\n\nWe disagree that the results are not surprising, and refer the reviewer to points 2 and 3 above. In particular the main conclusion is not heterogeneity over different auxiliary learning methods, it’s the strength of the effect of letting the policy learning algorithm influence the features, in particular the fact that it alone is enough to make curiosity-driven exploration work without any other feature learning method.\nWhile our method is indeed simpler than the ones in previously published work, it also works significantly better (see point 1). This should be considered a strength, not a weakness of the method.\n\n“Minor: The light green link color make the paper hard on the eye, I suggest using [hidelinks] for hyperref.\nFigure 2 is  very small and hard to read.”\n\nThank you for pointing this out, we will update the paper to correct this.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515112704141,"tcdate":1515112704141,"number":2,"cdate":1515112704141,"id":"S1dbYr2mf","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Comment","forum":"S1gWUifW0b","replyto":"Sy7lYfixz","signatures":["ICLR.cc/2018/Conference/Paper933/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper933/Authors"],"content":{"title":"Reply","comment":"Thank you for your comments, in addition to this comment please see our general rebuttal.\n\n“The main conclusion is that training with this intrinsic reward performs similar to some of the considered aux tasks.”\n\nThis is incorrect, we do not contrast intrinsic reward methods with auxiliary task methods. All experiments are performed only with the intrinsic reward (i.e. without any extrinsic rewards or end of episode signals). The difference is choice of features used for next state prediction. Feature spaces are defined by an auxiliary task, or lack of one, and whether the features are trained jointly with the policy. As it turns out, the most important part of the method is the dynamic between the features being trained by policy gradients and the predictor, not an auxiliary loss used to ground the features. This dynamic is what we refer to as ‘feature-bootstrapping’.\n\n“How robust are the empirical returns curve? (mean/std)”\n\nIn the case of pure exploration we considered it be more useful to show the max return, since it varies over time what aspects of the environment is most surprising to the agent and whether that correlates with the unseen extrinsic reward, but we can add this information to an appendix if you think it would be useful to readers. We found our results to be fairly robust to choice of random seed, and somewhat sensitive to choices of hyperparameters like learning rates, number of parallel environments, and gamma in PPO (although the relative performance of the different algorithms is robust to these hyperparameters).\n\n“Does this algorithm help with better exploration when combined with rewards coming from the environment?”\n\nIn this work we were concerned with the pure exploration setting with no extrinsic reward given at all, since our motivation was to find a way to make use of the many environments without any extrinsic reward functions. We expect combining intrinsic and extrinsic reward to improve results on games with non-trivial exploration, but this would involve per-task tuning of the coefficients for combining intrinsic and extrinsic reward, since the scale of our intrinsic reward is not determined a priori.\n\n“The curiosity signal is basically the same as in Pathak et al. (2017) with minor implementation level differences. Defining intrinsic rewards as error in the transition model can lead to hairy issues. This has been extensively studied in Ouyeder et al (c.f. What is intrinsic motivation? A typology of computational approaches). So the basic premise of the curiosity considered here is riddled with conceptual problems. This is my biggest issue with the setup.”\n\nWhile we discuss the limitations of the general approach in section 4.1 (especially as related to stochastic environments), the provided reference “What is intrinsic motivation? A typology of computational approaches” does not contain a discussion of the limitations of the approach. A related paper “Intrinsic Motivation Systems for Autonomous Mental Development” does contain such discussion, but it is not entirely applicable to our setup. The main limitation discussed in this latter paper is transitions in the environment that are either truly stochastic, or else, impossible for the agent to predict because of its limited capacity. Since our approach works by using error in feature space, rather than in observation space, a well-chosen feature space could reduce or solve this problem. Indeed, the feature space could ignore the aspects of the dynamics that are truly stochastic or impossible for the agent to predict. The extent to which this happens with the different ways of learning the features is not entirely clear and requires separate experimental validation (similar to what we presented in the paper). Since our paper was dealing with deterministic environments, questions related to truly stochastic environments (where the transition can meaningfully depend on a random choice, like in a lottery, rather than small amounts of irrelevant stochasticity, as in TV static or observation noise) were not considered there.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515112618037,"tcdate":1515112618037,"number":1,"cdate":1515112618037,"id":"S1f3_rhmz","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Comment","forum":"S1gWUifW0b","replyto":"S1gWUifW0b","signatures":["ICLR.cc/2018/Conference/Paper933/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper933/Authors"],"content":{"title":"Rebuttal","comment":"Thank you for your comments.\n\nTo clarify confusion over what our method is we briefly summarize it.\n\nThe method is to use the error of a next state predictor in some space as an intrinsic reward. One could choose the space to be the raw observation space, or some feature space. A good choice of the feature space is essential for the performance of the method. Stadie et al (2015) used an autoencoding loss to learn the features, Pathak et al. (2017) used an inverse dynamics loss. It is also natural to use the same features in your policy network as Pathak et al. (2017) and we do, in which case feature learning is an auxiliary loss to be added to your policy gradient loss. In our work we considered an inverse dynamics auxiliary loss, a hindsight experience replay loss, and no auxiliary loss at all.\n\nOur main contributions are:\n\n 1. Showing how far this style of intrinsic motivation can be pushed. Our most striking result is the progress on Mario, I encourage readers to watch this video ( https://youtu.be/57hHdoCnsUE ) of our most recent experiment where the agent can pass 6 levels in a row. Contrast this with the previous result by Pathak where the agent can only pass a third of the first level. While encouraging this initial result did not tell us much about the viability of the approach since we have observed that random agent is also able to progress one third into the first level. Our method represents an order of magnitude improvement over previous state of the art on exploration in Mario without privileged access to the environment (no rewards and no end of life signals). We also report results on Atari environments not considered in previous work on pure exploration (with the exception of Bellemare et al (2016) on Montezuma Revenge).\n\n2. Demonstrating that the method still works with no auxiliary loss. We believe that this is an important point overlooked by some of the reviews. It was very surprising to us that this could work at all, and we believe should be surprising for the reader as well.\nA good choice of features should encode aspects of the environment that the agent can control well, but also such that surprising transitions in the environment are reflected in this feature space. This is a somewhat fine balance to strike: autoencoder features are likely to encode too many aspects of the environment, even those an agent can’t control; random features are likely to undergo surprising transitions even for predictable transitions of the environment; IDF features might be myopic, only encoding the agent’s position on the screen, and not encoding aspects of the environment that the agent can control indirectly. The fact that all these choices don’t lead to a good performance is reflected in our experimental results (FRF, HER, IDF lines in our graphs), as well as in weaker results in previous published work.\nThe fact that policy gradients can learn a feature space that is more appropriate for exploration than any of the methods listed above is not expected a priori. A reasonable researcher trying to make a prediction about the performance of the method before looking at the experimental results in our paper could argue that the method shouldn’t work along the following lines. “Initially the rewards for the policy come from the surprise of a random predictor of random features. These rewards are likely not tied to meaningful aspects of the game, and hence must lead to extremely bad feature learning. Learning from further rewards, coming from surprise of a dynamics model in that feature space, is unlikely to improve these features, and so on.” We believe that arguments of this kind have implicitly precluded previous researchers from even trying the method without an auxiliary feature learning task. It is only after looking at the results of our experiments that the argument about the bootstrapping effect explained in our paper becomes more likely than the argument presented above for why the method shouldn’t work.\n\n3. Our results shed new light on the reasons why previous published work got the results that it did. Previous work has emphasized the importance of a properly chosen feature space to prevent the features from collapsing. We show that to the extent their choices of feature learning auxiliary tasks worked well, it was only a second order effect, overshadowed by another choice that they didn’t emphasize at all, and likely thought to be unimportant - allowing the policy to influence the features. We disentangle these two effects and show that the second effect is more prominent than the effect of a choice of an auxiliary task.\n\n4. We show experimentally that even the hardest tasks in the Doom environment proposed in Pathak et. al. are inadequate as an exploration benchmark, since even the simplest algorithm we proposed can solve it as well as any other algorithm, and even random agent explores a significant fraction of the maze. This observation should influence future research on exploration.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1513914194802,"tcdate":1513914112705,"number":1,"cdate":1513914112705,"id":"B1FWJbcMf","invitation":"ICLR.cc/2018/Conference/-/Paper933/Public_Comment","forum":"S1gWUifW0b","replyto":"S1gWUifW0b","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"summary of a reproducibility study on this paper","comment":"We tried to reproduce this paper in two different Atari environments (pong and seaquest) and were able to reproduce the authors's results in one (seaquest), although we ran for less iterations. Using the hyperparameters specified, we had difficulty stabilizing the learning process. We ended up adding a term to scale down the forward dynamics losses to stabilize training. \n\nOur report can be found here: https://www.sharelatex.com/read/rchbvvpxcmkr\n\nOur implementation can be found here: https://github.com/thomasehuang/Reproducing-Curiosity-Driven-Exploration-By-Bootstrapping-Features"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642532427,"tcdate":1511889131268,"number":3,"cdate":1511889131268,"id":"Sy7lYfixz","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Review","forum":"S1gWUifW0b","replyto":"S1gWUifW0b","signatures":["ICLR.cc/2018/Conference/Paper933/AnonReviewer3"],"readers":["everyone"],"content":{"title":"review","rating":"5: Marginally below acceptance threshold","review":"- This paper considers next step model prediction error as an intrinsic reward signal for RL agents. It combines this with several auxiliary tasks such as inverse dynamics prediction, hindsight experience replay and so on. The main conclusion is that training with this intrinsic reward performs similar to some of the considered aux tasks. So therefore the authors conclude that the curiosity reward likely encourages bootstrapping of purposeful features. \n\n- The curiosity signal is basically the same as in Pathak et al. (2017) with minor implementation level differences. Defining intrinsic rewards as error in the transition model can lead to hairy issues. This has been extensively studied in Ouyeder et al (c.f. What is intrinsic motivation? A typology of computational approaches). So the basic premise of the curiosity considered here is riddled with conceptual problems. This is my biggest issue with the setup. \n\n- How robust are the empirical returns curve? (mean/std)\n\n- Does this algorithm help with better exploration when combined with rewards coming from the environment?","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642532467,"tcdate":1511796122471,"number":2,"cdate":1511796122471,"id":"rJMoToYlz","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Review","forum":"S1gWUifW0b","replyto":"S1gWUifW0b","signatures":["ICLR.cc/2018/Conference/Paper933/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Limited novelty","rating":"4: Ok but not good enough - rejection","review":"The authors present a derivation of previous work of [1]. In particular they propose the method of using the error signal of a dynamics model as curiosity for exploration, such as [1], but without any additionaly auxiliary methods. This the author call Curiosity by Bootstrapping Feature (CBF).\n  \nIn particular they show over  a set of auxiliary learning methods (hindsight ER, inverse dynamics model[1]) there is\nnot a clear cut edge one method has over the other (or over using no auxilirary method all, that is CBF).\n\nOverall I think the novelty is too limited for acceptance. The main point of the authors (heterogeneous results\nover different auxilirary learning methods),  is not suprising at all, and to be expected. The method the authors introduce\nis just a submodule of already published results[1].\n\nFor instance, section 4 discusses challenges related to these class of approaches such as the presence of stochasticity. Had the authors proposed a solution to these challenges that would have benefited the paper greatly.\n\nMinor: The light green link color make the paper hard on the eye, I suggest using [hidelinks] for hyperref.\nFigure 2 is  very small and hard to read.\n\n\n[1] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by\nself-supervised prediction. In ICML, 2017","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1515642532506,"tcdate":1511591144213,"number":1,"cdate":1511591144213,"id":"SyelTYIgM","invitation":"ICLR.cc/2018/Conference/-/Paper933/Official_Review","forum":"S1gWUifW0b","replyto":"S1gWUifW0b","signatures":["ICLR.cc/2018/Conference/Paper933/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Lacking interesting results. ","rating":"6: Marginally above acceptance threshold","review":"Clarity \nThe paper is well written and clear. \n\nOriginality\nThe proposed Curiosity by Bootstrapping Features can be viewed as a simplified version of Pathak et al., 2017. But no significant advantage of CBF is demonstrated.  \n\nSignificance\n- The empirical results might be helpful in a better understanding of Pathak et al., 2017. \n- The average return could be a better metric. And some plots require standard errors. \n  \nPros:\n- The paper is well written and clear. \n- The paper provides additional results for Pathak et al., 2017.\n\nCons:\n- No effective new method is demonstrated. \n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]}},{"tddate":null,"ddate":null,"tmdate":1510092385571,"tcdate":1509137233260,"number":933,"cdate":1510092362273,"id":"S1gWUifW0b","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1gWUifW0b","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Curiosity-driven Exploration by Bootstrapping Features","abstract":"We introduce CBF, an exploration method that works in the absence of rewards or end of episode signal. CBF is based on intrinsic reward derived from the error of a dynamics model operating in feature space. It was inspired by (Pathak et al., 2017), is easy to implement, and can achieve results such as passing four levels of Super Mario Bros, navigating VizDoom mazes and passing two levels of SpaceInvaders. We investigated the effect of combining the method with several auxiliary tasks, but find inconsistent improvements over the CBF baseline.\n","pdf":"/pdf/7aab3713419f7b614e7aabc00779f7d5502f69f5.pdf","TL;DR":"A simple intrinsic motivation method using forward dynamics model error in feature space of the policy.","paperhash":"anonymous|curiositydriven_exploration_by_bootstrapping_features","_bibtex":"@article{\n  anonymous2018curiosity-driven,\n  title={Curiosity-driven Exploration by Bootstrapping Features},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1gWUifW0b}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper933/Authors"],"keywords":["exploration","intrinsic motivation","reinforcement learning"]},"nonreaders":[],"replyCount":9,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}