{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222558736,"tcdate":1512006138471,"number":3,"cdate":1512006138471,"id":"BJzWfyTlf","invitation":"ICLR.cc/2018/Conference/-/Paper1137/Official_Review","forum":"H1kMMmb0-","replyto":"H1kMMmb0-","signatures":["ICLR.cc/2018/Conference/Paper1137/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Application of existing classifier networks as components of a visual arithmetic RL problem. OK results, not particularly surprising.","rating":"2: Strong rejection","review":"Summary: The authors use RL to learn a visual arithmetic task, and are able to do this with a relatively small number of examples, presumably not including the number of examples that were used to pre-train the classifiers that pre-process the images. This appears to be a very straightforward application of existing techniques and networks.\n\nQuality: Given the task that the authors are trying to solve, the approach seems reasonable.\nClarity: The paper appears quite clearly written for the most part.\nOriginality & Significance: Unless I am missing something important, or misunderstanding something, I do not really understand what is significant about this work, and I don't see it as having originality.\n\nNitpick 1: top of Page 5, it says \"Figure ??\" \nNitpick 2: Section 2.3 says \"M means take the product, A means take the sum, etc\". Why choose exactly those terms that obscure the pattern, and then write \"etc\"? In Figure 1, \"X\" could mean multiply, or take the maximum, but by elimination, it means take the maximum. It would have only added a few characters to the paper to specify the notation here, e.g. \"Addition(A), Max (X), Min (N), Multiply (M)\". If the authors insist on making the reader figure this out by deduction, I recommend they just write \"We leave the symbols-operation mapping as a small puzzle for the reader.\"\n\nThe authors might find the paper \"Visual Learning of Arithmetic Operations” by  Yedid Goshen and Shmuel Peleg to also be somewhat relevant, although it's different from what they are doing.\n\nSection 3. The story from the figures seems to be that the authors' system works beats a CNN when there are very few examples. But significance of this is not really discussed in any depth other than being mentioned in corresponding places in the text, i.e. it's not really the focal story of the text. \n\nPros: Seems to work OK. Seems like a reasonable application of pre-trained nets to allow solving a different visual problem for which less data might be available.\n\nCons: Unless I am missing an important point, the results are unsurprising, and I am not clear what is novel or significant about them.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequential Coordination of Deep Models for Learning Visual Arithmetic","abstract":"Achieving machine intelligence requires a smooth integration of perception and reasoning. Yet the models we have developed to date tend to specialize in one or the other; sophisticated manipulation of symbols acquired from rich perceptual spaces has so far proved elusive. Consider a visual arithmetic task, where an agent must learn to solve mathematical expressions,captured in natural conditions (e.g. hand-written, with background). We propose a two-tiered architecture for tackling this problem. At the lower level we leverage a collection of pre-trained deep perceptual models that can be used to detect and extract representations of characters in the image.At the higher level, we use reinforcement learning to learn when to apply the perceptual networks and what transformations to apply to their outputs.The resulting model is able to solve a variety of tasks in the Visual Arithmetic domain, and has several advantages over standard convolutional models, including greatly improved sample efficiency.","pdf":"/pdf/3aabac9a13b73eaca48e53acec3f071ba9fb96b9.pdf","TL;DR":"We use reinforcement learning to train an agent to solve a set of visual arithmetic tasks using provided pre-trained perceptual modules and transformations of internal representations created by those modules.","paperhash":"anonymous|sequential_coordination_of_deep_models_for_learning_visual_arithmetic","_bibtex":"@article{\n  anonymous2018sequential,\n  title={Sequential Coordination of Deep Models for Learning Visual Arithmetic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1kMMmb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1137/Authors"],"keywords":["reinforcement learning","pretrained","deep learning","perception","algorithmic"]}},{"tddate":null,"ddate":null,"tmdate":1512222558776,"tcdate":1511971098738,"number":2,"cdate":1511971098738,"id":"BkQXYLhgz","invitation":"ICLR.cc/2018/Conference/-/Paper1137/Official_Review","forum":"H1kMMmb0-","replyto":"H1kMMmb0-","signatures":["ICLR.cc/2018/Conference/Paper1137/AnonReviewer1"],"readers":["everyone"],"content":{"title":"A good proposal with limited experimental evidence","rating":"3: Clear rejection","review":"The paper presents an interesting model to reuse specialized models trained for perceptual tasks in order to solve more complex reasoning tasks. The proposed model is based on reinforcement learning with an agent that interacts with an environment C, which is the combination of E and I, the external world and the interface, respectively. This abstraction is nicely motivated and contextualized with respect to previous work.\n\nHowever, the paper evaluates the proposed model in artificial tasks with limited reasoning difficulty: the tasks can be solved with simpler baseline models. The paper argues that the advantage of the proposed approach is data efficiency, which seems to be a side effect of having pre-trained modules rather than a clear superior reasoning capability. The paper discusses other advantages of the model, but these are not tested or evaluated either. A more convincing experimental setup would include complex reasoning tasks, and the evaluation of all the aspects mentioned as benefits: computational time, flexibility of computation, better accuracy, etc.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequential Coordination of Deep Models for Learning Visual Arithmetic","abstract":"Achieving machine intelligence requires a smooth integration of perception and reasoning. Yet the models we have developed to date tend to specialize in one or the other; sophisticated manipulation of symbols acquired from rich perceptual spaces has so far proved elusive. Consider a visual arithmetic task, where an agent must learn to solve mathematical expressions,captured in natural conditions (e.g. hand-written, with background). We propose a two-tiered architecture for tackling this problem. At the lower level we leverage a collection of pre-trained deep perceptual models that can be used to detect and extract representations of characters in the image.At the higher level, we use reinforcement learning to learn when to apply the perceptual networks and what transformations to apply to their outputs.The resulting model is able to solve a variety of tasks in the Visual Arithmetic domain, and has several advantages over standard convolutional models, including greatly improved sample efficiency.","pdf":"/pdf/3aabac9a13b73eaca48e53acec3f071ba9fb96b9.pdf","TL;DR":"We use reinforcement learning to train an agent to solve a set of visual arithmetic tasks using provided pre-trained perceptual modules and transformations of internal representations created by those modules.","paperhash":"anonymous|sequential_coordination_of_deep_models_for_learning_visual_arithmetic","_bibtex":"@article{\n  anonymous2018sequential,\n  title={Sequential Coordination of Deep Models for Learning Visual Arithmetic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1kMMmb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1137/Authors"],"keywords":["reinforcement learning","pretrained","deep learning","perception","algorithmic"]}},{"tddate":null,"ddate":null,"tmdate":1512222558814,"tcdate":1511628832669,"number":1,"cdate":1511628832669,"id":"ByFXl7Def","invitation":"ICLR.cc/2018/Conference/-/Paper1137/Official_Review","forum":"H1kMMmb0-","replyto":"H1kMMmb0-","signatures":["ICLR.cc/2018/Conference/Paper1137/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Sequential Coordination of Deep Models for Learning Visual Arithmetic","rating":"4: Ok but not good enough - rejection","review":"Summary: This work is a variant of previous work (Zaremba et al. 2016) that enables the use of (noisy) operators that invoke pre-trained neural networks and is trained with Actor-Critic. In this regard it lacks a bit of originality. The quality of the experimental evaluation is not great. The clarity of the paper could be improved upon but is otherwise fine. The existence of previous work (Zaremba et al. 2016) renders this work (including its contributions) not very significant. Relations to prior work are missing. But let's wait for the rebuttal phase. \n\nPros \n-It is confirmed that noisy operators (in the form of neural networks) can be used on the visual arithmetic task\n\nCons\n-Not very novel\n-Experimental evaluation is wanting\n\nThe focus of this paper is on integrating perception and reasoning in a single system. This is done by specifying an interface that consists of a set of discrete operations (some of which involve perception) and memory slots. A parameterized policy that can make use of these these operations is trained via Actor-Critic to solve some reasoning tasks (arithmetics in this case). \n\nThe proposed system is a variant of previous work (Zaremba et al. 2016) on the concept of interfaces, and similarly learns a policy that utilizes such an interface to perform reasoning tasks, such as arithmetics. In fact, the only innovation proposed in this paper is to incorporate some actions that invoke a pre-trained neural network to “read” the symbol from an image, as opposed to parsing the symbol directly. However, there is no reason to expect that this would not function in previous work (Zaremba et al. 2016), even when the network is suboptimal (in which case the operator becomes noisy and the policy should adapt accordingly). Another notable difference is that the proposed system is trained with Actor-Critic as opposed to Q-learning, but this is not further elaborated on by the authors. \n\nThe proposed system is evaluated on a visual arithmetics task. The input consists of a 2x2 grid of extended MNIST characters. Each location in the grid then corresponds to the 28 x 28 pixel representation of the digit. Actions include shifting the “fovea” to a different entry of the grid, invoking the digit NN or the operator NN which parse the current grid entry, and some symbolic operations that operate on the memory. The fact that the input is divided into a 2x2 grid severely limits the novelty of this approach compared to previous work (Zaremba et al. 2016). Instead it would have been interesting to randomly spawn digits and operators in a 56 x 56 image and maintain 4 coordinates that specify a variable-sized grid that glimpses a part of the image. This would make the task severely more difficult, given fixed pre-trained networks. The addition of the salience network is unclear to me in the context of MNIST digits, since any pixel that is greater than 0 is salient? I presume that the LSTM uses this operator to evaluate whether the current entry contains a digit or an operator. If so, wouldn’t simply returning the glimpse be enough?\n\nIn the experiments the proposed system is compared to three CNNs on two different visual arithmetic tasks, one that includes operators as part of the input and one that incorporates operators only in the tasks description. In all cases the proposed method requires fewer samples to achieve the final performance, although given enough samples all of the CNNs will solve the tasks. This is not surprising as this comparison is rather unfair. The proposed system incorporates pre-trained modules, whose training samples are not taken into account. On the other hand the CNNs are trained from scratch and do not start with the capability to recognize digits or operators. Combined with the observation that all CNNs are able to solve the task eventually, there is little insight in the method's performance that can be gained from this comparison. \n\nAlthough the visual arithmetics on a 2x2 grid is a toy task it would at least be nice to evaluate some of the policies that are learned by the LSTM (as done by Zaremba) to see if some intuition can be recovered from there. Proper evaluation on a more complex environment (or at least on that does not assume discrete grids) is much desired. When increasing the complexity (even if by just increasing the grid size) it would be good to compare to a recurrent method (Pyramid-LSTM, Pixel-RNN) as opposed to a standard CNN as it lacks memory capabilities and is clearly at a disadvantage compared to the LSTM.\n\nSome detailed comments are:\n\nThe introduction invokes evidence from neuroscience to argue that the brain is composed of (discrete) modules, without reviewing any of the counter evidence (there may be a lot, given how bold this claim is).\n\nFrom the introduction it is unclear why the visual arithmetic task is important.\n\nSeveral statements including the first sentence lack citations.\n\nThe contribution section is not giving any credit to Zaremba et al. (2016) whereas this work is at best a variant of that approach.\n\nIn the experiment section the role of the saliency detector is unclear.\n\nExperiment details are lacking and should be included.\n\nThe related work section could be more focused on the actual contribution being made.\n\nIt strikes me as odd that in the discussion the authors propose to make the entire system differentiable, since this goes against the motivation for this work.\n\n\nRelation to prior work:\n\np 1: The authors write: \"We also borrow the notion of an interface as proposed in Zaremba et al. (2016). An interface is a designed, task-specific machine that mediates the learning agent’s interaction with the external world, providing the agent with a representation (observation and action spaces) which is intended to be more conducive to learning than the raw representations. In this work we formalize an interface as a separate POMDP I with its own state, observation and action spaces.\" \n\nThis interface terminology for POMDPs was actually introduced in:\n\nJ.  Schmidhuber. Reinforcement learning in Markovian and non-Markovian environments. In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, Advances in Neural Information Processing Systems 3, NIPS'3, pages 500-506. San Mateo, CA: Morgan Kaufmann, 1991.\n\np 4: authors write: \"For the policy πθ, we employ a Long Short-Term Memory (LSTM)\" \n\nDo the authors use the (cited) original LSTM of 1997, or do they also use the forget gates (recurrent units with gates) that most people are using now, often called the vanilla LSTM, by Gers et al (2000)?\n\np 4: authors write: \"One obvious point of comparison to the current work is recent research on deep neural networks designed to learn to carry out algorithms on sequences of discrete symbols. Some of these frameworks, including the Differen-tiable Forth Interpreter (Riedel and Rocktäschel, 2016) and TerpreT (Gaunt et al., 2016b), achieve this by explicitly generating code, while others, including the Neural Turing Machine (NTM; Graves et al., 2014), Neural Random-Access Machine (NRAM; Kurach et al., 2015), Neural Programmer (NP; Neelakan- tan et al., 2015), Neural Programmer-Interpreter (NPI; Reed and De Freitas, 2015) and work in Zaremba et al. (2016) on learning algorithms using reinforcement learning, avoid gen- erating code and generally consist of a controller network that learns to perform actions in a (sometimes differentiable) external computational medium in order to carry out an algorithm.\"\n\nHere the original work should be mentioned, on differentiable neural stack machines: \n\nG.Z. Sun and H.H. Chen and  C.L. Giles and Y.C. Lee and D. Chen. Connectionist Pushdown Automata that Learn Context-Free Grammars. IJCNN-90, Lawrence Erlbaum, Hillsdale, N.J., p 577, 1990.\n\nMozer, Michael C and Das, Sreerupa. A connectionist symbol manipulator that discovers the structure of context-free languages. Advances in Neural Information Processing Systems (NIPS), p 863-863, 1993.\n\n\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Sequential Coordination of Deep Models for Learning Visual Arithmetic","abstract":"Achieving machine intelligence requires a smooth integration of perception and reasoning. Yet the models we have developed to date tend to specialize in one or the other; sophisticated manipulation of symbols acquired from rich perceptual spaces has so far proved elusive. Consider a visual arithmetic task, where an agent must learn to solve mathematical expressions,captured in natural conditions (e.g. hand-written, with background). We propose a two-tiered architecture for tackling this problem. At the lower level we leverage a collection of pre-trained deep perceptual models that can be used to detect and extract representations of characters in the image.At the higher level, we use reinforcement learning to learn when to apply the perceptual networks and what transformations to apply to their outputs.The resulting model is able to solve a variety of tasks in the Visual Arithmetic domain, and has several advantages over standard convolutional models, including greatly improved sample efficiency.","pdf":"/pdf/3aabac9a13b73eaca48e53acec3f071ba9fb96b9.pdf","TL;DR":"We use reinforcement learning to train an agent to solve a set of visual arithmetic tasks using provided pre-trained perceptual modules and transformations of internal representations created by those modules.","paperhash":"anonymous|sequential_coordination_of_deep_models_for_learning_visual_arithmetic","_bibtex":"@article{\n  anonymous2018sequential,\n  title={Sequential Coordination of Deep Models for Learning Visual Arithmetic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1kMMmb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1137/Authors"],"keywords":["reinforcement learning","pretrained","deep learning","perception","algorithmic"]}},{"tddate":null,"ddate":null,"tmdate":1510092379960,"tcdate":1509138953135,"number":1137,"cdate":1510092359543,"id":"H1kMMmb0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"H1kMMmb0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Sequential Coordination of Deep Models for Learning Visual Arithmetic","abstract":"Achieving machine intelligence requires a smooth integration of perception and reasoning. Yet the models we have developed to date tend to specialize in one or the other; sophisticated manipulation of symbols acquired from rich perceptual spaces has so far proved elusive. Consider a visual arithmetic task, where an agent must learn to solve mathematical expressions,captured in natural conditions (e.g. hand-written, with background). We propose a two-tiered architecture for tackling this problem. At the lower level we leverage a collection of pre-trained deep perceptual models that can be used to detect and extract representations of characters in the image.At the higher level, we use reinforcement learning to learn when to apply the perceptual networks and what transformations to apply to their outputs.The resulting model is able to solve a variety of tasks in the Visual Arithmetic domain, and has several advantages over standard convolutional models, including greatly improved sample efficiency.","pdf":"/pdf/3aabac9a13b73eaca48e53acec3f071ba9fb96b9.pdf","TL;DR":"We use reinforcement learning to train an agent to solve a set of visual arithmetic tasks using provided pre-trained perceptual modules and transformations of internal representations created by those modules.","paperhash":"anonymous|sequential_coordination_of_deep_models_for_learning_visual_arithmetic","_bibtex":"@article{\n  anonymous2018sequential,\n  title={Sequential Coordination of Deep Models for Learning Visual Arithmetic},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=H1kMMmb0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1137/Authors"],"keywords":["reinforcement learning","pretrained","deep learning","perception","algorithmic"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":false,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}