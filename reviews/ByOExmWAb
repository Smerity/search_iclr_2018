{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222556239,"tcdate":1511916781284,"number":3,"cdate":1511916781284,"id":"HyHlSKjlG","invitation":"ICLR.cc/2018/Conference/-/Paper1108/Official_Review","forum":"ByOExmWAb","replyto":"ByOExmWAb","signatures":["ICLR.cc/2018/Conference/Paper1108/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Locally better, but globally not always better","rating":"6: Marginally above acceptance threshold","review":"This paper proposes MaskGAN, a GAN-based generative model of text based on\nthe idea of recovery from masked text. \nFor this purpose, authors employed a reinceforcement learning approach to\noptize a prediction from masked text. Moreover, authors argue that the \nquality of generated texts is not appropriately measured by perplexities,\nthus using another criterion of a diversity of generated n-grams as well as\nqualitative evaluations by examples and by humans.\n\nWhile basically the approach seems plausible, the issue is that the result is\nnot compared to ordinary LSTM-based baselines. While it is better than a \nconterpart of MLE (MaskedMLE), whether the result is qualitatively better than\nordinary LSTM is still in question.\n\nIn fact, this is already appearent both from the model architectures and the\ngenerated examples: because the model aims to fill-in blanks from the text\naround (up to that time), generated texts are generally locally valid but not\nalways valid globally. This issue is also pointed out by authors in Appendix\nA.2. \nWhile the idea of using mask is interesting and important, I think if this\nidea could be implemented in another way, because it resembles Gibbs sampling\nwhere each token is sampled from its sorrounding context, while its objective\nis still global, sentence-wise. As argued in Section 1, the ability of \nobtaining signals token-wise looks beneficial at first, but it will actually\nbreak a global validity of syntax and other sentence-wise phenoma.\n\nBased on the arguments above, I think this paper is valuable at least\nconceptually, but doubt if it is actually usable in place of ordinary LSTM\n(or RNN)-based generation.\nMore arguments are desirable for the advantage of this paper, i.e. quantitative\nevaluation of diversity of generated text as opposed to LSTM-based methods.","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MaskGAN: Textual Generative Adversarial Networks from Filling-in-the-Blank","abstract":"Neural text generation models are often autoregressive language models or seq2seq models. Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality. Language models are typically trained via maximum likelihood and most often with teacher forcing. Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time. We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.","pdf":"/pdf/04e1869870812eb3844ee949a7f96e38f9cb4bf8.pdf","TL;DR":"Natural language GAN for filling in the blank","paperhash":"anonymous|maskgan_textual_generative_adversarial_networks_from_fillingintheblank","_bibtex":"@article{\n  anonymous2018maskgan:,\n  title={MaskGAN: Textual Generative Adversarial Networks from Filling-in-the-Blank},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByOExmWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1108/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1512222556285,"tcdate":1511895752338,"number":2,"cdate":1511895752338,"id":"rkgAfEoeG","invitation":"ICLR.cc/2018/Conference/-/Paper1108/Official_Review","forum":"ByOExmWAb","replyto":"ByOExmWAb","signatures":["ICLR.cc/2018/Conference/Paper1108/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Very thorough empirical study","rating":"6: Marginally above acceptance threshold","review":"Generating high-quality sentences/paragraphs is an open research problem that is receiving a lot of attention. This text generation task is traditionally done using recurrent neural networks. This paper proposes to generate text using GANs. GANs are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete nature. This paper's approach is to use an actor-critic to train the generator of the GAN and use the usual maximum likelihood with SGD to train the discriminator. The whole network is trained on the \"fill-in-the-blank\" task using the sequence-to-sequence architecture for both the generator and the discriminator. At training time, the generator's encoder computes a context representation using the masked sequence. This context is conditioned upon to generate missing words. The discriminator is similar and conditions on the generator's output and the masked sequence to output the probability of a word in the generator's output being fake or real. With this approach, one can generate text at test time by setting all inputs to blanks. \n\nPros and positive remarks: \n--I liked the idea behind this paper. I find it nice how they benefited from context (left context and right context) by solving a \"fill-in-the-blank\" task at training time and translating this into text generation at test time. \n--The experiments were well carried through and very thorough.\n--I second the decision of passing the masked sequence to the generator's encoder instead of the unmasked sequence. I first thought that performance would be better when the generator's encoder uses the unmasked sequence. Passing the masked sequence is the right thing to do to avoid the mismatch between training time and test time.\n\nCons and negative remarks:\n--There is a lot of pre-training required for the proposed architecture. There is too much pre-training. I find this less elegant. \n--There were some unanswered questions:\n            (1) was pre-training done for the baseline as well?\n            (2) how was the masking done? how did you decide on the words to mask? was this at random?\n            (3) it was not made very clear whether the discriminator also conditions on the unmasked sequence. It needs to but \n                  that was not explicit in the paper.\n--Very minor: although it is similar to the generator, it would have been nice to see the architecture of the discriminator with example input and output as well.\n\n\nSuggestion: for the IMDB dataset, it would be interesting to see if you generate better sentences by conditioning on the sentiment as well.\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MaskGAN: Textual Generative Adversarial Networks from Filling-in-the-Blank","abstract":"Neural text generation models are often autoregressive language models or seq2seq models. Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality. Language models are typically trained via maximum likelihood and most often with teacher forcing. Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time. We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.","pdf":"/pdf/04e1869870812eb3844ee949a7f96e38f9cb4bf8.pdf","TL;DR":"Natural language GAN for filling in the blank","paperhash":"anonymous|maskgan_textual_generative_adversarial_networks_from_fillingintheblank","_bibtex":"@article{\n  anonymous2018maskgan:,\n  title={MaskGAN: Textual Generative Adversarial Networks from Filling-in-the-Blank},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByOExmWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1108/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1512222556331,"tcdate":1511804220081,"number":1,"cdate":1511804220081,"id":"Sy4HaTtlz","invitation":"ICLR.cc/2018/Conference/-/Paper1108/Official_Review","forum":"ByOExmWAb","replyto":"ByOExmWAb","signatures":["ICLR.cc/2018/Conference/Paper1108/AnonReviewer3"],"readers":["everyone"],"content":{"title":"The paper discusses a mechanism of generating text samples using GAN and a in-filling mechanism of missing words conditional on the surrounding text. I feel the Reinforcement learning idea that has been introduced to employ a actor-critic training procedure could be computationally challenging and the motivation to do so is not explicitly clear.","rating":"6: Marginally above acceptance threshold","review":"Quality: The work focuses on a novel problem of generating text sample using GAN and a novel in-filling mechanism of words. Using GAN to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issues. As a remedy to these problems an in-filling-task conditioning on the surrounding text has been proposed. But, the use of the rewards at every time step (RL mechanism) to employ the actor-critic training procedure could be challenging computationally challenging.\n\nClarity: The mechanism of generating the text samples using the proposed methodology has been described clearly. However the description of the reinforcement learning step could have been made a bit more clear.\n\nOriginality: The work indeed use a novel mechanism of in-filling via a conditioning approach to overcome the difficulties of GAN training in text settings. There has been some work using GAN to generate adversarial examples in textual context too to check the robustness of classifiers. How this current work compares with the existing such literature?\n\nSignificance: The research problem is indeed significant since the use of GAN in generating adversarial examples in image analysis has been more prevalent compared to text settings. Also, the proposed actor-critic training procedure via RL methodology is indeed significant from its application in natural language processing.\n\npros:\n(a) Human evaluations applications to several datasets show the usefulness of MaskGen over the maximum likelihood trained model in generating more realistic text samples.\n(b) Using a novel in-filling procedure to overcome the complexities in GAN training.\n(c) generation of high quality samples even with higher perplexity on ground truth set.\n\ncons:\n(a) Use of rewards at every time step to the actor-critic training procure could be computationally expensive.\n(b) How to overcome the situation where in-filling might introduce implausible text sequences with respect to the surrounding words?\n(c) Depending on the Mask quality GAN can produce low quality samples. Any practical way of choosing the mask?","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"MaskGAN: Textual Generative Adversarial Networks from Filling-in-the-Blank","abstract":"Neural text generation models are often autoregressive language models or seq2seq models. Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality. Language models are typically trained via maximum likelihood and most often with teacher forcing. Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time. We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.","pdf":"/pdf/04e1869870812eb3844ee949a7f96e38f9cb4bf8.pdf","TL;DR":"Natural language GAN for filling in the blank","paperhash":"anonymous|maskgan_textual_generative_adversarial_networks_from_fillingintheblank","_bibtex":"@article{\n  anonymous2018maskgan:,\n  title={MaskGAN: Textual Generative Adversarial Networks from Filling-in-the-Blank},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByOExmWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1108/Authors"],"keywords":["Deep learning","GAN"]}},{"tddate":null,"ddate":null,"tmdate":1510092380630,"tcdate":1509138484308,"number":1108,"cdate":1510092359936,"id":"ByOExmWAb","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ByOExmWAb","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"MaskGAN: Textual Generative Adversarial Networks from Filling-in-the-Blank","abstract":"Neural text generation models are often autoregressive language models or seq2seq models. Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality. Language models are typically trained via maximum likelihood and most often with teacher forcing. Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time. We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.","pdf":"/pdf/04e1869870812eb3844ee949a7f96e38f9cb4bf8.pdf","TL;DR":"Natural language GAN for filling in the blank","paperhash":"anonymous|maskgan_textual_generative_adversarial_networks_from_fillingintheblank","_bibtex":"@article{\n  anonymous2018maskgan:,\n  title={MaskGAN: Textual Generative Adversarial Networks from Filling-in-the-Blank},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ByOExmWAb}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper1108/Authors"],"keywords":["Deep learning","GAN"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}