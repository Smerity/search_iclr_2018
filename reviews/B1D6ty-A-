{"notes":[{"tddate":null,"ddate":null,"tmdate":1512222677307,"tcdate":1511814141490,"number":3,"cdate":1511814141490,"id":"Syrb4g5gz","invitation":"ICLR.cc/2018/Conference/-/Paper506/Official_Review","forum":"B1D6ty-A-","replyto":"B1D6ty-A-","signatures":["ICLR.cc/2018/Conference/Paper506/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting approach to training Autoencoders","rating":"7: Good paper, accept","review":"In this paper an alternating optimization approach is explored for training Auto Encoders (AEs).\nThe authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase.\nThen they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD. The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs.\n\n\nSome comments on the theoretical part:\n-The theoretical part is partly misleading. While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer.\nRegarding the intermediate layers, we may indeed treat them as generalized linear models, but with non-monotone activations, and therefore the SLQC property does not apply.\nThe authors should mention this point.\n\n-Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting. \n\n-It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives. Concretely: Is there any known theory for such objectives? What guarantees can we hope to achieve?\n\n\nThe extension to muti-layer AEs makes sense and seems to works quite well in practice.\n\nThe experimental part is satisfactory, and seems to be done in a decent manner. \nIt will be useful if the authors could relate to the issue of parameter tuning for their algorithm.\nConcretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification.\n","confidence":"5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Autoencoders by Alternating Minimization","abstract":"We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle. DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks. It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem. We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively. DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations. In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.","pdf":"/pdf/ddd1eb0d8a70a14e1ad7b0de03910636c8d65fcb.pdf","TL;DR":"We utilize the alternating minimization principle to provide an effective novel technique to train deep autoencoders.","paperhash":"anonymous|training_autoencoders_by_alternating_minimization","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Autoencoders by Alternating Minimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1D6ty-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper506/Authors"],"keywords":["Deep Learning","Autoencoders","Alternating Optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222677352,"tcdate":1511661495031,"number":2,"cdate":1511661495031,"id":"HJJaJoveM","invitation":"ICLR.cc/2018/Conference/-/Paper506/Official_Review","forum":"B1D6ty-A-","replyto":"B1D6ty-A-","signatures":["ICLR.cc/2018/Conference/Paper506/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Some interesting ideas. But, not sure if they are applicable to the autoencoder problem and it is not clear if it outperforms SGD.","rating":"4: Ok but not good enough - rejection","review":"The authors propose an alternating minimization framework for training autoencoders and encoder-decoder networks. The central idea is that a single encoder-decoder network can be cast as an alternating minimization problem. Each minimization problem is not convex but is quasi-convex and hence one can use stochastic normalized gradient descent to minimize w.r.t. each variable. This leads to the proposed algorithm called DANTE which simply minimizes w.r.t. each variable using stochastic normalized gradient algorithm to minimize w.r.t. each variable The authors start with this idea and introduce a generalized ReLU which is specified via a subgradient function only whose local quasi-convexity properties are established. They then extend these idea to multi-layer encoder-decoder networks by performing greedy layer-wise training and using the proposed algorithms for training each layer. The ideas are interesting, but I have some concerns regarding this work.\n\nMajor comments:\n\n1. When dealing with a 2 layer network where there are 2 matrices W_1, W_2 to optimize over. It is not clear to me why optimizing over W_1 is a quasi-convex optimization problem? The authors seem to use the idea that solving a GLM problem is a quasi-convex optimization problem. However, optimizing w.r.t. W_1 is definitely not a GLM problem, since W_1 undergoes two non-linear transformations one via \\phi_1 and another via \\phi_2. Could the authors justify why minimizing w.r.t. W_1 is still a quasi-convex optimization problem?\n\n2. Theorem 3.4, 3.5 establish  SLQC properties with generalized RELU activations. This is an interesting result, and useful in its own right. However, it is not clear to me why this result is even relevant here. The main application of this paper is autoencoders, which are functions from R^d -> R^d. However, GLMs are functions from R^d ---> R. So, it is not at all clear to me how Theorem 3.4, 3.5 and eventually 3.6 are useful for the autoencoder problem that the authors care about. Yes they are useful if one was doing 2-layer neural networks for binary classification, but it is not clear to me how they are useful for autoencoder problems.\n\n3. Experimental results for classification are not convincing enough. If, one looks at Table 1. SGD outperforms DANTE on ionosphere dataset and is competent with DANTE on MNIST and USPS. \n\n4. The results on reconstruction do not show any benefits for DANTE over SGD (Figure 3). I would recommend the authors to rerun these experiments but truncate the iterations early enough. If DANTE has better reconstruction performance than SGD with fewer iterations then that would be a positive result.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Autoencoders by Alternating Minimization","abstract":"We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle. DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks. It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem. We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively. DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations. In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.","pdf":"/pdf/ddd1eb0d8a70a14e1ad7b0de03910636c8d65fcb.pdf","TL;DR":"We utilize the alternating minimization principle to provide an effective novel technique to train deep autoencoders.","paperhash":"anonymous|training_autoencoders_by_alternating_minimization","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Autoencoders by Alternating Minimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1D6ty-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper506/Authors"],"keywords":["Deep Learning","Autoencoders","Alternating Optimization"]}},{"tddate":null,"ddate":null,"tmdate":1512222677391,"tcdate":1511656906658,"number":1,"cdate":1511656906658,"id":"H1Q0pKwlf","invitation":"ICLR.cc/2018/Conference/-/Paper506/Official_Review","forum":"B1D6ty-A-","replyto":"B1D6ty-A-","signatures":["ICLR.cc/2018/Conference/Paper506/AnonReviewer2"],"readers":["everyone"],"content":{"title":"an attempt of new training method for DNNs","rating":"5: Marginally below acceptance threshold","review":"This paper presents an algorithm for training deep neural networks. Instead of computing gradient of all layers and perform updates of all weight parameters at the same time, the authors propose to perform alternating optimization on weights of individual layers. \n\nThe theoretical justification is obtained for single-hidden-layer auto-encoders. Motivated by recent work by Hazan et al 2015, the authors developed the local-quasi-convexity of the objective w.r.t. the hidden layer weights for the generalized RELU activation. As a result, the optimization problem over the single hidden layer can be optimized efficiently using the algorithm of Hazan et al 2015. This itself can be a small, nice contribution.\n\nWhat concerns me is the extension to multiple layers. Some questions are not clear from section 3.4:\n1. Do we still have local-quasi-convexity for the weights of each layer, when there are multiple nonlinear layers above it? A negative answer to this question will somewhat undermine the significance of the single-hidden-layer result.\n\n2. Practically, even if the authors can perform efficient optimization of weights in individual layers, when there are many layers, the alternating optimization nature of the algorithm can possibly result in overall slower convergence. Also, since the proposed algorithm still uses gradient based optimizers for each layer, computing the gradient w.r.t. lower layers (closer to the inputs) are still done by backdrop, which has pretty much the same computational cost of the regular backdrop algorithm for updating all layers at the same time. As a result, I am not sure if the proposed algorithm is on par with / faster than the regular SGD algorithm in actual runtime. In the experiments, the authors plotted the training progress w.r.t. the minibatch iterations, I do not know if the minibatch iteration is a proxy for actual runtime (or number of floating point operations).\n\n3. In the experiments, the authors found the network optimized by the proposed algorithm generalize better than regular SGD. Is this result consistent (across dataset, random initializations, etc), and can the authors elaborate the intuition behind?\n","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Training Autoencoders by Alternating Minimization","abstract":"We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle. DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks. It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem. We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively. DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations. In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.","pdf":"/pdf/ddd1eb0d8a70a14e1ad7b0de03910636c8d65fcb.pdf","TL;DR":"We utilize the alternating minimization principle to provide an effective novel technique to train deep autoencoders.","paperhash":"anonymous|training_autoencoders_by_alternating_minimization","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Autoencoders by Alternating Minimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1D6ty-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper506/Authors"],"keywords":["Deep Learning","Autoencoders","Alternating Optimization"]}},{"tddate":null,"ddate":null,"tmdate":1509739265648,"tcdate":1509124543397,"number":506,"cdate":1509739262994,"id":"B1D6ty-A-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1D6ty-A-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Training Autoencoders by Alternating Minimization","abstract":"We present DANTE, a novel method for training neural networks, in particular autoencoders, using the alternating minimization principle. DANTE provides a distinct perspective in lieu of traditional gradient-based backpropagation techniques commonly used to train deep networks. It utilizes an adaptation of quasi-convex optimization techniques to cast autoencoder training as a bi-quasi-convex optimization problem. We show that for autoencoder configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions, we can perform the alternations very effectively. DANTE effortlessly extends to networks with multiple hidden layers and varying network configurations. In experiments on standard datasets, autoencoders trained using the proposed method were found to be very promising when compared to those trained using traditional backpropagation techniques, both in terms of training speed, as well as feature extraction and reconstruction performance.","pdf":"/pdf/ddd1eb0d8a70a14e1ad7b0de03910636c8d65fcb.pdf","TL;DR":"We utilize the alternating minimization principle to provide an effective novel technique to train deep autoencoders.","paperhash":"anonymous|training_autoencoders_by_alternating_minimization","_bibtex":"@article{\n  anonymous2018training,\n  title={Training Autoencoders by Alternating Minimization},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1D6ty-A-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper506/Authors"],"keywords":["Deep Learning","Autoencoders","Alternating Optimization"]},"nonreaders":[],"replyCount":3,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}