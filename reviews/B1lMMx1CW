{"notes":[{"tddate":null,"ddate":null,"tmdate":1513130073245,"tcdate":1513130073245,"number":4,"cdate":1513130073245,"id":"B1WDuZAWz","invitation":"ICLR.cc/2018/Conference/-/Paper114/Official_Comment","forum":"B1lMMx1CW","replyto":"B1lMMx1CW","signatures":["ICLR.cc/2018/Conference/Paper114/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper114/Authors"],"content":{"title":"Thank you to reviewers","comment":"We would like to thank all the reviewers for their careful consideration of our paper, and very useful comments. \nWe provided detailed responses below and uploaded a new revision of the paper"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS","abstract":"We present a personalized recommender system using neural network for recommending\nproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.\nIt produces recommendations based on customer’s implicit feedback history such\nas purchases, listens or watches. Our key contribution is to formulate recommendation\nproblem as a model that encodes historical behavior to predict the future\nbehavior using soft data split, combining predictor and auto-encoder models. We\nintroduce convolutional layer for learning the importance (time decay) of the purchases\ndepending on their purchase date and demonstrate that the shape of the time\ndecay function can be well approximated by a parametrical function. We present\noffline experimental results showing that neural networks with two hidden layers\ncan capture seasonality changes, and at the same time outperform other modeling\ntechniques, including our recommender in production. Most importantly, we\ndemonstrate that our model can be scaled to all digital categories, and we observe\nsignificant improvements in an online A/B test. We also discuss key enhancements\nto the neural network model and describe our production pipeline. Finally\nwe open-sourced our deep learning library which supports multi-gpu model parallel\ntraining. This is an important feature in building neural network based recommenders\nwith large dimensionality of input and output data.","pdf":"/pdf/bff1431ea91c5678aff97176ccd1b5e75a30c7af.pdf","TL;DR":"Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website","paperhash":"anonymous|the_effectiveness_of_a_twolayer_neural_network_for_recommendations","_bibtex":"@article{\n  anonymous2018the,\n  title={THE UNREASONABLE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1lMMx1CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper114/Authors"],"keywords":["Recommender systems","deep learning","personalization"]}},{"tddate":null,"ddate":null,"tmdate":1515777178721,"tcdate":1513125957918,"number":3,"cdate":1513125957918,"id":"H1RrugAbf","invitation":"ICLR.cc/2018/Conference/-/Paper114/Official_Comment","forum":"B1lMMx1CW","replyto":"r1rOlgOlz","signatures":["ICLR.cc/2018/Conference/Paper114/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper114/Authors"],"content":{"title":"Review Rebuttal for Reviewer3","comment":"Thank you for your feedback\nIn addition to your comments we would like to highlight several points:\n1. Two methods of integrating time decay of purchases into the learning framework were proposed:\n1.1 Convolutional layer for exploring the shape of the time decay function.\n1.2 We explored properties of different neural network based recommenders: predictor and auto-encoder models and proposed a method of combining their properties by integrating time decay of purchases into the learning framework. The final model (“soft” split) can be interpreted as a generalized auto-encoder which has time decay on input layer and time decay on cost function. The evaluation is done on both internal and public data sets.\n2 Our relatively simple model can capture seasonality changes with daily re-training.\n3 We designed and open-sourced the core library which was used on these experiments. This library supports multi-gpu model parallel training and allows us to train large neural networks based recommender (model size can be more than several GB) in timely manner.\n4 Our approach is successfully scaled and outperforms existing recommenders on four different categories of one of the largest retail catalog.\n\nSeveral reasons of emphasizing the online production A/B test results are presented below:\nI would like to highlight the importance of reporting the online A/B test results for recommender systems which was done in this paper. The standard evaluation of real recommender system estimates KPI gain (for example number of purchases) and confidence level (p-value). \nIf p-value is low and KPI gain is high it means that we are confident that KPI gain has low probability of being random. \nIf p-value is high it means that we are not confident in results and it is highly probable that KPI gain is random.\n\nThat is why if  only offline metrics improvements of recommender system with no confidence evaluation is reported, then we do not know what is the probability of the offline gain being random. For example we report offline improvements on next week, but how about second, third week and etc. Even if we produce the full accuracy distribution over next several months it will not be real because of the second point below.\nSecond point about \"pure\" offline evaluation: it is done on purchases made by customers which were exposed to recommendations produced by different recommender (for example by legacy recommender). So again, offline metrics do not show real picture. In this case even if there is no gain in offline metrics we still can get KPI gain during online test and vice versa.\nDuring online A/B test the recommender loop is “closed”: we are evaluating KPI metrics on purchases which were done by customers who are exposed to the recommender which we are evaluating.\n\nIn conclusion offline evaluation is a preliminary test of the opportunity (which says that designed method can produce some recommendations) and only online A/B test shows real value of the designed approach. That is why we highlighted the last point in our paper: we demonstrated low p-value (less than 0.05) and increased KPI (number of purchases)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS","abstract":"We present a personalized recommender system using neural network for recommending\nproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.\nIt produces recommendations based on customer’s implicit feedback history such\nas purchases, listens or watches. Our key contribution is to formulate recommendation\nproblem as a model that encodes historical behavior to predict the future\nbehavior using soft data split, combining predictor and auto-encoder models. We\nintroduce convolutional layer for learning the importance (time decay) of the purchases\ndepending on their purchase date and demonstrate that the shape of the time\ndecay function can be well approximated by a parametrical function. We present\noffline experimental results showing that neural networks with two hidden layers\ncan capture seasonality changes, and at the same time outperform other modeling\ntechniques, including our recommender in production. Most importantly, we\ndemonstrate that our model can be scaled to all digital categories, and we observe\nsignificant improvements in an online A/B test. We also discuss key enhancements\nto the neural network model and describe our production pipeline. Finally\nwe open-sourced our deep learning library which supports multi-gpu model parallel\ntraining. This is an important feature in building neural network based recommenders\nwith large dimensionality of input and output data.","pdf":"/pdf/bff1431ea91c5678aff97176ccd1b5e75a30c7af.pdf","TL;DR":"Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website","paperhash":"anonymous|the_effectiveness_of_a_twolayer_neural_network_for_recommendations","_bibtex":"@article{\n  anonymous2018the,\n  title={THE UNREASONABLE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1lMMx1CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper114/Authors"],"keywords":["Recommender systems","deep learning","personalization"]}},{"tddate":null,"ddate":null,"tmdate":1512597748268,"tcdate":1512597748268,"number":2,"cdate":1512597748268,"id":"SJ3gtkUWz","invitation":"ICLR.cc/2018/Conference/-/Paper114/Official_Comment","forum":"B1lMMx1CW","replyto":"SJqZdRi1f","signatures":["ICLR.cc/2018/Conference/Paper114/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper114/Authors"],"content":{"title":"Review Rebuttal for Reviewer1 about benchmarking on public data sets and methodological contribution","comment":"Thank you for your feedback.\n1. Comments about benchmarking on public data sets:\nYou raised a good point about evaluation on public data sets. It was not done because of several reasons:\n1.1 There is no public data sets which have the same properties with our data: implicit feedbacks(purchase events + date of purchase), large number of products, large number of customers. \n1.2 Most of the papers are reporting RMSE or precision@K on randomly held out data sets, whereas we measure precision@K at particular time (future week). So, that estimated metrics are as close as possible to real production environment.\n\nWe would like to alleviate your concern about evaluation on public data sets.\nWe are going to pick MovieLens data [http://files.grouplens.org/datasets/movielens/ml-20m.zip] because it is related to one of the categories we use in the paper. \nWe are going to convert all rating to watches events (implicit feedbacks) by thresholding the ratings: 1 if rating >= 3, 0 otherwise. The same implicit feedback conversion was used in the paper [Vito Claudio Ostuni et. Al Top-N recommendations from implicit feedback leveraging linked open data. RecSys '13]\nWe are going to split the MovieLens data into past and future purchases. Then use past purchase for training the models and future for evaluation.\nIn the end we will compare accuracy metrics of our method with existing techniques on MovieLens data sets and report precision@K and PCC@K on future week (as described in point 1.2).\nPlease let me know if above approach can alleviate your concern about bench-marking on public data sets.\n\n2. Comments about our contribution:\n2.1 Yes, one of the focus of this paper is scaling neural network based recommender on all digital categories in real production environment.\nWe also open sourced the core library which is used in our experiments. It supports multi-gpu model parallelization. It allows us to train a neural networks with million of input and output dimensionalities (so that model size can be more than several gigabytes) in timely manner.\n\n2.2 In addition to that we did methodological contribution (which was a key for success in running these models in production):\nWe proposed to use convolutional layer for exploring the shape of the time decay. \nWe proposed different methods for increasing precision@K and PCC@K of neural network based recommender. We presented results on different data sets such as video, audiobooks, ebooks, music\nThese data sets have different properties: \na). For example on video data sets (which are popularity biased) we showed that predictor model combined with time decay on input data improves precision@K only, we also showed how to combine predictor model with auto-encoder so that PCC@K can be increased 2 times without significant reduction of precision@K. \nb). On other data sets like ebooks and audio-books (which are less popularity biased then video data) we showed that combination of predictor and autoencoder models increases both precision@K and PCC@K in comparison with predictor model. \nThe goal of this project was to find a solution which can be scaled to all digital categories of retail catalog. It makes it different with other referred papers where one category is picked and then model is specifically designed for it.\n\nWe will add above comments in the next revision of the paper.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS","abstract":"We present a personalized recommender system using neural network for recommending\nproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.\nIt produces recommendations based on customer’s implicit feedback history such\nas purchases, listens or watches. Our key contribution is to formulate recommendation\nproblem as a model that encodes historical behavior to predict the future\nbehavior using soft data split, combining predictor and auto-encoder models. We\nintroduce convolutional layer for learning the importance (time decay) of the purchases\ndepending on their purchase date and demonstrate that the shape of the time\ndecay function can be well approximated by a parametrical function. We present\noffline experimental results showing that neural networks with two hidden layers\ncan capture seasonality changes, and at the same time outperform other modeling\ntechniques, including our recommender in production. Most importantly, we\ndemonstrate that our model can be scaled to all digital categories, and we observe\nsignificant improvements in an online A/B test. We also discuss key enhancements\nto the neural network model and describe our production pipeline. Finally\nwe open-sourced our deep learning library which supports multi-gpu model parallel\ntraining. This is an important feature in building neural network based recommenders\nwith large dimensionality of input and output data.","pdf":"/pdf/bff1431ea91c5678aff97176ccd1b5e75a30c7af.pdf","TL;DR":"Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website","paperhash":"anonymous|the_effectiveness_of_a_twolayer_neural_network_for_recommendations","_bibtex":"@article{\n  anonymous2018the,\n  title={THE UNREASONABLE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1lMMx1CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper114/Authors"],"keywords":["Recommender systems","deep learning","personalization"]}},{"tddate":null,"ddate":null,"tmdate":1512588672182,"tcdate":1512588672182,"number":1,"cdate":1512588672182,"id":"SJOFHTH-M","invitation":"ICLR.cc/2018/Conference/-/Paper114/Official_Comment","forum":"B1lMMx1CW","replyto":"B1RdHXTef","signatures":["ICLR.cc/2018/Conference/Paper114/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper114/Authors"],"content":{"title":"Review Rebuttal for Reviewer2 about vanishing gradient, explanation of lower performance of LSTM and paper title","comment":"Thank you for your feedback.\n\n1. Comments about vanishing gradient:\nWe acknowledged that we did not add detailed info about vanishing gradient of predictor model(feedforward neural network). More details with experimental results are presented below:\nWe use ReLU activation function to mitigate vanishing gradient in predictor model.\nWith increase of the depth (number of hidden layer) of predictor model, accuracy metrics can degrade significantly (vanishing gradient is one of the reason of such effect). That is why we measured the impact of the NN depth on Precision@1, and observed that with increasing the NN depth, Precision@1 is going down as follow (even with ReLU):\nDepth                1            2        3         4         5         6\nPrecision@1   0.072   0.072   0.07   0.068  0.067  0.065\nOne of the method of mitigating the accuracy degradation (due to depth of NN) is residual neural networks [K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.]. We explored residual NN with predictor model on our data sets, and observed that it mitigates vanishing gradient effect, so that precisoion@1 stayed the same regardless of the depth of the neural network: around 0.072. But it does not improve accuracy metrics in comparison with two layers NN. That is why we picked neural network model with number of hidden layer no more than 2.\nWe will add these comments with experimental results in the new paper revision.\n\n2. Comments about LSTM performance:\nLSTM is well applied on sequences like text, speech etc. These sequences has “strong” grammatical rules, which are well captured by LSTM. We explain lower accuracy of LSTM by our data properties (or lack of “strong” grammatical rules in sequences of purchases in our data). For example on ebooks data, if one customer buy books in order: “Harry Potter”, “Golden Compass”, “Inkheart”, another customer can buy these books in different order:  “Inkheart”, “Harry Potter”, “Golden Compass” and another one in different order, etc. So these purchases can be in any order and “long” term dependencies can be noisy.\nAnother important properties of our data(video, ebooks) is the popularity of the recommended products at particular date. Our approach (predictor model) is modeling these properties by re-training the model every day and predicting the next purchases which are popular in the current week, whereas LSTM is recommending only next purchases (which are not necessary popular at current week). \nWe can expect better performance of LSTM on other categories of products (where order of purchases is more important), for example probability of buying a game for a cell phone after purchasing a cell phone is higher than probability of buying these products in reversed order. \n\n3. Comment about paper title:\nWe will rename it to:  “THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS”\n\nWe will add above comments in the next paper revision."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS","abstract":"We present a personalized recommender system using neural network for recommending\nproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.\nIt produces recommendations based on customer’s implicit feedback history such\nas purchases, listens or watches. Our key contribution is to formulate recommendation\nproblem as a model that encodes historical behavior to predict the future\nbehavior using soft data split, combining predictor and auto-encoder models. We\nintroduce convolutional layer for learning the importance (time decay) of the purchases\ndepending on their purchase date and demonstrate that the shape of the time\ndecay function can be well approximated by a parametrical function. We present\noffline experimental results showing that neural networks with two hidden layers\ncan capture seasonality changes, and at the same time outperform other modeling\ntechniques, including our recommender in production. Most importantly, we\ndemonstrate that our model can be scaled to all digital categories, and we observe\nsignificant improvements in an online A/B test. We also discuss key enhancements\nto the neural network model and describe our production pipeline. Finally\nwe open-sourced our deep learning library which supports multi-gpu model parallel\ntraining. This is an important feature in building neural network based recommenders\nwith large dimensionality of input and output data.","pdf":"/pdf/bff1431ea91c5678aff97176ccd1b5e75a30c7af.pdf","TL;DR":"Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website","paperhash":"anonymous|the_effectiveness_of_a_twolayer_neural_network_for_recommendations","_bibtex":"@article{\n  anonymous2018the,\n  title={THE UNREASONABLE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1lMMx1CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper114/Authors"],"keywords":["Recommender systems","deep learning","personalization"]}},{"tddate":null,"ddate":null,"tmdate":1515642388958,"tcdate":1512023414497,"number":3,"cdate":1512023414497,"id":"B1RdHXTef","invitation":"ICLR.cc/2018/Conference/-/Paper114/Official_Review","forum":"B1lMMx1CW","replyto":"B1lMMx1CW","signatures":["ICLR.cc/2018/Conference/Paper114/AnonReviewer2"],"readers":["everyone"],"content":{"title":"A good applied paper with a novel approach and good experimental results","rating":"7: Good paper, accept","review":"This paper presents a practical methodology to use neural network for recommending products to users based on their past purchase history. The model contains three components: a predictor model which is essentially a RNN-style model to capture near-term user interests, a time-decay function which serves as a way to decay the input based on when the purchase happened, and an auto-encoder component which makes sure the user's past purchase history get fully utilized, with the consideration of time decay. And the paper showed the combination of the three performs the best in terms of precision@K and PCC@K, and also with good scalability. It also showed good online A/B test performance, which indicates that this approach has been tested in real world.\n\nTwo small concerns:\n1. In Section 3.3. I am not fully sure why the proposed predictor model is able to win over LSTM. As LSTM tends to mitigate the vanishing gradient problem which most likely would exist in the predictor model. Some insights might be useful there.\n2. The title of this paper is weird. Suggest to rephrase \"unreasonable\" to something more positive. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS","abstract":"We present a personalized recommender system using neural network for recommending\nproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.\nIt produces recommendations based on customer’s implicit feedback history such\nas purchases, listens or watches. Our key contribution is to formulate recommendation\nproblem as a model that encodes historical behavior to predict the future\nbehavior using soft data split, combining predictor and auto-encoder models. We\nintroduce convolutional layer for learning the importance (time decay) of the purchases\ndepending on their purchase date and demonstrate that the shape of the time\ndecay function can be well approximated by a parametrical function. We present\noffline experimental results showing that neural networks with two hidden layers\ncan capture seasonality changes, and at the same time outperform other modeling\ntechniques, including our recommender in production. Most importantly, we\ndemonstrate that our model can be scaled to all digital categories, and we observe\nsignificant improvements in an online A/B test. We also discuss key enhancements\nto the neural network model and describe our production pipeline. Finally\nwe open-sourced our deep learning library which supports multi-gpu model parallel\ntraining. This is an important feature in building neural network based recommenders\nwith large dimensionality of input and output data.","pdf":"/pdf/bff1431ea91c5678aff97176ccd1b5e75a30c7af.pdf","TL;DR":"Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website","paperhash":"anonymous|the_effectiveness_of_a_twolayer_neural_network_for_recommendations","_bibtex":"@article{\n  anonymous2018the,\n  title={THE UNREASONABLE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1lMMx1CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper114/Authors"],"keywords":["Recommender systems","deep learning","personalization"]}},{"tddate":null,"ddate":null,"tmdate":1515642388998,"tcdate":1511682157156,"number":2,"cdate":1511682157156,"id":"r1rOlgOlz","invitation":"ICLR.cc/2018/Conference/-/Paper114/Official_Review","forum":"B1lMMx1CW","replyto":"B1lMMx1CW","signatures":["ICLR.cc/2018/Conference/Paper114/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Good use of known techniques to build a production ready recommender model","rating":"6: Marginally above acceptance threshold","review":"Authors describe a procedure of building their production recommender system from scratch, begining with formulating the recommendation problem, label data formation, model construction and learning. They use several different evaluation techniques to show how successful their model is (offline metrics, A/B test results, etc.)\n\nMost of the originality comes from integrating time decay of purchases into the learning framework. Rest of presented work is more or less standard.\n\nPaper may be useful to practitioners who are looking to implement something like this in production.","confidence":"4: The reviewer is confident but not absolutely certain that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS","abstract":"We present a personalized recommender system using neural network for recommending\nproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.\nIt produces recommendations based on customer’s implicit feedback history such\nas purchases, listens or watches. Our key contribution is to formulate recommendation\nproblem as a model that encodes historical behavior to predict the future\nbehavior using soft data split, combining predictor and auto-encoder models. We\nintroduce convolutional layer for learning the importance (time decay) of the purchases\ndepending on their purchase date and demonstrate that the shape of the time\ndecay function can be well approximated by a parametrical function. We present\noffline experimental results showing that neural networks with two hidden layers\ncan capture seasonality changes, and at the same time outperform other modeling\ntechniques, including our recommender in production. Most importantly, we\ndemonstrate that our model can be scaled to all digital categories, and we observe\nsignificant improvements in an online A/B test. We also discuss key enhancements\nto the neural network model and describe our production pipeline. Finally\nwe open-sourced our deep learning library which supports multi-gpu model parallel\ntraining. This is an important feature in building neural network based recommenders\nwith large dimensionality of input and output data.","pdf":"/pdf/bff1431ea91c5678aff97176ccd1b5e75a30c7af.pdf","TL;DR":"Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website","paperhash":"anonymous|the_effectiveness_of_a_twolayer_neural_network_for_recommendations","_bibtex":"@article{\n  anonymous2018the,\n  title={THE UNREASONABLE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1lMMx1CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper114/Authors"],"keywords":["Recommender systems","deep learning","personalization"]}},{"tddate":null,"ddate":null,"tmdate":1515642389037,"tcdate":1510889474097,"number":1,"cdate":1510889474097,"id":"SJqZdRi1f","invitation":"ICLR.cc/2018/Conference/-/Paper114/Official_Review","forum":"B1lMMx1CW","replyto":"B1lMMx1CW","signatures":["ICLR.cc/2018/Conference/Paper114/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Good industrial/empirical paper with some surprising findings.","rating":"6: Marginally above acceptance threshold","review":"The paper proposes a new neural network based method for recommendation.\n\nThe main finding of the paper is that a relatively simple method works for recommendation, compared to other methods based on neural networks that have been recently proposed.\n\nThis contribution is not bad for an empirical paper. There's certainly not that much here that's groundbreaking methodologically, though it's certainly nice to know that a simple and scalable method works.\n\nThere's not much detail about the data (it is after all an industrial paper). It would certainly be helpful to know how well the proposed method performs on a few standard recommender systems benchmark datasets (compared to the same baselines), in order to get a sense as to whether the improvement is actually due to having a better model, versus being due to some unique attributes of this particular industrial dataset under consideration. As it is, I am a little concerned that this may be a method that happens to work well for the types of data the authors are considering but may not work elsewhere.\n\nOther than that, it's nice to see an evaluation on real production data, and it's nice that the authors have provided enough info that the method should be (more or less) reproducible. There's some slight concern that maybe this paper would be better for the industry track of some conference, given that it's focused on an empirical evaluation rather than really making much of a methodological contribution. Again, this could be somewhat alleviated by evaluating on some standard and reproducible benchmarks.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS","abstract":"We present a personalized recommender system using neural network for recommending\nproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.\nIt produces recommendations based on customer’s implicit feedback history such\nas purchases, listens or watches. Our key contribution is to formulate recommendation\nproblem as a model that encodes historical behavior to predict the future\nbehavior using soft data split, combining predictor and auto-encoder models. We\nintroduce convolutional layer for learning the importance (time decay) of the purchases\ndepending on their purchase date and demonstrate that the shape of the time\ndecay function can be well approximated by a parametrical function. We present\noffline experimental results showing that neural networks with two hidden layers\ncan capture seasonality changes, and at the same time outperform other modeling\ntechniques, including our recommender in production. Most importantly, we\ndemonstrate that our model can be scaled to all digital categories, and we observe\nsignificant improvements in an online A/B test. We also discuss key enhancements\nto the neural network model and describe our production pipeline. Finally\nwe open-sourced our deep learning library which supports multi-gpu model parallel\ntraining. This is an important feature in building neural network based recommenders\nwith large dimensionality of input and output data.","pdf":"/pdf/bff1431ea91c5678aff97176ccd1b5e75a30c7af.pdf","TL;DR":"Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website","paperhash":"anonymous|the_effectiveness_of_a_twolayer_neural_network_for_recommendations","_bibtex":"@article{\n  anonymous2018the,\n  title={THE UNREASONABLE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1lMMx1CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper114/Authors"],"keywords":["Recommender systems","deep learning","personalization"]}},{"tddate":null,"ddate":null,"tmdate":1516568486389,"tcdate":1508995592352,"number":114,"cdate":1509739473826,"id":"B1lMMx1CW","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"B1lMMx1CW","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"THE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS","abstract":"We present a personalized recommender system using neural network for recommending\nproducts, such as eBooks, audio-books, Mobile Apps, Video and Music.\nIt produces recommendations based on customer’s implicit feedback history such\nas purchases, listens or watches. Our key contribution is to formulate recommendation\nproblem as a model that encodes historical behavior to predict the future\nbehavior using soft data split, combining predictor and auto-encoder models. We\nintroduce convolutional layer for learning the importance (time decay) of the purchases\ndepending on their purchase date and demonstrate that the shape of the time\ndecay function can be well approximated by a parametrical function. We present\noffline experimental results showing that neural networks with two hidden layers\ncan capture seasonality changes, and at the same time outperform other modeling\ntechniques, including our recommender in production. Most importantly, we\ndemonstrate that our model can be scaled to all digital categories, and we observe\nsignificant improvements in an online A/B test. We also discuss key enhancements\nto the neural network model and describe our production pipeline. Finally\nwe open-sourced our deep learning library which supports multi-gpu model parallel\ntraining. This is an important feature in building neural network based recommenders\nwith large dimensionality of input and output data.","pdf":"/pdf/bff1431ea91c5678aff97176ccd1b5e75a30c7af.pdf","TL;DR":"Improving recommendations using time sensitive modeling with neural networks in multiple product categories on a retail website","paperhash":"anonymous|the_effectiveness_of_a_twolayer_neural_network_for_recommendations","_bibtex":"@article{\n  anonymous2018the,\n  title={THE UNREASONABLE EFFECTIVENESS OF A TWO-LAYER NEURAL NETWORK FOR RECOMMENDATIONS},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=B1lMMx1CW}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper114/Authors"],"keywords":["Recommender systems","deep learning","personalization"]},"nonreaders":[],"replyCount":7,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}