{"notes":[{"tddate":null,"ddate":null,"tmdate":1512371657002,"tcdate":1512371657002,"number":2,"cdate":1512371657002,"id":"S1WASuMbf","invitation":"ICLR.cc/2018/Conference/-/Paper982/Official_Comment","forum":"ryTp3f-0-","replyto":"BJ448noJM","signatures":["ICLR.cc/2018/Conference/Paper982/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper982/Authors"],"content":{"title":"Reply to AnonReviewer2","comment":"We would like to thank the reviewer for their detailed and thoughtful feedback.\n\nIn our revision, we will significantly expand our comparison to related work on learning from demonstrations. The summary is provided below:\n\nPrevious work on learning from demonstration fall into two broad categories:\n    1) Using demonstrations to directly update policy parameters (e.g., behavioral cloning, IRL, etc.).\n    2) Using demonstrations to guide or constrain exploration.\n\nOur method belongs to category (2). The core idea is to explore trajectories that lie in a \"neighborhood\" surrounding an expert demonstration. In our work, the neighborhood is defined by a workflow, which only permits action sequences analogous to the demonstrated actions.\n\nOther methods in category (2) also explore the neighborhood surrounding a demonstration, using shaping rewards (Brys et al. 2015, Hussein et al. 2017) or off-policy sampling (Levine & Koltun, 2013). A key difference is that we define our neighborhood in terms of action-similarity, rather than state-similarity. This distinction is particularly important for the web tasks we study: we can easily and intuitively describe how two actions are analogous (e.g., \"they both type a username into a textbox\"), while it is harder to decide if two web page states are analogous (e.g., the email inboxes of two different users will have completely different emails, but they could still be analogous, depending on the task.)\n\nRegarding overfitting: our workflow policy does not overfit to demonstrations because the demonstrations are merely used to induce a workflow lattice -- the actual parameters of the workflow policy are learned through trial-and-error reinforcement learning, for which there is infinite data.\n\nThe workflow policy maintains a distribution over possible workflows. Some workflows define a very small neighborhood of trajectories surrounding an expert demonstration (tight), while others impose almost no constraints (loose). As the workflow policy is trained, it converges to the tightest workflow that can successfully generalize.\n\nOur final neural policy also does not overfit, because it is trained on a replay buffer of successful episodes discovered by the workflow policy, which is much larger than the original set of demonstrations.\n\nWe would like to also quickly address the other questions, which we will be sure to clarify in the paper:\n    - In Equation 4, we do not have access to the environment model p(s_t | s_{t-1}, a_{t-1}). We merely state that our \n      sampling procedure produces episodes e following the distribution p(e | g) of Equation 4, which we do not \n      compute.\n    - DOM stands for Document Object Model, the standard tree-based representation of a web page.\n    - The \"Steps\" column in Table 1 is the number of steps needed to complete the task under the optimal behavior \n      (e.g., by a human expert). It is a rough measure of task difficulty and is not related to model performance.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration","abstract":"Reinforcement learning (RL) agents improve through trial-and-error, but when re- ward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying emails, where a single mis- take can ruin the entire sequence of actions. A common remedy is to “warm-start” the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level “workflows” which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., “Step 1: click on a textbox; Step 2: enter some text”). Our exploration pol- icy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent’s ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 10x.","pdf":"/pdf/b1663898b2b7a6a4f5e649a9db8949078b349f2d.pdf","TL;DR":"We solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations","paperhash":"anonymous|reinforcement_learning_on_web_interfaces_using_workflowguided_exploration","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryTp3f-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper982/Authors"],"keywords":["reinforcement learning","sparse rewards","web","exploration"]}},{"tddate":null,"ddate":null,"tmdate":1512371374351,"tcdate":1512371374351,"number":1,"cdate":1512371374351,"id":"HyInEOM-f","invitation":"ICLR.cc/2018/Conference/-/Paper982/Official_Comment","forum":"ryTp3f-0-","replyto":"H1asng9lG","signatures":["ICLR.cc/2018/Conference/Paper982/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper982/Authors"],"content":{"title":"Reply to AnonReviewer3","comment":"We would like to thank the reviewer for their helpful feedback!\n\nThe key distinction between our work and most existing hierarchical RL approaches (e.g., options, MAXQ) is that our hierarchical structures (workflows) are inferred from demonstrations, rather than manually crafted or learned from scratch.\n\nWe try to keep the constraint language for describing workflow steps as minimal and general as possible. The main part of the language is just an element selector (elementSet) which selects either (1) things that share a specified property, or (2) things that align spatially, both of which are applicable in many typical RL domains (game playing, robot navigation, etc.)\n\nIn our experiments (Figure 3), WGE (red) consistently performs equally or better than behavioral cloning (green). There are some easy tasks in the benchmark (e.g., click-button), where both WGE and the baselines have perfect performance. But in more difficult tasks (Table 1), WGE greatly improves over baselines by an average of 42% absolute success rate.\n\nRegarding the comparison with Shi17:\n        - Shi17 used ~200 demonstrations per task, whereas we achieve superior performance with only 3-10.\n        - In addition to pixel-level data, the model of Shi17 actually also uses the DOM tree to compute text alignment \n          features. Our DOMNet uses the DOM structure more explicitly, which indeed produces better performance.\n        - Our DOMNet+BC+RL baseline separates the contribution of DOMNet from Workflow-Guided Exploration. Table 1 \n          and Figure 3 illustrate that both are important.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration","abstract":"Reinforcement learning (RL) agents improve through trial-and-error, but when re- ward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying emails, where a single mis- take can ruin the entire sequence of actions. A common remedy is to “warm-start” the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level “workflows” which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., “Step 1: click on a textbox; Step 2: enter some text”). Our exploration pol- icy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent’s ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 10x.","pdf":"/pdf/b1663898b2b7a6a4f5e649a9db8949078b349f2d.pdf","TL;DR":"We solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations","paperhash":"anonymous|reinforcement_learning_on_web_interfaces_using_workflowguided_exploration","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryTp3f-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper982/Authors"],"keywords":["reinforcement learning","sparse rewards","web","exploration"]}},{"tddate":null,"ddate":null,"tmdate":1512222834107,"tcdate":1511816357514,"number":2,"cdate":1511816357514,"id":"H1asng9lG","invitation":"ICLR.cc/2018/Conference/-/Paper982/Official_Review","forum":"ryTp3f-0-","replyto":"ryTp3f-0-","signatures":["ICLR.cc/2018/Conference/Paper982/AnonReviewer3"],"readers":["everyone"],"content":{"title":"Interesting, but hard to know the novelty","rating":"7: Good paper, accept","review":"This paper introduces a new exploration policy for Reinforcement Learning for agents on the web called \"Workflow Guided Exploration\". Workflows are defined through a DSL unique to the domain.\n\nThe paper is clear, very well written, and well-motivated. Exploration is still a challenging problem for RL. The workflows remind me of options though in this paper they appear to be hand-crafted. In that sense, I wonder if this has been done before in another domain. The results suggest that WGE sometimes helps but not consistently. While the experiments show that DOMNET improves over Shi et al, that could be explained as not having to train on raw pixels or not enough episodes.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration","abstract":"Reinforcement learning (RL) agents improve through trial-and-error, but when re- ward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying emails, where a single mis- take can ruin the entire sequence of actions. A common remedy is to “warm-start” the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level “workflows” which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., “Step 1: click on a textbox; Step 2: enter some text”). Our exploration pol- icy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent’s ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 10x.","pdf":"/pdf/b1663898b2b7a6a4f5e649a9db8949078b349f2d.pdf","TL;DR":"We solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations","paperhash":"anonymous|reinforcement_learning_on_web_interfaces_using_workflowguided_exploration","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryTp3f-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper982/Authors"],"keywords":["reinforcement learning","sparse rewards","web","exploration"]}},{"tddate":null,"ddate":null,"tmdate":1512222834149,"tcdate":1510880812299,"number":1,"cdate":1510880812299,"id":"BJ448noJM","invitation":"ICLR.cc/2018/Conference/-/Paper982/Official_Review","forum":"ryTp3f-0-","replyto":"ryTp3f-0-","signatures":["ICLR.cc/2018/Conference/Paper982/AnonReviewer2"],"readers":["everyone"],"content":{"title":"The paper has good empirical results, but algorithmic novelty is small.  It could benefit from more concrete comparisons in the literaure","rating":"5: Marginally below acceptance threshold","review":"SUMMARY\n\nThe paper deals with the problem of training RL algorithms from demonstration and applying them to various web interfaces such as booking flights.  Specifically, it is applied to the Mini world of Bids benchmark (http://alpha.openai.com/miniwob/).\n\nThe difference from existing work is that rather than training an agent to directly mimic the demonstrations, it uses demonstrations to constrain exploration. By pruning away bad exploration directions. \n\nThe idea is to  build a lattice of workflows from demonstration and randomly sample sequence of actions from this lattice that satisfy the current goal.    Use the sequences of actions to sample trajectories and use the trajectories to learn the RL policy.\n\n\n\nCOMMENTS\n\n\nIn effect, the workflow sequences provide more generalization than simply mimicking, but It not obvious, why they don’t run into overfitting problems.  However experimentally the paper performs better than the previous approach.\n\nThere is a big literature on learning from demonstrations that the authors could compare with, or explain why their work is different.  \n\nIn addition, they make general comparison to RL literature such as hierarchy rather than more concrete comparisons with the problem at hand (learning from demonstrations.)\n\nWhat does DOM stand for?  The paper is not self-contained.  For example, what does DOM stand for?  \n\n\nIn the results of table 1 and Figure 3.  Why more steps mean success?\n\nIn equation 4 there seems to exist an environment model.  Why do we need to use this whole approach in the paper then?  Couldn’t  we just do policy iteration?\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration","abstract":"Reinforcement learning (RL) agents improve through trial-and-error, but when re- ward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying emails, where a single mis- take can ruin the entire sequence of actions. A common remedy is to “warm-start” the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level “workflows” which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., “Step 1: click on a textbox; Step 2: enter some text”). Our exploration pol- icy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent’s ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 10x.","pdf":"/pdf/b1663898b2b7a6a4f5e649a9db8949078b349f2d.pdf","TL;DR":"We solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations","paperhash":"anonymous|reinforcement_learning_on_web_interfaces_using_workflowguided_exploration","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryTp3f-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper982/Authors"],"keywords":["reinforcement learning","sparse rewards","web","exploration"]}},{"tddate":null,"ddate":null,"tmdate":1510092383090,"tcdate":1509137617433,"number":982,"cdate":1510092360960,"id":"ryTp3f-0-","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"ryTp3f-0-","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration","abstract":"Reinforcement learning (RL) agents improve through trial-and-error, but when re- ward is sparse and the agent cannot discover successful action sequences, learning stagnates. This has been a notable problem in training deep RL agents to perform web-based tasks, such as booking flights or replying emails, where a single mis- take can ruin the entire sequence of actions. A common remedy is to “warm-start” the agent by pre-training it to mimic expert demonstrations, but this is prone to overfitting. Instead, we propose to constrain exploration using demonstrations. From each demonstration, we induce high-level “workflows” which constrain the allowable actions at each time step to be similar to those in the demonstration (e.g., “Step 1: click on a textbox; Step 2: enter some text”). Our exploration pol- icy then learns to identify successful workflows and samples actions that satisfy these workflows. Workflows prune out bad exploration directions and accelerate the agent’s ability to discover rewards. We use our approach to train a novel neural policy designed to handle the semi-structured nature of websites, and evaluate on a suite of web tasks, including the recent World of Bits benchmark. We achieve new state-of-the-art results, and show that workflow-guided exploration improves sample efficiency over behavioral cloning by more than 10x.","pdf":"/pdf/b1663898b2b7a6a4f5e649a9db8949078b349f2d.pdf","TL;DR":"We solve the sparse rewards problem on web UI tasks using exploration guided by demonstrations","paperhash":"anonymous|reinforcement_learning_on_web_interfaces_using_workflowguided_exploration","_bibtex":"@article{\n  anonymous2018reinforcement,\n  title={Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=ryTp3f-0-}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper982/Authors"],"keywords":["reinforcement learning","sparse rewards","web","exploration"]},"nonreaders":[],"replyCount":4,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}