{"notes":[{"tddate":null,"ddate":null,"tmdate":1515290289715,"tcdate":1515290289715,"number":13,"cdate":1515290289715,"id":"BJ930g14M","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"Hk88g60Zf","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer3"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/AnonReviewer3"],"content":{"title":"reply to revision","comment":"The authors have addressed my major issue so I have changed my vote to accept."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1515141433768,"tcdate":1515141433768,"number":12,"cdate":1515141433768,"id":"SyfSY3hmf","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"Revised version addressing the reviewers comments","comment":"We thank the reviewers for their careful consideration of our paper and for offering improvements to the manuscript. We have submitted a new version of the paper that addresses the comments of the reviewers:\n\nRelated work on hyperbolic embeddings of graphs has been added. The paper ‘Poincaré Embeddings for Learning Hierarchical Representations’, which appeared concurrently with ours and was mentioned by several reviewers is now explicitly referenced and the differences between this work and our own are described and quantitatively evaluated.\n\nAn explanation of how our embedding sacrifices some of the properties of the hyperbolic space and why this is acceptable in the context of learning embeddings has been added.\n\nThe appendix has been merged with the main text to include Eq 17.  as several reviewers identified that this inhibited readability.\n\nThe results section has been improved by including a comparison with the hyperbolic method of ‘Poincaré Embeddings for Learning Hierarchical Representations’ and the charts have been regenerated with larger labels and displayed in an order that is consistent with the tables. A new results table has been added to ease quantitative comparisons between methods. An explanation of how the embedding diagrams in Fig 5 should be interpreted has also been added."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1513241231688,"tcdate":1513241231688,"number":11,"cdate":1513241231688,"id":"rydq92kzf","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"Hk6Ye6CWG","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/AnonReviewer2"],"content":{"title":"Let's see it in writing","comment":"Okay, I think that's a reasonable point. From this description, I'm having a hard time determining if this is an important or a minor insight, but it sounds like there's a valuable insight.\n\nBut I'll make you an offer: if you update the paper to discuss these matters in detail, then I'll re-review the paper from scratch. Please also take comments from the other reviewers into account. If you can document that there's an empirical benefit to your parametrization (you already hinted at that), then that's also great (given the short time available, I acknowledge that this may not be possible)."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1513177221465,"tcdate":1513177221465,"number":10,"cdate":1513177221465,"id":"Hk6Ye6CWG","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"rkKEbeO-z","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"It allows for a more efficient better behaved optimizer","comment":"Thank you, we appreciate your sympathy. \n\nThe Poincare representation has circular symmetry such that all elements on the boundary of a circle centered at the origin have the same distortion to the Euclidean length. By representing the system in spherical co-ordinates the update equations are greatly simplified with the radial updates being exactly the Euclidean updates and the angular update a simple modification of the Euclidean. This simplicity could explain why we are able to get compelling results after just 5 epochs while “Poincaré Embeddings for Learning Hierarchical Representations” (Nickel and Kiela, NIPS 2017) require a 20 epoch burn in period. \n\nIn addition, the heuristic of  “Poincaré Embeddings for Learning Hierarchical Representations” (Nickel and Kiela, NIPS 2017) that places points that the optimizer pushes outside of the boundary of the Poincare ball is troubling as it requires arbitrarily moving some  points an infinite hyperbolic distance, which seems like an undesirable property for an optimizer. This problem does not exist in natural coordinates as the Poincare ball boundary is at infinity.\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1513177165820,"tcdate":1513177165820,"number":9,"cdate":1513177165820,"id":"Hk88g60Zf","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"rynh2mGgf","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"Thank you for your helpful comments","comment":"Thank you for your helpful comments. We will add comparisons with more graph embedding methods and expand the literature review in future versions."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1513177139747,"tcdate":1513177139747,"number":8,"cdate":1513177139747,"id":"r1hExT0WG","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"HJ5atCj-G","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"This seems to be a duplicate comment","comment":"This seems to be a duplicate comment"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1513177109632,"tcdate":1513177109632,"number":7,"cdate":1513177109632,"id":"SJaMeTRbz","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"SyVv9AjWG","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"Thank you for your suggestions on how to improve the paper","comment":"Thank you for your suggestions on how to improve the paper. We do not claim that the non-Euclidean embeddings are new, only that doing so through a neural network is and that a change in geometry can markedly improve the representations learned by the very popular SkipGram / Word2vec model. We agree that eq. 17 should be moved to the main body and that it would improve readability to re-order some of the tables."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1513177078157,"tcdate":1513177078157,"number":6,"cdate":1513177078157,"id":"SJRleTC-G","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"HkfDunaZf","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"Both papers were written at the same time and the approaches differ","comment":"Our paper is similar to “Poincaré Embeddings for Learning Hierarchical Representations” (Nickel and Kiela, NIPS 2017). However, both were written independently at the same time. While they are similar, there are several important differences between them. Our work uses a spherical coordinate system, instead of cartesian coordinates, which we believe is more elegant as it exploits the symmetry of the Poincare ball. In addition, we use natural hyperbolic coordinates that extend radially (0,inf) instead of (0,1). Natural coordinates remove the numerical issues at the boundary of the ball. In the paper by Nickel and Kiela, this is a problem as the optimization will push points to values greater than 1. To fix this, they introduce a heuristic that moves points a small distance back inside the ball when the optimizer pushes them outside. As most of the space is towards the edge of the ball, many of the points in their system are effectively placed solely by the heuristic. This is not a problem in natural coordinates and we simply switch back coordinate systems on completion of the optimization process. In addition, we use a different similarity metric based on cosine similarity instead of the hyperbolic distance.\n\nDespite the similarity, it is worthwhile publishing this paper as it provides a complementary perspective on the problem and grows the evidence base for this as a powerful technique that other researchers can employ. Our paper shows that the principle of hyperbolic embedding can be achieved using a cosine-similarity approach as well as the distance-similarity approach used in the other paper, and so demonstrates a more general applicability of the idea. An example of where this has previously occurred are Auto-Encoding Variational Bayes (Kingma and Welling, ICLR 2014) https://arxiv.org/abs/1312.6114 and later Stochastic Backpropagation and Approximate Inference in Deep Generative Models Rezende et al., ICML 2014) https://arxiv.org/abs/1401.4082\nBoth papers contained the same idea and appeared on arxiv within a month, but provided different perspectives.\n\nIn response to the other comments, eq. 17 was removed as it is well known, but we agree that this affects readability and will include this in the main text of future versions. Similarly we will increase the font size of the axis labels. The comparisons of hyperbolic and Euclidean embeddings are showing that in hyperbolic space the classes are more easily separable. This is important as the results use a logistic regression classifier.\n"},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1513121409243,"tcdate":1513121409243,"number":3,"cdate":1513121409243,"id":"r1YtIJCbM","invitation":"ICLR.cc/2018/Conference/-/Paper92/Public_Comment","forum":"S1xDcSR6W","replyto":"SylAxmQeM","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"Completely agree - These are not hyperbolic embeddings","comment":"Since hyperbolic space is not a vector space (only a metric space) it doesn't make sense to define an inner product between two points on the manifold. This is a significant flaw in the paper."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1515642530789,"tcdate":1513109594331,"number":4,"cdate":1513109594331,"id":"HkfDunaZf","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Review","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer4"],"readers":["everyone"],"content":{"title":"Concerns about novelty","rating":"4: Ok but not good enough - rejection","review":"The authors present a method to embed graphs in hyperbolic space, and show that this approach yields stronger attribute predictions on a set of graph datasets. I am concerned by the strong similarity between this work and Poincaré Embeddings for Learning Hierarchical Representations (https://arxiv.org/abs/1705.08039). The latter has been public since May of this year, which leads me to doubt the novelty of this work.\n\nI also find the organization of the paper to be poor.\n- There is a surprisingly high number of digressions.\n- For some reason, Eq 17 is not included in the main paper. I would argue that this equation is one of the most important equations in the paper, given that it is the one you are optimizing.\n- The font size in the main result figure is so small that one cannot hope to parse what the plots are illustrating.\n- I am not sure what insights the readers are supposed to gain from the visual comparisons between the Euclidean and Poincare embeddings. \n\nDue to the poor presentation, I actually have difficulty making sense of the evaluation in this paper (it would help if the text was legible). I think this paper requires significant work and it not suitable for publication in its current state.\n\nAs a kind of unrelated note. It occurs to me that papers on hyperbolic embeddings tend to evaluate evaluate on attribute or link prediction. It would be great if authors would also evaluate these pretrained embeddings on downstream applications such as relation extraction, knowledge base population etc.","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1515642530827,"tcdate":1512987228395,"number":3,"cdate":1512987228395,"id":"SyVv9AjWG","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Review","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer1"],"readers":["everyone"],"content":{"title":"Nice idea but missing literature, poor paper organisation, and experiments that could be more complete","rating":"5: Marginally below acceptance threshold","review":"The authors present a neural embedding technique using a hyperbolic space.\nThe idea of embedding data into a space that is not Euclidean is not new.\nThere have been attempts to project onto (hyper)spheres.\nAlso, the proposal bears some resemblance with what is done in t-SNE, where an (exponential) distortion of distances is induced. Discussing this potential similarity would certainly broaden the readership of the paper.\n\nThe organisation of the paper might be improved, with a clearer red line and fewer digressions.\nThe call to the very small appendix via eq. 17 is an example.\nThe position of Table in the paper is odd as well.\nThe order of examples in Fig.5 differs from the order in the list.\n\nThe experiments are well illustrative but rather small sized.\nThe qualitative assessment is always interesting and it is completed with some label prediction task.\nDue the geometrical consideretations developed in the paper, other quality criteria like e.g. how well neighbourhoods are preserved in the embeddings would give some more insights.\n\nAll in all the idea developed in the paper sounds interesting but the paper organisation seems a bit loose and additional aspects should be investigated. ","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1512987073603,"tcdate":1512987073603,"number":5,"cdate":1512987073603,"id":"HJ5atCj-G","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer1"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/AnonReviewer1"],"content":{"title":"The paper somehow overlooks literature from dimensionality reduction","comment":"The authors present a neural embedding technique using a hyperbolic space.\nThe idea of embedding data into a space that is not Euclidean is not new.\nThere have been attempts to project onto (hyper)spheres.\nAlso, the proposal bears some resemblance with what is done in t-SNE, where an (exponential) distortion of distances is induced. Discussing this potential similarity would certainly broaden the readership of the paper.\n\nThe organisation of the paper might be improved, with a clearer red line and fewer digressions.\nThe call to the very small appendix via eq. 17 is an example.\nThe position of Table in the paper is odd as well.\nThe order of examples in Fig.5 differs from the order in the list.\n\nThe experiments are well illustrative but rather small sized.\nThe qualitative assessment is always interesting and it is completed with some label prediction task.\nDue the geometrical consideretations developed in the paper, other quality criteria like e.g. how well neighbourhoods are preserved in the embeddings would give some more insights.\n\nAll in all the idea developed in the paper sounds interesting but the paper organisation seems a bit loose and additional aspects should be investigated. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1512730929081,"tcdate":1512730929081,"number":4,"cdate":1512730929081,"id":"rkKEbeO-z","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"Sk2scfwWM","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer2"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/AnonReviewer2"],"content":{"title":"I'm sympathetic, but I need stronger arguments","comment":"I'm sympathetic to your position; on a personal level I understand the frustration of seeing a large body of work reduced simply because somebody else got there infinitesimally faster.\n\nWith that in mind, I'm willing to listen to the arguments of why this paper brings \"new perspective\" to the table. You bring up two key differences:\n\n1) You use a natural parametrization of the Poincare ball, which you state is more \"elegant\". I see that there is some benefit to having an unconstrained optimization vs one which is constrained. Is that what you mean by \"elegant\" or is there more to this claimed elegance? Can you empirically quantify the benefits of your parametrization?\n\n2) You use a cosine-based similarity measure rather than the standard hyperbolic distance. You mention this is a difference, but does it also offer benefits? From a geometric point of view, I guess it seems more \"elegant\" to use the standard hyperbolic distance. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1512676003601,"tcdate":1512676003601,"number":3,"cdate":1512676003601,"id":"Sk2scfwWM","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"HyiISdgef","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"Both papers were written at the same time and the approaches differ","comment":"Our paper is similar to “Poincaré Embeddings for Learning Hierarchical Representations” (Nickel and Kiela, NIPS 2017). However, both were written independently at the same time. While they are similar, there are several important differences between them. Our work uses a spherical coordinate system, instead of cartesian coordinates, which we believe is more elegant as it exploits the symmetry of the Poincare ball. In addition, we use natural hyperbolic coordinates that extend radially (0,inf) instead of (0,1). Natural coordinates remove the numerical issues at the boundary of the ball. In the paper by Nickel and Kiela, this is a problem as the optimization will push points to values greater than 1. To fix this, they introduce a heuristic that moves points a small distance back inside the ball when the optimizer pushes them outside. As most of the space is towards the edge of the ball, many of the points in their system are effectively placed solely by the heuristic. This is not a problem in natural coordinates and we simply switch back coordinate systems on completion of the optimization process. In addition, we use a different similarity metric based on cosine similarity instead of the hyperbolic distance.\n\nDespite the similarity, it is worthwhile publishing this paper as it provides a complementary perspective on the problem and grows the evidence base for this as a powerful technique that other researchers can employ. Our paper shows that the principle of hyperbolic embedding can be achieved using a cosine-similarity approach as well as the distance-similarity approach used in the other paper, and so demonstrates a more general applicability of the idea. An example of where this has previously occurred are Auto-Encoding Variational Bayes (Kingma and Welling, ICLR 2014) https://arxiv.org/abs/1312.6114 and later Stochastic Backpropagation and Approximate Inference in Deep Generative Models Rezende et al., ICML 2014) https://arxiv.org/abs/1401.4082\nBoth papers contained the same idea and appeared on arxiv within a month, but provided different perspectives.\n\n"},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1512030625134,"tcdate":1512030625134,"number":2,"cdate":1512030625134,"id":"SyFo-HpeM","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"Sy6BFU_eG","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"We are mostly concerned with learning high quality embeddings","comment":"We agree with the comment that the similarity is somewhat heuristic and inspired by the hyperbolic geometry. We care about the quality of the embeddings most of all. The use of heuristics to find good embeddings is quite common in the literature. For instance, negative sampling is often used, which abandons the strict maximization of the log probability of the softmax to learn more efficiently. What we are doing is in the same spirit as negative sampling."},"nonreaders":[],"replyCount":0,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1511708997116,"tcdate":1511708997116,"number":2,"cdate":1511708997116,"id":"Sy6BFU_eG","invitation":"ICLR.cc/2018/Conference/-/Paper92/Public_Comment","forum":"S1xDcSR6W","replyto":"r1aJmPBef","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"The similarity function still doesn't reflect the hyperbolic geometry.","comment":"What is misleading is to describe these embeddings as inheriting the geometrical properties of the hyperbolic space, which isn't true.  \n\nIt is true that the similarity function defined by the authors inherits some of the properties of the hyperbolic space, as they explained it above. \n\nHowever, it should be emphasized that it is only vaguely related to the hyperbolic metric via some heuristic, and that most properties characterizing a space endorsed with a hyperbolic structure will not be satisfied by the word-embedding space.\n\nNamely, with this similarity measure, one loses the possibility to use the conformality of the hyperbolic metric, which gave us closed forms to compute curvature tensors, volume elements, the metric tensor, the exponential map and geodesics... Which should be clearly stated at the beginning of the paper, in order to not mislead readers expecting a real exploitation of hyperbolic geometry in the embedding space. "},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1511514852988,"tcdate":1511514852988,"number":1,"cdate":1511514852988,"id":"r1aJmPBef","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Comment","forum":"S1xDcSR6W","replyto":"SylAxmQeM","signatures":["ICLR.cc/2018/Conference/Paper92/Authors"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference/Paper92/Authors"],"content":{"title":"We define a similarity in the objective function for points in hyperbolic space. We do not claim that this is a dot product.","comment":"\nHyperbolic space is not a vector space and so does not have a globally defined inner-product. We have defined a measure of similarity between embedded points, which is a cosine similarity weighted by the distance in hyperbolic space. \nThe hyperbolic metric comes into this because it is the hyperbolic distances from the origin (using this metric) that weight the cosine distance.\nThe net effect is that when the coordinates of points are updated, the updates in angular directions (ie. perpendicular to the radial direction) are suppressed for points far away from the origin, by a factor related to their distance from the origin in the hyperbolic space. This has the desired effect of allowing many peripheral points to be mutually distant, while simultaneously close to central points, as, for example, in figures 3 and 4.\nThe intention is to be able to use the machinery of neural embeddings while also getting the useful geometric properties of hyperbolic space."},"nonreaders":[],"replyCount":1,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1511366855619,"tcdate":1511366855619,"number":1,"cdate":1511366855619,"id":"SylAxmQeM","invitation":"ICLR.cc/2018/Conference/-/Paper92/Public_Comment","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["(anonymous)"],"readers":["everyone"],"writers":["(anonymous)"],"content":{"title":"These embeddings are not in the hyperbolic space as claimed...","comment":"This paper is completely wrong: by changing the dot-product, you cannot talk about a hyperbolic space anymore. \n\nThe dot-product given by the authors has nothing to do with the hyperbolic riemannian metric.\n\n"},"nonreaders":[],"replyCount":2,"writable":false,"revisions":false,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1515642530871,"tcdate":1511304371649,"number":2,"cdate":1511304371649,"id":"rynh2mGgf","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Review","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer3"],"readers":["everyone"],"content":{"title":"nice paper, good concepts, not enough experiments. ","rating":"7: Good paper, accept","review":"This paper proposes tree vertex embeddings over hyperbolic space. The conditional predictive distribution is the softmax of <v1, v2>_H = ||v1|| ||v2|| cos(theta1-theta2), and v1, v2 are points  defined via polar coordinates (r1,theta1), and (r2,theta2).\nTo evaluate, the authors show some qualitative embeddings of graph and 2-d projections, as well as F1 scores in identifying the biggest cluster associated with a class. \n\nThe paper is well motivated, with an explanation of the technique as well as its applications in tree embedding in general. I also like the evaluations, and shows a clear benefit of this poincare embedding vs euclidean embedding.\n\nHowever, graph embeddings are now a very well explored space, and this paper does not seem to mention or compare against other hyperbolic (or any noneuclidean) embedding techniques. From a 2 second google search, I found several sources with very similar sounding concepts:\n\nMaximilian Nickel, Douwe Kiela, Poincaré Embeddings for Learning Hierarchical Representations\n\nA Cvetkovski, M Crovella, Hyperbolic Embedding and Routing for Dynamic Graphs\n\nYuval Shavitt, Tomar Tankel, Hyperbolic Embedding of Internet Graph for Distance Estimation and Overlay Construction\n\nThomas Bläsius, Tobias Friedrich, Anton Krohmer, andSören Laue. Efficient Embedding of Scale-Free Graphs in the Hyperbolic Plane\n\nI think this paper does have some novelty in applying it to the skip-gram model and using deep walk, but it should make more clear that using hyperbolic space embeddings for graphs is a popular and by now, intuitive construct. Along the same lines, the benefit of using the skip-gram and deep-walk techniques should be compared against some of the other graph embedding techniques out there, of which none are listed in the experiment section. \n\nOverall, a detailed comparison against 1 or 2 other hyperbolic graph embedding techniques would be sufficient for me to change my vote to accept. \n\n\n","confidence":"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1515642530911,"tcdate":1511191891076,"number":1,"cdate":1511191891076,"id":"HyiISdgef","invitation":"ICLR.cc/2018/Conference/-/Paper92/Official_Review","forum":"S1xDcSR6W","replyto":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference/Paper92/AnonReviewer2"],"readers":["everyone"],"content":{"title":"Done before?","rating":"4: Ok but not good enough - rejection","review":"== Preamble ==\n\nAs promised, I have read the updated paper from scratch and this is my revised review. My original review is kept below for reference. My original review had rating \"4: Ok but not good enough - rejection\".\n\n== Updated review ==\n\nThe revised improves upon the original submission in several ways and, in particular, does a much better job at positioning itself within the existing body of literature. The new experiments also indicate that the proposed model offer some improvement over Nickel & Douwe, NIPS 2017).\n\nI do have remaining concerns that unfortunately still prevent me from recommending acceptance:\n\n- Throughout the paper it is argued that we should embed into a hyperbolic space. Such a space is characterized by its metric, but the proposed model do not use a hyperbolic metric. Rather it relies on a heuristic similarity measure that is inspired by the hyperbolic metric. I understand that this may be a practical choice, but then I find it misleading that the paper repeatedly states that points are embedded into a hyperbolic space (which is incorrect). This concern was also raised on this forum prior to the revision.\n\n- The resulting optimization is one of the key selling points of the proposed method as it is unconstrained while Nickel & Douwe resort to a constrained optimization. Clearly unconstrained optimization is to be preferred. However, it is not entirely correct (from what I understand), that the resulting optimization is indeed unconstrained. Nickel & Douwe work under the constraint that |x| < 1, while the proposed model use polar coordinates (r, theta): r in (0, infinity) and theta in (0, 2 pi]. Note that theta parametrize a circle, and therefore wrapping may occur (this should really be mentioned in the paper). The constraints on theta are quite easy to cope with, so I agree with the authors that they have a more simple optimization problem. However, this is only true since points are embedded on the unit disk (2D). Should you want to embed into higher dimensional spaces, then theta need to be confined to live on the unit sphere, i.e. |theta| = 1 (the current setting is just a special-case of the unit sphere). While optimizing over the unit sphere is manageable it is most definitely a constrained optimization problem, and it is far from clear that it is much easier than working under the Nickel & Douwe constraint, |x| < 1.\n\nOther comments:\n- The sentence \"even infinite trees have nearly isometric embeddings in hyperbolic space (Gromov, 2007)\" sounds cool (I mean, we all want to cite Gromov), but what does it really mean? An isometric embedding is merely one that preserves a metric, so this statement only makes sense if the space of infinite trees had a single meaningful metric in the first place (it doesn't; that's a design choice).\n\n- In the \"Contribution\" and \"Conclusion\" sections it is claimed that the paper \"introduce the new concept of neural embeddings in hyperbolic space\". I thought that was what Nickel & Douwe did... I understand that the authors are frustrated by this parallel work, but at this stage, I don't think the present paper can make this \"introducing\" claim.\n\n- The caption in Figure 2 miss some indication that \"a\" and \"b\" refer to subfigures. I recommend \"a\" --> \"a)\" and \"b\" --> \"b)\".\n\n- On page 4 it is mentioned that under the heuristic similarity measure some properties of hyperbolic spaces are lost while other are retained. From what I can read, it is only claimed that key properties are kept; a more formal argument (even if trivial) would have been helpful.\n\n\n== Original Review ==\n\nThe paper considers embeddings of graph-structured data onto the hyperbolic Poincare ball. Focus is on word2vec style models but with hyperbolic embeddings. I am unable to determine how suitable an embedding space the Poincare ball really is, since I am not familiar enough with the type of data studied in the paper. I have a few minor comments/questions to the work, but my main concern is a seeming lack of novelty:\nThe paper argues that the main contribution is that this is the first neural embedding onto a hyperbolic space. From what I can see, the paper\n\n  Poincaré Embeddings for Learning Hierarchical Representations\n  https://arxiv.org/abs/1705.08039\n\nconsider an almost identical model to the one proposed here with an almost identical motivation and application set. Some technicalities appear different, but (to me) it seems like the main claimed novelties of the present paper has already been out for a while. If this analysis is incorrect, then I encourage the authors to provide very explicit arguments for this in the rebuttal phase.\n\nOther comments:\n*) It seems to me that, by construction, most data will be pushed towards the boundary of the Poincare ball during the embedding. Is that a property you want?\n*) I found it rather surprising that the log-likelihood under consideration was pushed to an appendix of the paper, while its various derivatives are part of the main text. Given the not-so-tight page limits of ICLR, I'd recommend to provide the log-likelihood as part of the main text (it's rather difficult to evaluate the correctness of a derivative when its base function is not stated).\n*) In the introduction must energy is used on the importance of large data sets, but it appears that only fairly small-scale experiments are considered. I'd recommend a better synchronization.\n*) I find visual comparisons difficult on the Poincare ball as I am so trained at assuming Euclidean distances when making visual comparisons (I suspect most readers are as well). I think one needs to be very careful when making visual comparisons under non-trivial metrics.\n*) In the final experiment, a logistic regressor is fitted post hoc to the embedded points. Why not directly optimize a hyperbolic classifier?\n\nPros:\n+ well-written and (fairly) well-motivated.\n\nCons:\n- It appears that novelty is very limited as highly similar work (see above) has been out for a while.\n\n","confidence":"3: The reviewer is fairly confident that the evaluation is correct"},"writers":[],"nonreaders":[],"replyCount":1,"writable":false,"revisions":true,"tags":[],"forumContent":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]}},{"tddate":null,"ddate":null,"tmdate":1515156075705,"tcdate":1508952663717,"number":92,"cdate":1509739487864,"id":"S1xDcSR6W","invitation":"ICLR.cc/2018/Conference/-/Blind_Submission","forum":"S1xDcSR6W","signatures":["ICLR.cc/2018/Conference"],"readers":["everyone"],"writers":["ICLR.cc/2018/Conference"],"content":{"title":"Hybed: Hyperbolic Neural Graph Embedding","abstract":"Neural embeddings have been used with great success in Natural Language Processing (NLP) where they provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph-based tasks, embeddings in high-dimensional Euclidean spaces have been learned.\nHowever, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but a negatively curved hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that hyperbolic embeddings significantly outperform Euclidean embeddings on vertex classification tasks for several real-world public datasets. ","pdf":"/pdf/c5a4cd965e6f379dbe477e9d5d8e341fe2547a95.pdf","TL;DR":"We learn neural embeddings of graphs in hyperbolic instead of Euclidean space","paperhash":"anonymous|hybed_hyperbolic_neural_graph_embedding","_bibtex":"@article{\n  anonymous2018hybed:,\n  title={Hybed: Hyperbolic Neural Graph Embedding},\n  author={Anonymous},\n  journal={International Conference on Learning Representations},\n  year={2018},\n  url={https://openreview.net/forum?id=S1xDcSR6W}\n}","authors":["Anonymous"],"authorids":["ICLR.cc/2018/Conference/Paper92/Authors"],"keywords":["embeddings","hyperbolic space","neural networks","geometry"]},"nonreaders":[],"replyCount":20,"writable":false,"revisions":true,"tags":[],"forumContent":null,"tauthor":"ICLR.cc/2018/Conference"}],"limit":2000,"offset":0}