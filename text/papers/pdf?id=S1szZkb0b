Under review as a conference paper at ICLR 2018
Learning a set of interrelated tasks by using a succession of motor policies for a socially guided intrinsically motivated learner
Anonymous authors Paper under double-blind review
ABSTRACT
We propose an active learning architecture, capable of organizing its learning process to learn complex motor policies (which are succession of primitive motor policies) achieving multiple outcomes: Socially Guided Intrinsic Motivation at High Level (SGIM-HL). The learner can generalize over its experience to continuously learn new outcomes, by choosing actively what and how to learn guided by empirical measures of its own progress. In this paper, we are considering the learning of a set of interrelated complex outcomes hierarchically organized. We introduce a new framework called "procedures", which enables the autonomous discovery of how to combine previously learned skills in order to learn increasingly more complex motor policies (combinations of primitive motor policies). Our architecture can actively decide which outcome to focus on and which exploration strategy to apply. Those strategies could be autonomous exploration, or active social guidance, where it relies on the expertise of a human teacher providing demonstrations at the learner's request. We show on a simulated environment that our new architecture is capable of tackling the learning of complex motor policies, to adapt the complexity of its policies to the task at hand. We also show that our "procedures" increases the agent's capability to learn complex tasks.
1 INTRODUCTION
Recently, efforts in the robotic industry and academic field have been made for integrating robots in previously human only environments. In such a context, the ability for service robots to continuously learn new skills, autonomously or guided by their human counterparts, has become necessary. They would be needed to carry out multiple tasks, especially in open environments, which is still an ongoing challenge in robotic learning. Those tasks can be independent and self-contained but they can also be complex and interrelated, needing to combine learned skills from simpler tasks to be tackled efficiently.
The range of tasks those robots need to learn can be wide and even change after the deployment of the robot, we are therefore taking inspiration from the field of developmental psychology to give the robot the ability to learn. Taking a developmental robotic approach (Lungarella et al. (2003)), we combine the approaches of active motor skill learning of multiple tasks, interactive learning and strategical learning into a new learning algorithm and we show its capability to learn a mapping between a continuous space of parametrised outcomes (sometimes referred to as tasks) and a space of parametrised motor policies (sometimes referred to as actions).
1.1 ACTIVE MOTOR SKILL LEARNING OF MULTIPLE TASKS
Classical techniques based on Reinforcement Learning (Theodorou et al. (2010), Stulp & Schaal (2011)) still need an engineer to manually design a reward function for each particular task. Intrinsic motivation, which triggers curiosity in human beings according to developmental psychology (Deci & Ryan (1985)), was introduced in highly-redundant robots to make them learn a wider range of tasks.

Under review as a conference paper at ICLR 2018
In knowledge-based approaches (Oudeyer & Kaplan (2009)), the learner was driven by unexpected outcomes (Barto et al. (2004)). However those approaches encounter limitations when the sensorimotor dimensionality increases. Another approach using competence progress measures successfully drove the learner's exploration through goal-babbling (Baranes & Oudeyer (2010), Rolf et al. (2010)).
However when the dimension of the outcome space increases, these methods become less efficient (Baranes & Oudeyer (2013)) due to the curse of dimensionality, or when the reachable space of the robot is small compared to its environment. Thus, heuristics such as social guidance can help by driving its exploration towards interesting and reachable space fast.
1.2 INTERACTIVE LEARNING
Combining intrinsically motivated learning and imitation Nguyen et al. (2011) has bootstrapped exploration by providing efficient human demonstrations of motor policies and outcomes.
Also, such a learner has been shown to be more efficient if requesting actively a human for help when needed instead of being passive, both from the learner or the teacher perspective (Cakmak et al. (2010)). This approach is called interactive learning and it enables a learner to benefit from both local exploration and learning from demonstration. Information could be provided to the robot using external reinforcement signals (Thomaz & Breazeal (2008)), actions (Grollman & Jenkins (2010)), advice operators (Argall et al. (2008)), or disambiguation among actions (Chernova & Veloso (2009)). Another advantage of introducing imitation learning techniques is to include nonrobotic experts in the learning process (Chernova & Veloso (2009)). One of the key element of these hybrid approaches is to choose when to request human information or learn in autonomy such as to diminish the teacher's attendance.
1.3 STRATEGIC LEARNING
Approaches enabling the learner to choose either what to learn (which outcome to focus on) or how to learn (which strategy to use such as imitation) are called strategic learning (Lopes & Oudeyer (2012)). They aim at giving an autonomous learner the capability to self-organize its learning process.
Some work has been done to enable a learner to choose on which task space to focus. The SAGGRIAC algorithm (Baranes & Oudeyer (2010)), by self-generating goal outcomes fulfils this objective. Other approaches focused on giving the learner the ability to change its strategy (Baram et al. (2004)) and showed it could be more efficient than each strategy taken alone.
Fewer studies have been made to enable a learner to choose both its strategy and its target outcome. The problem was introduced and studied in Lopes & Oudeyer (2012), and was implemented for an infinite number of outcomes and policies in continuous spaces by the SGIM-ACTS algorithm (Nguyen & Oudeyer (2012)). This algorithm is capable of organizing its learning process, by choosing actively both which strategy to use and which outcome to focus on. It relies on the empirical evaluation of its learning progress. It could choose among autonomous exploration driven by intrinsic motivation and low-level imitation of one of the available human teachers to learn more efficiently. It showed its potential to learn on a real high dimensional robot a set of hierarchically organized tasks (Duminy et al. (2016)), which is why we consider it to learn complex motor policies.
1.4 LEARNING COMPLEX MOTOR POLICIES
In this article, we tackle the learning of complex motor policies, which we define as combinations of simpler policies. We described it more concretely as a sequence of primitive policies.
A first approach to learning complex motor policies is to use via-points (Stulp & Schaal (2011)). Via-points enable the definition of complex motor policies. Those via-points are in the robot motor policy space. When increasing the size of the complex policy (by chaining more primitive actions together), we can tackle more complex tasks. However, this would increase the difficulty for the learner to tackle simpler tasks which would be reachable using less complex policies. We wanted to enable the learner to decide autonomously the complexity of the policy necessary to solve a task. Options (Sutton et al. (1999)) are a different way to tackle this problem by offering temporally

Under review as a conference paper at ICLR 2018
abstract actions to the learner. However each option is built to reach one particular task and they have only been tested for discrete tasks and actions, in which a small number of options were used, whereas our new proposed learner is to be able to create an unlimited number of complex policies.
As we aim at learning a hierarchical set of interrelated complex tasks, our algorithm could also benefit from this task hierarchy (as Forestier & Oudeyer (2016) did for learning tool use with simple primitive policies only), and try to reuse previously acquired skills to build more complex ones. Barto et al. (2013) showed that building complex actions made of lower-level actions according to the task hierarchy can bootstrap exploration by reaching interesting outcomes more rapidly.
Adapting SGIM-ACTS to learn complex motor policies of unlimited size, while providing it with a new mechanism called "procedures" (see Section 2.2) which proposes to combine known policies according to their outcome, we developed a new algorithm called Socially Guided Intrinsic Motivation at High-Level (SGIM-HL) capable of taking task hierarchy into account to learn a set of complex interrelated tasks using adapted complex policies. We will describe an experiment, on which we have tested our algorithm, and we will present and analyze the results.
2 OUR APPROACH
Inspired by developmental psychology, we combine interactive learning and autonomous exploration in a strategic learner, which learning process is driven by intrinsic motivation. This learner also takes task hierarchy into account to reuse its previously learned skills while adapting the complexity of its policy to the complexity of the task at hand.
In this section, we formalize our learning problem and explain the principles of SGIM-HL.
2.1 PROBLEM FORMALIZATION
In our approach, an agent can perform motions through the use of policies , parametrized by   , and those policies have an effect on the environment, which we call the outcome   . The agent is then to learn the mapping between  and : it learns to predict the outcome  of each policy  (the forward model M ), but more importantly, it learns which policy to choose for reaching any particular outcome (an inverse model L). The outcomes  can be of different dimensionality and thus be split in task spaces i  . The policies do not only consist of one primitive but of a succession of a finite number of primitives (encoded by the same set of parameters   ) that are executed sequentially by the agent. Hence, policies also are of different dimensionality and are split in policy spaces i   (where i corresponds to the number of primitives used in each policy of i). Complex policies are represented by concatenating the parameters of each of its primitive policies in execution order. When trying to reach an outcome g with a policy , the learner will compute its competence d(, g), by comparing it with the outcome  it actually reached (using normalized Euclidean distance), and use it to improve the accuracy of its inverse model L. The improvement in competence, called competence progress p(g), is the metric that drives the interest of the learner.
2.2 PROCEDURES
As this algorithm tackles the learning of complex hierarchically organized tasks, exploring and exploiting this hierarchy could ease the learning of the more complex tasks. We define procedures as a way to encourage the robot to reuse previously learned skills, and chain them to build more complex ones. More formally, a procedure ti tj is built by choosing two previously known outcomes (ti, tj  ), and chain the policies that reached them.
Executing a procedure ti tj means building the complex policy  corresponding to the succession of both policies i and j and execute it (where i and j reach respectively ti and tj).
It is important to note that even if procedures are task-oriented and take advantage from the task hierarchy, they are only a criteria to select interesting motor policies to be combined. Furthermore, ti and tj might be unknown from the learner. Hence, ti and tj can be updated (see Algo. 1) to subtasks t1 and t2 which are feasible by the learner according to its current skill set. And the complex policy built from the succession of the policies 1 and 2 (associated with t1 and t2) is then executed by the learner. Also the learner is not limited in the procedures it builds and the

Under review as a conference paper at ICLR 2018

outcome spaces used to create those procedures are not manually programmed but discovered by the learner.
Algorithm 1 Procedure modification before execution
Input: (ti, tj)  2 Input: inverse model L
t1 Nearest-Neighbour(ti) t2 Nearest-Neighbour(tj) 1  L(t1) 2  L(t2) return  = 1 2

2.3 SOCIALLY GUIDED INTRINSIC MOTIVATION AT HIGH LEVEL

Procedures Exploration Strategies
Policies Exploration Strategies

 = mimic procedural teacher n

 = explore procedures autonomously

Strategy Level
Outcome Space
Exploration
Procedural Space
Exploration

Request to teacher n
Correspondence

tdi+tdj
Mimic Procedure

Goal Directed Procedure Optimization

ti+tj

g d

Policy Space Exploration

Procedure execution
r, 

Select Goal Outcome & Strategy

 = explore policies
autonomously

 = mimic policy
teacher n

progress Outcome & Strategy
Interest Mapping

g d

Request to teacher n
Correspondence

d

Goal Directed Policy Optimization

Mimic Policy

r, 

Figure 1: SGIM-HL architecture

The SGIM-HL algorithm (see Fig. 1) learns by episodes, which first starts by choosing both which outcome g   to target and which data collection strategy  to use for learning it. This choice is biased by the empirical progress the learner has made in each region Rn of the outcome space . It is made according to a sampling mode, chosen randomly at the beginning of the episode:
· mode 1: exploration mode which choose  and g   at random (has a probability p1 of being used);
· mode 2: choose the outcome region Rn() and thus the strategy  with a probability proportional to its interest value, and then generate a goal randomly inside it;
· mode 3: choose the outcome region and strategy like in mode 2, but generate a goal g close to the already experienced one with the highest interest value of the region.
The available strategies are: autonomous exploration of the policy space, autonomous exploration of the procedure space, mimicry of one of the available policy teachers and mimicry of one of the available procedural teachers. You can see the pseudo-code for the SGIM-HL algorithm in Algo. 2.
In an episode under the autonomous exploration of the policy space strategy, the learner tries to optimize a policy  to produce g the best by choosing between random exploration of policies and local optimization, following the SAGG-RIAC algorithm Baranes & Oudeyer (2010) (GoalDirected Policy Optimization(g)). Local optimization is done using multivariate linear regression to interpolate from the known policies reaching an outcome close to g.
In an episode under the autonomous exploration of the procedural space strategy, the learner builds a procedure ti tj such as to reproduce the goal outcome g the best (Goal-Directed Procedure Optimization(g)). It chooses between random exploration of procedures (which builds procedures by generating two subtasks at random), and local procedure optimization, which works the same

Under review as a conference paper at ICLR 2018

as local optimization of policies except with procedures. The procedure built is then modified and executed, following Algo. 1.
In an episode under the mimicry of one policy teacher strategy, the learner requests a demonstration d from the chosen teacher. d is selected by the teacher as the closest from the goal outcome g in its demonstration repertoire. The learner then repeats the demonstrated policy (Mimic Policy(d)).
In an episode under the mimicry of one procedural teacher strategy, the learner requests a procedural demonstration tdi tdj which is built by the chosen teacher according to a preset function which depends on the target outcome g. Then the learner tries to reproduce the demonstrated procedure by refining and executing it, following Algo. 1 (Mimic Procedure(tdi tdj)).
After each episode, the learner stores the policies and procedures executed along with their reached outcomes in its episodic memory. Then it updates its interest model according to the progress it has made (including the outcome spaces reached but not targeted) in order to choose the strategy and task in the next episode. The interest interest(, ) of each outcome added depends on both the progress p() made and the cost K() of the strategy used: interest(, ) = p()/K(). The outcomes reached and the goal are added along with their interest measure in their corresponding region, which is then splat when exceeding a fixed number of points to discriminate the regions of high and low interest for the learner. The method used is described in Nguyen & Oudeyer (2012).
Algorithm 2 SGIM-HL
Input: the different strategies 1, ..., n Initialization: partition of outcome spaces R  i{i} Initialization: episodic memory Memo  
loop g,   Select Goal Outcome and Strategy(R) if  = Mimic policy teacher i strategy then (d, d)  ask and observe demonstrated policy to teacher i Memo  Mimic Policy(d) else if  = Mimic procedural teacher i strategy then (tdi tdj, d)  ask and observe demonstrated procedure to teacher i Memo  Mimic Procedure(tdi tdj) else if  = Autonomous exploration of procedures strategy then Memo  Goal-Directed Procedure Optimization(g) else  = Autonomous exploration of policies strategy Memo  Goal-Directed Policy Optimization(g) end if Update L-1 with collected data Memo R  Update Outcome and Strategy Interest Mapping(R,Memo,g)
end loop

When the learner computes nearest neighbours to select policies or procedures to optimize (when choosing local optimization in any of both autonomous exploration strategies and when refining procedures), it actually uses a performance metric (1) which takes into account the complexity of the policy chosen:

perf = d(, g)n

(1)

where d(, g) is the normalized Euclidean distance between the target outcome g and the outcome  reached by the policy,  is a constant and n is equal to the size of the policy (the number of
primitives chained).

3 EXPERIMENT
We designed an experiment with a simulated robotic arm, which can move in its environment and interact with objects in it. It can learn an infinite number of tasks, organized as 6 hierarchically

Under review as a conference paper at ICLR 2018

(x, y, z)

z
(x, y)

(x, y, z)

(x, y, z)
y

(xa, ya)

(x, y, z)
x
(xb, yb)

Figure 2: Experimental setup: a robotic arm, can interact with the different objects in its environment (a pen and two joysticks). Both joysticks enable to control a video-game character (represented in top-right corner). A grey floor limits its motions and can be drawn upon using the pen (a possible drawing is represented).
organized types of tasks. The robot is capable of performing complex policies of unrestricted size (i.e. consisting of any number of primitives), with primitive policies highly redundant and of a high dimensionality.
3.1 SIMULATION SETUP
The Fig. 2 shows a representation of the 3D environmental setup (contained in a cube delimited by x  [-1; 1], y  [-1; 1] and z  [-1; 1]. The learning agent is a robotic arm of 3 joints with the base centred on the horizontal plane, able to rotate freely along the vertical axis (each link of the robot has a length of 0.33). The robot can also change its vertical position to move in a different horizontal plane. The robot can grab objects in this environment, by hovering its arm tip (shown in blue in the figure) close to them. The robot can interact with the following:
· Floor (below z = 0.0): limits the motions of the robot, slightly elastic which enable the robot to go down to z = -0.2 by forcing on it;
· Pen: can be moved around and draw on the floor, broken if forcing too much on the floor (when z <= -0.3);
· Joystick 1 (the left one on the figure): can be moved inside a cube-shaped area (automatically released otherwise, position normalized for this area), its x-axis position control a video-game character x position on the screen when grabbed by the robot;
· Joystick 2 (the right one on the figure): can be moved inside a cube-shaped area (automatically released otherwise, position normalized for this area), its y-axis position control a video-game character y position on the screen when grabbed by the robot;
· Video-game character: can be moved on the screen by using the two joysticks, its position is refreshed only at the end of a primitive policy execution for the manipulated joystick.
The robot grabber can only handle one object. When it touches a second object, it breaks, releasing both objects.

Under review as a conference paper at ICLR 2018

The robot always starts from the same position before executing a policy, and primitives are executed sequentially without getting back to this initial position. Whole complex policies are recorded with their outcomes, but each step of the complex policy execution is recorded.
3.2 EXPERIMENT VARIABLES
3.2.1 POLICY SPACES
The motions of each of the three joints of the robot are encoded using a one-dimensional Dynamic Movement Primitive (Pastor et al. (2009)), defined by the system:

 v = K(g - x) - Dv + (g - x0)f (s)  x = v  s = -s

(2) (3) (4)

where x and v are the position and velocity of the system; s is the phase of the motion; x0 and g are the starting and end position of the motion;  is a factor used to temporally scale the system (set to fix the length of a primitive execution); K and D are the spring constant and damping term fixed for the whole experiment;  is also a constant fixed for the experiment; and f is a non-linear term used to shape the trajectory called the forcing term. This forcing term is defined as:

f (s) = i ii(s)s i i(s)

(5)

where i(s) = exp(-hi(s - ci)2) with centers ci and widths hi fixed for all primitives. There are 3 weights i per DMP.

The weights of the forcing term and the end positions are the only parameters of the DMP used by the robot. The starting position of a primitive is set by either the initial position of the robot (if it is starting a new complex policy) or the end position of the preceding primitive. The robot can also set its position on the vertical axis z for every primitive. Therefore a primitive policy  is parametrized by:

 = (a0, a1, a2, z)

(6)

where ai = (0(i), 1(i), 2(i), g(i)) corresponds to the DMP parameters of the joint i, ordered from base to tip, and z is the fixed vertical position. When combining two or more primitive policies
(0 , 1 , ...), in a complex policies , the parameters (0, 1, ...) are simply concatenated together from the first primitive to the last.

3.2.2 TASK SPACES
The task spaces the robot learns are hierarchically organized and defined as:
· 0: the position (x0, y0, z0) of the end effector of the robot in Cartesian coordinates at the end of a policy execution;
· 1: the position (x1, y1, z1) of the pen at the end of a policy execution if the pen is grabbed by the robot;
· 2: the first (xa, ya) and last (xb, yb) points of the last drawn continuous line on the floor if the pen is functional (xa, ya, xb, yb);
· 3: the position (x3, y3, z3) of the first joystick at the end of a policy execution if it is grabbed by the robot;
· 4: the position (x4, y4, z4) of the second joystick at the end of a policy execution if it is grabbed by the robot;
· 5: the position (x5, y5) of the video-game character at the end of a policy execution if moved.

Under review as a conference paper at ICLR 2018
3.3 THE TEACHERS
To help the SGIM-HL learner, procedural teachers were available so as to provide procedures for every outcome spaces but 0. Each teacher was only giving procedures useful for its own outcome space, and was aware of the task representation. They all had a cost of 5. The rules used to provide procedures are the following:
· ProceduralTeacher1 (1): t1 t0 with t1  1 corresponding to the pen initial position and t0  0 corresponding to the desired final pen position;
· ProceduralTeacher2 (2): t1 t0 with t1  1 corresponding to the point on the z = 1.0 plane above the first point of the desired drawing, and t0  0 corresponding to the desired final drawing point;
· ProceduralTeacher3 (3): t3 t0 with t3  3 and t3 = (0, 0, 0), t0  0 corresponding to the end effector position corresponding to the desired final position of the first joystick;
· ProceduralTeacher4 (4): t4 t0 with t4  4 and t4 = (0, 0, 0), t0  0 corresponding to the end effector position corresponding to the desired final position of the second joystick;
· ProceduralTeacher5 (5): t3 t4 with t3  3 and t3 = (x, 0, 0) with x corresponding to the desired x-position of the video-game character, t4  4 and t4 = (0, y, 0) with y corresponding to the desired y-position of the video-game character.
We also added policy teachers corresponding to the same outcome spaces to bootstrap the robot early learning process. The strategy attached to each teacher has a cost of 10. Each teacher was capable to provide demonstrations (as policies executable by the robot) linearly distributed in its outcome space:
· MimicryTeacher1 (1): 15 demonstrations; · MimicryTeacher2 (2): 25 demonstrations; · MimicryTeacher3 (3): 18 demonstrations; · MimicryTeacher4 (4): 18 demonstrations; · MimicryTeacher5 (5): 9 demonstrations;
3.4 EVALUATION METHOD
To evaluate our algorithm, we created a benchmark dataset for each outcome space i, linearly distributed across the outcome space dimensions, for a total of 27,600 points. The evaluation consists in computing the normalized Euclidean distance between each of the benchmark outcome and their nearest neighbour in the learner dataset. Then we compute the mean distance to benchmark for each outcome space. The global evaluation is the mean evaluation for the 6 outcome spaces. This process is then repeated across the learning process at predefined and regularly distributed timestamps.
Then to asses our algorithm efficiency, we compare its results with 4 other algorithms:
· RandomPolicy: performs random exploration of the policy space ; · SAGG-RIAC: performs autonomous exploration of the policy space  guided by intrinsic
motivation; · Random-HL: performs both random exploration of policies and procedures; · SAGG-HL: performs both autonomous exploration of the procedural space and the policy
space, guided by intrinsic motivation; · SGIM-HL: interactive learner driven by intrinsic motivation. Choosing between au-
tonomous exploration strategies (either of the policy space or the procedural space) and mimicry of one of the available teachers (either policy or procedural teachers).
Each algorithm was run 5 times on this setup. Each run, we let the algorithm performs 25,000 iterations (complex policies executions). The value of  for this experiment is 1.2. The probabilities to choose either of the sampling mode of SGIM-HL are p1 = 0.15, p2 = 0.65, p3 = 0.2.

Under review as a conference paper at ICLR 2018
4 RESULTS
Figure 3: Evaluation of all algorithms
The Fig. 3 shows the global evaluation of all the tested algorithms, which corresponds to the mean error made by each algorithm to reproduce the benchmarks with respect to the number of complete complex policies tried. The algorithms capable of performing procedures (Random-HL, SAGG-HL and SGIM-HL) have errors that drop to levels lower than the others, moreover since the beginning of the learning process (shown on Fig. 3). It seems that the procedures bootstrap the exploration, enabling the learner to progress further. Indeed, the autonomous learners RandomPolicy and SAGGRIAC, have significantly better performance when they can use procedures and are thus upgraded to respectively Random-HL and SAGG-HL. We can also see that the SGIM-HL algorithm has a very quick improvement in global evaluation owing to the bootstrapping effect of the different teachers. It goes lower to the final evaluation of the RandomPolicy and SAGG-RIAC (0.17) after only 500 iterations. If we look at the evaluation on each individual outcome space (Fig. 4), we can see that SGIM-HL outperforms the other algorithms, except for the outcome space 5 where SAGG-HL is better, due to the fact that SAGG-HL practised much more than SGIM-HL (500 iterations where the goal was in 5 against 160) on this outcome space. SGIM-HL is much better than the other algorithms on the two joysticks outcome spaces (3 and 4). This is not surprising given the fact that those outcome spaces require precise policies. Indeed, if the end-effector gets out of the area where it can control the joystick, the latter is released, thus potentially ruining the attempt. So on these outcome spaces working directly on carefully crafted policies can alleviate this problem, while using procedures might be tricky, as the outcomes used don't take into account the motion trajectory but merely its final state. SGIM-HL was provided with such policies by the policy teachers. Also if we compare the results of the learners without procedures (RandomPolicy and SAGG-RIAC) with the others, we can see that they learn less on any outcome space but 0 (which was the only outcome space reachable using only single primitive policies and that could not benefit from using the task hierarchy to be learned) and especially for 1, 2 and 5 which were the most hierarchical in this setup. So the procedures helped when learning any potentially hierarchical task in this experiment. We further analyzed the results of our SGIM-HL learner. We looked in its learning process to see which pairs of teachers and target outcomes it has chosen (Fig. 5). It was capable to request demonstrations from the relevant teachers depending on the task at hand, except for the outcome space 0 which had no human teachers and therefore could not find a better teacher to help it. Indeed, for the outcome space 2, the procedural teacher (ProceduralTeacher2) specially built for this outcome space was greatly chosen. We wanted to see if our SGIM-HL learner adapts the complexity of its policies to the working task. We draw 1,000,000 goal outcomes for each of the 0, 1 and 2 subspaces (chosen because they are increasingly complex) and we let the learner choose the known policy that would reach the closest outcome. Fig. 6 shows the results of this analysis.

Under review as a conference paper at ICLR 2018
Figure 4: Evaluation of all algorithms per outcome space (for 0, all evaluations are superposed)
Figure 5: Choices of teachers and target outcomes of the SGIM-HL learner As we can see on those three interrelated outcome subspaces (Fig. 6), the learner is capable to adapt the complexity of its policies to the outcome at hand. It chooses longer policies for the 1 subspace (policies of size 2 and 3 while using mostly policies of size 1 and 2 for 0) and even longer for the 2 subspace (using far more policies of size 3 than for the others). It shows that our learner is capable to correctly limit the complexity of its policies instead of being stuck into always trying longer and longer policies.

Under review as a conference paper at ICLR 2018
Figure 6: Number of policies selected per policy size for three increasingly more complex outcome spaces by the SGIM-HL learner
5 CONCLUSION AND FUTURE WORK
With this experiment, we show the capability of SGIM-HL to tackle the learning of a set of multiple interrelated complex tasks. It successfully uses complex motor policies to learn a wider range of tasks. It was capable to correctly choose the most adapted teachers to the target outcome when available. Though it was not limited in the size of policies it could execute, the learner shows it could adapt the complexity of its policies to the task at hand. The procedures greatly improved the learning capability of autonomous learners, as we can see by the difference between the Random-HL and SAGG-HL learners and the RandomPolicy and SAGGRIAC ones. Our SGIM-HL shows it was capable to use procedures to exploit both the task hierarchy of this experimental setup and previously learned skills. However this new framework of procedures could be better exploited, if it could be recursive (defined as a binary tree structure), allowing the refinement process to select lower-levels procedures as one of the policy component. This process could also be used inside the strategical decisions made by the learner when selecting what and how to learn. This choice of strategy could then be also recursive, allowing the learner to optimize both components of a procedure at the same time to produce the wanted procedures, instead of using the current one-step refinement process. Also, the procedures are here only combinations of two subtasks, it could be interesting to see if the process can extend to combinations of any number of subtasks. Finally, proving the potency of our SGIM-HL learner on a real-world application could show the algorithm potency. We are currently designing such an experiment with a real robotic platform. ACKNOWLEDGMENTS This work was supported by public funds (Ministe`re de l'Education Nationale, de l'Enseignement Supe´rieur et de la Recherche, FEDER (fonds europe´en de de´veloppement re´gional (FEDER)), Re´gion Bretagne, Conseil Ge´n´ral du Finiste`re) and by Institut Mines Te´le´com, received in the framework of the VITAAL project
REFERENCES
Brenna D Argall, Brett Browning, and Manuela Veloso. Learning robot motion control with demonstration and advice-operators. In Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on, pp. 399­404. IEEE, 2008.
Yoram Baram, Ran El Yaniv, and Kobi Luz. Online choice of active learning algorithms. Journal of Machine Learning Research, 5(Mar):255­291, 2004.

Under review as a conference paper at ICLR 2018
Adrien Baranes and Pierre-Yves Oudeyer. Intrinsically motivated goal exploration for active motor learning in robots: A case study. In Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on, pp. 1766­1773. IEEE, 2010.
Adrien Baranes and Pierre-Yves Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1):49­73, 2013.
Andrew G Barto, Satinder Singh, and Nuttapong Chentanez. Intrinsically motivated learning of hierarchical collections of skills. In Proceedings of the 3rd International Conference on Development and Learning, pp. 112­19, 2004.
Andrew G Barto, George Konidaris, and Christopher Vigorito. Behavioral hierarchy: exploration and representation. In Computational and Robotic Models of the Hierarchical Organization of Behavior, pp. 13­46. Springer, 2013.
Maya Cakmak, Crystal Chao, and Andrea L Thomaz. Designing interactions for robot active learners. IEEE Transactions on Autonomous Mental Development, 2(2):108­118, 2010.
Sonia Chernova and Manuela Veloso. Interactive policy learning through confidence-based autonomy. Journal of Artificial Intelligence Research, 34(1):1, 2009.
E.L. Deci and Richard M. Ryan. Intrinsic Motivation and self-determination in human behavior. Plenum Press, New York, 1985.
Nicolas Duminy, Sao Mai Nguyen, and Dominique Duhaut. Strategic and interactive learning of a hierarchical set of tasks by the poppy humanoid robot. In Development and Learning and Epigenetic Robotics (ICDL-EpiRob), 2016 Joint IEEE International Conference on, pp. 204­209. IEEE, 2016.
Se´bastien Forestier and Pierre-Yves Oudeyer. Curiosity-driven development of tool use precursors: a computational model. In 38th Annual Conference of the Cognitive Science Society (CogSci 2016), pp. 1859­1864, 2016.
Daniel H Grollman and Odest Chadwicke Jenkins. Incremental learning of subtasks from unsegmented demonstration. In Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on, pp. 261­266. IEEE, 2010.
Manuel Lopes and Pierre-Yves Oudeyer. The strategic student approach for life-long exploration and learning. In Development and Learning and Epigenetic Robotics (ICDL), 2012 IEEE International Conference on, pp. 1­8. IEEE, 2012.
Max Lungarella, Giorgio Metta, Rolf Pfeifer, and Giulio Sandini. Developmental robotics: a survey. Connection Science, 15(4):151­190, 2003.
Sao Mai Nguyen and Pierre-Yves Oudeyer. Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner. Paladyn Journal of Behavioural Robotics, 3(3): 136­146, 2012.
Sao Mai Nguyen, Adrien Baranes, and Pierre-Yves Oudeyer. Bootstrapping intrinsically motivated learning with human demonstrations. In IEEE International Conference on Development and Learning, 2011.
Pierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational approaches. Frontiers in neurorobotics, 1:6, 2009.
Peter Pastor, Heiko Hoffmann, Tamim Asfour, and Stefan Schaal. Learning and generalization of motor skills by learning from demonstration. In Robotics and Automation, 2009. ICRA'09. IEEE International Conference on, pp. 763­768. IEEE, 2009.
Matthias Rolf, Jochen J Steil, and Michael Gienger. Goal babbling permits direct learning of inverse kinematics. IEEE Transactions on Autonomous Mental Development, 2(3):216­229, 2010.
Freek Stulp and Stefan Schaal. Hierarchical reinforcement learning with movement primitives. In Humanoid Robots (Humanoids), 2011 11th IEEE-RAS International Conference on, pp. 231­238. IEEE, 2011.

Under review as a conference paper at ICLR 2018
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181­ 211, 1999.
Evangelos Theodorou, Jonas Buchli, and Stefan Schaal. Reinforcement learning of motor skills in high dimensions: A path integral approach. In Robotics and Automation (ICRA), 2010 IEEE International Conference on, pp. 2397­2403. IEEE, 2010.
Andrea L Thomaz and Cynthia Breazeal. Experiments in socially guided exploration: Lessons learned in building robots that learn with and without human teachers. Connection Science, 20 (2-3):91­110, 2008.

