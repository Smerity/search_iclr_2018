Under review as a conference paper at ICLR 2018
TRAINING NEURAL MACHINES WITH PARTIAL TRACES
Anonymous authors Paper under double-blind review
ABSTRACT
We present a novel approach for training neural abstract architectures which incorporates (partial) supervision over the machine's interpretable components. To cleanly capture the set of neural architectures to which our method applies, we introduce the concept of a differential neural computational machine (NCM) and show that several existing architectures (e.g., NTMs, NRAMs) can be instantiated as a NCM and can thus benefit from any amount of additional supervision over their interpretable components. Based on our method, we performed a detailed experimental evaluation with both, the NTM and NRAM architectures, and showed that the approach leads to significantly better convergence and generalization capabilities of the learning phase than when training using only input-output examples.
1 INTRODUCTION
Recently, there has been substantial interest in neural abstract machines that can induce programs from examples Feser et al. (2015; 2016); Frankle et al. (2016); Gaunt et al. (2017); Graves et al. (2014; 2016); Kaiser & Sutskever (2016); Kurach et al. (2016); Reed & de Freitas (2016); Vinyals et al. (2015); Zaremba & Sutskever (2015); Zhang et al. (2015). While significant progress has been made towards learning interesting algorithms Graves et al. (2016), ensuring the training of these machines converges to the desired solution can be very challenging. Interestingly however, even though these machines differ architecturally, they tend to rely on components (e.g., neural memory) that are more interpretable than a typical neural network (e.g., an LSTM). A key question then is:
Can we somehow provide additional amounts of supervision for these interpretable components during training so to bias the learning towards the desired solution?
In this work we investigate this question in depth. We refer to the type of supervision mentioned above as partial trace supervision, capturing the intuition that more detailed information, beyond inputoutput examples, is provided during learning. To study the question systematically, we introduce the notion of a differential neural computational machine (NCM), a formalism which allows for clean characterization of the neural abstract machines that fall inside our class and that can benefit from any amount of partial trace information. We show that common architectures such as Neural Turing Machines (NTMs) and Neural Random Access Machines (NRAMs) can be phrased as NCMs, useful also because these architectures form the basis for many recent extensions, e.g., Graves et al. (2016); Grefenstette et al. (2015); Kaiser & Sutskever (2016). We also explain why other machines such as the Neural Program Interpreter (NPI) Reed & de Freitas (2016) or its recent extensions (e.g., the Neural Program Lattice Li et al. (2017)) cannot be instantiated as an NCM and are thus restricted to require large (and potentially prohibitive) amounts of supervision. We believe the NCM abstraction is a useful step in better understanding how different neural abstract machines compare when it comes to additional supervision. We then present NCM loss functions which abstractly capture the concept of partial trace information and show how to instantiate these for both the NTM and the NRAM. We also performed an extensive evaluation for how partial trace information affects training in both architectures. Overall, our experimental results indicate that the additional supervision can substantially improve convergence while leading to better generalization and interpretability.
To provide an intuition for the problem we study in this work, consider the simple task of training an NTM to flip the third bit in a bit stream (called Flip3rd) ­ such bitstream tasks have been extensively studied in the area of program synthesis (e.g., Jha et al. (2010); Raychev et al. (2016)). An example input-output pair for this task could be [0, 1, 0, 0]  [0, 1, 1, 0]. Given a set of such
1

Under review as a conference paper at ICLR 2018

(a) Overfitting & No Traces (b) Generalizing & No Traces

(c) Generalizing & Traces

Figure 1: Traces of locations accessed by read/write heads for the Flip3rd task in three different training setups. The y-axis represents time (descending); x-axis represents head locations. First two NTMs are trained without partial trace information. White represents the distribution of the write head; orange the distribution of the read head; (b) and (c) generalize and are more interpretable than (a); (c) was trained using partial trace information and is more interpretable than (b).

examples, our goal is to train an NTM that solves this task. An example NTM that generalizes well and is understandable is shown in Figure 1c. Here, the y-axis is time (descending), the x-axis is the accessed memory location, the white squares represent the write head of the NTM, and the orange squares represent the read head. As we can see, the model writes the input sequence to the tape and then reads from the tape in the same order. However, in the absence of richer supervision, the NTM (and other neural architectures) can easily overfit to the training set ­ an example of an overfitting NTM is shown in Figure 1a. Here, the traces are chaotic and difficult to interpret. Further, even if the NTM generalizes, it can do so with erratic reads and writes, an example of which is shown in Figure 1b. Here, the NTM learns to read from the third bit (circled) with a smaller weight than from other locations, and also reads and writes erratically near the end of the sequence. This model is less interpretable than the one in Figure 1c because it is unclear how the model knows which the third bit actually is, or why a different read weight would help flip that bit.
In this work we will develop principled ways for guiding the training of a neural abstract machine towards the behavior shown in Figure 1c. For instance, for Flip3rd, providing partial trace information on the NTM's read heads for 10% of the input-output examples is sufficient to bias the learning towards the NTM shown in Figure 1c 100% of the time.
2 NEURAL COMPUTATIONAL MACHINES
To capture the essence of our method and illustrate its applicability, we now define the abstract notion of a neural computational machine (NCM). NCMs mimic classic computational machines with a controller and a memory, and generalize multiple existing architectures. Our approach for supervision with partial trace information applies to all neural architectures expressible as NCMs. A useful feature of the NCM abstraction is that it clearly delineates end-to-end differentiable architectures (Graves et al. (2014)'s NTM, Kurach et al. (2016)'s NRAM), which can train with little to no trace supervision, from architectures that are not end-to-end differentiable (Reed & de Freitas (2016)'s NPI) and hence require a certain minimum amount of trace information. In the follow-up section, we show how to phrase two existing neural architectures (NTMs and NRAMs) as an NCM.
An NCM is a triple of functions: a processor, a controller, and a loss:
Processor The processor  : W × C × M  B × M performs a pre-defined set of commands C, which might involve manipulating memories in M . The commands may produce additional feedback in B. Also, the processor's operation may depend on parameters in W .
Controller The controller  : W × B × Q × I  C × Q × O decides which operations the machine performs at each step. It receives external inputs from I and returns external outputs in O. It can also receive feedback from the processor and command it to do certain operations (e.g., memory read). The decisions the controller takes may depend on its internal state (from Q). The controller can also depend on parameters in W . For instance, if the controller is a neural network, then the network's weights will range over W .
2

Under review as a conference paper at ICLR 2018

Q IC
 M OB
(a) NCM

e

x

a
write

w
controller address
r
y w, r
m read

(b) NTM

ct at bt
M
t ft controller
(c) NRAM

rt,1 rt.2
READ ADD SUB LT WRITE
rt+1,1 rt+1,2

M t

Figure 2: (a) is depiction of the generic NCM structure (b) is a high-level overview of an NTM and (c) is a high level overview of the NRAM architecture. The controller outputs a circuit, which in this case contains the modules READ, ADD, SUB, LT, and WRITE. The controller encodes the two inputs to the modules as probability vectors, a, b and c, over the possible choices. The most likely choice is shown in green. The only input to the controller are the registers r1 and r2.

Loss Function The loss function Le : Trace × E  R indicates how close a trace   Trace of an execution of the machine (defined below) is to a behavior from a set E. The loss function provides
a criterion for training a machine to follow a prescribed set of behaviors, and hence we impose certain
differentiability conditions. We require that the loss surface is continuous and piecewise differentiable with respect to the weights w  W for all examples e and inputs x with traces  (w, x):

l(w; x, e) = Le( (w, x), e)

(1)

Execution The execution of the machine begins with an input sequence x = {xt}1n and initial values of the controller state q0, memory m0, and processor feedback b0. At each time step t = 1 . . . n,
controller and processor take turns executing according to the following equations:

(ct, qt, yt) = (w, bt-1, qt-1, xt) (bt, mt) = (w, ct, mt-1)

(2)

A trace  (w, x, b0, m0, q0) = {(ct, bt, qt, yt, mt)}1n records these quantities' values at each time step. We will occasionally write C, B, . . . for the trace projected onto one of its components c, b, . . . .

NCMs Note that the differentiability conditions that we impose on the loss do not imply that any of the NCM functions ,  and Le are continuous or differentiable. They indeed can be highly discontinuous as in NCMs like Weston et al. (2014)'s memory networks with a hard attention mechanism, or as in Reed & de Freitas (2016)'s neural programmer-interpreters. In order to fix these discontinuities and recover a differentiable loss surface, these architectures train with strong supervision only: the training examples e  E must provide a value for every traced quantity that comes from a discontinuous parameter.
In contrast, what we call differentiable neural computational machines (NCM), have ,  and Le continuous and piecewise differentiable. In this case, the loss surface is differentiable with respect to every parameter. Thus, there is no need to specify corresponding values in the examples, and so we can train with as much trace information as available.

3

Under review as a conference paper at ICLR 2018

3 NTMS AND NRAMS AS NCMS
We now show how NTMs and NRAMs can be instantiated as NCMs.

NTM as NCM An Neural Turing Machine (NTM) Graves et al. (2014) (Figure 2b) has access to a memory M  Rc×n of c cells of n real numbers each. We suppose the machine has one read head and one write head, whose addresses are, respectively, the probability vectors r, w  [0, 1]{1...c}. At every time step, the read head computes the expected value m  Rn of a random cell at index i  r. This value together with the current input are fed into a controller neural network, which then decides on several commands. It decides what fraction e  Rn to erase and how much a  Rn to
add to the cells underneath the write head. The write head stores the tape expected after a random modification at index i  w. Then the controller indicates the head movement with two probability vectors r, w  [0, 1]{-1,0,+1} which are convolved with the respective head addresses (the actual
addressing mechanism is more involved, but we omit it for brevity) Finally, the controller produces
the current output value. In terms of NCMs, the NTM's variables fall into the following classes:

I/O xI yO

State qQ
(r, w, M)  M

Communication (e, a, r, w)  C
mB

Each of these variables change over time according to certain equations (see Appendix A for details). The processor  and the controller  functions for each time step satisfy:

((et, at, rt, wt), qt, yt) = (w, mt, qt-1, xt) (mt+1, (rt, wt, Mt)) = ((et, at, rt, wt), (rt-1, wt-1, Mt-1)).

(3)

The standard loss function Le for the NTM simply includes a term, such as cross-entropy or L2 distance, for the machine output at every time step. Each of these compare the machine output to the respective values contained in the examples e  E.

NRAM as NCM A Neural Random Access Machine (NRAM) Kurach et al. (2016) is a neural machine designed for ease of pointer (de-)referencing. An NRAM has a variable sized memory M  Rc×c whose size varies between runs. It also has access to a register file r  Rn×c with a constant number n of registers. Both the memory and the registers store probability vectors over {1 . . . c}. The controller receives no external inputs, but at each time step reads the probability that a register assigns to 0. It also produces no external output, except a probability f  [0, 1] for termination at the current time step. The output of the run is considered to be the final memory state.
Unlike the NTM, computation in the NRAM is performed by a fixed sequence of modules. Each module implements a simple integer operation/memory manipulation lifted to probability vectors. For example, addition lifts to convolution, while memory access is like that of the NTM. At every time step the controller organizes the sequence of modules into a circuit, which is then executed. The circuit is encoded by a pair of probability distributions per module, as shown in Figure 2c. These distributions specify respectively which previous modules or registers will provide a given module first/second arguments. The distributions are stacked in the matrices a and b . A similar matrix c is responsible for specifying what values should be written to the registers at the end of the time step. The NCM instantiation of an NRAM is the following:

I/O {1} = I
ft  O

State
qt  Q (rt, Mt)  M

Communication
(at, bt, ct)  C rt,-,0  B

The equations that determine these quantities can be found in Appendix B. The processor function  and the controller function  expressed in terms of these quantities are:

((at, bt, ct), ht, ft) = (w, r(t-1),-,0, ht-1, 1) (rt,-,0, (rt, Mt)) = ((at, bt, ct), (rt-1, Mt-1)).

(4)

4

Under review as a conference paper at ICLR 2018

The loss of the NRAM is more complex than the NTM loss: it is an expectation with respect to the probability distribution p of termination time, as determined by the termination probabilities ft (see Appendix B). For every t = 1 . . . k, the loss considers the negative log likelihood that the i-th memory cell at that time step equals the value ei provided in the example, independently for each i:

Le(, e) = - pt log(Mt,i,ei ).
t<k i<c

(5)

4 SUBTRACE SUPERVISION OF NCMS

Incorporating supervision during NCM training can be helpful with: (i) convergence: additional bias may steer the minimization of the NCM's loss function Le, as much as possible, away from local minima that do not correspond to good solutions, (ii) interpretability: the bias can also be useful in guiding the NCM towards learning a model that is more intuitive/explainable to a user (especially if the user already has an intuition on what it is that parts of the model should do), and (iii) generalization: the bias can steer the NCM towards solutions which minimize not just the loss on example of difficulties it has seen, but on significantly more difficult examples.

The way we provide additional supervision to NCMs is, by encoding, for example, specific commands
issued to the processor, into extra loss terms. Let us illustrate how we can bias the learning with an NTM. Consider the task of copying the first half of an input sequence {xt}12l into the second half of the machine's output {yt}21l, where the last input xl from the first half is a special value indicating that the first half ended. Starting with both heads at position 1, the most direct solution is to consecutively
store the input to the tape during the first half of the execution, and then recall the stored values
during the second half. In such a solution, we expect the head positions to be:

one-hot(t) if t = 1 . . . l

one-hot(1) if t = 1 . . . l

wt = p(t) = one-hot(l) if t  l + 1 rt = q(t) = one-hot(t - l) if t  l + 1 (6)

To incorporate this information into the training, we add loss terms that measure the cross-entropy (H) between p(t) and wt as well as between q(t) and rt. Importantly, we need not add terms for every time-step, but instead we can consider only the corner cases where heads change direction:
H(p(t), wt) + H(q(t), rt).
t=1,l+1,2l

4.1 GENERIC SUBTRACE LOSS FOR NCMS

We now describe the general shape of the extra loss terms for arbitrary NCMs. Since, typically, we can interpret only the memory and the processor in terms of well-understood operations, we will consider loss terms only for the memory state and the communication flow between the controller and the processor. We leave the controller's hidden state unconstrained ­ this also permits us to use the same training procedure with different controllers.

The generic loss is expressed with four loss functions for the different components of an NCM trace:

LC : C × EC  R LO : O × EO  R

LB : B × EB  R LM : M × EM  R

(7)

For each part   {C, B, O, M }, we provide hints (t, v, µ)   that indicate a time step t at which the hint applies, an example v  E for the relevant component, and a weight w  R of the hint. The weight is included to account for hints having a different importance at different time-steps, but also
to express our confidence in the hint, e.g., hints coming from noisy sources would get less weight.

A subtrace  is a collection of hints used for a particular input-output example e. We call it a subtrace because, typically, it contains hints for a proper subset of the states traced by the NCM during execution. The net loss for a given input-output example and subtrace equals the original loss Le added to the weighted losses for all the hints, scaled by a constant factor :

L(, (, e)) = Le(, e) + 

{C,B,O,M} (t,v,µ) µL(,t, v) {C,B,O,M } (t,v,µ) µ

(8)

5

Under review as a conference paper at ICLR 2018

READ

INC

r0

WRITE

r0

r1 ONE

ADD

READ

r1

Figure 3: An example circuit for the task of adding one to all memory cells. The arrows for register updates are shown in red. Technically, modules take two arguments, but some ignore an argument, such as INC or READ. For them, we show only the relevant arrows.

4.2 SUBTRACES FOR NTM

For NTMs, we allow hints on the output y, the addresses r and w, and the tape M. We include extra loss terms for the memory state only (all other loss terms are zero):

LM ((rt, wt, Mt), (wr, v)) = H(v, wt) LM ((rt, wt, Mt), (rd, v)) = H(v, rt)

(9)

Unlike the output and addresses, values on the tape are interpreted according to an encoding internal to the controller (which emerges only during training). Forcing the controller to use a specific encoding for the tape, as we do with NTM output, can have a negative effect on training (in our experiments, training diverged consistently). To remedy this, we do not apply loss to the tape directly but to a decoded version of a cell on the tape. While a decoder might find multiple representations and overfit, we found that it forced just enough consistency to improve the convergence rate. The decoder itself is an auxiliary network  trained together with the NTM, which takes a single cell from memory as input. The output of the decoder is compared against the expected value which should be in that cell:

LM ((-, -, Mt), (tape, i, v)) = H((Mt,i), v).

(10)

For all subtraces we provide in our experiments with NTMs, the hints have the same unit weight.

4.3 SUBTRACES FOR NRAM

For NRAMs, we hint which connections should be present in the circuit the controller constructs at each step, including the ones for register updates. An example circuit is shown in Figure 3. In terms of an NCM, this amounts to providing loss for commands and no loss for anything else. We set the loss to the negative log likelihood of the controller choosing specific connections revealed in the hint:

LC ((at, bt, ct), (module, m, i, j)) = - log(at,m,i) - log(bt,m,j) LC ((at, bt, ct), (register, r, i)) = - log(ct,r,i)

(11)

In our experiments, we observed that assigning higher weight to hints at earlier timesteps is crucial for convergence of the training process. For a hint at time-step t, we use the weight µ = (t + 1)-2. A possible reason for why this helps is that the machine's behavior at later time-steps is highly dependent on its behavior at the early time-steps. Thus, the machine cannot reach a later behavior that is right before it fixes its early behavior. Unless the behavior is correct early on, the loss feedback from later time-steps will be mostly noise, masking the feedback from early time-steps.

Other Architectures The NCM can be instantiated to architectures as diverse as a common LSTM network or End-To-End Differentiable Memory Networks. Any programming inducing neural network with at least partially interpretable intermediate states for which the dataset contains additional hints could be considered a good candidate for application of this abstraction.

5 EXPERIMENTAL EVALUATION
We evaluated our NCM supervision method on the NTM and NRAM architectures. For each of the two architectures we implemented a variety of tasks and experimented with different setups of trace supervision. The main questions that we address are: (i) does trace supervision help convergence, interpretability, and generalization? (ii) how much supervision is needed to train such models? Below, we summarize our findings ­ further details are provided in the appendix.

6

Under review as a conference paper at ICLR 2018

Figure 4: Relative percentage of training instances which generalized out of ten runs per task for the NTM. We provide a subtrace 100% of the time and use  = 1. x-axis shows the type of supervision.

density 
baseline addr+val address value read write corner

100 100 100 100 50 50 50 50 10 10 10 10 1 1 1 1 1 0.3 0.1 0.03 1 0.3 0.1 0.03 1 0.3 0.1 0.03 1 0.3 0.1 0.03
45 30 50 50 50 50 50 70 80 60 80 40 40 40 60 40 10 0 60 40 40 20 70 80 90 90 70 60 50 50 50 60 60 60 50 40 60 80 70 40 10 50 10 40 70 50 30 40 60 40 60 70 40 30 80 90 90 100 70 80 50 30 50 60 30 0 30 50 20 30 40 60 20 40 40 40 40 20 50 70 50 50 70 80 80 40 40 90 70 80 70 60 40 50 60 60 40

Figure 5: The number of initial runs which generalized for Flip3rd. The first dimension listed in the rows controls the execution details revealed in a subtrace, while the second dimension (the density column) controls the proportion of examples that receive extra subtrace supervision.

Subtraces For NTMs We measured how often we successfully trained an NTM that achieves strong generalization. We consider a model to generalize if relative to the training size limit n, it achieves perfect accuracy on all of tests of size  1.5n, and perfect accuracy on 90% of the tests of size  2n. Figure 4 reports the average improvement compared to a baseline using only I/O examples. We ran experiments with four different tasks and various types of hints (cf. Appendices C, E). Some of the hint types are: read and write specify respective head addresses for all time steps; address combines the previous two; corner reveals the head addresses, but only when the heads change direction; value gives value for a single cell. Except for three cases, trace supervision helped improve generalization. Here, RepeatFlip3d is most challenging, with baseline generalizing only 5% of the time (cf. Appendix I). Here we have the largest improvement with extra supervision: corner type of hints achieve eight-fold increase in success rate, reaching 40%. Another task with an even larger ratio is RepeatCopyTwice (cf. Appendix), where success increases from 15.5% to 100%.

In addition to this experiment, we performed an extensive evaluation of different setups, varying the

global  parameter of the loss Eq. 8, and providing hints for just a fraction of the examples. The full

results are in Appendix I; here we provide those for RepeatFlip3d in Table 5. The table reveals

that the efficacy of our method heavily depends on these two parameters. The best results in this case

are

for

the

read/corner

type

of

hints

1 2

/

1 10

of

the

time,

with





{0.1, 1}.

The

best

results

for

other

tasks are achieved with different setups. Generally, our conclusion is that training with traces 50%

of the time usually improves performance (or does not lower it much) when compared to the best

method. This observation raises the interesting question of what the best type and amount of hints are

for a given task.

Finally, we observed that in all cases where training with trace supervision converged, it successfully learned the head movements/tape values we had intended. This show that trace supervision can bias the architecture towards more interpretable behaviors. In those cases, the NTM learned consistently sharper head positions/tape values than the baseline, as Figure 6 shows for Flip3rd.

Effect of Subtraces For NRAMs Ease of generalization for the NRAM is a known issue, with Neelakantan et al. (2015) reporting that ListK for example generalizes poorly, even when trained with noise in the gradient, curriculum learning, and an entropy bonus. We observed that when run on an indefinite number of examples with the correct number of timesteps and a correct module

7

Under review as a conference paper at ICLR 2018
(a) (b) Figure 6: Execution traces of two NTMs trained on Flip3rd until generalization. First is baseline (no trace supervision); second is trained with corner hints. Time flows top to bottom. The first pane from every pair shows the value written to tape; second shows head locations. Figures show that a little extra supervision helps the NTM write sharper 0­1 values and have more focused heads.

(a) Error Rates for NRAM

(b) Permute with noise for NRAM

Figure 7: (a) average number of errors after training had completed for NRAM. Observe that full training results in a significantly higher percent of generalization after training stopped. (b) shows the distribution of errors to problem length for Permute (one character of noise in 10% of samples).

sequence, Swap and Increment would in fact occasionally generalize perfectly, but did not have the resources to run such indefinite tests with Permute, ListK, and Merge.
Figure 7a demonstrates that when training had finished, either because it had ended early or had reached 5000 training examples (our upper bound), generalization would in fact be on average significantly better than the baseline the more hints that were used for all tasks. Here, number of hints used seemed to be a sufficient predictor for the quality of the trained model.
Robustness to Noise The effect of increasing supervision on the quality of the trained model was so strong that not even noise in the input was able to significantly hinder generalization. In Figure 7b, we corrupted a single character in the output examples for the Permute problem in 10% of the examples. We found that without any extra hints, no convergence was seen after training was complete, whereas with just corner subtraces, the generalization was nearly optimal.
6 CONCLUSION
We presented a method for incorporating (any amount of) additional supervision into the training of neural abstract machines. The basic idea was to provide this supervision (called partial trace information) over the interpretable components of the machine and to thus more effectively guide the learning towards the desired solution. We introduced the NCM architecture in order to precisely capture the neural abstract machines to which our method applies. We showed how to formulate partial trace information as abstract loss functions, how to instantiate common neural architectures such as NTMs and NRAMs as NCMs and concretize the NCM loss functions. Our experimental results indicate that partial trace information is effective in biasing the learning of both NTM's and NRAM's towards better converge, generalization and interpretability of the resulting models.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett (eds.). Advances in Neural Information Processing Systems 28, 2015. URL http://papers.nips.cc/ book/advances-in-neural-information-processing-systems-28-2015.
John K Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations from input-output examples. In ACM SIGPLAN Notices, volume 50, pp. 229­239. ACM, 2015.
John K Feser, Marc Brockschmidt, Alexander L Gaunt, and Daniel Tarlow. Differentiable functional program interpreters. arXiv preprint arXiv:1611.01988, 2016.
Jonathan Frankle, Peter-Michael Osera, David Walker, and Steve Zdancewic. Example-directed synthesis: a type-theoretic interpretation. ACM SIGPLAN Notices, 51(1):802­815, 2016.
Alexander L. Gaunt, Marc Brockschmidt, Nate Kushman, and Daniel Tarlow. Lifelong perceptual programming by example. In under review for ICLR (2017). URL https://arxiv.org/ abs/1611.02109.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014. URL http://arxiv.org/abs/1410.5401.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwin´ska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471­ 476, Oct 2016. ISSN 0028-0836. URL http://dx.doi.org/10.1038/nature20101.
Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded memory. In Cortes et al. (2015), pp. 1828­1836. URL http://papers. nips.cc/paper/5648-learning-to-transduce-with-unbounded-memory.
Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. Oracle-guided component-based program synthesis. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1, pp. 215­224. ACM, 2010.
Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In Kingsbury & Bengio (2016). URL http://arxiv.org/abs/1511.08228.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Brian Kingsbury and Samy Bengio (eds.). 4th International Conference on Learning Representations, May 2-4, 2016, San Juan, Puerto Rico, 2016. URL http://www.iclr.cc/doku.php?id= iclr2016:main.
Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. Neural random-access machines. ERCIM News, 2016(107), 2016. URL http://ercim-news.ercim.eu/en107/special/ neural-random-access-machines.
Chengtao Li, Daniel Tarlow, Alexander L. Gaunt, Marc Brockschmidt, and Nate Kushman. Neural program lattices. In under review for ICLR (2017). URL https://openreview.net/pdf? id=HJjiFK5gx.
Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.
Veselin Raychev, Pavol Bielik, Martin T. Vechev, and Andreas Krause. Learning programs from noisy data. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2016, St. Petersburg, FL, USA, January 20 - 22, 2016, pp. 761­774, 2016. doi: 10.1145/2837614.2837671. URL http://doi.acm.org/10.1145/ 2837614.2837671.
9

Under review as a conference paper at ICLR 2018
Scott Reed and Nando de Freitas. Neural programmer-interpreters. In Kingsbury & Bengio (2016). URL https://arxiv.org/abs/1511.06279.
under review for ICLR (ed.). 5th International Conference on Learning Representations, April 2426, 2017, Toloun, France, 2017. URL http://www.iclr.cc/doku.php?id=ICLR2017: main.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Cortes et al. (2015), pp. 2692­2700. URL http://papers.nips.cc/paper/5866-pointer-networks.
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.
Wojciech Zaremba and Ilya Sutskever. Learning to execute. CoRR, abs/1410.4615, 2014. URL http://arxiv.org/abs/1410.4615.
Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines. CoRR, abs/1505.00521, 2015. URL http://arxiv.org/abs/1505.00521.
Wei Zhang, Yang Yu, and Bowen Zhou. Structured memory for neural turing machines. CoRR, abs/1510.03931, 2015. URL http://arxiv.org/abs/1510.03931.
10

Under review as a conference paper at ICLR 2018

A NTM EQUATIONS

The controller for the NTM consists of the networks , y, e, a, r, w, which operate on the variables:

x ­ in q ­ controller state r ­ read address r ­ change in r e ­ erase M ­ tape

y ­ out m ­ read value

w ­ write address w ­ change in w a ­ add

(12)

The equations that describe NTM executions are:

rt = r(qt) wt = w(qt)
yt = y(qt) et = e(qt) at = a(qt)

rt = address(rt, rt-1, Mt-1) wt = address(wt, wt-1, Mt-1) mt = rtMt-1 Mt = Mt-1 - (wt  et) Mt-1 + wt  at qt = (xt, qt-1, mt).

(13)

B NRAM EQUATIONS

The controller of the NRAM consists of the networks , a, b, c, f , which operate on the variables:

a ­ lhs circuit

b ­ rhs circuit

c ­ register inputs o ­ module outputs

r ­ register state M ­ memory tape h ­ controller state f ­ stop probability. (14)

The equations that describe the NRAM execution are:

at = softmax(a(qt)) bt = softmax(b(qt)) ct = softmax(c(qt)) ft = f (qt)
qt = (qt-1, rt,-,0)
pt = ft (1 - fi)
i<t

At,i = (r1, . . . , rR, o0, . . . , oi-1)T at,i Bt,i = (r1, . . . , rR, o0, . . . , oi-1)T bt,i rt,i = (r1, . . . , rR, o1, . . . , oQ)T ct,i

ot,i,k =

At,i,aBt,i,b[mi(a, b) = k]

0a,b<M

ot, = MtAt,

Mt = (J - At,)J T · Mt-1 + At,BtT,

pT = 1 - pi
i<T

i < M i < M i < R i / {, }, k < M
(15) (16)

11

Under review as a conference paper at ICLR 2018
C SETUP FOR NTM
For all of our NTM experiments we use a densely connected feed-forward controller. There are two architectural differences from the original NTM Graves et al. (2014) that helped our baseline performance: (1) the feed-forward controller, the erase and the add gates use tanh activation; (2) the output layer uses softmax. In the original architecture these are all logistic sigmoids. For the newly introduced tape decoder (active only during training) we used two alternative implementations: a tanh-softmax network, and a single affine transformation. We tested the NTM's learning ability on five different tasks for sequence manipulation, two of which have not been previously investigated in this domain. These tasks can be found in Appendix E. We performed experiments using several combination of losses as summarized in Appendix F. The observed training performance per task is shown in Appendix I, with rows corresponding to the different loss setups. The corner setup differs from the address setup in that the example subtraces were defined only for a few important corner cases. For example in RepeatCopyTwice, the write head was provided once at the beginning of the input sequence, and once at the end. Similarly, the read head was revealed at the beginning and at the end of every output repetition. In all other setups we provide full subtraces (defined for all time steps). The supervision amount can be tuned by adjusting the  weight from Equation 8. Further, we can also control the fraction of examples which get extra subtrace supervision (the density row in Figure I). The performance metric we use is the percentage of runs that do generalize after 100k iterations for the given task and supervision type. By generalize we mean that the NTM has perfect accuracy on all testing examples up to 1.5× the size of the max training length, and also perfect accuracy on 90% of the testing examples up to 2× the maximum training length. We used a feed-forwad controller with 2 × 50 units, except for RepeatCopyTwice, which uses 2 × 100 units. For training we used the Adam optimizer Kingma & Ba (2014), a learning rate of 10-3 for all tasks except RepeatFlip3d and Flip3rd which use 5 · 10-4. The lengths of the training sequences for the first four tasks are from 1 to 5, whereas the generalization of the model was tested with sequences of lengths up to 20. For Flip3rd and RepeatFlip3d, the training sequence length was up to 16, whereas the testing sequences have maximum length of 32.
D SETUP FOR NRAM
Like in the NTM, we use a densely connected two layer feed forward controller for our experiments, and use ReLU as the activation function. We make no modifications to the original architecture, and use noise with the parameter  = 0.3 as suggested by Neelakantan et al. (2015), and curriculum learning as described by Zaremba & Sutskever (2014). We stop training once we get to a difficulty specified by the task, and increase the difficulty once 0 errors were found on a new testing batch of 10 samples. Each training iteration trains with 50 examples of the currently randomly sampled difficulty. Regardless of whether the model had converged, training is stopped after 5000 samples were used. Such a low number is used to replicate the potential conditions under which such a model might be used. As with the NTM, the Adam optimizer was used. The specific tasks we use are described in Appendix G, and the specific kinds of supervision we give are described in Appendix H. The  we used here was 40. The system was implemented using PyTorch.
12

Under review as a conference paper at ICLR 2018
E NTM TASKS
Every input sequence ends with a special delimiter xE not occurring elsewhere in the sequence Copy ­ The input consists of generic elements, x1 . . . xnxE. The desired output is x1 . . . xnxE. RepeatCopyTwice ­ The input is again a sequence of generic elements, x1 . . . xnxE. The
desired output is the input copied twice x1 . . . xnx1 . . . xnxE. Placing the delimiter only at the end of the output ensures that the machine learns to keep track of the number of copies. Otherwise, it could simply learn to cycle through the tape reproducing the given input indefinitely. We kept the number of repetitions fixed in order to increase baseline task performance for the benefit of comparison. DyckWords ­ The input is a sequence of open and closed parentheses, x1 . . . xnxE. The desired output is a sequence of bits y1 . . . ynxE such that yi = 1 iff the prefix x1 . . . xi is a balanced string of parentheses (a Dyck word). Both positive and negative examples were given. Flip3rd ­ The input is a sequence of bits, x1x2x3 . . . xnxE. The desired output is the same sequence of bits but with the 3rd bit flipped: x1x2x¯3 . . . xnxE. Such a task with a specific index to be updated (e.g., 3rd) still requires handling data dependence on the contents of the index (unlike say the Copy task). RepeatFlip3d ­ The input is a sequence of bits, x1x2x3x4x5x5 . . . xE. The desired output is the same sequence of bits but with every 3rd bit flipped: x1x2x¯3x4x5x¯6 . . . xE.
F NTM SUBTRACES
addr+val value address/corner
write read Figure 8: A heirarchy of supervision types (but not quantities) for NTMs.
value traces provide hints for the memory at every timestep as explained in Equation 10. read ­ provides a hint for the address of the read head at every timestep. write ­ provides a hint for the address of the write head at every timestep. address ­ provides hints for the address of both the read and the write head at every timestep. addr+val ­ provides value, read and write hints for every timestep. corner ­ provides hints for the address of both the read and the write head at every "important"
timestep - we decided what important means here depends on which task we are referring to. In general, we consider the first and last timesteps important, and also any timestep where a head should change direction. For example, in RepeatCopyTwice for an example of size n with e repeats, we'd provide the heads at timesteps 0, n, 2n, 3n . . . , en.
13

Under review as a conference paper at ICLR 2018

G NRAM TASKS

Below we describe all the tasks we experimented with. We predominantly picked tasks that the NRAM is known to have trouble generalizing on. We did not introduce any new tasks, and more detailed descriptions of these tasks can be found in Kurach et al. (2016).

Swap ­ Provided two numbers, a and b and an array p, swap p[a] and p[b]. All elements but that in the last memory cell are not zero.
Increment ­ Given an array p, return the array with one added to each element. All elements but that in the last cell for the input are not zero. Elements can be zero in the output.
Permute ­ Given two arrays p and q return a new array s such that s[i] = q[p[i]]. The arrays p and q are preceded by a pointer, a, to array q. The output is expected to be a, s[0] . . . , s[n], q[0], q[n].
ListK ­ Given a linked list in array form, and an index k return the value at node k.
Merge ­ given arrays p and q, and three pointers a, b, c to array p, q, and the output sequence (given as zeros initially), place the sorted combination of p and q into the output location.

The following table describes the specific NRAM instantiation used for each task. The default sequence (def) is the one described by Kurach et al. (2016). The number of timesteps is usually dependent on the length of the problem instance, M (equivalently the word size or difficulty), and in the case of ListKwas given with respect to the argument k. The difficulty (D) was simply the length of the sequence used.

Task Swap Increment Permute ListK Merge

No. Regs 5 2 4 6 8

Module Sequence def
def + R R + def + R + W
def def + def + def

Timesteps 7
M +2 M +3 k+5 M +3

Learn Rate 0.01 0.01 0.05 0.05 0.05

Start D 6 4 7 9 13

End D 10 10 12 16 16

H NRAM SUBTRACES

For each of the tasks listed Appendix G, we hand coded a complete circuit for every module and every timestep we would provide. The following subtraces types describe how we provide hints based on this circuit.
None ­ provides no hints. Full ­ provides the entire circuit. SingleHint ­ provides a random hint at a random timestep. SingleTimestep ­ provides the entire circuit at a random timestep. Corners ­ provides the entire circuit at the first and last timesteps. Registers ­ provides hints for the registers at every timestep. Modules ­ provides hints for the modules at every timestep.

14

Under review as a conference paper at ICLR 2018

I NTM RESULTS

density 
baseline addr+val address value read write corner

100 100 100 100 50 50 50 50 10 10 10 10 1 1 1 1 1 0.3 0.1 0.01 1 0.3 0.1 0.01 1 0.3 0.1 0.01 1 0.3 0.1 0.01
52.5 100 100 100 70 100 100 100 40 60 80 40 30 30 50 60 10 100 100 100 50 90 100 90 30 80 90 70 30 50 30 40 50 100 100 70 40 80 20 40 10 10 20 40 30 60 40 20 10 90 80 70 50 60 20 50 20 40 40 60 20 70 30 40 10 60 70 80 60 80 80 40 40 50 70 50 40 50 60 50 40 100 100 100 50 100 90 60 70 70 20 50 30 50 60 20 30
(a) Copy

density 
baseline addr+val address value read write corner

100 100 100 50 50 50 10 10 10 1 0.3 0.03 1 0.3 0.03 1 0.3 0.0
15.5 90 100 60 90 80 40 80 20 10 90 90 90 100 100 40 100 60 0 80 70 0 50 50 10 30 30 20 50 30 30 20 60 30 20 60 10 30 30 20 10 30 40 20 0 10 60 50 40 50 60 10 20 40 20

11 1 1 0.3 0.03
10 0 0 0 20 30 10 30 0 10 10 0 10 20 20 10 20 0

(b) RepeatCopyTwice

density 
baseline address read corner

100 100 100 100 50 50 50 50 10 10 10 10 1 1 1 1 1 0.3 0.1 0.01 1 0.3 0.1 0.01 1 0.3 0.1 0.01 1 0.3 0.1 0.01
45 70 90 60 80 90 90 90 50 80 50 90 80 100 80 70 70 80 90 70 70 100 100 70 50 50 60 70 70 80 70 50 50 60 100 80 80 80 90 90 90 60 60 100 50 90 80 80 50
(c) DyckWords

density 
baseline addr+val address value read write corner

100 100 100 100 50 50 50 50 10 10 10 10 1 1 1 1 1 0.3 0.1 0.03 1 0.3 0.1 0.03 1 0.3 0.1 0.03 1 0.3 0.1 0.03
45 30 50 50 50 50 50 70 80 60 80 40 40 40 60 40 10 0 60 40 40 20 70 80 90 90 70 60 50 50 50 60 60 60 50 40 60 80 70 40 10 50 10 40 70 50 30 40 60 40 60 70 40 30 80 90 90 100 70 80 50 30 50 60 30 0 30 50 20 30 40 60 20 40 40 40 40 20 50 70 50 50 70 80 80 40 40 90 70 80 70 60 40 50 60 60 40
(d) Flip3rd

density

100 100 100 100 50 50 50 50 10 10 10 10 1 1 1 1

 0.5 0.3 0.1 1 0.53 0.31 00..013 11 00..353 00..131 000...00133 11 00..353 00..131 000..0.0133 1 0.35 0.13 00..013

baseline

5

addr+val

30 20 20 40 30 30 10 10 40 10 0 20 10 10 0 10

address

20 50 30 30 30 40 20 40 20 40 20 0 0 0 20 0

value

0 0 20 20 0 0 0 0 10 10 10 0 0 0 0 0

read 30 10 40 20 10 30 20 40 30 10 0 10 20 0 10 20

write 0 0 0 10 0 0 0 0 10 10 0 0 20 20 30 0

corner

40 40 60 20 50 30 10 30 10 10 0 0 0 10 0 10

(e) RepeatFlip3d

Figure 9: Baselines have generalization on over 40 different initial weights. Other tests use 10.

15

Under review as a conference paper at ICLR 2018
Which Details to Reveal for NTM? The first dimension listed in the rows of the tables of Figure I controls the execution details revealed in a Subtrace. We use subtraces showing either the addresses without the tape values, only the read heads or the write heads, or even weaker supervision in a few corner cases. In tasks Copy Figure 9a), RepeatCopyTwice (Figure 9b) and DyckWords (Figure 9c), it is frequently the case that when the NTM generalizes without supervision, it converges to an algorithm which we are able to interpret. For them, we designed the addr+val traces to match this algorithm, and saw increases in generalization frequency of at least 45%. It can be concluded that when additionally provided supervision reflects the interpretable "natural" behavior of the NTM, the learning becomes significantly more robust to changes in initial weights. Additionally, for tasks Flip3rd (Figure 9d) and RepeatFlip3d (Figure 9e), both the baseline and other supervision types are outperformed by training with read supervision. It is also notable that corner supervision in RepeatFlip3d achieves highest improvement over the baseline, 60% over 5%. In essence, this means that providing only a small part of the trace can diminish the occurrence of local minima in the loss function. How Often to Reveal for NTM? The second dimension controls the proportion of examples that receive extra subtrace supervision (the density columns in Figure I). For Flip3rd, RepeatCopyTwice and DyckWords we observed that having only a small number of examples with extra supervision leads to models which are more robust to initial weight changes than the baseline, although not necessarily always as robust as providing supervision all the time. A couple of interesting cases stand out. For Flip3rd with 10% corner subtraces and  = 1, we find a surprisingly high rate of generalization. Providing address traces 10% of the time when training RepeatCopyTwice leads to better performance all the time. For RepeatFlip3d, write traces at 1% frequency and  = 0.1 generalize 30% of the time vs. 5% for baseline. While the type of trace which works best varies per task, for each task there exists a trace which can be provided only 1% of the time and still greatly improve the performance over the baseline. This suggests that a small amount of extra supervision can improve performance significantly, but the kind of supervision may differ. It is an interesting research question to find out how the task at hand relates to the optimal kind of supervision.
16

Under review as a conference paper at ICLR 2018

J NRAM RESULTS

Subtrace Type \Task None
SingleHint Corners
SingleTimestep Registers Modules Full

Permute
36 24 29 21 48 48 26

Swap
44 38 23 52 58 58 33

Increment
58 28 36 29 73 107 32

ListK
41 22 22 28 54 54 44

Merge
13 12 9 12 21

PermuteNoise
49 59 72 60 12

Figure 10: The number of runs which completed for each task and subtrace type. The Data in the graphs below is taken by averaging the results of these runs.

Subtrace Type \Task None
SingleHint Corners
SingleTimestep Registers Modules Full

Permute
6290.29 5565.22 4468.85 6259.05 6618.12 6523.16 4919.33

Swap
5505.22 3700.64 6195.75 2662.35 5774.61 5781.99 4110.14

Increment
3500.13 4318.20 3199.86 4042.18 3839.18 2335.99 3758.99

ListK
5880.11 6574.59 6601.16 5076.17 6185.54 6183.74 3216.01

Figure 11: The average time (in seconds) to finish training for each task and subtrace type. For most tasks it is clear that Full traces while introducing extra computations to individual timesteps, reduce the amount of time to finish training over not using supervision.

Subtrace Type \Task None
SingleHint Corners
SingleTimestep Full
Registers Modules

ListK
95.08 93.61 94.47 36.91 12.77 93.22 93.70

Swap
91.52 2.41 99.09 1.75 11.01 93.44 95.57

Permute
99.97 57.55 16.40 47.79 7.83 99.97 86.48

Increment
99.91 14.86 2.14 13.77 9.89 90.36 40.87

Merge
99.96 100.0 100.0 100.0 78.44
-

PermuteNoise
99.99 56.90 20.79 33.60 23.57
-

Figure 12: The average number of errors on the test set for each task and subtrace type once trained.

Figure 13: Comparing average generalization to Figure 14: Comparing average generalization to

sequence length for Swap

sequence length for Increment

17

Under review as a conference paper at ICLR 2018

Figure 15: Comparing average generalization to Figure 16: Comparing average generalization to

sequence length for Permute

sequence length for ListK

Figure 17: Comparing average generalization to sequence length for Merge

18

