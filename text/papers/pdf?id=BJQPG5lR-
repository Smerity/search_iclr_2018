Under review as a conference paper at ICLR 2018
VARIABLE ACTIVATION NETWORKS: A SIMPLE METHOD TO TRAIN DEEP FEED-FORWARD NETWORKS WITHOUT SKIP-CONNECTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Novel architectures such as ResNets have enabled the training of very deep feedforward networks via the introduction of skip-connections, leading to state-of-theart results in many applications. Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients). In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections. The proposed method poses the learning of weights in deep networks as a constrained optimization problem where the presence of skip-connections is penalized by Lagrange multipliers. This allows for skip-connections to be introduced during the early stages of training and subsequently phased out in a principled manner. We demonstrate the benefits of such an approach with experiments on MNIST, fashion-MNIST, CIFAR-10 and CIFAR-100 where the proposed method is shown to outperform many architectures without skip-connections and is often competitive with ResNets.
1 INTRODUCTION
The representation view of deep learning suggests that neural networks learn an increasingly abstract representation of input data in a hierarchical fashion (Zeiler & Fergus, 2014; Goodfellow et al., 2016; Greff et al., 2016). Such representations may then be exploited to perform various tasks such as image classification, machine translation and speech recognition.
A natural conclusion of the representation view is that deeper networks will learn more detailed and abstract representations as a result of their increased capacity. However, in the case of feed-forward networks it has been observed that performance deteriorates beyond a certain depth, even within training data. Recently, Residual Networks (ResNets; He et al. 2016a) and Highway Networks (Srivastava et al., 2015) have demonstrated that introducing various flavors of skip-connections or gating mechanisms makes it possible to train increasingly deep networks.
A widely held hypothesis explaining the success of ResNets is that the introduction of skipconnections serves to improve the conditioning of the optimization manifold as well as the statistical properties of gradients employed during training. Raiko et al. (2012) and Schraudolph (2012) show that the introduction of specially designed skip-connections serves to diagonalize the Fisher information matrix, thereby bringing standard gradient steps closer to the natural gradient. More recently, Balduzzi et al. (2017) demonstrated that the introduction of skip-connections helps retain the correlation structure across gradients. This is contrary to the gradients of deep feed-forward networks, which resemble white noise. More generally, the skip-connections are thought to reduce the effects of vanishing gradients by introducing a linear component to the activation function (He et al., 2016b).
Despite the success of ResNets and Highway Networks, some issues still remain. For instance, it is often not clear what the actual depth of the network is nor is the contribution of each layer to the final representation obvious. Greff et al. (2016) suggest that the level of representation remains constant across blocks of multiple layers and only varies when the dimensionality changes, typically through projections. A related issue is feature re-use, whereby higher layers may not necessarily learn useful representations. Indeed, Zagoruyko & Komodakis (2016) demonstrate that it is better to increase
1

Under review as a conference paper at ICLR 2018

the width of ResNets than the depth, suggesting that perhaps only a few layers are learning useful representations.
The goal of this work is to address some of the aforementioned issues by leveraging some of the desirable optimization properties of ResNets in the context of neural networks that lack skipconnections. We approach the task of learning parameters for a deep network under the framework of constrained optimization. This strategy allows us to introduce skip-connections penalized by Lagrange multipliers into the architecture of our network. In our setting, skip-connections play an important role during the initial training of the network and are subsequently removed in a principled manner. Throughout a series of experiments we demonstrate that such an approach leads to improvements in generalization error when compared to architectures without skip-connections and is competitive with ResNets in some cases.
The contributions of this work are as follows:
· We propose a novel alternative for training deep feed-forward networks without skipconnections. The proposed method introduces skip-connections which are penalized by Lagrange multipliers. This allows for the presence of skip-connections to be iteratively phased out during training in a principled manner. The proposed method is thereby able to enjoy the optimization benefits associated with skip-connections during the early stages of training.
· A number of benchmark datasets are used to demonstrate the empirical capabilities of the proposed method. In particular, the proposed method consistently out-performs deep networks trained using traditional back-propagation and is competitive with ResNets in some cases.

2 RELATED WORK

The hierarchical nature of many feed-forward networks is loosely inspired by the structure of the visual cortex where neurons in early layers capture simple features (e.g., edges) which are subsequently aggregated in deeper layers (Hubel & Wiesel, 1962). This interpretation of neural networks suggests that the depth of a network should be maximized, thereby allowing the network to learn more abstract (and hopefully useful) representations (Bengio et al., 2013). However, a widely reported phenomenon is that deeper networks are more difficult to train. This is the result of optimization challenges such as vanishing and shattered gradients (Hochreiter et al., 2001; Balduzzi et al., 2017).

In the past these challenges have been partially addressed via the use of supervised and unsupervised pre-training (Bengio et al., 2009) and more recently through careful parameter initialization (Glorot & Bengio, 2010; He et al., 2015) and batch normalization (Ioffe & Szegedy, 2015). In the past couple of years further improvements have been obtained via the introduction of skip-connections. ResNets (He et al., 2016a;b) replace traditional activation functions within each layer with a residual block consisting of a residual function F together with a skip-connection. Formally, the residual block is defined as:

xl+1 = Fl(xl, Wl) + Wlxl

(1)

where Fl : Rn  Rn represents some combination of affine transformation, nonlinearity and batch normalization parameterized by Wl. The matrix Wl parameterizes a linear projection to ensure the dimensions are aligned1. More generally, ResNets can be considered a special case of Highway
Networks (Srivastava et al., 2015) where the output of each layer is defined as:

xl+1 = Fl(xl, Wl) · T (xl, Hl) + xl · (1 - T (xl, Hl)),

(2)

where · denotes element-wise multiplication. In Highway Networks the output of each layer is determined by a gating function

T (xl, Hl) = sigmoid (Hlxl)

inspired from LSTMs. We note that both ResNets and Highway Networks were introduced with the explicit goal of training deeper networks. Inspired by the success of the these methods, many

1Unless stated otherwise we will assume F retains the dimension of xl and set Wl to the identity.

2

Under review as a conference paper at ICLR 2018

variations have been proposed. Huang et al. (2016a) propose DenseNet, where skip-connections are passed from all previous activations. Huang et al. (2016b) propose to shorten networks during training by randomly dropping entire layers, leading to better gradient flow and information propagation, while using the full network at test time.
Recently, the goal of learning deep networks without skip-connections has begun to receive more attention. Zagoruyko & Komodakis (2017) propose a novel re-parameterization of weights in feedforward networks which they call the Dirac parameterization. Instead of explicitly adding a skipconnection, they model the weights as a residual of the Dirac function, effectively moving the skipconnection inside the non-linearity. In related work, Balduzzi et al. (2017) propose to initialize weights in a CReLU activation function in order to preserve linearity during the initial phases of training. This is achieved by initializing the weights in a mirrored block structure. During training the weights are allowed to diverge, resulting in non-linear activations.
Finally, we note that while the aforementioned approaches have sought to train deeper networks via modifications to the network architecture (i.e., by adding skip-connections) success has also been obtained by modifying the non-linearities (Clevert et al., 2015; Klambauer et al., 2017).

3 VARIABLE ACTIVATION NETWORKS

The goal of this work is to train deep feed-forward networks without skip-connections. To set
notation, we denote x0 as the input and xL as the output of a feed-forward network with L layers. Given training data {y, x0} it is possible to learn parameters {Wl}Ll=1 by locally minimizing some objective function

{W^ l}lL=1 = arg min C y, xL; {Wl}lL=1 .

(3)

First-order methods are typically employed due to the complexity of the objective function in equation (3). However, directly minimizing the objective is not practical in the context of deep networks: beyond a certain depth performance quickly deteriorates on both test and training data. Such a phenomenon, often termed the degradation problem (He et al., 2016b), does not occur in the presence of skip-connections. Accordingly, we take inspiration from ResNets and propose to modify equation (1) in the following manner:

xl+1 = l · Fl(xl, Wl) + (1 - l) · xl

(4)

where l  [0, 1]n determines the convex combination between the non-linearity and the skipconnection. More specifically, l is a vector were the entry i dictates the balance between nonlinearity and skip-connection for the neuron i in layer l. Due to the variable nature of parameters l in equation (4), we refer to networks employing such an activation as Variable Activation Networks
(VAN).

The objective of the proposed method is to train a feed-forward network under the constraint that
l = 1 for all layers, l. When the constraint is satisfied all skip-connections are removed. The advantage of such a strategy is that we only require l = 1 at the end of training. This allows us to initialize l to some other value, thereby relaxing the optimization problem and obtaining the advantages associated with ResNets during the early stages of training. In particular, whenever
l = 1 information is allowed to flow through the skip-connections, alleviating issues associated with shattered and vanishing gradients.

As a result of the equality constraint on l, the proposed activation function effectively does not introduce any additional parameters. All remaining weights can be trained by solving the following
constrained optimization problem:

{W^ l}Ll=1 = argmin C y, xL; {Wl, l}lL=1 such that l = 1 for l = 1, . . . , L.

(5)

The associated Lagrangian takes the following simple form (Boyd & Vandenberghe, 2004):

L
L = C y, xL; {Wl, l}lL=1 + lT (l - 1),
l=1

(6)

3

Under review as a conference paper at ICLR 2018

where each l  Rn are the Lagrange multipliers associated with the constraints on l. In practice, we iteratively update l via stochastic gradients descent (SGD) steps of the form:

C l  l -  l - l

(7)

where  is the step-size parameter for SGD. Throughout the experiments we will often take the

nonlinearity in Fl to be ReLU in which case the activation function corresponds to the parametric

ReLU,

for

which

the

term

C l

can

be

efficiently

computed

(He

et

al.,

2015).

Although

not

strictly

required, we clip the values l to ensure they remain in the interval [0, 1]n.

From equation (6), we have that the gradients with respect to Lagrange multipliers are of the form:

l  l +  (l - 1) ,

(8)

We note that since we require l  [0, 1]n, the values of l are monotonically decreasing. As the value of Lagrange multiplier decreases, this in turn pushes l towards 1 in equation (7). We set the step-size for the Lagrange multipliers,  , to be a fraction of . The motivation behind such a choice
is to allow the network to adjust as we enforce the constraint on l.

3.1 INTUITIONS FROM A TOY MODEL

To provide concrete intuition for the proposed method we consider training a toy network consisting of a single sigmoidal unit. Consider a network mapping scalar x to to F(x, w) = tanh(wx) and propose to learn scalar w via traditional gradient descent with the following cost:

C(w) = 1 2

y - F(x, w)

22.

It is evident from the form of the tanh function that minimizing such a loss will suffer from vanishing gradients when wx is large. Indeed,

dC = -x(y - tanh(wx)) sech(wx)2  0 as wx  ±, dw

hence gradients may vanish due to poor initialization2 of w or large input x, leading to slow con-
vergence. The motivation behind the proposed method follows from the fact that linear activation functions do not suffer from vanishing gradients. Hence we recast our sigmoidal activation F(x, w) as one extreme of a convex combination of the linear and tanh activations, parameterized by the scalar :
F(x, w, ) =  F(x, w) + (1 - ) wx,   [0, 1].

Thus at the  = 0 bound F(x, w, ) reduces to the linear activation function, while at  = 1 we recover the tanh activation function. The proposed method therefore considers

C^(w, ) = 1 2

y - F(x, w, )

22,

subject to the constraint that  = 1 at the end of training. Such a constraint is enforced by formulating the associated Lagrangian as

L(w, , ) = C^(w, ) + (1 - ).

which can be solve by iteratively apply the following update rules:

w  w -  C^ ,    -  C^ -  ,    -  ( - 1) . w 

In the limit, these dynamics yield  = 1, w = w at their stationary point, just like ordinary gradient descent on C(w). The intuition motivating our approach is that by employing a constrained optimization approach we allow  temporary freedom to assume values less than 1, for which the activation function is more linear. This leads to gradients whose magnitude is larger thereby speeding up the process of learning. Figure 1 demonstrates that this is indeed the case on some toy data.

2We note that typical initialization schemes for weights, such as Xavier or He initialization, would avoid such degeneracy in this toy network. However, this is not generally the case in deeper networks.

4

Under review as a conference paper at ICLR 2018

(w) |w w * |

2.0 GD VAN
1.5 VAN ( * )
1.0
0.5
0.0 100 101 102 103 104 105 iterations

1
0 100 101 102 103 104 105 iterations

6
4
2
0 100 101 102 103 104 105 iterations

Figure 1: Training a single unit on a single example (x = 1; y = tanh(3x)). Left: True loss i.e.

for  = 1. Middle: Time course of . Right: Time course of weight error. Ordinary gradient

descent (GD, green) is slow due to vanishing gradients. VAN (blue) is faster because  is allowed

to be non-zero, allowing it to take a larger initial steps. Note that  dips briefly back towards zero

because

of

the

contribution

of

the

-

C^(w,) 

to

its

dynamics,

but

is

pushed

back

to

1

by

the

Lagrange

multipliers. Optimizing  to minimize the true loss C(w) rather than C^(w, ) results in the fastest

convergence (orange).

Furthermore, in the context of a simple network such as the one considered here, we can compute

the optimal value of  at each iteration. If we update w according to the gradient of C such that

w

=

-



C^(w,) w

,

the

resulting

first-order

approximation

to

the

change

in

the

cost

function

is

C(w, )

=

C(w) w

=

- C(w)

C^(w,

) .

w w w

We can then define (w) as the   [0, 1] that minimizes this first-order change in the true loss for

some fixed w:

(w) argmin C(w, ) = argmax C(w) C^(w, ) .

[0,1]

[0,1] w

w

Notice that only the second term depends on . Because the domain of maximization of  contains 1 as a special case we have that C(w, (w))  C(w, 1). Hence if (w) can be found, descent according to the gradient of C^(w, ) would (locally) reduce the loss at least as much standard
gradient descent.

For the case of a single tanh unit considered here, finding (w) requires minimizing a simple quadratic over [0, 1]. Using this value of (w) yields a dramatic improvement in convergence time (see Figure 1). However, for networks with multiple layers, the optimal choice of  values across layers interact leading to an intractable optimization problem. The use of Lagrange multipliers constitutes a computationally efficient alternative in such settings.

4 EXPERIMENTS
We first demonstrate the capabilities of the proposed method using a simple, non-convolutional architecture on the MNIST and Fashion-MNIST datasets (Xiao et al., 2017) in Section 4.1. More extensive comparisons are then considered on the CIFAR datasets (Krizhevsky & Hinton, 2009) in Section 4.2.
4.1 MNIST AND FASHION-MNIST
Networks of varying depths were trained on both MNIST and Fashion-MNIST datasets. Following Srivastava et al. (2015) the networks employed in this section were thin, with each layer containing 50 hidden units. In all networks the first layer was a fully connected plain layer followed by l layers and a final softmax layer. The proposed method is benchmarked against several popular architectures such as ResNets and Highway Networks as well as the recently proposed DiracNets (Zagoruyko & Komodakis, 2017). Plain networks without skip-connections are also considered. For all architectures the ReLU activation function was employed. For both ResNets and VAN the residual function consisted of batch-normalization followed by ReLU.

5

Under review as a conference paper at ICLR 2018
Figure 2: Results on MNIST (left) and fashion-MNIST (right) for various different architectures as the depth of the network varies from 1 to 30. Mean average test accuracy over 10 independent training sessions is shown. We note that with the exception of plain networks, the performance of all remaining architectures is stable as the number of layers increases.
The depth of the network varied from l = 1 to l = 30 hidden layers. All networks were trained using SGD with momentum. The learning rate is fixed at 0.001 and the momentum parameter at 0.9. Training consisted of 50 epochs with a batch-size of 128. In the case of VAN networks the l values were initialized to 0.5 · 1 for all layers. As such, during the initial stages of training VAN networks had the same activation function as ResNets.
RESULTS
The results are shown in Figure 2 where the test accuracy is shown as a function of the network depth for both the MNIST and Fashion-MNIST datasets. In both cases we find that the performance of plain networks deteriorates significantly once the network depth exceeds some critical value (approximately 10 layers). As would be expected, this is not the case for ResNets, Highway Networks and DiracNets as such architectures have been explicitly designed to avoid this behavior. We note that VAN networks obtain competitive results across all depths. This is particularly true in the context of shallow networks (l  10 layers) where VAN networks obtain the highest test accuracies. In particular, we note that VAN networks outperform plain networks in this context, suggesting that the introduction of variable skip-connections may lead to convergence to more desirable local optima.
4.2 CIFAR
As a more realistic benchmark we consider the CIFAR-10 and CIFAR-100 datasets. These consist of 60000 32×32 pixel color images with 10 and 100 classes respectively. The datasets are divided into 50000 training images and 10000 test images. We follow He et al. (2016a) and train deep convolutional networks consisting of four blocks each consisting of n residual layers. The residual function is of the form conv-BN-ReLU-conv-BN-ReLU. This corresponds to the pre-activation function (He et al., 2016b). The convolutional layers consist of 3 × 3 filters with downsampling at the beginning of blocks 2, 3 and 4. The network ends with a fully connected softmax layer, resulting in a depth of 8n + 2. The architecture is described in Table 1. Networks were trained using SGD with momentum over 165 epochs. The learning rate was set to 0.1 and divided by 10 at the 82nd and 125th epoch. The momentum parameter was set to 0.9. Networks were trained using mini-batches of size 128. Data augmentation followed Lee et al. (2015): this involved random cropping and horizontal flips. Weights were initialized following He et al. (2015). As in Section 4.1, we initialize l = 0.5 for all layers. On CIFAR-10 we ran experiments with n  {1, 2, 3, 4, 5, 6, 8, 10} yielding networks with depths ranging from 10 to 82. For CIFAR-100 experiments were run with n  {1, 2, 3, 4}.
6

Under review as a conference paper at ICLR 2018

Figure 3: Left: Results on CIFAR-10 dataset are shown as the depth of networks increase. We note

that the performance of both VAN and plain networks deteriorates as the depth increases, but the

effect is far less pronounced for plain networks. Right: Training and test error curves are shown for

networks

with

26

layers.

We

also

plot

the

mean



residuals:

1 L

lL=1(1 - l)2 on the right axis.

RESULTS

Results for experiments on CIFAR-10 are shown in Figure 3. The left panel shows the mean test accuracy over five independent training sessions for ResNets, VANs and plain networks. While plain networks provide competitive results for networks with fewer than 30 layers, their performance quickly deteriorates thereafter. We note that a similar phenomenon is observed in VAN networks but the effect is not as dramatic. In particular, the performance of VANs is similar to ResNets for networks with up to 40 layers. Beyond this depth, ResNets outperform VAN by an increasing margin. This may be a result of the ensemble nature of ResNets (Veit et al., 2016), which does not play a significant role until the depth of the network increases. A comparison of the performance of VAN networks in provided in Table 2. We note that while VAN networks do not outperform ResNets, they do outperform other alternatives such as Highway networks and FitNets (Romero et al., 2014) when networks of similar depths considered. In the case of both VANs and Highway Networks the best performance is obtained with networks of 18 and 19 layers respectively while ResNets continue to improve their performance as depth increases.

Training curves for a plain and VAN network are shown on the right hand side of Figure 3. The

thick gold line indicates the mean residuals of the l parameters across all layers. This is defined as

1 L

Ll=1(1-l)2 and is a measure of the extent to which skip-connections are present in the network.

Recall that if all l values are set to one then all skip-connections are removed (see equation (4)).

From Figure 3, it follows that skip-connections are effectively removed from the VAN network at

approximately the 80th iteration.

Table 1: Architecture of varying depth employed on CIFAR-10 and CIFAR-100 datasets. Downsampling occurs at the beginning of block 2, 3 and 4 with a stride of two.

Layer name Output size

conv1

32 × 32

Block 1

32 × 32

Block 2

16 × 16

Block 3

8×8

Block 4

4×4

Convolution 3 × 3, 64

3×3,64 3×3,64

×n

3×3,128 3×3,128

×n

3×3,256 3×3,256

×n

3×3,512 3×3,512

×n

Figure 4 provides results on the CIFAR-100 dataset. This dataset is considerably more challenging as it consists of a larger number of classes as well as fewer examples per class. As in the case of
7

Under review as a conference paper at ICLR 2018

Figure 4: Left: Results on CIFAR-100 dataset are shown as the depth increases from 10 to 34 layers.

We note that the performance of both VAN and plain networks deteriorates as the depth increases,

but the effect is far less pronounced for plain networks. Right: Training and test error curves are

shown

for

VAN

and

plain

networks

with

18

layers.

The

mean



residuals,

1 L

Ll=1(1 - l)2, are

shown in gold along the right axis.

CIFAR-10, we observe a fall in the performance of both VAN and plain networks beyond a certain depth (in this case approximately 20 layers). Despite this drop in performance, Table 2 indicates that the performance of VAN networks with both 18 and 26 layers is competitive with many alternatives proposed in the literature such as Highway Networks and FitNets.
Training curves are shown on the right hand side of Figure 4. As in the equivalent plot for CIFAR-10, the introduction and subsequent removal of skip-connections during training leads to improvements in generalization error.

Table 2: Comparison of VAN networks to results of other convolutional networks on CIFAR-10 and CIFAR-100. For VAN networks we report the best value as well as the mean and standard deviation over five independent training runs (in brackets). Results are ordered by performance on CIFAR-10.

Architecture
Highway Network FitNet Highway Network DiracNet (width-1) ELU VAN (ours) VAN (ours) DiracNet (width-2) ResNet

# Layers
32 19 19 34 18 26 18 34 164

CIFAR-10 (test error %)
8.80 8.39 7.54 7.10 6.55 6.22 (6.55 ± 0.24) 6.17 (6.35 ± 0.13) 5.60 5.46

CIFAR-100 (test error %)
35.04 32.39
24.28 28.31 (29.14 ± 0.48) 27.15 (27.45 ± 0.41) 26.72 24.33

5 DISCUSSION
This manuscript presents a simple method for training deep feed-forward networks without skipconnections. The goal of the proposed method is to exploit many of the optimization benefits associated with skip-connections when training plain networks. This is achieved by posing network training as a constrained optimization problem where skip-connections are introduced during the early stages of training and subsequently phased out in a principled manner using Lagrange multipliers. Throughout a series of experiments we demonstrate that VAN networks obtain competitive results on both the CIFAR-10 and CIFAR-100 datasets.
The method proposed in this work has focused on networks without skip-connections due to the well-documented optimization challenges associated with training such networks, and to emphasize the ability of this approach to address such issues. However, the proposed method is quite general and can be readily extended to networks which do allow for skip-connections.
8

Under review as a conference paper at ICLR 2018
REFERENCES
David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? arXiv preprint arXiv:1702.08591, 2017.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798­1828, 2013.
Yoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R in Machine Learning, 2(1):1­127, 2009.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249­256, 2010.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http: //www.deeplearningbook.org.
Klaus Greff, Rupesh K Srivastava, and Ju¨rgen Schmidhuber. Highway and residual networks learn unrolled iterative estimation. arXiv preprint arXiv:1612.07771, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, pp. 630­645. Springer, 2016b.
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, Ju¨rgen Schmidhuber, et al. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016a.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European Conference on Computer Vision, pp. 646­661. Springer, 2016b.
David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. The Journal of physiology, 160(1):106­154, 1962.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Gu¨nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In Advances in Neural Information Processing Systems, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeplysupervised nets. In Artificial Intelligence and Statistics, pp. 562­570, 2015.
Tapani Raiko, Harri Valpola, and Yann LeCun. Deep learning made easier by linear transformations in perceptrons. In Artificial Intelligence and Statistics, pp. 924­932, 2012.
9

Under review as a conference paper at ICLR 2018
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.
Nicol N Schraudolph. Centering neural network gradient factors. In Neural Networks: Tricks of the Trade, pp. 205­223. Springer, 2012.
Rupesh K Srivastava, Klaus Greff, and Ju¨rgen Schmidhuber. Training very deep networks. In Advances in neural information processing systems, pp. 2377­2385, 2015.
Andreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of relatively shallow networks. In Advances in Neural Information Processing Systems, pp. 550­558, 2016.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
Sergey Zagoruyko and Nikos Komodakis. Diracnets: Training very deep neural networks without skip-connections. arXiv preprint arXiv:1706.00388, 2017.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pp. 818­833. Springer, 2014.
10

