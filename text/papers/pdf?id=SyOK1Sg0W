Adaptive Quantization of Neural Networks
Anonymous Author(s) Affiliation Address email
Abstract
1 Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various 2 classification problems, their deployment onto resource constrained edge comput3 ing devices remains challenging, due to their large size and complexity. Several 4 recent studies have reported remarkable results in reducing this complexity through 5 quantization of DNN models. However, these studies usually do not consider 6 the change in loss when performing quantization, nor do they take the disparate 7 importance of DNN connections to the accuracy into account. We address these 8 issues in this paper by proposing a new method, called adaptive quantization, which 9 simplifies a trained DNN model by finding a unique, optimal precision for each 10 connection weight such that the increase in loss is minimized. The optimization 11 problem at the core of this method iteratively uses the loss function gradient to 12 determine an error margin for each weight and assign it a precision accordingly. 13 Since this problem uses linear functions, it is computationally cheap and, as we will 14 show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, 15 and SVHN datasets showed that the proposed method can achieve near or better 16 than state-of-the-art reduction in model size with similar error rate. Furthermore, 17 it can achieve compressions close to floating-point model compression methods 18 without loss of accuracy.
19 1 Introduction
20 Deep Neural Networks (DNNs) have achieved incredible accuracy in applications ranging from 21 computer vision (Simonyan and Zisserman 2014) to speech recognition (Hinton et al. 2012) and 22 natural language processing (Devlin et al. 2014). One of the key enablers of the unprecedented 23 success of DNNs is the availability of very large model sizes. While the increase in model size 24 improves the classification accuracy, it inevitably increases the computational complexity and memory 25 requirement needed to train and store the network. This poses challenges in deploying these large 26 models in resource-constrained edge computing environments, such as mobile devices. This challenge 27 motivates neural network compression, which exploits the redundancy of neural networks to achieve 28 drastic reductions in model sizes. The state-of-the-art neural network compression techniques include 29 weight quantization (Courbariaux et al. 2015), weight pruning (Han et al. 2015), weight sharing (Han 30 et al. 2015), and low rank approximation (Zhao et al. 2017). For instance, weight quantization has 31 previously shown good accuracy with fixed-point 16-bit and 8-bit precisions (Suda et al. 2016; Qiu 32 et al. 2016). Recent work attempts to push that even further towards reduced precision and has trained 33 models with 4-bit, 2-bit, and 1-bit precisions using quantized training methods (Hubara et al. 2016a; 34 Zhou et al. 2016; Hubara et al. 2016b; Courbariaux et al. 2015).
35 Although these quantization methods can significantly reduce model complexity, they generally have 36 two key constraints. First, while performing quantization they do not take the change in the loss 37 function into account. This can result in a disparity between the quantized model accuracy and the 38 objective accuracy. Previous works get around this issue by devising quantized learning schemes that
Submitted to 6th Conference on Learning Representations (ICLR 2018). Do not distribute.

39 essentially try to solve the problem after the fact. Such schemes have the disadvantage of converging 40 very slowly compared to full-precision learning methods. Second, they do not differentiate between 41 different connections in their approaches. This is despite the fact that it has been shown that different 42 weights are not equally important to the accuracy of the network (). Disregarding this difference 43 constraints these methods to either use higher precisions for quantization or use deeper networks. 44 Both of these solutions diminish the compression.
45 In this paper, we address the aforementioned issues by proposing adaptive quantization. To take 46 the disparate importance of different connections into account, this method quantizes each weight 47 of a trained network by a unique precision. This way, weights that impact the accuracy the most 48 can be represented using a higher precision, while low-impact weights are represented with only 49 fewer bits or are pruned. Consequently, it can reduce the model size significantly while maintaining 50 a certain accuracy. The proposed method monitors the accuracy by incorporating the loss function 51 into an optimization problem to minimize the models. The output of the optimization problem is 52 an error margin associated to each weight. This margin is computed based on the loss function 53 gradient of the weight and is used to determine its precision. We will show that the proposed 54 optimization problem has a closed-form approximate solution, which can be iteratively applied to the 55 same network to minimize its size, constraining the loss. We test the proposed method using three 56 classification benchmarks comprising MNIST, CIFAR-10, and SVHN. We show that, across all these 57 benchmarks, we can achieve near or better compressions compared to state-of-the-art quantization 58 techniques. Furthermore, we can achieve similar compressions compared to state-of-the-art pruning 59 and weight-sharing techniques which inherently require more computational resources for inference.

60 2 Proposed Quantization Algorithm
61 Despite their remarkable classification accuracies, large DNNs assimilate redundancies. Several 62 recent works have studied these redundancies by abstracting the network from different levels and 63 search for, in particular, redundant filters (DenseNets ) and redundant connections (Deep Compression 64 ). In this work, we present an even more fine-grained study of redundancy and extend it to fixed-point 65 quantization of network parameters. That is, we approximate the minimum size of the network when 66 each parameter is allowed to have a distinct number of precision bits. Our goal here is to represent 67 parameters with high precisions only when they are critical to the accuracy of the network. In this 68 sense, our approach is similar to the weight pruning which eliminated all but the essential parameters, 69 producing a sparse network. In the rest of this section, we first formally define the problem of 70 minimizing the network size as an optimization problem. Then, we propose a trust region technique 71 to approximately solve this problem. We will show that, in each iteration of the trust region method, 72 our approach has a straightforward closed-form solution. Finally, we will explain how the hyper 73 parameters in the algorithm are chosen and discuss the implications of the proposed technique.

74 2.1 Problem Definition
75 A formal definition of the optimization problem that was discussed previously is presented here. 76 Then, key characteristics of the objective function are derived. We will use these characteristics later 77 when we solve the optimization problem.
78 We minimize the aggregate bit-width of all network parameters while monitoring the training loss 79 function. Due to the quantization noise, this function deviates from its optimum which was achieved 80 by training. This effect can be controlled by introducing an upper bound on the loss function in order 81 to maintain it reasonably close to the optimum. Consequently, solution of this minimization problem 82 represents critical parameters with high precision to sustain high accuracy and assigns low precisions 83 to ineffectual ones or prunes them. The problem described here can be defined in formal terms as 84 below.

n
min NQ(W ) = Nq(i)
W i=1

(1)

l(W )  l

(2)

2

85 Here, n is the number of model parameters, W = [1...n]T is a vector of model parameters, i 86 is the i-th model parameter, and the function Nq(i) is the minimum number of bits required to 87 represent i in its fixed-point format. As a result, NQ(W ) is the total number of bits required to 88 represent the model. In addition, l(W ) is the loss of the model over the training set (or a minibatch
89 of the training set). l ( l(W0)) is a constant upper bound on the loss of the neural network, which is 90 used to bound the accuracy loss, and W0 is the vector of initial model parameters before quantization.
91 The optimization problem presented in equations 1 and 2 is difficult to solve because of its non-smooth 92 objective function. However, a smooth upper limit can be found for it.
93 Lemma 2.1. Given a vector of network parameters W , a vector of tolerance values T = [1...n]T , 94 and a vector of quantized network parameters Wq = [1q...nq ]T , such that each iq, i  [1, n] solves 95 the constrained optimization function:

96 We have that:

min
q

Nq

(q

)

|q - i|  i

(3) (4)

NQ(W )  NQu (T ) 97 Where NQu (T ) is a smooth function, defined as:

(5)

n
NQu (T ) = - log2 i
i=1

(6)

98 Proof. The problem of equation 4 can be easily solved using algorithm 1, which simply checks all 99 possible solutions one-by-one. Here, we allocate at most 32 bits to each parameter. This should be 100 more than enough as previous works have demonstrated that networks can be easily quantized to 101 even 8 bits without significant loss of accuracy.

Algorithm 1 Quantization of a parameter
procedure QUANTIZE_PARAMETER(i, i) q  0 Nq  0 while Nq  32 do if |q - i|  i then break
end if q  round(2Nq )/2Nq Nq  Nq + 1 end while return Nq, q end procedure

102

Algorithm

1

estimates

i

with

a

rounding

error

of

up

to

.1
2Nq +1

This

error

needs

to

be

consistent

with

103 the constraint of equation 4:

1 2Nq(i)+1  i  Nq(i) + 1  - log2 i 104 The minimization in equation 4 entails that:

(7)

Nq(i) = - log2 i - 1  - log2 i

(8)

3

105 Consequently:
106

nn
NQ(W ) = Nq(i)  - log2 i = NQu (T )
i=1 i=1

(9)

107 2.2 Solving the Optimization Problem
108 The optimization problem of equations 1 and 2 presents two key challenges. First, as was mentioned 109 previously, the objective function is non-smooth. Second, not only is the constraint (equation 2) 110 non-linear, but also it is unknown. In this section, we present a method that circumvents these 111 challenges and provides an approximate solution for this problem.
112 We sidestep the first challenge by using the upper bound of the objective function, NQu (T ). Particu113 larly, we approximate the minimum of NQ(W ) by first finding the optimal T for NQu (T ) and then 114 calculating the quantized parameters using algorithm 1. This optimization problem can be defined as: 115 minimize NQu (T ) such that if each i is perturbed by, at most, i, the loss constraint of equation 2 is 116 not violated:

117

min
T

NQu (T

)

(10)

l(W0 + W )  l

(11)

W = [1...n]T such that i  [1, n] : |i|  i

(12)

118 It is important to note that although the upper bound NQu (T ) is smooth over all its domain, it is tight 119 only when i  [1, n] : i = 2-k, k  N. This difference between the objective function and NQu (T )
120 means that it is possible to iteratively reduce NQ(W ) by repeating the steps of the indirect method, 121 described above, and improve our approximation of the optimum quantization.

122 Such an iterative algorithm would also help address the second challenge. Specifically, it allows us 123 to use a simple model of the loss function (e.g. a linear or quadratic model) as a stand-in for the 124 complex loss function. If in some iteration of the algorithm the model is inaccurate, it can be adjusted 125 in following iteration. In our optimization algorithm, we will use a linear bound of loss function and 126 adopt a Trust Region method to track its accuracy.

127 Trust region optimization methods iteratively optimize an approximation of the objective function. In

128 order to guarantee the accuracy of this model, they further define a neighborhood around each point

129 in the domain, called the trust region, inside which the model is a "sufficiently good" approximation

130 of the objective function. The quality of the approximation is evaluated each iteration and the size of

131 the trust region is updated accordingly. In our algorithm we adopt a similar technique, but apply it to

132 the constraint instead of the objective function.

133 Lemma 2.2. We refer to ml : Rn  R as an accurate estimation of l(W ) in a subdomain of l like 134 D  Rn, if:

W  D : ml(W )  l  l(W )  l

(13)

135 Let the radius  specify a spherical trust region around the point W0 where the loss function l is 136 accurately estimated by its first-order Taylor series expansion. Then, the constraint of equation ??,

137 when T 2  , is equivalent to:

mlu(T ) = l(W0) + GT T  l 138 Where G = [g1...gn]T and gi = |[W l]i|, i  [1, n].

(14)

139 Proof. Since T 2  , then W as defined in equation ?? is inside the trust region. Therefore:

mlu(T

)

=

max{l(W0)
W

+

W

l(W0)T

W

}



l

(15)

140

4

141 Consequently, if we define such a trust region, we can simply the following problem as our subproblem 142 in each iteration. We will present the solution for this subproblem in the next section.

min
T

NQu (T

)

mul (T )  l

(16) (17)

143 Algorithm 2 summarizes the proposed method of solving the original optimization problem (equation
144 1). This algorithm first initializes the trust region radius  and the loss function bound l. Then, it 145 iteratively solves the subproblem of equation 17 and calculates the quantized parameters. If the loss 146 function corresponding the quantized parameters violates the loss bound of equation 2, it means that 147 the linear estimation was inaccurate over the trust region. Thus, we reduce the radius of the trust 148 region and in the solve the subproblem over a smaller trust region in the next iteration. In this case, 149 the calculated quantized parameters are rejected. Otherwise, we update the parameters. In addition,
150 we grow the trust region unless if it grows larger than an upper bound on this radius (Delta). We use 151 two measures to examine convergence of the solution: the loss function and the trust region radius.
152 If the loss function of the quantized model converges to the loss bound l or the trust region radius 153 becomes very small, indicated by values and , respectively. Note that the protocol to update the 154 trust region radius as well as the trust region convergence condition used in this algorithm here are 155 commonly used in trust region methods.

Algorithm 2 Adaptive Quantization

k0

Initialize ,  and l while l - l(W k)  and    do

Solve Trust Region Subproblem (Section 2.3) U = Quantize(W k, T k)

if l(U )  l then

W k+1  U

  min(2k, )

else

k+1



1 2

k

end if

k  k+1

end while

156 2.3 Trust Region Subproblem 157 We can directly solve the subproblem of equation ?? by writing its KKT conditions:

NQu + mlu = 0 (mlu - l) = 0

(18) (19)

158

The

gradient

of

the

objective

function,

NQu

=

-

1 ln 2

[

1 1

...

1 n

]

cannot

be

zero.

Hence,

psi

>

0

and

159 mlu - l = 0.

NQu

+

mul

=

0



i



[1, n]

:

1 -
ln 2i

+

gi



i



[1, n]

:

igi

=



1 ln 2



GT T

=

n  ln 2

mul

-

l

=

0



l(W ) +

n  ln 2

-l

=

0



n  ln 2

=

l

- l(W ) n



i



[1, n]

:

i

=

l

- l(W ) ngi

160 If for the resulting tolerance vector T > , we scale T so that its norm would be equal to .

5

161 This solution is correct only when gi > 0 for all i. It is possible, however, that there exists some i for 162 which gi = 0. In this case, we use the following equation to calculate the optimal solution T .

i =

d ngi
|i|

gi > 0 gi = 0

(20)

163 We treat singular points this way for three reasons. First, a gradient of zero with respect to i means 164 that the cost is not very sensitive to its value. Thus, it might be possible to eliminate it quickly. Second,
165 this insensitivity of the cost to i means that large deviations in its value would not significantly 166 affect the cost. Finally, setting the value of i to a large value relative to other tolerances reduces 167 their values after normalization to the trust region radius. Thus, the effect of a singularity on other 168 parameters is reduced. After finding tolerance values, we once again calculate T k by normalizing T 
169 to the trust region radius.

170 2.4 Choosing Hyper-parameters

171 The hyper-parameters, , and l, determin the speed of convergence and the final classification

172 accuracy. Smaller trust regions result in slower convergence, while larger trust regions produce higher

173 174

cehrrooorsreat2e0s. Fno. rSim,islainrlcye,

remaining the higher

values could ultimately be the loss bound l, the lower

represented by 0 bits (pruned), we the accuracy, while small values of

175 loss bound prevent effective model size reduction. We choose this value in our algorithm on the flye.

176 That is, we start off by conservatively choosing a small value for l (e.g. l = l(W0)) and then increase 177 it everytime the algorithm 2 converges. Algorithm 3 provides the details of this process. At the

178 begining the loss bound is chosen to be the loss of the floating-point trained model. After quantizing

179 the model using this bound, the bound value is increased by Scale. These steps are repeated Steps

180 times. In our experiments in the next section Scale and Steps are set to 1.1 and 20, respectively.

Algorithm 3 Adaptive Quantization
Initialize Steps and Scale l  l(W0) while Steps > 0 do
Quantize model using Algorithm 2. Steps  Steps - 1 l  l  Scale end while

181 3 Evaluation and Discussion
182 We evaluate the performance of the proposed quantization method in compressing trained NN models 183 using a set of classification datasets. In our evaluation, we train three networks using three common 184 datasets. Then, we use the adaptive quantization method to quantize these trained models. In order to 185 achieve the smallest model sizes, we apply the quantization method three times and after each time 186 we retrain the network for one epoch in the full precision domain. We will experimentally show that 187 three passes of quantization is sufficient for achieving good accuracy as well as model size reduction. 188 In this section, we discuss the benchmarks used in these experiments and analyze their quantization 189 results.
190 3.1 Benchmarks
191 We use MNIST (LeCun et al. 1998a), CIFAR-10 (Krizhevsky and Hinton 2009), and SVHN (Netzer 192 et al. 2011) in our experiments. MNIST is a small dataset containing 28 × 28 images of handwritten 193 digits from 0 to 9. For this dataset, we use the LeNet-5 network (LeCun et al. 1998b). For both 194 CIFAR-10, a dataset of 32 × 32 images in 10 classes, and SVHN, a large dataset of 32 × 32 real-world 195 images of digits, we employ the same organization of convolution layers and fully connected layers 196 as networks used in BNN (Hubara et al. 2016b). These networks are based on VGG (Simonyan and 197 Zisserman 2014) but have parameters in full precision. The only difference is that in the case of
6

198 CIFAR-10, we use 4096 neurons instead of the 1024 neurons used in BNN, and we do not use batch 199 normalization layers. We train these models using a Cross entropy loss function. For CIFAR-10, we 200 also add an l2 regularization term. Table 1 shows the specifications of the baseline trained models.

Dataset

Table 1: Baseline models

Model

Model Size Error Rate

MNIST LeNet-5 (LeCun et al. 1998b)

CIFAR-10 (Hubara et al. 2016b)

SVHN

(Hubara et al. 2016b)

12M b 612M b 110M b

0.65% 9.08% 7.26%

201 3.2 Execution Time
202 The key contributors to the computational load of the proposed technique are back propagation (to 203 calculate gradients) and quantization (algorithm 2). Both these operations can be completed in O(n) 204 complexity. We implement the proposed quantization on Intel Core i7 CPU (3.5 GHz) with Titan X 205 GPU performing training and quantization. The timing results of the algorithm have been summarized 206 in table 2.

Table 2: Timining results for training and quantization of the benchmark models in seconds Dataset Training Quantization

MNIST CIFAR-10 SVHN

120 4560 1920

300 4320 2580

207 3.3 Quantization Results

208 We evaluate the performance of the quantization algorithm by analyzing the compression rate of the 209 models and their respective error rates after each pass of quantization without retraining. As discussed 210 in section 2, the quantization algorithm will try to reduce parameter precision while maintaining a 211 minimum classification accuracy loss. Here we present the results of these experiments.

212 Figure 1a depicts the error rate of LeNet trained on the MNIST dataset as it is compressed through 213 three passes of adaptive quantization and retraining. As this figure shows, the second and third passes 214 tend to have smaller error rates compared to the first pass. But, at the end of the second pass the 215 error rate is higher than the first one. This can be improved by increasing the number of epochs for
216 retraining or choosing a lower cost bound (C). By the end of the third pass, the highest compression 217 rate is achieved. However, due to its small difference in compression rate compared to its preceding 218 pass, we do not expect to achieve more improvements by continuing the retraining process.

219 We also evaluate the convergence speed of the algorithm by measuring the compression rate (1 -

220

sizeof quantizedmodel sizeof originalmodel

)

of

the

model

after

each

iteration.

The

results

of

this

experiment

for

one

pass

221 of quantization have been depicted in Figure 1b. As shown in the figure, the size of the model is

222 reduced quickly during the initial iterations. This portion of the experiment corresponds to the part of

223 Figure 1a where the error rate is constant. However, as quantization continues, the model experiences

224 diminishing returns in size reduction. After 25 iterations, little reduction in model size is achieved.

225 The lower density of data points past this point is due to the generated steps failing the test on the

226 upper bound on cost in Algorithm 2. Consequently, the algorithm reduces the trust region size and

227 recalculates the tolerance steps.

7

(a) Error rate increase as the model is compressed when retraining is applied

(b) Convergence of quantization

Figure 1: Quantization of LeNet-5 model trained on MNIST dataset

228 4 Comparison with Related Approaches

229 The generality of the proposed technique makes it adaptable to other model compression techniques. 230 In this section, we review some of the most notable of these techniques and examine the relationship 231 between their approach and ours. Specifically, we will explain how the proposed approach subsumes 232 previous ones and how it can be specialized to implement them in an improved way.
233 Pruning: In pruning, small parameters in a trained network are set to zero, which means that their 234 corresponding connections are eliminated from the network. Han et al. showed that by using pruning, 235 the number of connections in the network can be reduced substantially (Han et al. 2015). Adaptive 236 quantization confirms similar observations that in a NN, most connections can be eliminated. In fact, 237 adaptive quantization eliminates 5.6×, 5.6×, and 5.3× of the connections in the networks trained for 238 MNIST, CFAR, and SVHN, respectively. These number are lower compared to their corresponding 239 values achieved using deep compression. The reason for this difference is that deep compression 240 amortizes for the loss of accuracy due to pruning of a large number of parameters by using full 241 precision in the remaining ones. In contrast, adaptive quantization eliminates fewer connections and 242 instead quantizes the remaining parameters.
243 While adaptive quantization identifies connections that can be eliminated automatically, deep com244 pression identifies them by setting parameters below a certain threshold to zero. However, this 245 technique might not be suitable in some cases. For example, we consider the network model trained 246 for CIFAR-10. This network has been trained using L2 regularization, which helps avoid over fitting. 247 As a result, the trained model has a large population of small-valued parameters. Looking at Figure 2a 248 which shows the distribution of parameter values in the CIFAR-10 model, we see that most populate 249 a small range around zero. Such cases can make choosing a good threshold for pruning difficult.
250 Weight Sharing: The goal of weight sharing is to create a small dictionary of parameters. The 251 parameters are grouped into bins whose members will all have the same value. Therefore, instead 252 of storing parameters themselves, the weights can be replaced by the index in the dictionary corre253 sponding to that parameter's bin. Deep compression implements weight sharing by applying k-means 254 clustering to trained network parameters (Han et al. 2015). Similarly, Smaragh et al. implements 255 weight sharing by iteratively applying k-means clustering and retraining in order to find the best 256 dictionary (Sarmagh et al. 2017). In both of these works, the number of dictionary entries are fixed in 257 advance. Adaptive quantization, on the other hand, produces the bins as a byproduct and does not 258 make assumptions on the total number of bins.
259 Deep compression also identifies that the accuracy of the network is less sensitive to the fully 260 connected layers compared to the convolution layers. Because of that it allocates a smaller dictionary 261 for storing them. Looking at figures 2b and 2c, we can see that results of adaptive quantization are 262 consistent with this observation. These figures show the distribution of quantization bits for one 263 convolution layer and one fully connected layer in the quantized CIFAR-10 network. It is clear from 264 these figures that parameters in the fully connected layer, on average require fewer quantization bits.
265 Binarization and Quantization: Binarized Neural Networks (Courbariaux et al. 2015; Hubara 266 et al. 2016b) and Quantized Neural Networks (Hubara et al. 2016a) can reduce model size by 267 assuming the same quantization precision for all parameters in the network or all parameters in a layer.

8

(a) Distribution of parameter values.

(b) Distribution of Quantization bits in the first convolution layer

(c) Distribution of Quantization bits in the first fully connected layer

Figure 2: Distributions of parameter values and quantization bits for the CIFAR-10 network. Zero elements have been excluded because of their large number.

(a) MNIST

(b) CIFAR-10

(c) SVHN

Figure 3: Trade-off between accuracy and error rate for benchmark datasets. BNN-2048, BNN-24096, BNN-Theano are from Hubara et al. (2016b); BinaryConnect is from Courbariaux et al. (2015); and Deep Compression is from Han et al. (2015)

268 These works often design quantized training algorithms to maintain the same parameter precisions 269 throughout training. These techniques can be slower than full-precision training. In contrast, adaptive 270 quantization allows for unique quantization precisions for each parameter. This way, for a trained 271 model, it can find a notably smaller network, indicating the limits of quantization. In addition, it does 272 not require quantized training.
273 In order to compare adaptive quantization with previous works, we quantize trained models with 274 different training cost bounds. This way, for each dataset, we produce several models with different 275 sizes and error rates. We illustrate the results of these experiments in Figure 3, showing the trade-off 276 between model size and error rate.
277 In both MNIST and CIFAR-10, adaptive quantization produces curves below the ones achieved in 278 BNN (Hubara et al. 2016b) and BinaryConnect (Courbariaux et al. 2015), and it shows a smaller 279 model size while maintaining the same error rate or the same model size with a smaller error rate. In 280 the case of SVHN, the BNN achieves a better result compared to adaptive quantization. This is due 281 in part to the error rate of the full-precision model being higher than the BNN model.
282 Comparison of adaptive quantization against deep compression (when pruning is applied) for the 283 MNIST set shows some similarity between the two techniques, although deep compression achieves 284 a slightly smaller error rate for the same model size. This advantage comes from two differences 285 between the two techniques. First, deep compression uses full precision, floating point parameters 286 while adaptive quantization uses quantized parameters, which requires less computational complexity 287 (fixed-point operation vs. floating-point operation). Second, after pruning the network, deep compres288 sion performs a complete retraining of the network. In contrast, adaptive quantization performs little 289 retraining.
290 Furthermore, adaptive quantization can be used to identify groups of parameters that need to be repre291 sented with more precision. In a more general sense, the flexibility of adaptive quantization allows it 292 to be specialized for more constrained forms of quantization. For example, if the quantization requires 293 that all parameters in the same layer have the same quantization precision, adaptive quantization 294 could find the best model. This could be implemented by solving the same minimization problem 295 as in Equation ??, except the length of T would be equal to the number of layers. In this case, gi in

9

296 Equation ?? would be

jJi

|

C j

(jk )|,

where

Ji

is

the

set

of

parameter

indices

belonging

to

layer

297 i. The rest of the solution would be the same.

298 5 Conclusion
299 In this work, we quantize neural network models such that only parameters critical to the accuracy are 300 represented with high accuracy. The goal is to minimize data movement and simplify computations 301 needed for inference in order to accelerate implementations on resource constrained hardware. 302 To achieve acceleration, the proposed technique prunes unnecessary parameters or reduces their 303 precision. Combined with existing fixed-point computation techniques such as SWAR or Bit304 pragmatic computation , we expect these small fixed-point models to achieve fast inference with 305 high accuracy. We have confirmed the effectiveness of this technique through experiments on several 306 benchmarks. Through this technique, our experiments show, DNN model sizes can be reduced 307 significantly without loss of accuracy. The resulting models are significantly smaller than state-of308 the-art quantization technique. Furthermore, it can provide similar results to floating-point model 309 compression techniques.

310 References
311 K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," 312 arXiv preprint arXiv:1409.1556, 2014.
313 G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, 314 T. N. Sainath et al., "Deep neural networks for acoustic modeling in speech recognition: The 315 shared views of four research groups," IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82­97, 316 2012.
317 J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. M. Schwartz, and J. Makhoul, "Fast and robust neural 318 network joint models for statistical machine translation." in ACL (1). Citeseer, 2014, pp. 1370­ 319 1380.
320 M. Courbariaux, Y. Bengio, and J.-P. David, "Binaryconnect: Training deep neural networks with 321 binary weights during propagations," in Advances in Neural Information Processing Systems, 2015, 322 pp. 3123­3131.
323 S. Han, H. Mao, and W. J. Dally, "Deep compression: Compressing deep neural networks with 324 pruning, trained quantization and huffman coding," arXiv preprint arXiv:1510.00149, 2015.
325 L. Zhao, S. Liao, Y. Wang, J. Tang, and B. Yuan, "Theoretical properties for neural networks with 326 weight matrices of low displacement rank," arXiv preprint arXiv:1703.00144, 2017.
327 N. Suda, V. Chandra, G. Dasika, A. Mohanty, Y. Ma, S. Vrudhula, J.-s. Seo, and Y. Cao, "Throughput328 optimized opencl-based fpga accelerator for large-scale convolutional neural networks," in Pro329 ceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. 330 ACM, 2016, pp. 16­25.
331 J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T. Tang, N. Xu, S. Song et al., "Going 332 deeper with embedded fpga platform for convolutional neural network," in Proceedings of the 333 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2016, 334 pp. 26­35.
335 I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, "Quantized neural networks: Train336 ing neural networks with low precision weights and activations," arXiv preprint arXiv:1609.07061, 337 2016.
338 S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, "Dorefa-net: Training low bitwidth convolutional 339 neural networks with low bitwidth gradients," arXiv preprint arXiv:1606.06160, 2016.
340 I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, "Binarized neural networks," in 341 Advances in Neural Information Processing Systems, 2016, pp. 4107­4115.
342 Y. LeCun, C. Cortes, and C. J. Burges, "The mnist database of handwritten digits," 1998.

10

343 A. Krizhevsky and G. Hinton, "Learning multiple layers of features from tiny images," 2009. 344 Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, "Reading digits in natural images 345 with unsupervised feature learning," in NIPS workshop on deep learning and unsupervised feature 346 learning, vol. 2011, no. 2, 2011, p. 5. 347 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document 348 recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278­2324, 1998. 349 M. Sarmagh, M. Ghasemzadeh, and F. Koushanfar, "Customizing neural networks for efficient 350 fpga implementation," The 25th IEEE International Symposium on Field-Programmable Custom 351 Computing Machines (FCCM 2017), 2017.
11

