Under review as a conference paper at ICLR 2018
IMPROVING GENERALIZATION WITH WASSERSTEIN
REGULARIZATION
Anonymous authors Paper under double-blind review
ABSTRACT
Natural gradients are expensive to calculate but have been shown to both improve convergence speed and generalization. We point out that the natural gradient is the optimal update when one regularizes the Kullbeck-Leibler divergence between the output distributions of successive updates. The natural gradient can thus be seen as a regularization term upon the change in the parameterized function. With this intuition, we propose it is possible to achieve the same effect more efficiently by choosing and regularizing a simpler metric of two distributions' similarity. The resulting algorithm, which we term Wasserstein regularization, explicitly penalizes changes in predictions on a held-out set. It can be interpreted as a way of regularizing that encourages simple functions. Experiments show that the Wasserstein regularization is efficient and leads to considerably better generalization.
1 INTRODUCTION
The natural gradient is a well-studied and commonly approximated method for the optimization of neural networks. It is often cast as an adaptive strategy: to move quickly when the cost changes slowly, but slowly when the cost changes quickly. In simplest mathematical terms, one performs the natural gradient by computing an expectation of the covariance of gradients EP [JJT ], and then multiplying the current gradients by its inverse. This covariance matrix is known as the Fisher information metric, and the natural gradient step is in the direction F -1J. There are many interpretation of why this modification improves gradient descent.
Amari originally developed the natural gradient in the light of information geometry (Amari et al. (1996); Amari (1998)). Supposing that some direction is parameter space are more informative of the network's outputs, i.e. have a larger impact, we wish to scale updates by each dimension's informativeness. To do so, in fact, is Fisher efficient (Amari et al. (2000)). The natural gradient is the optimal update when the loss geometry is not Euclidean but Riemannian, as Amari argued it would be (Amari (1997)). We can also explain the natural gradient by relating that the Fisher metric is the Hessian of the Kullbeck-Leibler (KL) divergence of the output distribution, expanded around the current parameterization. As mentioned in Pascanu & Bengio (2013), this implies that the natural gradient takes strides of constant distance in the space of output distributions. This intuition forms the basis of our work.
The number of parameters in neural networks typically prohibits calculating and storing a full gradient covariance matrix, let alone inverting one. All implementations of the natural gradient in the literature are necessarily approximate. The list of approximate natural gradient algorithms is long, and includes some of the most popular optimizers beyond SGD. The diagonal Adagrad algorithm, for example, uses a diagonal approximation of the Fisher computed over training examples (the 'empirical Fisher') (Duchi et al. (2011)). Interestingly, the 'full matrix' Adagrad was not originally seen as the empirical natural gradient, but rather as a method to use the empirical Fisher metric (there, the 'Mahalanobis norm') to provably reduce the regret bound during learning. Related adaptive methods like Adadelta and RMSprop can be seen also be seen as employing related diagonal approximations of the empirical Fisher that differ in their approximation (Zeiler (2012)). Other natural gradient methods of the past decade include TONGA (Roux et al. (2008)), the 'natural conjugate gradient' of (Pascanu & Bengio (2013)), and the recent K-FAC algorithm (Martens & Grosse (2015)). K-FAC is likely the most accurate scalable approximation to date, reported to converge in 14x fewer iterations than SGD with momentum (and in less real time). Martens and others note that in certain cases
1

Under review as a conference paper at ICLR 2018

the Fisher is equivalent to the Generalized Gauss-Newton matrix (a positive semi-definite approximation to the Hessian), and is thus conceptually related to 2nd order methods (Martens (2014); Pascanu & Bengio (2013); Amari et al. (2000)). The natural gradient thus lies at the core of a cluster of methods that spans both advanced techniques and successful heuristics.
In this work, we examine the natural gradient from its starting intuition as a bond between the distribution of a network's output and its parameters. We point out that it can be viewed as the result of a regularizing functional change. Specifically, that the natural gradient is the optimal update when one penalizes the incremental change in the output distribution as measured by the KL divergence. The KL divergence is, however, not the only measure of similarity between two distributions. We propose that it would be more computationally feasible to regularize a term related to the Earth Mover's Distance, or Wasserstein distance, between the outputs of successive updates. We find that Wasserstein regularization improves generalization over stochastic gradient descent at little extra cost.

1.1 THE NATURAL GRADIENT AS REGULATOR OF FUNCTIONAL CHANGE

A simple way to characterize a function is to examine the function's outputs for a set of inputs. If we define a set of network parameters , we will write the distribution of the network's outputs (given

our input distribution X) as P. Let us plan to regularize the change in P throughout optimization. For this, we need a measure of similarity between two distributions. One such measure is the

Kullbeck-Leibler (KL) divergence. The KL divergence from Pt to Pt+1 is defined as

DKL(Pt+1 Pt ) =

log

Pt (x) Pt+1 (x)

P (x)dx

(1)

P is the density of P. To ensure the output distribution changes little throughout optimization, we

define a new cost function

C = C0 + DKL(Pt+1 Pt )

(2)

where C0 is the original cost function and  is a hyperparameter that controls the importance of this

regularization term. Optimization would be performed with respect to the proposed update t+1.

In information theory, the KL divergence from Pt to Pt+1 is called the relative entropy of Pt+1 with respect to Pt . We can thus equivalently speak of regularizing the change in the entropy of the output distribution throughout learning. If we assume that the initial parameterization leads to an
output distribution with maximal entropy (as would be the case if, reasonably, the inputs are treated
nearly equally by the initial parameterization), and the average update decreases output entropy, then
regularizing the relative entropy between optimization steps will lead to a solution with maximal
entropy.

Evaluating the KL divergence directly is problematic because it is infeasible to define the output

density P everywhere. One can obtain a more calculable form by expanding DKL(Pt+1 Pt ) around t to second order with respect to . The Hessian of the KL divergence is the Fisher information metric F . If J is the the Jacobian J = (C0), the Fisher metric is defined as EP [JJT ], i.e. the expectation (over the output distribution) of the covariance matrix of the gradients. With

  (t+1 - t), we can rewrite our regularized cost function as

C



C0

+

1 T 2

F 

(3)

To optimize C via gradient descent we first replace C0 with its first order approximation.

C  J T  +  T F  2

(4)

At each evaluation, J is evaluated before any step is made, and we seek the value of  that minimizes Equation 2. By setting the derivative to zero, we can see that this value is

 = 1 F -1J 

(5)

When  = 1 this update is exactly equal to the natural gradient. Thus, the natural gradient emerges as the optimal update when one explicitly regularizes the change in the output distribution during learning.

2

Under review as a conference paper at ICLR 2018

2 PROPOSED ALGORITHMS

2.1 NATURAL GRADIENT BY GRADIENT DESCENT

Since many approximations of the natural gradient already exist, we will not empirically evaluate the method here. In order to lay the conceptual groundwork for Wasserstein regularization, we propose a natural gradient algorithm of a similar style. We suggest this natural gradient method that may prove faster than current methods if implemented efficiently.

Previous work on the natural gradient has aimed to approximate F -1 as best and as cheaply as

possible.

This is equivalent to minimizing Equation 2 (i.e.

J 

+

 2

T

F

)

with

a

single

iteration of a second-order optimizer. For very large neural networks, however, it is much cheaper

to calculate matrix-vector products than to approximately invert a large matrix. It is possible that

the natural gradient may be more accessible via gradient descent, which would be performed during

each update step as an inner loop.

We describe this idea at high level in Algorithm 1. After an update step is proposed by a standard optimizer, the algorithm iteratively corrects this update step towards the natural gradient. To start with a good initial proposed update, it is better to use a fast diagonal approximation of the natural gradient (such as Adagrad or RMSprop) as the main optimizer. Each additional correction requires just one matrix-vector product after the gradients are calculated. Depending on the quality of the proposed update, the number of iterations required is likely to be small, and even a small number of iterations will improve the update.

Algorithm 1 Natural gradient by gradient descent. This algorithm can be paired with any optimizer to increase its similarity to the natural gradient.

Require: n Require:  1: procedure 2:   0 3: while t not converged do 4: 0  RMSprop(t) 5: for i < n do 6: i+1 = i - (J + F i)
7:    + 

Number of corrective steps. May be 1. Learning rate for corrective step

Initialize parameters

Use any optimizer to get proposed update

Corrective loop

Step

towards

1 

F

-1J

8: return t

Since the Fisher matrix F can be calculated from the covariance of gradients, it never needs to be fully stored. Instead, for a array of gradients G of size (# parameters, # examples), we can write

F  = (GGT ) = G(GT )

(6)

The choice of G is an important one. It cannot be a vector of aggregated gradients (i.e. J), as that would destroy covariance structure and would result in a rank-1 Fisher matrix. If we choose G to be the array of per-example gradients on the minibatch, F is known as the 'empirical Fisher'. As explained in Martens (2014) and in Pascanu & Bengio (2013), the proper method is to calculate G from the predictive (output) distribution of the network, P(y|x). This can be done as in Martens & Grosse (2015) by sampling randomly from the output distribution and re-running backpropagation on these fictitious targets, using (by necessity) the activations from the minibatch. Alternatively, as done in Pascanu & Bengio (2013), one may also use unlabeled or validation data to calculate G on each batch. Failure to use a different batch will result in a type of overfitting, as gradients become, in a sense, judges of their own trustworthiness on the test set.

2.2 WASSERSTEIN REGULARIZATION
We arrived at the natural gradient because we wished to decrease the change in the output distribution throughout learning. However, the KL divergence is not the only measure of similarity between distributions. One simple regularization term that measures the change in our function f(x) is the

3

Under review as a conference paper at ICLR 2018

norm of the differences of the outputs over N examples:

N

C = C0 + N

ft (xi) - ft+(xi)

i=0

(7)

We implement an l2 norm. This cost function imposes a penalty upon the difference between the output of the current network at time t and the proposed network at t + 1. In analogy with WGANs,
we call this modification Wasserstein regularization.

Our implementation of the Wasserstein regularizer, displayed as Algorithm 2, takes some lessons

from

the

natural

gradient.

If

the

scale

of

the

effect

is

to

be

the

same,



might

be

quite

large

(

1 2

).

Just

as the natural gradient is the optimal solution to Equation 4 at each step, here we seek the optimal

solution to Equation 7. Thus we seek to converge to a  at each update step, where



= argmin


J T  +  N N
i=0

ft (xi) - ft+(xi)

(8)

Minimization can be performed in an inner loop, as in Algorithm 1. As in the natural gradient by gradient descent, we first propose some 0 = - C0 and then iteratively correct this proposal by gradient descent towards  . If the norm is an l2 norm we can be guaranteed convergence. While a naive regularizer might simply add the derivative of the Wasserstein term after 0 has been proposed (essentially alternating between minimizing each term), we expect a faster overall convergence for this joint minimization over J and the Wasserstein term since all updates will reduce the original cost. Finally, just as the empirical Fisher is improper for the natural gradient, we will evaluate the Wasserstein distance on validation data (or, at least, a difference batch than used for the initial proposed update). It would also be possible to use unlabeled data. Using a different batch of data for the inner loop allows Wasserstein regularization to ensure that the update does not overfit to the training set at the expense of its previous behavior.

Algorithm 2 Wasserstein regularization. Implements Equation 8.

Require: n

Number of corrective steps. May be 1.

Require: 

Learning rate for corrective step

1: procedure

2:   0

Initialize parameters

3: while t not converged do 4: draw X  Px 5: J  C0(X) 6: 0  SGD.apply(J)

Draw training batch Calculate gradients Apply any optimizer to obtain proposed update

7: for i < n do

Corrective loop

8: drawXV  Px

Draw validation batch

9:

gw

=

J

+

 N

t +i

N i=0

ft (xi) - ft+i (xi)

For xi  XV

10: i+1  i - (gw)

11:    + 

12: return t

Wasserstein regularization is computationally cheaper than the natural gradient. We are not required to approximately invert any large matrix F , nor are we required to calculate the per-example gradients that comprise G. To compute G efficiently it is required that a deep learning framework implement forward-mode differentiation, which is currently not supported in popular frameworks. When the validation batch XV is drawn anew for each corrective iteration (step 8 in Algorithm 2), Wasserstein regularization requires an additional two forward passes and one backwards pass for each correction i < n, for a total of 2 + 3n passes each outer step. This can be reduced by 1 pass for i  1 if XV is drawn anew just for i = 0.
A similar measure of distance between distributions is used in the training of Wasserstein Generative Adversarial Networks (WGAN) (Arjovsky et al. (2017)), from which our method gets its name. As a generative network, the WGAN is trained to produce a distribution of outputs that is as similar as possible to the training distribution. While the original GAN sought to minimize the KL divergence between these two distributions (Radford et al. (2015)), it was found by Arjovsky et al. that a

4

Under review as a conference paper at ICLR 2018

measure based on the norm of the difference outputs is more tractable and leads to improved training. Here we wish to compare f (xi) on a per-example basis, and are not interested in the shape of the output distribution per se (i.e. without regard to inputs), as in the GAN application. Still, it is worth reviewing the theory behind the WGAN, as insights will transfer.

The WGAN minimizes the 1st Wasserstein distance between two distributions, also known as the Earth Mover's Distance. This distance is defined as:

DW (Pa, Pb) = inf E(x,y)~ [ x - y ]
 (Pa ,Pb )

(9)

Intuitively, the Wasserstein distance measures the cost of the best transport plan of probability distri-

bution "Earth" that would shift one distribution onto the other, where cost is volume times distance.

Arjovsky et al. (2017) demonstrates two crucial features of the Wasserstein distance. First, the au-

thors show it is a weak distance, in the sense that more convergent series of distributions exist under that metric, which has the favorable implication that the mapping   P is more likely to be continuous. This is useful for defining continuously differentiable loss functions. Second, the authors

apply the Kantorovich-Rubenstein theorem to show that if f is 1-Lipschitz, then the Wasserstein

distance between the output distributions parameterized by fa and fb is equivalent to

1N

1N

DW (Pa, Pb) = N

fa (xi) - N

fb (xi)

i=0 i=0

(10)

Here, the sums need not be over the same set of examples. To ensure that f is 1-Lipschitz in practice,

the weights  can be clipped (Arjovsky et al. (2017)), though it was later found more effective to

penalize the norm of the gradient towards 1 (Gulrajani et al. (2017)). This result is the cost term

of WGANs. While very similar to our term, we note that we evaluate the difference over identical

examples xi, which is not the case for GANs.

If it were important to ensure that f be 1-Lipschitz in our application, we could follow Gulrajani

et al. (2017) and implement a 2-sided l2 norm upon the gradients. The cost would then be

C = C0 + DW (Pt , Pt+1 ) + (C0 - 1)T (C0 - 1)

(11)

We found that the addition of this term does not empirically improve our results. This is likely be-

cause we are able to evaluate the differences pairwise over identical examples rather than comparing

averages. This guarantee of locality likely removes the need for small function gradients.

3 RESULTS
We demonstrate our method on training canonical networks at the task of MNIST digit classification. While this is a simple benchmark by modern standards, our intention is not to break new records but to show that Wasserstein regularization is grounded and effective.
We focus on the clean example of training a dense multilayer perceptron without any additional tricks. We employ an 850-90-50-10 architecture, and do not use dropout or batch normalization. This ensures that the task is sufficiently difficult. We plan to test the interaction of this method with such strategies in the near future. As can be seen in Figure 1, Wasserstein regularization notably improves performance on the test set.
We found that  values of in the range [0, 1] produced the best results. It was necessary to tune the learning rate on the correction step, but we found that learning rates around one order of magnitude less than the learning rate applied on the outer loop tended to work best. Wasserstein regularization allows a higher SGD learning rate than would otherwise converge. To produce Figure 1, we used an SGD learning rate of 0.5, an inner learning rate of 0.05, and  = 0.3. The batch size for the training batch X was 128, and the batch size for the "validation" batch was 256. While the examples in the validation batch were different than the training batch, they were also drawn from the train set. SGD optimization followed standard procedures, with the learning rate tuned and set to 0.1.
While Wasserstein regularization converges more quickly than SGD, it requires a larger number of passes through the computational graph. In Figure 2, we plot the same results as Figure 1 but choose the total number of passes as the x-axis. It can be seen that when the number of innerloop corrections n = 1, Wasserstein regularization converges in similar compute time as SGD (but generalizes better).

5

Under review as a conference paper at ICLR 2018
Figure 1: Test and train accuracy on MNIST digit classification for a dense multilayer perceptron without dropout or normalization. Traces and envelopes represent the mean ± standard deviation of the traces of 10 runs. Wasserstein regularization converges faster than SGD and generalizes better both for n = 1 and n = 10 corrections to the gradient. An interesting effect of Wasserstein regularization is that the variance of the test error across runs is much lower.
Figure 2: Test and train accuracy on MNIST digit classification for a dense multilayer perceptron, plotted against the total passes performed (where total passes = inference passes + backprop passes). The x axis has been truncated at the run time of SGD. Traces and envelopes represent the mean ± standard deviation of the traces of 10 runs.
4 DISCUSSION
It is slightly counterintuitive that we benefit from actively limiting the change in the output distribution. The point of learning, after all, is to change a model we are given away from its factory settings. A straightforward explanation is that our algorithm penalizes directions that are very sensitive controls of the outputs, similar to the natural gradient. Alternatively, since we evaluate the change in the output distribution on different data than the gradient, Wasserstein regularization asks
6

Under review as a conference paper at ICLR 2018
the model to train on the current examples only in ways that will not affect what has already been learned for other examples.
We also believe that a deeper justification for the success of Wasserstein regularization lies in a different mystery, namely, the ability for gradient descent to produce networks that generalize well in the first place. As many have done before us, we note that successful networks often have more parameters than the number of unique training examples. This in principle enables them to trivially memorize the training data without learning interesting structure. Indeed, recent work has showed that networks can correctly classify images with shuffled labels, even when the images are themselves random pixels (Zhang et al. (2016)). Large networks are thus perfectly capable of memorizing training examples, yet generally choose not to.
A distinguishing feature of Stochastic Gradient Descent (SGD) is that large jumps through parameter space are generally prohibited. It is natural to suggest that this feature encourages function simplicity. A random initialization is a simple function, in that it likely a function that does nothing and treats all example classes the same. Solutions closer to the initialization thus likely parameterize simpler functions. Furthermore, a network is much more likely to overfit if it is able to sample a large volume of parameter space. Our hypothesis is that SGD encourages generalization by limiting both the path length of parameter space explored and the distance to the initial parameters. As evidence for this notion, consider that Early Stopping, the practice of halting training before training error is minimized, often improves generalization for SGD.
It is not distance in parameter space that we wish to regularize for its own sake, but rather distance in the space of functions the network parameterizes. SGD works for this purpose for neural networks because there is is likely to be a reasonably smooth mapping between these two spaces. The natural gradient and Wasserstein regularization, however, work by directly regularizing changes in function space rather than relying on parameter space as a heuristic. These methods discourage excessively exploration in function space, and promote finding a solution that is closer (in output distribution) to the initialized function.
It is worth mentioning that there is an interesting connection between the natural gradient and techniques that normalize and whiten gradients. The term F -1J, after all, simply ensures that steps are made in a parameter space that is whitened by the covariance of the gradients. Techniques that aim to normalize gradients have the effect that SGD becomes more similar to the natural gradient. Forcing activations to have zero-mean activations and zero slope, for example, ensures that the Fisher is diagonal Raiko et al. (2012). Other activation whitening methods are also known to speed convergence. It appears that many approaches with this aim have been forwarded in the literature Simard et al. (1998), Schraudolph & Sejnowski (1996), Crammer et al. (2009), Wang et al. (2013), Le Cun et al. (1991), Schraudolph (1998). We suspect a similar effect is at play for Batch Normalization, as well Ioffe & Szegedy (2015). By normalizing and whitening the gradients, or by proxy, the activations, these various methods ensure that parameter space is a better proxy for function space, thus allowing SGD to better implicitly regularize the function parameterized by the network.
We find it interesting to ask if there is support in neuroscience for learning rules that diminish the size of changes when that change would have a large effect on other tasks. It is unlikely that the nervous system performs precisely the natural gradient or Wasserstein regularization, but there is some evidence that some analog is in play. One otherwise perplexing finding is that behavioral learning rates in motor tasks are dependent on the direction of an error but independent of the magnitude of that error Fine & Thoroughman (2006). This result is not expected by most models of gradient descent, but would be expected if the size of the change in the output distribution (i.e. behavior) were regulated to be constant. Regularization upon behavior change (rather than synaptic change) would predict that neurons that are central to many actions, like neurons in motor pools of the spinal cord, would learn very slowly after early development, despite the fact that their gradient to the error on any one task (if indeed it is calculated) is likely to be quite large. Given the human general resistance to overfitting during learning, and the great variety of roles of neurons, it is likely that some type of regularization of behavioral and perceptual learning is at play.
ACKNOWLEDGMENTS
Use unnumbered third level headings for the acknowledgments. All acknowledgments, including those to funding agencies, go at the end of the paper.
7

Under review as a conference paper at ICLR 2018
REFERENCES
Shun-ichi Amari. Neural learning in structured parameter spaces-natural riemannian gradient. In Advances in neural information processing systems, pp. 127­133, 1997.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251­ 276, 1998.
Shun-ichi Amari, Andrzej Cichocki, and Howard Hua Yang. A new learning algorithm for blind signal separation. In Advances in neural information processing systems, pp. 757­763, 1996.
Shun-Ichi Amari, Hyeyoung Park, and Kenji Fukumizu. Adaptive method of realizing natural gradient learning for multilayer perceptrons. Neural Computation, 12(6):1399­1409, 2000.
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Koby Crammer, Alex Kulesza, and Mark Dredze. Adaptive regularization of weight vectors. In Advances in neural information processing systems, pp. 414­422, 2009.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Michael S Fine and Kurt A Thoroughman. Motor adaptation to single force pulses: sensitive to direction but insensitive to within-movement pulse placement and magnitude. Journal of neurophysiology, 96(2):710­720, 2006.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to neural-network learning. Physical Review Letters, 66(18):2396, 1991.
James Martens. New perspectives on the natural gradient method. arXiv preprint arXiv:1412.1193, 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International Conference on Machine Learning, pp. 2408­2417, 2015.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint arXiv:1301.3584, 2013.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tapani Raiko, Harri Valpola, and Yann LeCun. Deep learning made easier by linear transformations in perceptrons. In Artificial Intelligence and Statistics, pp. 924­932, 2012.
Nicolas L Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradient algorithm. In Advances in neural information processing systems, pp. 849­856, 2008.
Nicol Schraudolph. Accelerated gradient descent by factor-centering decomposition. 1998.
Nicol N Schraudolph and Terrence J Sejnowski. Tempering backpropagation networks: Not all weights are created equal. In Advances in neural information processing systems, pp. 563­569, 1996.
Patrice Simard, Yann LeCun, John Denker, and Bernard Victorri. Transformation invariance in pattern recognitiontangent distance and tangent propagation. Neural networks: tricks of the trade, pp. 549­550, 1998.
8

Under review as a conference paper at ICLR 2018 Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
gradient optimization. In Advances in Neural Information Processing Systems, pp. 181­189, 2013. Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
9

