Under review as a conference paper at ICLR 2018
ASSESSING GENERALIZATION CAPABILITY OF CONVOLUTIONAL NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, we assess the capability of Convolutional Neural Networks (CNNs) in generalizing to images with different distribution than the training data, specifically images with similar shapes but different colors. We show that, although CNNs do not intrinsically classify objects based on their shapes, they can learn to do so when trained with enough number of images with the same shape and different colors. In experiments, we use original and negative images of training data as such images. Through systematic experiments, we investigate the role of training data, model architecture, initialization and regularization techniques on model generalization to negative images. We conclude that although CNNs can memorize any training data, they only learn and generalize the structures.
1 INTRODUCTION
Recently, it has been shown that state-of-the-art Convolutional Neural Networks (CNNs) trained with stochastic gradient descent (SGD) have enough capacity to "memorize" the training data, even when training images or labels are randomized Zhang et al. (2016). Memorization is often associated with overfitting the training data and, hence, large generalization error, defined as the difference between training error and test error. Yet, CNNs are known to be able to generalize well to the data that are similarly distributed as the training data.
For an image classifier, the space of possible inputs is much larger than the size of the training data. Hence, models with identical performance on samples that are similarly distributed as training data can perform qualitatively differently on unseen distributions. Of course, most of possible inputs are random images that the model is not expected to recognize. Therefore, it remains to be determined what distributions we expect the model to generalize to and how different factors, such as network architecture, initialization method and regularization techniques, affect the generalization capability.
The distributions that we are interested in can be determined according to the human cognitive system. It has been shown that humans display "shape bias" when assigning a name to new items Landau et al. (1988), i.e., they weight shape more heavily than they do other dimensions of perceptual similarity, such as size or texture. CNN models are also designed to take into account the spatial structure of image data. In fact, Ritter et al. (2017) have shown that CNNs trained on ImageNet display shape bias as well, although there is high variance in the bias depending on initialization. Arpit et al. (2017) also argued that deep networks learn simple patterns in training data first, before memorizing.
To probe the CNNs' behavior on shapes and patterns, we train and test them with specifically designed images. Given an image, one simple way to generate another image with the same shape but different colors is image complementing. The transformed image, called negative image, is an image with reversed brightness. Unlike typical transformations used for training or testing machine learning models, image complementing causes a large pixel-wise perturbation to the original images. It, however, maintains the shape and structure (e.g., edges) and semantics of images, as negative images are often easily recognizable by humans.
In this paper, we study the ability of CNNs in generalizing the shapes, by training and testing them on negative images. Through extensive systematic experiments, we examine the effect of training data, model architecture, initialization and regularization techniques on model generalization to negative images. Our contributions are summarized in the following.
1

Under review as a conference paper at ICLR 2018
· We show that it is possible to design different CNN models that achieve similar accuracy on original images, but perform significantly differently on negative images, suggesting that CNN models do not intrinsically display shape bias.
· We then show that, when negative images of some classes are included in training data, CNNs correctly recognize negative images of other classes as well. This is however true, only if the number of negative images in training data is large enough and also the model is trained with batch normalization. Hence, when properly tuned, CNNs can indeed learn to be invariant to color.
· We also investigate the effect of augmenting training data with original and negative images of an unrelated dataset. We show that such data augmentation with an unrelated dataset significantly improves model generalizability to negative images.
· We examine the role of initialization and demonstrate that a different initialization can significantly affect the model generalization performance. Specifically, a model that is initialized by training with original and negative images of a dataset shows shape bias on another dataset as well. Moreover, the shape bias property does not fade away after finetuning only with original images of the second dataset.
· We finally show that CNNs do not learn the shape bias property from a dataset that lacks any structure, e.g., a dataset with random images. Moreover, a model with shape bias cannot generalize this property to random images, i.e., it does not perform similarly on random images and their negatives. In conclusion, although CNN models can fit any training data, they only learn and generalize the structures.
2 RELATED WORKS
Deep convolutional neural networks have achieved human level performance on a variety of computer vision tasks. However, their generalization performance is not well understood. Recently, there has been an interest in exploring various aspects of the generalization of CNNs by both theoretical and empirical analysis Zhang et al. (2016); Arpit et al. (2017); Neyshabur et al. (2017); Lin et al. (2017); Xu & Raginsky (2017); Yan & Zhou (2017). It has been shown that state-of-the-art CNNs trained with SGD are capable of memorizing the training dataset, contradicting their low generalization error Zhang et al. (2016). Arpit et al. (2017), however, argued that learning is data dependent and, even in the presence of noise examples, networks trained with SGD learn structures before memorizing. This follows the works suggesting that SGD generally results in simpler models Sjo¨berg & Ljung (1995); Yao et al. (2007).
Deep neural networks are known to be capable of approximating any measurable function given sufficient capacity Cybenko (1989); Hornik et al. (1989). These works determine the set of hypotheses a model can express, but do not specify what hypothesis can be reached by training a network on a dataset using a particular method. In fact, it is known that when training the same architecture with the same dataset, different optimization methods may result in different generalization performances, even if they all lead to zero training error Neyshabur et al. (2015); Keskar et al. (2016). In this paper, we further investigate generalizability to images with different distribution than the training data, and evaluate the effect of various components of training on the generalization error.
From the practical perspective, it is important to determine how deep neural networks perform on related but not identical distributions as of the training data. This problem has been studied in literature of transfer learning Pan & Yang (2010); Yosinski et al. (2014) and domain adaptation BenDavid et al. (2010); Saenko et al. (2010); Gopalan et al. (2011); Donahue et al. (2014). The goal is to use models and features learned on one dataset/domain for another dataset/domain with minimal fine-tuning Pan & Yang (2010). In this paper, we examine transferability of features from original to negative images. To the best of our knowledge, generalizing to negative images is not studied in transfer learning or domain adaptation literature.
Another approach for improving generalization is data augmentation via domain-specific transformations Krizhevsky et al. (2012); Chatfield et al. (2014). Image transformations are also used to evaluate the performance of trained models Paulin et al. (2014). Most conventional data transformations only introduce slight variations to the image, and hence are limited in evaluating the generalization capabilities of trained models. In contrast, we use image complementing transforma-
2

Under review as a conference paper at ICLR 2018
Regular Images
Negative Images
Figure 1: Examples of regular images and their corresponding negative images of datasets MNIST, notMNIST and CIFAR10.
tion, which yields images that are semantically similar to original images, but very different in pixel space. Moreover, we show that augmenting training data even with an unrelated dataset improves the model generalization.
3 SETUP FOR SIMULATIONS
Experiments are performed on image datasets MNIST LeCun et al. (1998), CIFAR10 Krizhevsky (2009) and notMNIST Bulatov (2011). MNIST is a dataset of handwritten digits. CIFAR10 dataset consists of natural color images in 10 classes. notMNIST is a dataset similar to MNIST with 10 classes of letters A-J taken from different fonts. notMNIST dataset contains more than 500, 000 images, from which we randomly select 60, 000 images for training and validation.
We also train and test the models with negative images. A negative image is defined as the image complement of the original image, in which the light pixels appear dark and vice versa. Let X be an image and Xi,n  [0, 1] be the i-th pixel in the n-th color channel. The negative image is defined as X, where Xi,n = 1 - Xi,n. Image complementing is a simple transformation that preserves the shape and structure (e.g., edges) of the image. Moreover, for MNIST, CIFAR10 and notMNIST datasets, since the classes are very distinct, image complementing is unlikely to change the groundtruth label. 1 Figure 1 shows examples of original and negative images of MNIST, notMNIST and CIFAR10 datasets. In the rest of the paper, we refer to original images as regular images.
We consider a small version of VGG-16 model Simonyan & Zisserman (2014), which we call sVGG, with 6 convolutional layers followed by two fully connected layers. Similar to VGG-16, we use 3 × 3 convolution kernels and 2 × 2 max-pooling. 2 For MNIST, we also consider 1- and 2layer multi-layer perceptrons (MLPs) with ReLU activation function and 1000 hidden nodes per layer. The models are trained using SGD and, unless otherwise stated, with batch normalization in all layers. In all experiments, 20% of training images are held out and used for validation. We stop training, when validation accuracy is the highest and training accuracy is 100%. In cases that training accuracy does not reach 100%, we train the models for 500 epochs. When validation accuracy is not meaningful (e.g., training with random images), we report the test accuracy of the last epoch. Results are averaged over five experiments.
4 TESTING ON NEGATIVE IMAGES
In this section, we examine the performance of CNNs on negative images by designing three experiments performed on MNIST and CIFAR10 datasets. The experiments on MNIST are illustrated in Figure 2 and the results on CIFAR10 are provided in Table 1.
In the first experiment, we train the sVGG model only with regular images. As can be seen in Figure 2 and Table 1, the accuracy on negative test images is significantly lower then the accuracy on regular images. We repeated the experiment without using batch normalization in training, and obtained 12.2% and 21.4% accuracy respectively on MNIST and CIFAR10 negative test images, which signifies the importance of batch normalization in generalization. We will explore the role of
1In datasets with very high number of closely related classes, such as ImageNet or CIFAR100, image complementing may change the ground-truth label of the image.
2The sVGG structure is as follows: (conv 3 × 3 × 16, ReLU, conv 3 × 3 × 16, ReLU, max pool 2 × 2, conv 3 × 3 × 32, ReLU, conv 3 × 3 × 32, ReLU, max pool 2 × 2, conv 3 × 3 × 48, ReLU, conv 3 × 3 × 48, ReLU, max pool 2 × 2, FC-128, ReLU, FC-128, ReLU, FC-10, softmax).
3

Under review as a conference paper at ICLR 2018

Training images Labels: 0 1 2 3 4 5 6 7 8 9
Experiment 1:

Test images 01 234 5 6 78 9

Accuracy
99.5% 28.3%

Experiment 2:

99.5% 99.5%

Experiment 3:

99.5% 0%
99.5%

Figure 2: Illustration of the three experiments on MNIST. The figure shows one image and the corresponding label as a representative of each class. For example, in experiment 3, regular training images of digit 0 and negative training images of digit 9 are mapped to class zero, and the accuracy on negative test images with correct labels is 0% and accuracy on negative test images with modified labels is 99.5%. In all cases, accuracy on training data is 100%.

batch normalization more in the next section. We also performed the experiment with MNIST and using a softmax classifier and 1- and 2-layer MLP models and achieved 0% and around 8% accuracy on negative images, respectively for softmax classifiers and MLPs. Hence, regarding generalization to images with similar shapes and different colors, the CNN model (with batch normalization) indeed performs much better than MLPs.
In the second experiment, we train the models with both regular and negative images. In this case, the accuracy of CNN model on negative images is similar to the accuracy on regular images. We again did the experiment with MNIST and using a softmax classifier and the MLP models. For the softmax classifier, the accuracy even on training data is about 15%, since the data is not linearly separable. The two MLP models, however, yield high accuracy on both regular and negative images.
In the third experiment, we train the models with both regular and negative images, but the labels of negative images are changed to (i + 1) mod 10, where i is the ground-truth label. In this case, the model cannot classify images solely based on the shape pattern, since training images in separate classes have the same shapes. Therefore, while the model can certainly achieve 100% training accuracy, the question is whether it can generalize to regular images and also to negative images with modified labels. The answer is yes for both MNIST and CIFAR10 datasets. Specifically, for MNIST, the accuracy on regular and negative test images (with original labels) is 99.5% and 0%, respectively, and the accuracy on negative images with modified labels is 99.5%. We did the experiment with MNIST and MLP classifiers and obtained similar results.
The results on CIFAR10 is similar to MNIST. Note that, unlike MNIST, CIFAR10 consists of natural images. Therefore, it is interesting that the CNN model generalizes to both regular test images and negative test images with modified labels. For instance, negative images of "automobile" class can be classified as "bird," yet the model would achieve high accuracy on both classes of "automobile" and "bird." Specifically, compared with the case where the model was only trained on regular images, the accuracy on regular test images decreases only about 4%, with most of newly misclassified images are labeled as (i + 1) mod 10. Similar result holds for negative test images, i.e., only about 4% of negative test images are classified as their true label. 3
In this section, we designed three CNN models that perform similarly on regular test images, but very differently on negative images. The results show that CNN models do not intrinsically display shape bias. Specifically, in the second and third experiments, we guided the models to respectively weight shape and color more, and yet they can achieve similar test accuracy on regular test images. The results also suggest that the accuracy on images that are distributed similarly as training data
3The possibility for the model to yield high accuracy both on regular test images and on negative test images with modified labels can be attributed to the dataset bias discussed in Torralba & Efros (2011).
4

Under review as a conference paper at ICLR 2018

Table 1: Accuracy of sVGG model on regular and negative images of CIFAR10 test data. In experiment 1, the model is only trained with regular images. In experiment 2, the model is trained with both regular and negative images. In experiment 3, the model is also trained with both regular and negative images, but the labels of negative images are changed to (i + 1) mod 10, where i is the ground-truth label. In all cases, accuracy on training data is 100%.

Experiment 1
Accuracy on (regular , negative)
79.7% , 38.7%

Experiment 2
Accuracy on (regular , negative)
78.3% , 78.5%

Experiment 3
Accuracy on (regular , negative , negative with modified labels)
75.4% , 4.2%, 75.7%

is not representative of the behavior of machine learning models in the wild. That is, models with identical performance on regular images can behave qualitatively differently on a distinct yet related distribution. Investigating the security implications of this observation is left for future work.

5 TRAINING WITH NEGATIVE IMAGES

In this section, we investigate whether CNNs can learn to be "invariant to color." That is, if negative images of some classes are included in training data, does the CNN model correctly recognize negative images of other classes? To answer this question, for both MNIST and CIFAR10 datasets, we train the sVGG model with all regular images and negative images of 9 classes of training data. We then test the model on negative training images of the excluded class. We do the experiment on all 10 classes and report the average accuracy. Figure 3 illustrates the experiment.

Training images Labels: 0 1 2 3 4 5 6 7 8 9

Test images 9

Accuracy with BN w/o BN

MNIST

97.7%

0%

CIFAR10

75.8% 15.6%

Figure 3: An illustration of partially training the model with negative images. BN is batch normalization. In this experiment, we train the CNN model with all regular images and negative images of 9 classes, and test it on the negative images of the excluded class. The experiment is conducted on all classes and the average accuracy is reported. In all cases, accuracy on training data is 100%.
Recall from Figure 2 that, for MNIST dataset, when the model is only trained with regular images, the accuracy on negative images is 28.3%. By including negative images of 9 classes into the training data, the accuracy on negative images of the excluded class jumps to 97.7%. We repeated the experiment with MNIST and with 1- and 2-layer MLPs. For MLP models, when including negative images of 9 classes in training data, the accuracy on negative images of the excluded class is 0%. Therefore, unlike MLPs which classify inputs based on raw pixel intensities, CNNs are able to classify objects based on their shapes.
To gain an insight into intermediate feature layers, we visualized the outputs of convolutional layers of the sVGG model trained on MNIST. Two cases are considered: 1) training only with regular images, and 2) training with all regular images and also negative images of classes 0 to 8. The trained models are then tested with a regular and a negative image of digit 9. Figure 4 shows outputs of second convolutional layer with the most number of active neurons. As can be seen, the outputs of the first model are different for regular and negative images. However, for the second model, the first two convolutional layers already provided the invariance to color.
The results on CIFAR10 is similar to MNIST. The accuracy of sVGG on negative images of the excluded class jumps from 38.7% to 75.8%, when negative images of 9 classes are included in training data. Essentially, by including some of negative images in training data, the model learns to

5

Under review as a conference paper at ICLR 2018

(a) CNN model only trained with regular images. (b) CNN model trained with all regular images and also negative images of classes 0 to 8.
Figure 4: Visualizations of outputs of second convolutional layer. Out of 16 outputs, five of them with the most number of active neurons are shown. In each figure, the top and bottom rows show the outputs on a regular image of digit 9 and its negative, respectively. The results show that, by simultaneously training with regular and negative images, the model becomes invariant to color.

weight edge patterns more then the color information. Hence, it can generalize significantly better to other negative images as well.
Role of regularization and batch normalization. We examined the effect of L2 and Dropout Srivastava et al. (2014) regularizations on model ability to generalize to negative images of the excluded class. Our experimental results show that regularizing the model slows down and stabilizes the training process, but does not affect the generalization.
We also evaluated the role of batch normalization Ioffe & Szegedy (2015) and found that it fundamentally impacts the model behavior on negative images. Essentially, in our experiment, since the model is trained and tested on different distributions, it must deal with the covariate shift problem Shimodaira (2000). Batch normalization is specifically designed to reduce the internal covariate shift of deep neural networks, by fixing the means and variances of layer inputs within each minibatch. As can be seen in Figure 3, without batch normalization, the accuracy on negative images of the excluded class is 0% and 15.6%, respectively for MNIST and CIFAR10 datasets, which shows a dramatic decrease compared to 97.7% and 75.8% accuracy with batch normalization. This behavior was consistent across all of our experiments that involved training and testing on different distributions. In the rest of the paper, we only report the experimental results with batch normalization.

6 LEARNING SHAPE BIAS ACROSS DATASETS

In this section, we extend the experiments by combining two datasets and examine whether the CNN model can transfer features learned on one dataset to another. The experiments are conducted on MNIST and notMNIST datasets, since they contain images of similar types. We examine the effect of augmenting MNIST dataset with notMNIST dataset, the model accuracy versus the number of negative images included in training data and the role of initialization on model generalizability. Finally, we eTrxaainminigniemwaghesether the features caTenstbiemtargaenssferred to or from a dataset with random images.

Labels: 0 1 2 3 4 5 6 7 8 9

0 1 2 3 4 5 6 7 8 9 Accuracy

6.1 TRAINING WITH MNIST AND NOTMNIST DATASETS

99.8%

We train the sVGG model simultaneously with regular and negative images of notMNIST and reg-

ular images of MNIST. The model has 20 number of classes, i.e., each class in the two datasets is

assigned to a unique label. The experiment is illustrated in Figure 5. When tested on MNIST nega-

tive images, the model accuracy reaches 99.8%, almost as if it was trained also on them. In essence,

the model learns the class representations using the MNIST images and improves its basic under-

standing of objects, e.g., displaying shape bias, using regular and negative images of notMNIST.

Training images
Labels: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

Test images
10 11 12 13 14 15 16 17 18 19

Accuracy

99.8%

Figure 5: An illustration of training the model with regular and negative images of notMNIST and regular images of MNIST. Images of MNIST and notMNIST datasets are mapped to different labels.

Training images

6

Labels: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

Test images
10 11 12 13 14 15 16 17 18 19

Accuracy

Under review as a conference paper at ICLR 2018

Accuracy versus different number of negative images. We also investigate whether any number of notMNIST negative images can always lead to better generalization to MNIST negative images. Figure 6 shows the accuracy on MNIST negative images versus different number of notMNIST regular and negative images included in training data. As can be seen, training with few negative images causes the accuracy to decrease. As an example, recall from Figure 2 that, when trained with regular images, the accuracy on MNIST negative images is 28.3%. By including only 10 to 100 notMNIST negative images in training data, the model accuracy on MNIST negative images drops to 0%. The accuracy, however, reaches 99% when the model is trained with 10000 notMNIST regular and negatives images. This implies that the CNN model learns to classify objects by their shapes, only when it is trained with a large enough number of regular and negative image pairs.

Accuracy

1
0.8
0.6
0.4
0.2
0 100 101 102 103 104
Number of Negative Images
Figure 6: Accuracy of sVGG model on MNIST negative images versus number of regular and negative images of notMNIST dataset included in training data. The model is trained with regular and negative images of notMNIST and regular images of MNIST.

6.2 ROLE OF DATA AUGMENTATION

We train the sVGG model again with regular and negative images of notMNIST and regular images of MNIST, but with 10 number of classes. That is, all images are mapped to labels 0 to 9, e.g., images of digit 0 of MNIST and images of letter A of notMNIST are assigned the label 0. The experiment is illustrated in Figure 7. Similar to the case of training with 20 labels, when tested on negative images of MNIST, the model accuracy reaches 99.8%.
Assigning images of the two datasets to the same labels has a practical implication. Most data augmentation techniques include transformed version of images into the training data. Our results, however, show that one can augment a target dataset with another dataset that contains similar type of images, though with unrelated labels. Moreover, in the case that the two datasets do not have common classes, each class of the external dataset can be randomly labeled. Augmenting a target dataset with unrelated data is especially helpful, when the dataset size is small and it is difficult to obtain samples from the classes of target dataset.

Training images
Labels: 0 1 2 3 4 5 6 7 8 9

Test images
01 2345 6 789

Accuracy

99.8%

Figure 7: An illustration of training the model with regular and negative images of notMNIST and regular images of MNIST. Images of MNIST and notMNIST datasets are mapped to same labels.

Training images
6.3 ROLE OF INITIALIZATION
Labels: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

Test images
10 11 12 13 14 15 16 17 18 19

Now, we examine the effect of initialization on the ability of CNNs to generalize to negative images. For this, we first train the sVGG model with regular and negative images of notMNIST dataset for 100 epochs, and then fine-tune with MNIST regular images for another 100 epochs. We consider two cases for the first phase of training: 1) training with notMNIST regular and negative images with correct labels, 2) training with notMNIST regular and negative images, with labels of negative images being modified to (i + 1) mod 10, where i is the ground-truth label.

Accuracy 99.8%

Figures 8a and 8b show the model accuracy on MNIST regular and negative images versus epoch number for the two cases. While accuracy on MNIST images in not meaningful during the first phase of training, notably for the first casTera, iintinisg simimagielsar for both MNIST regular andTensetgimataigvees images.

Labels: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

10 11 12 13 14 15 16 17 18 19

7

Accuracy 99.8%

Under review as a conference paper at ICLR 2018

Accuracy Accuracy KL Divergence

1 1 102
Using correct labels for notMNIST negative images

Using modified labels for notMNIST negative images

0.8 0.8

Accuracy on MNIST Regular Images Accuracy on MNIST Negative Images

Accuracy on MNIST Regular Images Accuracy on MNIST Negative Images

101

0.6 0.6

0.4 0.4

100

0.2 0.2 10-1

0 0 10-2

0

50 100 150 200 0

50 100 150 200

0

50 100 150 200

Epoch Number

Epoch Number

Epoch Number

(a) Accuracy versus epoch number (b) Accuracy versus epoch number (c) KL divergence between model

in the first case.

in the second case.

outputs on MNIST regular and neg-

ative images in both cases.

Figure 8: Role of initialization and fine-tuning on shape bias. The CNN model is trained with notMNIST regular and negative images for 100 epochs and then fine-tuned with MNIST regular images for another 100 epochs. Two cases are considered for the first phase of training: 1) training with notMNIST images with correct labels, 2) training with notMNIST images, with labels of negative images being modified to (i + 1) mod 10, where i is the ground-truth label. Throughout training, we test the model on MNIST regular and negative images. Only in the first case, the model performs similarly on MNIST regular and negative images, implying that it learns to be invariant to color.

This implies that, when correctly trained with regular and negative images of one dataset, the model performs similarly on regular and negative images of another dataset as well and, hence, is displaying shape bias. In the first case, after fine-tuning the model with MNIST regular images, the accuracy on negative images increases and remains on par with the accuracy on regular images, i.e., the shape bias property does not fade away when the model is fine-tuned only with regular images. In the second case, the model accuracy on MNIST negative images converges to about 30%, which is similar to the case where the model was initialized randomly.

We also use KL-divergence metric to measure the similarity of the model output probability vectors on regular and negative images. Let P (X) be the model output probability vector for an image X. The KL-divergence from P (X) to P (1 - X) is defined as follows:

DKL(P (X) P (1 - X)) =

i

P (X)i

log

P

P (X)i (1 - X

)i

.

We define D = EX [DKL(P (X) P (1 - X))], i.e., the average KL-divergence of model outputs on all pairs of regular and negative images. Figure 8c shows the value of D versus epoch number for the two cases. As can be seen, in the first case (training with correct labels) and during the first phase of training, the KL divergence remains low and on par with the KL divergence in the second phase. This further demonstrates that the CNN model learns to be invariant to color. However, in the second case (training with notMNIST negative images with wrong labels), the average KL divergence is significantly higher throughout the training. In conclusion, a different initialization can lead to a qualitatively different model, e.g., in the first case, it is as if the shape bias property is encoded into the model at initialization.

6.4 GENERALIZATION TO/FROM RANDOM IMAGES
In this subsection, we first examine whether the CNN model can learn the shape bias property from a dataset that lacks any structure, e.g., a dataset with random images. We then investigate how a model with shape bias property performs on random images and their negatives. In experiments, we generate a dataset, with the same size as MNIST, consisting of random images with uniform distribution in [0, 1]. The images are labeled randomly between 0 to 9.
For answering the first question, we train the sVGG model with random images and their negatives and with the regular images of MNIST. The model is trained with 10 labels, i.e., random images and MNIST images are mapped to the same set of classes. Our experimental results show that the model accuracy on MNIST negative images is about 30%, implying that including random images and their negatives in the training data does not improve the model generalization to MNIST negative images. In the second experiment, we train the sVGG model with notMNIST regular and negative

8

Under review as a conference paper at ICLR 2018
images and also with random images. The test accuracy on negatives of random images is 10%. In conclusion, although CNN models can fit any dataset, they only learn and generalize the structures.
7 CONCLUSION
In this paper, we examined the ability of CNNs in generalizing to images with similar shapes but different colors. In experiments, we used original and negative images of training data as such images. Through experiments, we evaluated the role of various components of training algorithms in model generalizability. We showed that CNNs can learn to classify objects based on their shapes, when trained with enough number of original and negative image pairs. As a future work, we will explore biologically-inspired models that provide shape bias by design. Such a model will potentially learn more efficiently with smaller data and generalize better to images of different domains.
REFERENCES
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International Conference on Machine Learning, pp. 233­242, 2017.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1):151­175, 2010.
Y Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available: http://yaroslavvb.blogspot.it/2011/09/notmnist-dataset.html, 2011.
Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2(4):303­314, 1989.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pp. 647­655, 2014.
Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Domain adaptation for object recognition: An unsupervised approach. In Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 999­1006. IEEE, 2011.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Barbara Landau, Linda B Smith, and Susan S Jones. The importance of shape in early lexical learning. Cognitive development, 3(3):299­321, 1988.
9

Under review as a conference paper at ICLR 2018
Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits, 1998.
Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well? Journal of Statistical Physics, 168(6):1223­1247, 2017.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2422­2430, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. arXiv preprint arXiv:1706.08947, 2017.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345­1359, 2010.
Mattis Paulin, Je´ro^me Revaud, Zaid Harchaoui, Florent Perronnin, and Cordelia Schmid. Transformation pursuit for image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3646­3653, 2014.
Samuel Ritter, David GT Barrett, Adam Santoro, and Matt M Botvinick. Cognitive psychology for deep neural networks: A shape bias case study. arXiv preprint arXiv:1706.08606, 2017.
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. Computer Vision­ECCV 2010, pp. 213­226, 2010.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227­244, 2000.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Jonas Sjo¨berg and Lennart Ljung. Overtraining, regularization and searching for a minimum, with application to neural networks. International Journal of Control, 62(6):1391­1407, 1995.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929­1958, 2014.
Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 1521­1528. IEEE, 2011.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. arXiv preprint arXiv:1705.07809, 2017.
Zhennan Yan and Xiang Sean Zhou. How intelligent are convolutional neural networks? arXiv preprint arXiv:1709.06126, 2017.
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289­315, 2007.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pp. 3320­3328, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
10

