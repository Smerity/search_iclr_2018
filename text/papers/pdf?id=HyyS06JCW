Under review as a conference paper at ICLR 2018
INTERACTIVE BOOSTING OF NEURAL NETWORKS FOR SMALL-SAMPLE IMAGE CLASSIFICATION
Anonymous authors Paper under double-blind review
ABSTRACT
Neural networks have recently shown excellent performance on numerous classification tasks. These networks often have a large number of parameters and thus require much data to train. When the number of training data points is small, however, a network with high flexibility will quickly overfit the training data, resulting in a large model variance and a poor generalization performance. To address this problem, we propose a new ensemble learning method called InterBoost for small-sample image classification. In the training phase, InterBoost first randomly generates two complementary datasets to train two base networks of the same structure, separately, and then next two complementary datasets for further training the networks are generated through interaction (or information sharing) between the two base networks trained previously. This interactive training process continues iteratively until a stop criterion is met. In the testing phase, the outputs of the two networks are combined to obtain one final score for classification. Experimental results on UIUC-Sports (UIUC) and LabelMe (LM) datasets demonstrate that the proposed ensemble method outperforms existing ones. Moreover, the confusion matrices of the two base networks trained by our method are shown to be complementary. Detailed analysis of the method is provided for an in-depth understanding of its mechanism.
1 INTRODUCTION
Image classification is an important application of machine learning and data mining. Recent years have witnessed tremendous improvement in large-scale image classification due to the advances of deep learning (Simonyan & Zisserman, 2014; Szegedy et al., 2015; Krizhevsky et al., 2012; Gu et al., 2015). Despite recent breakthroughs in applying deep networks, one persistent challenge is classification with a small number of training data points (Santoro et al., 2016). Small-sample classification is important, not only because humans learn a concept of class without millions or billions of data but also because many kinds of real-world data have a small quantity. Given a small number of training data points, a large network will inevitably encounter the overfitting problem, even when dropout (Srivastava et al., 2014) and weight decay are applied during training (Zhang et al., 2016). This is mainly because a large network represents a large function space, in which many functions can fit a given small-sample dataset, making it difficult to find the underlying true function that is able to generalize well. As a result, a neural network trained with a small number of data points usually exhibits a large variance.
Ensemble learning is one way to reduce the variance. According to bias-variance dilemma (Geman et al., 1992), there is a trade-off between the bias and variance contributions to estimation or classification errors. The variance is reduced when multiple models or ensemble members are trained with different datasets and are combined for decision making, and the effect is more pronounced if ensemble members are accurate and diverse (Granitto et al., 2005).
There exist two classic strategies of ensemble learning (Zhou et al., 2002; Schwenk & Bengio, 1998). The first one is Bagging (Zhou, 2012) and variants thereof. This strategy trains independent classifiers on bootstrap re-samples of training data and then combines classifiers based on some rules, e.g. weighted average. Bagging methods attempt to obtain diversity by bootstrap sampling, i.e. random sampling with replacement. There is no guarantee to find complementary ensemble members and new datasets constructed by bootstrap sampling will contain even fewer data points,
1

Under review as a conference paper at ICLR 2018
which can potentially make the overfitting problem even more severe. The second strategy is Boosting (Schwenk & Bengio, 2000; Moghimi et al., 2016) and its variants. This strategy starts from a classifier trained on the available data and then sequentially trains new member classifiers. Taking Adaboost (Zhou, 2012) as an example, a classifier in Adaboost is trained according to the training error rates of previous classifiers. Adaboost works well for weak base classifiers. If the base classifier is of high complexity, such as a large neural network, the first base learner will overfit the training data. Consequently, either the Adaboost procedure is stopped or the second classifier has to be trained on data with original weights, i.e. to start from the scratch again, which in no way is able to ensure the diversity of base networks.
In addition, there also exist some "implicit" ensemble methods in the area of neural networks. Dropout (Srivastava et al., 2014), DropConnect (Wan et al., 2013) and Stochastic Depth techniques (Huang et al., 2016) create an ensemble by dropping some hidden nodes, connections (weights) and layers, respectively. Snapshot Ensembling (Huang et al., 2017) is a method that is able to, by training only one time and finding multiple local minima of objective function, get many ensemble members, and then combines these members to get a final decision. Temporal ensembling, a parallel work to Snapshot Ensembling, trains on a single network, but the predictions made on different epochs correspond to an ensemble prediction of multiple sub-networks because of dropout regularization (Laine & Aila, 2017). These works have demonstrated advantages of using an ensemble technique. In these existing "implicit" ensemble methods, however, achieving diversity is left to randomness, making them ineffective for small-sample classification.
Therefore, there is a need for new ensemble learning methods able to train diverse and complementary neural networks for small-sample classification. In this paper, we propose a new ensemble method called InterBoost for training two base neural networks with the same structure. In the method, the original dataset is first re-weighted by two sets of complementary weights. Secondly, the two base neural networks are trained on the two re-weighted datasets, separately. Then we update training data weights according to prediction scores of the two base networks on training data, so there is an interaction between the two base networks during the training process. When base networks are trained interactively with the purpose of deliberately pushing each other in opposite directions, they will be complementary. This process of training network and updating weights is repeated until a stop criterion is met.
In this paper, we present the training and test procedure of the proposed ensemble method and evaluate it on the UIUC-Sports dataset (Li & Fei-Fei, 2007) and the LabelMe dataset (Russell et al., 2008) with a comparison to Bagging, Adaboost, SnapShot Ensembling and other existing methods. Experimental results show the superior performance of the proposed ensemble learning method.
2 THE PROPOSED INTERBOOST METHOD
In this section, we present the proposed method in detail, followed by discussion.
2.1 INITIALIZATION OF COMPLEMENTARY TRAINING DATASETS
We are given a training dataset {xd, yd}, d {1, 2, ..., D}, where yd is the true class label of xd. We assign a weight to the point {xd, yd}, which is used for re-weighting the loss of the data point in the loss function of neural network. It is equivalent to changing the distribution of training dataset and thus changing the optimization objective of neural network. We randomly assign a weight 0 < W1d < 1 to {xd, yd} for training the first base network, and then assign a complementary weight W2d = 1 - W1d to {xd, yd} for training the second base network.
2.2 INTERBOOST TRAINING
The core idea of the InterBoost method is to train two base neural networks interactively. This is in contrast to Boosting, where base networks are typically trained in sequence, namely the subsequent network or learner is trained on a dataset with new data weights that are updated using the error rate performance of the previous base network.
As shown in Figure 1, the procedure contains multiple iterations. It first trains a number of epochs for two base networks using two complementary datasets {xd, yd, W1(d1)} and {xd, yd, W2(d1)}, d 
2

Under review as a conference paper at ICLR 2018

Network
{xd, yd, W1(d1)} training P (yd|xd, 1(1))

{xd, yd, W1(d2)}

Network

training

{xd, xd, W1(dn)}

P (yd|xd, 1(n))

{xd, yd, W2(d1)}

P (yd|xd, 2(1))

Network

training

{xd, yd, W2(d2)}

{xd, yd, W2(dn)}

P (yd|xd, 2(n))

Network

training

Figure 1: InterBoost training procedure. n is the number of iteration. W1d and W2d are the weights of data point {xd, yd}, d {1, 2, ..., D} for two base networks. 1 and 2 are the parameters of two
base neural networks. W1(dn) + W2(dn) = 1 and 0 < W1(dn), W2(dn) < 1. P (yd|xd, i(n)), i  {1, 2} is the probability that the ith base network can classify xd correctly after nth iteration.

{1, 2, ..., D}, separately, and then iteratively update data weights based on the probabilities that the two base networks classify xd correctly, namely P (yd|xd, 1(1)) and P (yd|xd, 2(1)), where 1 and 2 are parameters of the two base networks. During the iterative process, the weights always have the constraints W1d + W2d = 1 and 0 < W1d, W2d < 1. That is, they are always kept complementary to ensure the trained networks are complementary. Training networks and updating data weights run
alternately until a stop condition is met.

To compute 1(n) and 2(n) in the nth iteration, we minimize weighted loss functions are follows.

D

L(1n) =

W1(dn)L(xd, yd, 1(n-1))

d=1

(1)

D

L(2n) =

W2(dn)L(xd, yd, 2(n-1))

d=1

(2)

where L(xd, yd, 1) and L(xd, yd, 2) are loss functions of of xd for two base networks, respectively.

To update W1d and W2d, we devise the following updating rule: If the prediction probability of a data point in one base network is higher than that in another, its weight in next iteration for training this network will be smaller than its weight for training another base network. In this way, a base network will be assigned a larger weight for a data point on which it does not perform well. Hence the interaction make it be trained on diverse datasets in sequence, which can be considered as "implicit" Adaboost. Moreover, considering the fact that the two networks are always trained based on loss functions with different data weights, this interaction makes them diverse and complementary.

Figure 2: Function w1 = p2/(p1 + p2) (left) and function w1 = ln(p1)/(ln(p1) + ln(p2)) (right), where 0 < p1, p2 < 1.
To implement the rule of updating data weights, a simple method is to use function w1 = p2/(p1 + p2), and then assign W1d = w1 and W2d = 1 - W1d. Here, for convenience, we use p1 and p2 to represent the probabilities that the point xd is classified by the two base networks correctly.
3

Under review as a conference paper at ICLR 2018

Moreover, this is problematic, as illustrated on the left side of Figure 2. For example, when both p1 and p2 are large and close to each other, w1 will be close to 0.5. In this situation, there will be no big difference between W1d and W2d. In addition, this situation will occur frequently as neural networks with high flexibility will fit the data well. As a result, the function have difficulty to make
a data point have different weights in two base networks.

Instead, we use function w1 = ln(p1)/(ln(p1) + ln(p2)), as shown on the right side of Figure 2, to update data weights. It is observed that the function is more sensitive to the small differences

between p1 and p2 when they are both large. Specifically, for {xd, yd}, d  {1, 2, ..., D}, we update its weights W1(dn) and W2(dn) by Equation (3) and (4).

W1(dn)

=

ln P (yd|xd, 1(n-1)) ln P (yd|xd, 1(n-1)) + ln P (yd|xd, 2(n-1))

(3)

W2(dn) = 1 - W1(dn)

(4)

The training procedure of InterBoost is described in Algorithm 1. First, two base networks are

trained by minimizing loss functions L1 and L2, respectively. Secondly, weights of data point on training data are recalculated using Equation (3) and (4) on the basis of the prediction results

from two base networks. We repeat the two steps until the proposed ensemble network achieves a

predefined performance on the validation dataset or the maximum iteration number is reached.

Algorithm 1 InterBoost training procedure
Input: Training set X = {(xd, yd)|d  {1, 2, ..., D}}, validation set V = {(xd, yd)|d  {1, 2, ..., V }} and maximum number of iterations N .
Steps: Initialize weights for each data point, W1(d1), W2(d1), and parameters of two base neural networks 1(0) and 2(0). n  0, val acc  0. repeat nn+1 Update 1(n) and 2(n) by minimizing (1) and (2) Update W1(dn+1), W2(dn+1), d  {1, 2, ..., D}, according to (3) and (4) Computing accuracy on V, temp acc, by (5) if temp acc  val acc then val acc  temp acc 1  1(n) 2  2(n) end if until val acc == 1 or n == N return Parameters of two base neural networks, 1 and 2

2.3 INTERBOOST PREDICTION
Through the interactive and iterative training process, the two networks are expected to be well trained over various regions of the problem space, represented by the data. In other words, they become "experts" with different knowledge. Therefore, we adopt a simple fusion strategy of linearly combining the prediction results of two networks with equal weights and choose the index class with a maximum prediction value as the final label, as detailed in (5).

O(xnew) = arg max {P (c | xnew, 1) + P (c | xnew, 2)},
c{1,2,...,C}

(5)

where P (c | xnew, i), i  {1, 2} is the cth class probability of the unseen data point xnew from the ith network, and O(xnew) is the final classification label of the point xnew. Because base networks
or "experts" have different knowledge, in the event that one base network makes a wrong decision,

it is quite possible that another network will correct it.

4

Under review as a conference paper at ICLR 2018

Complementary datasets

{xd, W1(d1)} {xd, W1(d2)}

{xd, W2(d1)} {xd, W2(d2)}

Diverse datasets

{xd, W1(dn)}

{xd, W2(dn)}

Figure 3: Generated training datasets during the InterBoost training process. The datasets on the left side are for the first base network, and the datasets on the right side are for the second base network.

2.4 DISCUSSION ON INTERBOOST

During the training process, we always keep the constraints W1d+W2d = 1 and 0 < W1d, W2d < 1, to ensure the base networks diverse and complementary. Equation (3) and (4) are designed for updating weights of data points, so that the weight updating rule is sensitive to small differences be-
tween prediction probabilities from two base networks to prevent premature training. Furthermore,
if the prediction of a data point in one network is more accurate than another network, its weight in
next round will be smaller than its weight for another network, thus making the training of individual
network on more different regions.

The training process generates many diverse training dataset pairs, as shown in Figure 3. That is, each base network will be trained on these diverse datasets in sequence, which is equivalent to that an "implicit" ensemble is applied on each base network. Therefore, the base network will get more and more accurate during training process. At the same time, the two networks are complementary to each other.

In each iteration, determination of the number of epochs for training base networks is also crucial. If the number is too large, the two base networks will fit training data too well, making it difficult to change data weights of to generate diverse datasets. If it is too small, it is difficult to obtain accurate base classifiers. In experiments, we find that a suitable epoch number in each iteration is the ones that make the classification accuracy of the base network fall in the interval of (0.9, 0.98).

Similar to Bagging and Adaboost, our method has no limitation on the type of neural networks. In

addition, it is straightforward to extend the proposed ensemble method for multiple networks, just

by keeping

H i=1

Wid

=

1,

d



{1, 2, ..., D},

in

which

H

is

the

number

of

base

networks

and

0 < Wid < 1.

3 EXPERIMENTAL RESULTS AND DISCUSSION
3.1 DATASETS AND PREPROCESSING
Considering our focus on small-sample image classification, we choose the following datasets.
· LabelMe datase (LM): A subset of scene classification dataset from (Russell et al., 2008). The dataset contains 8 classes of natural scene images: coast, forest, highway, inside city, mountain, open country, street and tall building. We randomly select 210 images for each class, so the total number of images is 1680.
· UIUC-Sports dataset (UIUC): A 8 class sports events classification dataset 1 from (Li & Fei-Fei, 2007). The dataset contains 8 classes of sports scene images. The total number of images is 1579. The numbers of images for different classes are: bocce (137), polo (182), rowing (250), sailing (190), snowboarding (190), rock climbing (194), croquet (236) and badminton (200).
1http://vision.stanford.edu/lijiali/Resources.html

5

Under review as a conference paper at ICLR 2018

Table 1: Comparison of average accuracies on two datasets: UIUC-Sports dataset (UIUC) and a subset of LabelMe dataset (LM). Methods include SupDocNADE model (SNADE), SVM with a polynomial kernel, Softmax classifier, Fully connected network (FC), Bagging, Adaboost, SnapShot Ensembling (SnapShot) and the proposed InterBoost method (Ours). Each method except for SupDocNADE and SVM runs 60 rounds, and the mean values of classification accuracies are reported in the Table.

Datasets SNADE SVM Softmax FC Bagging Adaboost SnapShot Ours

UIUC

0.753

0.520 0.672

0.810 0.801

0.822

0.825

0.840

LM

0.833

0.800 0.833

0.854 0.855

0.868

0.878

0.887

[3] We directly cite the results of SupDocNADE from Zheng et al. (2014) on UIUC and LM datasets.

For the LM dataset, we split the whole dataset into three parts: training, validation and test datasets. Both training and test datasets contain 800 data points, in which each class contains 100 data points. The validation dataset contains 8 classes, and each class contains 10 data points.
For the UIUC dataset, we also split the whole dataset into three parts as above. In this dataset, the number of data points in each class, however, is different. We first randomly choose 10 data points for every class to form validation dataset, resulting in 80 data points in total. The remaining parts of the dataset are divided equally into training and test datasets.
For small-sample classification, good discriminative features are crucial. For both LM and UIUC datasets, we first resize the images into the same size of 256 × 256 and then directly use the VGG16 (Simonyan & Zisserman, 2014) network trained on the ImageNet dataset without any additional tunning, to extract image features. Finally, we only reserve the features of last convolutional layer and simply flatten them. Hence, final feature dimensions for each image is 512 × 8 × 8 = 32768.
3.2 BASE NETWORKS
Considering the small number of data points in the two datasets, we only use fully connected network with two layers. In the first layer, the activation function is Rectified Linear Unit function (Relu). In the second layer, the activation function is Softmax. We tried different numbers of hidden units, from 1024 to 32, and found overfitting is more severe if the number of hidden units is larger. Finally, we set the number of hidden layer units as 32. We did not adopt the dropout technique, simply because we found there was no difference between with and without dropout, and we set the parameter of L2 norm about network weights as 0.01. We used minibatch gradient descent to minimize the softmax cross entropy loss. The optimization algorithm is RMSprop, the initial learning rate is 0.001, and the batch size is 32.
3.3 CLASSIFICATION ACCURACIES
In order to evaluate the classification performance of the proposed InterBoost method on LM and UIUC datasets, we use the training, validation and test datasets described above, and compare it with (1) SupDocNADE (Zheng et al., 2014) (SNADE), (2) SVM with a polynomial kernel (SVM), (3) Softmax classifier (Bishop, 2006), (4) Fully connected network (FC), (5) Bagging, (6) Adaboost and (7) SnapShot Ensembling (SnapShot) (Huang et al., 2017).
It is noted that SNADE, which is a neural autoregressive topic model, has achieved the state-of-theart performance on bag of words representation with SIFT features. For the method, we directly cite the results of Zheng et al. (2014). For SVM, we use the Libsvm package (Chang & Lin, 2011). For SnapShot, we adopt the code published by the author of it. For Softmax, FC, Bagging, Adaboost and our method, we implement them based on Keras framework (Chollet, 2015), in which FC is the base network of Bagging, Adaboost, SnapShot and the proposed method. The code of the proposed method and the codes of the other referred methods and two datasets used in the experiment can be found on an anonymous webpage2 on DropBox .
2https://goo.gl/9PG3V5
6

Under review as a conference paper at ICLR 2018
On the two datasets, we test different epochs ranging from 50 to 800 for Adaboost. It is found that the performance is similar when the epoch number of training base networks is set as 800 and 500 (the details are described in Figure 6 of Appendix). Hence, epoch numbers for FC, Softmax and the base network of bagging are also set as 800 . For SnapShot, we get a snapshot network every 800 epochs, and the number of snapshot is 2. For the base networks of our method, we choose 8 iterations, each iteration has 100 epochs, and thus the total epoch number remains the same as that of Bagging, Adaboost and Snapshot. SnapShot and SVM do not use a validation dataset, so we merge the train and validation datasets into a new training dataset while the test dataset remains unchanged. We run these methods on the two datasets 60 rounds each. The average accuracies are reported in Table 1. From Table 1, it can be seen that our method has the best performance with an accuracy of 84.0% on the UIUC dataset, and also has the best performance with an accuracy of 88.7% on the LM dataset. Our method performs significantly better than the two classical classifiers SVM and Softmax, and SNADE on the two datasets. It is worth to mention that FC is the base network of bagging, Adaboost, SnapShot and our method. The average accuracy of our method is 3.9% absolutely higher than that of FC on the UIUC dataset, and 3.3% absolutely higher than that of FC on the LM dataset. Our method also performs better than other three ensemble methods, Bagging, Adaboost and Snapshot. In summary, our method demonstrates superior performance on both LM and UIUC datasets, showing its ability to reduce the overfitting problem and work well on small-sample classification.
Figure 4: Comparison of accuracies obtained by FC, Bagging, Adaboost, SnapShot and our method via box plot on UIUC and LM datasets. The central mark is the median, and the edges of the box are the 25th and 75th percentiles. The outliers are marked individually. In two top boxplots, each method runs 60 rounds. Epoch number of FC and base network in ensemble methods is 800 epochs, in which our method has 8 iterations, and each base network in each iteration is trained 100 epochs. In two bottom boxplots, each method runs 30 rounds. Epoch number of FC and base network in other methods is 10000 epochs, in which our method has 100 iterations, and each base network in each iteration is trained 100 epochs.
7

Under review as a conference paper at ICLR 2018
3.4 VARIANCE OF CLASSIFICATION ACCURACIES
To further evaluate the robustness and stability of the proposed method, in Figure 4, we show boxplots of accuracies obtained by FC, Bagging, Adaboost, SnapShot and our method on the two datasets. In all experiments, the number of base network in all ensemble methods is still set as 2, and the number of epoch for training base networks is the same for all methods.
Here we show two sets of experiments. In the first set, the epoch number for both FC and base networks in four ensemble methods is set as 800. To keep the same epoch number of base network, our method has 8 iterations, and each base network in each iteration is trained 100 epochs. Each method runs 60 rounds (two top boxplots of Figure 4.) In the second set, the epoch number for both FC and base network in four ensemble methods is set as 10000. To keep the same epoch number of base network, our method has 100 iterations, and each base network is trained 100 epochs in each iteration. Each method in this set of experiments runs 30 rounds (two bottom boxplots of Figure 4).
From the first set of experiments, we can see that among these five methods, the box of the FC method is the largest on both datasets. The box of our method is most compact, in which the maximum value and the lower quartile of accuracies are both higher than those of other three ensemble methods. It is worth to mention that there are some low outliers in the accuracies of FC, Bagging, Adaboost and SnapShot, but there is no low outlier in the accuracies of our method.
Moreover, from the second set of experiments, we can find that when the iteration number is set as 100, the boxes of our method become more compact than those in the first set, and also more compact than other methods. That means when iteration number is creased, our method has smaller fluctuation than other methods, and thus it reduces variance significantly. More information about this set of experiments can be found in Table 4 of Appendix.
3.5 CLASSIFICATION ACCURACIES FOR DIFFERENT TRAINING DATASET SIZES
It is well-known that when the number of training data points is reduced, the overfitting problem will get more severe. To further demonstrate the performance of our method for mitigating the overfitting problem, we decrease the size of the training dataset while keeping the sizes of validation and test datasets. More details about the sizes of training, validation and test datasets can be found in Table 3 of Appendix.
For all experiments here, each method runs 60 rounds. The epoch number for both FC and base networks in four ensemble methods is set as 800. Our method has 8 iterations, and each base network in each iteration is trained 100 epochs. We then compute average accuracies and standard deviations, as well as p-values of our method and other referring methods, which are reported in Table 2. In the Table, the notation DatasetName-n denotes the configuration in which the number of training data in the named dataset is reduced by n data points for every class from the original training dataset, while the validation and test data are kept unchanged. For SnapShot, we still merge the the decreased training data and validation datasets into a new training dataset while the test dataset remains unchanged.
It can be observed from Table 2 that on all experiments, average accuracies of Bagging, Adaboost, SnapShot and our method perform better than FC. On LM-50 dataset, SnapShot has competitive performance with our method. Except for it, the proposed method has higher average accuracies than those of all other methods. In addition, on LM-50 dataset, the p-values of the Student's T-test of our method and SnapShot is 0.0639. Except for it, the p-values are all smaller than the threshold (0.05), which means the null-hypothesis that our method and the referred methods have the same mean value but different variances is rejected. This indicates that the classification performance obtained by the proposed method is statistically significantly better than the referred methods except for the experiments of SnapShot on LM-50 dataset.
In summary, our method still has better performance than other methods when the number of training data points become small.
8

Under review as a conference paper at ICLR 2018

Table 2: Comparison of classification accuracies obtained by different methods on UIUC and LM
datastes when the training datasets are reduced. Mean values and standard deviations (in bracket) are listed at the top of cells of the table and p-values of our method and a referred method in the Student's t-Test are listed at the bottom of cells. Each method runs 60 rounds on each dataset. The notation DatasetName-n denotes the configuration in which the training dataset in the named dataset is reduced by n data points for every class from the original training dataset, while the
validation and test datasets are kept unchanged.

UIUC p-value UIUC-20 p-value UIUC-30 p-value UIUC-40 p-value UIUC-50 p-value
LM p-value LM-10 p-value LM-30 p-value LM-50 p-value LM-70 p-value

FC 0.810(0.0110) 3.45E-11 0.769(0.0168) 3.78E-26 0.754(0.0185) 3.80E-20 0.736(0.0152) 5.82E-21 0.697(0.0222) 1.51E-17
FC 0.854(0.0222) 1.36E-16 0.841(0.0254) 8.69E-15 0.825(0.0182) 3.24E-15 0.774(0.0296) 1.40E-08 0.775(0.0226) 4.96E-12

Bagging 0.801(0.0122) 1.21E-13 0.773(0.0120) 3.20E-32 0.753(0.0116) 1.20E-31 0.736(0.0135) 2.90E-32 0.703(0.0161) 6.60E-18
Bagging 0.855(0.0142) 1.45E-25 0.850(0.0213) 5.95E-12 0.828(0.0159) 8.38E-15 0.774(0.0278) 1.76E-9 0.766(0.0226) 8.90E-11

Adaboost 0.822(0.0071) 4.90E-8 0.783(0.0130) 5.10E-16 0.774(0.0104) 1.80E-8 0.753(0.0110) 1.30E-7 0.721(0.0180) 1.00E-4
Adaboost 0.868(0.0156) 2.70E-11 0.863(0.0152) 6.65E-6 0.838(0.0167) 7.45E-7 0.778(0.0186) 2.13E-10 0.782(0.0198) 9.70E-9

SnapShot 0.825(0.0073) 2.14E-7 0.798(0.0086) 9.56E-6 0.780(0.0101) 5.33E-4 0.755(0.0120) 6.53E-6 0.720(0.0110) 2.25E-7
SnapShot 0.878(0.0111) 1.95E-4 0.864(0.0152) 1.94E-5 0.844(0.0145) 0.0012 0.808(0.0183) 0.0639 0.778(0.0159) 1.32E-13

Ours 0.840(0.0067) N/A 0.807(0.0103) N/A 0.787(0.0113) N/A 0.765(0.0117) N/A 0.734(0.0158) N/A
Ours 0.887(0.0100) N/A 0.877(0.0148) N/A 0.853(0.0153) N/A 0.802(0.0178) N/A 0.803(0.0156) N/A

3.6 DISCUSSION
We make several remarks about the experimental results on the two datasets as follows. First, InterBoost outperforms bagging. The reason is that bootstrap sampling in bagging makes the training data of each network contain fewer original data points, and thus although it can construct two diverse base networks, each of the base networks is less accurate. In our InterBoost method, each base network is trained on multiple diverse datasets iteratively and thus is more accurate than those in Bagging.
Secondly, InterBoost outperforms Adaboost. The reason is that in Adaboost, if the first classifier fits training data too well and has no or few errors, little or no changes can be introduced to the weights of data point for training the second classifier. It will end up with training the base network twice on the original dataset and then combining their results. Therefore, the diversity between base networks only relies on the randomness in training neural networks, such as random initialization of a network and shuffling data points in a dataset. However, in our InterBoost method, datasets for training two base networks are always kept complementary during the training process. Furthermore, the InterBoost training is an iterative process, generating a matrix of datasets for training, as illustrated in Figure 3.
In the end, InterBoost outperforms SnapShot. The reason is that in SnapShot, multiple base networks are obtained by finding different local minimum of loss function used to train network. It can make the base networks be different, but it is difficult to be sure that the base networks are enough diverse or complementary. When the weights of two base networks have no big difference, ensemble performance will have no big improvement.
InterBoost can construct two diverse networks, every network has different classes which the network is good at classifying, which is illustrated in Figure 5. Figure 5 presents the confusion matrices
9

Under review as a conference paper at ICLR 2018
Figure 5: Confusion matrices of the proposed InterBoost method on the LM test dataset. The first base network (the left one), with an overall accuracy of 76.4%, has higher accuracies on "forest", "inside city", "open country" and "street" classes. The second base network (the middle one), with an overall accuracy of 79.3%, has higher accuracies on "coast", "highway", `mountain" and "tall building" classes. They jointly give an accuracy of 89.1%.
of our method and its two base networks on the LM dataset without reducing size of training data. From the three matrices, it is observed that the first base network has higher accuracies on "forest", "inside city", "open country" and "street" classes, which means the base network is good at distinguishing these four classes. The second base network, however, has higher accuracies on other four classes, and is good at classifying other four classes. Therefore, even though the first base network (the left one) has an accuracy of 76.4% on the test dataset, and the second one (the middle one) has an accuracy of 79.3%, which are similar to that of a single base network trained alone on the original dataset, our InterBoost method has an accuracy of 89.1% on the test dataset. Therefore, the proposed InterBoost method is able to train two diverse base networks, and obtains a better generalization performance. Due to the space limitation, we list the confusion matrices of Adaboost and Snapshot on LM dataset in Figure 7 of Appendix.
4 CONCLUSION
In the paper, we have proposed an ensemble method called InterBoost for training neural networks for small-sample classification and detailed the training and test procedures. In the training procedure, the two base networks share information with each other in order to push each other optimized in different directions. At the same time, each base network is trained on diverse datasets iteratively. In the end, two complementary base networks are obtained. Experimental results on UIUC-Sports (UIUC) and LabelMe (LM) datasets confirmed that our ensemble method (1) can obtain two complementary base networks, (2) has better generalization performance than other ensemble methods, and (3) reduces variances significantly. Future work includes increasing the number of networks, experimenting on different types of network as well as different kinds of data to evaluate the effectiveness of the InterBoost method.
REFERENCES
Christopher M.. Bishop. Pattern recognition and machine learning. Springer, 2006. Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology, 2(3):27, 2011. Franc¸ois Chollet. Keras:deep learning library for python. runs on tensorflow, theano, or cntk. URL
https://github.com/fchollet/keras, 2015. Stuart Geman, Elie Bienenstock, and Rene´ Doursat. Neural networks and the bias/variance dilemma.
Neural Computation, 4(1):1­58, 1992. Pablo M Granitto, Pablo F Verdes, and H Alejandro Ceccatto. Neural network ensembles: evaluation
of aggregation algorithms. Artificial Intelligence, 163(2):139­162, 2005.
10

Under review as a conference paper at ICLR 2018
Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, and Gang Wang. Recent advances in convolutional neural networks. arXiv preprint arXiv:1512.07108, 2015.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European Conference on Computer Vision, pp. 646­661. Springer, 2016.
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. In International Conference on Learning Representations (ICLR), 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097­1105, 2012.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In International Conference on Learning Representations (ICLR), 2017.
Li-Jia Li and Li Fei-Fei. What, where and who? classifying events by scene and object recognition. In IEEE 11th International Conference on Computer Vision, pp. 1­8, 2007.
Mohammad Moghimi, Serge J Belongie, Mohammad J Saberian, Jian Yang, Nuno Vasconcelos, and Li-Jia Li. Boosted convolutional neural networks. In The British Machine Vision Conference, 2016.
Bryan C Russell, Antonio Torralba, Kevin P Murphy, and William T Freeman. Labelme: a database and web-based tool for image annotation. International Journal of Computer Vision, 77(1):157­ 173, 2008.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. One-shot learning with memory-augmented neural networks. arxiv preprint. arXiv preprint arXiv:1605.06065, 2016.
Holger Schwenk and Yoshua Bengio. Training methods for adaptive boosting of neural networks. In Advances in Neural Information Processing Systems, pp. 647­653, 1998.
Holger Schwenk and Yoshua Bengio. Boosting neural networks. Neural Computation, 12(8):1869­ 1887, 2000.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929­1958, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 1­9, 2015.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th international conference on machine learning (ICML), pp. 1058­1066, 2013.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Yin Zheng, Yu-Jin Zhang, and Hugo Larochelle. Topic modeling of multimodal data: an autoregressive approach. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 1370­1377, 2014.
Zhi-Hua Zhou. Ensemble methods: foundations and algorithms. CRC press, 2012.
Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: many could be better than all. Artificial Intelligence, 137(1-2):239­263, 2002.
11

Under review as a conference paper at ICLR 2018
APPENDIX
Figure 6: Comparison of accuracies obtained by Adaboost under via boxplot on LM datasets. The horizontal axis represents the epoch number for training base networks in Adaboost. The vertical axis is the accuracy. The central mark is the median, and the edges of the box are the 25th and 75th percentiles. The outliers are marked individually. Each method runs 60 rounds.
Figure 7: Confusion matrices of Adaboost and SnapShot on LM dataset. Accuracies of two base networks in Adaboost on test dataset are 86.88% and 85.88%, respectively, and the final accuracy of Adaboost is 86.75%. Accuracies of two base networks in SnapShot on test dataset are 88.75% and 84.88%, respectively, and the final accuracy of Snapshot is 87.0%.
12

Under review as a conference paper at ICLR 2018

Table 3: The sizes of training, validation, test dataset in all datasets in the paper. The first column is the name of dataset, the following columns are the specific classes. We list the sizes of training, validation and test data each class on top, middle and bottom, respectively.

UIUC UIUC-20 UIUC-30 UIUC-40 UIUC-50
LM LM-10 LM-30 LM-50 LM-70

badmin. 95 10 95 75 10 95 65 10 95 55 10 95 45 10 95
coast 100 10 100 90 10 100 70 10 100 50 10 100 30 10 100

bocce 63 10 63 40 10 63 30 10 63 20 10 63 10 10 63
forest 100 10 100 90 10 100 70 10 100 50 10 100 30 10 100

croquet 113 10 113 93 10 113 83 10 113 63 10 113 53 10 113
highw. 100 10 100 90 10 100 70 10 100 50 10 100 30 10 100

polo 86 10 86 86 10 86 76 10 86 66 10 86 56 10 86
insid. 100 10 100 90 10 100 70 10 100 50 10 100 30 10 100

rockcl. 92 10 92 72 10 92 62 10 92 52 10 92 42 10 92
mount. 100 10 100 90 10 100 70 10 100 50 10 100 30 10 100

rowing 120 10 120 100 10 120 90 10 120 80 10 120 70 10 120
openc. 100 10 100 90 10 100 70 10 100 50 10 100 30 10 100

sailing 90 10 90 70 10 90 60 10 90 50 10 90 40 10 90
street 100 10 100 90 10 100 70 10 100 50 10 100 30 10 100

snowb. 90 10 90 70 10 90 60 10 90 50 10 90 40 10 90
tallb. 100 10 100 90 10 100 70 10 100 50 10 100 30 10 100

Total 749 80 749 589 80 749 509 80 749 429 80 749 349 80 749
Total 800 80 800 720 80 800 560 80 800 400 80 800 240 80 800

Table 4: Comparison of classification accuracies obtained by different methods on UIUC and LM
datastes. Mean values and standard deviations (in bracket) are listed at the top of cells of the table and p-values of our method and a referred method in the Student's t-Test are listed at the bottom of cells. The epoch number of FC and base network in other ensemble methods is 10000, in which our method has 100 iterations, and each base network in each iteration is trained 100 epochs. Each method runs 30 rounds,

UIUC p-value
LM p-value

FC 0.809(0.0117) 1.75E-16 0.862(0.02766) 1.11E-7

Bagging 0.795(0.0104) 2.92E-23 0.858(0.0182) 6.83E-13

Adaboost 0.801(0.0287) 9.50E-9 0.864(0.014968) 3.36E-14

SnapShot 0.826(0.0085) 5.01E-14 0.876(0.0118) 3.40E-11

Ours 0.843(0.0035) N/A 0.898(0.0042) N/A

13

