Under review as a conference paper at ICLR 2018
THE POWER OF DEEPER NETWORKS
FOR EXPRESSING NATURAL FUNCTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
It is well-known that neural networks are universal approximators, but that deeper networks tend in practice to be more powerful than shallower ones. We shed light on this by proving that the total number of neurons m required to approximate natural classes of multivariate polynomials of n variables grows only linearly with n for deep neural networks, but grows exponentially when merely a single hidden layer is allowed. We also provide evidence that when the number of hidden layers is increased from 1 to k, the neuron requirement grows exponentially not with n but with n1/k, suggesting that the minimum number of layers required for practical expressibility grows only logarithmically with n.
1 INTRODUCTION
Deep learning has lately been shown to be a very powerful tool for a wide range of problems, from image segmentation to machine translation. Despite its success, many of the techniques developed by practitioners of artificial neural networks (ANNs) are heuristics without theoretical guarantees. Perhaps most notably, the power of feedforward networks with many layers (deep networks) has not been fully explained. The goal of this paper is to shed more light on this question and to suggest heuristics for how deep is deep enough.
It is well-known (Cybenko, 1989; Funahashi, 1989; Hornik et al., 1989) that neural networks with a single hidden layer can approximate any function under reasonable assumptions, but it is possible that the networks required will be extremely large. Recent authors have shown that some functions can be approximated by deeper networks much more efficiently (i.e. with fewer neurons) than by shallower ones. However, many of the functions in question are complicated or arise from "existence proofs" without explicit constructions. Some other results apply only to types of network rarely used in practice.
It is important and timely to extend this work to make it more concrete and actionable, by deriving resource requirements for approximating natural classes of functions using today's most common neural network architectures. Lin et al. (2017) recently proved that it is exponentially more efficient to use a deep network than a shallow network when Taylor-approximating the product of input variables. In the present paper, we move far beyond this result in the following ways: (i) we use standard uniform approximation instead of Taylor approximation, (ii) we show that the exponential advantage of depth extends to all general sparse multivariate polynomials, and (iii) we address the question of how the number of neurons scales with the number of layers. Our results apply to standard feedforward neural networks and are borne out by empirical tests.
Our primary contributions are as follows:
· It is possible to achieve arbitrarily close approximations of simple multivariate and univariate polynomials with neural networks having a bounded number of neurons (see §3).
· Such polynomials are exponentially easier to approximate with deep networks than with shallow networks (see §4).
· The power of networks improves rapidly with depth; for natural polynomials, the number of layers required is at most logarithmic in the number of input variables, where the base of the logarithm depends upon the layer width (see §5).
1

Under review as a conference paper at ICLR 2018
2 RELATED WORK
Deeper networks have been shown to have greater representational power with respect to various notions of complexity, including piecewise linear decision boundaries (Montufar et al., 2014) and topological invariants (Bianchini & Scarselli, 2014). Recently, Poole et al. (2016) and Raghu et al. (2016) showed that the trajectories of input variables attain exponentially greater length and curvature with greater network depth. Work including Daniely (2017); Eldan & Shamir (2016); Telgarsky (2016) shows that there exist functions that require exponential width to be approximated by a shallow network. Mhaskar et al. (2016), in considering compositional functions with this property, inquire whether explicit examples must be pathologically complicated, a question which we answer here in the negative.
Various authors have also considered the power of deeper networks of types other than the standard feedforward model. The problem has also been posed for sum-product networks (Delalleau & Bengio, 2011) and restricted Boltzmann machines (Martens et al., 2013). Cohen et al. (2016) showed, using tools from tensor decomposition, that shallow arithmetic circuits can express only a measure-zero set of the functions expressible by deep circuits. A weak generalization of this result to convolutional neural networks was shown in Cohen & Shashua (2016).
3 THE POWER OF APPROXIMATION
In this paper, we will consider the standard model of feedforward neural networks (also called multilayer perceptrons). Formally, the network may be considered as a multivariate function N (x) = Ak(· · · (A2(A1x)) · · · ), where A1, . . . , Ak are constant matrices and  denotes a scalar nonlinear function applied element-wise to vectors. The constant k is referred to as the depth of the network. The neurons of the network are the entries of the vectors (A · · · (A2(A1x)) · · · ), for
= 1, . . . , k - 1. These vectors are referred to as the hidden layers of the network.
Two notions of approximation will be relevant in our results: -approximation, also known as uniform approximation, and Taylor approximation. Definition 3.1. For constant > 0, we say that a network N (x) -approximates a multivariate function f (x) (for x in a specified domain (-R, R)n) if supx |N (x) - f (x)| < . Definition 3.2. We say that a network N (x) Taylor-approximates a multivariate polynomial p(x) of degree d if p(x) is the dth order Taylor polynomial of N (x).
The following proposition shows that Taylor approximation implies -approximation for homogeneous polynomials. The reverse implication does not hold. Proposition 3.3. Suppose that the network N (x) Taylor-approximates the homogeneous multivariate polynomial p(x). Then, for every , there exists a network N (x) that -approximates p(x), such that N (x) and N (x) have the same number of neurons in each layer.
Proof. Suppose that N (x) = Ak(· · · (A2(A1x)) · · · ) and that p(x) has degree d. Let µ = supx |N (x) - p(x)|. If µ  , then we are done. Otherwise, let  = /µ, so that  < 1. Let A1 = A1, Ak = Ak/d, and A = A for = 2, 3, . . . , k - 1. Then, for N (x) = Ak(· · · (A2(A1x)) · · · ), we observe that N (x) = N (x)/d. Since N (x) is a Taylor approximation of p(x), we can write N (x) as p(x) + E(x), where E(x) is a Taylor series with terms of degree at least d + 1. Since  < 1, we have:
|N (x) - p(x)| = |N (x)/d - p(x)| = |p(x)/d + E(x)/d - p(x)| = |E(x)/d|  |N (x) - p(x)|  µ = .
We conclude that N (x) is an -approximation of p(x), as desired.
For a fixed nonlinear function , we consider the total number of neurons (excluding input and output neurons) needed for a network to approximate a given function. Remarkably, it is possible to
2

Under review as a conference paper at ICLR 2018

attain arbitrarily good approximations of a (not necessarily homogeneous) multivariate polynomial by a feedforward neural network, even with a single hidden layer, without increasing the number of neurons past a certain bound.
Theorem 3.4. Suppose that p(x) is a degree-d multivariate polynomial and that the nonlinearity  has nonzero Taylor coefficients up to degree d. Let mk(p) be the minimum number of neurons in a depth-k network that -approximates p. Then, the limit lim 0 mk(p) exists (and is finite).

Proof. We show that lim 0 m1(p) exists; it follows immediately that lim 0 mk(p) exists for every k, since an -approximation to p with depth k can be constructed from one with depth 1.
Let p1(x), p2(x), . . . , ps(x) be the monomials of p(x), so that p(x) = i pi(x). We claim that each pi(x) can be Taylor-approximated by a network N i(x) with one hidden layer. This follows, for example, from the proof in Lin et al. (2017) that products can be Taylor-approximated by networks with one hidden layer, since each monomial is the product of several inputs (with multiplicity); we prove a far stronger result about N i(x) later in this paper (see Theorem 4.1).
Suppose now that N i(x) has mi hidden neurons. By Proposition 3.3, we conclude that since pi(x) is homogeneous, it may be -approximated by a network Ni(x) with mi hidden neurons, where  = /s. By combining the networks Ni(x) for each i, we can define a network N (x) = i Ni(x) with i mi neurons. Then, we have:
|N (x) - p(x)|  |Ni(x) - pi(x)|
i
  = s = .
i
Hence, N (x) is an -approximation of p(x), implying that m1(p)  i mi for every . Thus, lim 0 m1(p) exists, as desired.

This theorem is perhaps surprising, since it is common for -approximations to functions to require

ever-greater complexity, approaching infinity as  0. For example, the function exp(| - x|) may

be approximated on the domain (-, ) by Fourier sums of the form

m k=0

am

cos(kx).

However,

in order to achieve -approximation, we need to take m  1/ terms. By contrast, we have

shown that a finite neural network architecture can achieve arbitrarily good approximations merely

by altering its weights.

Note also that the assumption of nonzero Taylor coefficients cannot be dropped from Theorem 3.4. For example, the theorem is false for rectified linear units (ReLUs), which are piecewise linear and do not admit a Taylor series. This is because -approximating a non-linear polynomial with a piecewise linear function requires an ever-increasing number of pieces as  0.

Theorem 3.4 allows us to make the following definition:
Definition 3.5. Suppose that a nonlinear function  is given. For p a multivariate polynomial, let mkuniform(p) be the minimum number of neurons in a depth-k network that -approximates p for all arbitrarily small. Set muniform(p) = mink mkuniform(p). Likewise, let mkTaylor(p) be the minimum number of neurons in a depth-k network that Taylor-approximates p, and set mTaylor(p) = mink mkTaylor(p).

In the next section, we will show that there is an exponential gap between mu1niform(p) and muniform(p) and between m1Taylor(p) and mTaylor(p) for various classes of polynomials p.

4 THE INEFFICIENCY OF SHALLOW NETWORKS
In this section, we compare the efficiency of shallow networks (those with a single hidden layer) and deep networks at approximating multivariate polynomials. Proofs of our main results are included in the Appendix.

3

Under review as a conference paper at ICLR 2018

4.1 MULTIVARIATE POLYNOMIALS

Our first result shows that uniform approximation of monomials requires exponentially more neu-

rons in a shallow than a deep network.

Theorem 4.1. Let p(x) denote the monomial x1r1 x2r2 · · · xnrn , with d =

n i=1

ri.

Suppose

that

the

nonlinearity  has nonzero Taylor coefficients up to degree 2d. Then, we have:

(i) mu1niform(p) = (ii) muniform(p) 

in=1(ri + 1), ni=1(7 log2(ri) + 4),

where x denotes the smallest integer that is at least x.

We can prove a comparable result for mTaylor under slightly weaker assumptions on . Note that

by setting r1 = r2 = . . . = rn = 1, we recover the result of Lin et al. (2017) that the product of n numbers requires 2n neurons in a shallow network but can be Taylor-approximated with linearly

many neurons in a deep network.

Theorem 4.2. Let p(x) denote the monomial x1r1 xr22 · · · xnrn , with d =

n i=1

ri.

Suppose

that



has

nonzero Taylor coefficients up to degree d. Then, we have:

(i) m1Taylor(p) = (ii) mTaylor(p) 

in=1(ri + 1), in=1(7 log2(ri) + 4).

It is natural now to consider the cost of approximating general polynomials. However, without

further constraint, this is relatively uninstructive because polynomials of degree d in n variables

live within a space of dimension

n+d d

, and therefore most require exponentially many neurons for

any depth of network. We therefore consider polynomials of sparsity c: that is, those that can be

represented as the sum of c monomials. This includes many natural functions.

The following theorem, when combined with Theorems 4.1 and 4.2, shows that general polynomials p with subexponential sparsity have exponentially large mu1niform(p) and mT1aylor(p), but subexponential muniform(p) and mTaylor(p).
Theorem 4.3. Let p(x) be a multivariate polynomial of degree d and sparsity c, having monomials q1(x), q2(x), . . . , qc(x). Suppose that the nonlinearity  has nonzero Taylor coefficients up to degree 2d. Then, we have:

(i)

m1uniform(p) 

1 c

maxj

mu1niform(qj ).

(ii) muniform(p)  j muniform(qj ).

These statements also hold if muniform is replaced with mTaylor.

4.2 UNIVARIATE POLYNOMIALS
As with multivariate polynomials, depth can offer an exponential savings when approximating univariate polynomials. We show below (Proposition 4.4) that a shallow network can approximate any degree-d univariate polynomial with a number of neurons at most linear in d. The monomial xd requires d + 1 neurons in a shallow network (Proposition 4.5), but can be approximated with only logarithmically many neurons in a deep network. Thus, depth allows us to reduce networks from linear to logarithmic size, while for multivariate polynomials the gap was between exponential and linear. The difference here arises because the dimensionality of the space of univariate degree-d polynomials is linear in d, which the dimensionality of the space of multivariate degree-d polynomials is exponential in d.
Proposition 4.4. Suppose that the nonlinearity  has nonzero Taylor coefficients up to degree d. Then, mT1aylor(p)  d + 1 for every univariate polynomial p of degree d.
Proof. Pick a0, a1, . . . , ad to be arbitrary, distinct real numbers. Consider the Vandermonde matrix A with entries Aij = aji . It is well-known that det(A) = i<i (ai - ai) = 0. Hence, A is

4

Under review as a conference paper at ICLR 2018

invertible, which means that multiplying its columns by nonzero values gives another invertible matrix. Suppose that we multiply the jth column of A by j to get A , where (x) = j jxj is the Taylor expansion of (x).
Now, observe that the ith row of A is exactly the coefficients of (aix), up to the degree-d term. Since A is invertible, the rows must be linearly independent, so the polynomials (aix), restricted to terms of degree at most d, must themselves be linearly independent. Since the space of degree-d univariate polynomials is (d + 1)-dimensional, these d + 1 linearly independent polynomials must span the space. Hence, m1Taylor(p)  d + 1 for any univariate degree-d polynomial p. In fact, we can fix the weights from the input neuron to the hidden layer (to be a0, a1, . . . , ad, respectively) and still represent any polynomial p with d + 1 hidden neurons.
Proposition 4.5. Let p(x) = xd, and suppose that the nonlinearity (x) has nonzero Taylor coefficients up to degree 2d. Then, we have:
(i) m1uniform(p) = d + 1.
(ii) muniform(p)  7 log2(d) .
These statements also hold if muniform is replaced with mTaylor.
Proof. Part (i) follows from part (i) of Theorems 4.1 and 4.2 by setting n = 1 and r1 = d.
For part (ii), observe that we can Taylor-approximate the square x2 of an input x with three neurons in a single layer:
1 ((x) + (-x) - 2(0)) = x2 + O(x4 + x5 + . . .). 2 (0)
We refer to this construction as a square gate, and the construction of Lin et al. (2017) as a product gate. We also use identity gate to refer to a neuron that simply preserves the input of a neuron from the preceding layer (this is equivalent to the skip connections in residual nets).
Consider a network in which each layer contains a square gate (3 neurons) and either a product gate or an identity gate (4 or 1 neurons, respectively), according to the following construction: The square gate squares the output of the preceding square gate, yielding inductively a result of the form x2k , where k is the depth of the layer. Writing d in binary, we use a product gate if there is a 1 in the 2k-1-place; if so, the product gate multiplies the output of the preceding product gate by the output of the preceding square gate. If there is a 0 in the 2k-1-place, we use an identity gate instead of a product gate. Thus, each layer computes x2k and multiplies x2k-1 to the computation if the 2k-1-place in d is 1. The process stops when the product gate outputs xd.
This network clearly uses at most 7 log2(d) neurons, with a worst case scenario where d + 1 is a power of 2. Hence mTaylor(p)  7 log2(d) , with muniform(p)  mTaylor(p) by Proposition 3.3 since p is homogeneous.

5 HOW EFFICIENCY IMPROVES WITH DEPTH

We now consider how mukniform(p) scales with k, interpolating between exponential in n (for k = 1) and linear in n (for k = log n). In practice, networks with modest k > 1 are effective at representing natural functions. We explain this theoretically by showing that the cost of approximating the product polynomial drops off rapidly as k increases.

By repeated application of the shallow network construction in Lin et al. (2017), we obtain the following upper bound on mkuniform(p), which we conjecture to be essentially tight. Our approach is reminiscent of tree-like network architectures discussed e.g. in Mhaskar et al. (2016), in which
groups of input variables are recursively processed in successive layers.

Theorem 5.1. Let p(x) equal the product x1x2 · · · xn, and suppose  has nonzero Taylor coefficients up to degree n. Then, we have:

mkuniform(p) = O n(k-1)/k · 2n1/k .

(1)

5

Under review as a conference paper at ICLR 2018

Proof. We construct a network in which groups of the n inputs are recursively multiplied up to Taylor approximation. The n inputs are first divided into groups of size b1, and each group is multiplied in the first hidden layer using 2b1 neurons (as described in Lin et al. (2017)). Thus, the first hidden layer includes a total of 2b1 n/b1 neurons. This gives us n/b1 values to multiply, which are in turn divided into groups of size b2. Each group is multiplied in the second hidden layer using 2b2 neurons. Thus, the second hidden layer includes a total of 2b2 n/(b1b2) neurons.
We continue in this fashion for b1, b2, . . . , bk such that b1b2 · · · bk = n, giving us one neuron which is the product of all of our inputs. By considering the total number of neurons used, we conclude

k
mTkaylor(p) 
i=1



n
i j=1

bj

2bi

=

k i=1

k
 bj  2bi .
j=i+1

(2)

By Proposition 3.3, mkuniform(p)  mTkaylor(p) since p is homogeneous. Setting bi = n1/k, for each i, gives us the desired bound (1).

In fact, we can solve for the choice of bi such that the upper bound in (2) is minimized, under the condition b1b2 · · · bk = n. Using the technique of Lagrange multipliers, we know that the optimum
occurs at a minimum of the function

L(bi, ) :=

k
n - bi
i=1


kk

+ 

bj  2bi .

i=1 j=i+1

Differentiating L with respect to bi, we obtain the conditions

i-1
0 = - bj +
j=i h=1

k j=h+1

bj

bi


k

2bh + (log 2) 

bj 2bi , for 1  i  k

j=i+1

k
0 = n - bj.
j=1

(3) (4)

Dividing (3) by

k j=i+1

bj

and

rearranging

gives

us

the

recursion

bi = bi-1 + log2(bi-1 - 1/ log 2). Thus, the optimal bi are not exactly equal but very slowly increasing with i (see Figure 1).

(5)

The following conjecture states that the bound given in Theorem 5.1 is (approximately) optimal.
Conjecture 5.2. Let p(x) equal to the product x1x2 · · · xn, and suppose that  has all nonzero Taylor coefficients. Then, for small enough, we have:

mukniform(p) = 2(n1/k),

(6)

i.e., the exponent grows as n1/k for n  .
We empirically tested Conjecture 5.2 by training ANNs to predict the product of input values x1, . . . , xn with n = 20 (see Figure 2). The rapid interpolation from exponential to linear width aligns with our predictions.
In our experiments, we used feedforward networks with dense connections between successive layers. In the figure, we show results for (x) = tanh(x) (note that this behavior is even better than expected, since this function actually has numerous zero Taylor coefficients). Similar results were also obtained for rectified linear units (ReLUs) as the nonlinearity, despite the fact that this function does not even admit a Taylor series. The number of layers was varied, as was the number of neurons within a single layer. The networks were trained using the AdaDelta optimizer (Zeiler, 2012) to minimize the absolute value of the difference between the predicted and actual values. Input variables xi were drawn uniformly at random from the interval [0, 2], so that the expected value of the output would be of manageable size.
6

Under review as a conference paper at ICLR 2018

Figure 1: The optimal settings for {bi}ik=1 as n varies are shown for k = 1, 2, 3. Observe that the bi converge to n1/k for large n, as witnessed
by a linear fit in the log-log plot. The exact val-
ues are given by equations (4) and (5).

Figure 2: Performance of trained networks in approximating the product of 20 input variables, ranging from red (high error) to blue (low error). The error shown here is the expected absolute difference between the predicted and actual product. The curve w = n(k-1)/k · 2n1/k for n = 20 is shown in black. In the region above and to the right of the curve, it is possible to effectively approximate the product function (Theorem 5.1).

Eq. (6) provides a helpful rule of thumb for how deep is deep enough. Suppose, for instance, that we wish to keep typical layers no wider than about a thousand ( 210) neurons. Eq. (6) then implies n1/k < 10, i.e., that the number of layers should be at least
k > log10 n.
It would be very interesting if one could show that general polynomials p in n variables require a superpolynomial number of neurons to approximate for any constant number of hidden layers. The analogous statement for Boolean circuits - whether the complexity classes T C0 and T C1 are equal - remains unresolved and is assumed to be quite hard. Note that the formulations for Boolean circuits and deep neural networks are independent statements (neither would imply the other) due to the differences between computation on binary and real values. Indeed, work such as Cohen et al. (2016); Montufar et al. (2014); Telgarsky (2016) has already proven gaps in expressivity for real-valued neural networks of different depths, for which the analogous results remain unknown in Boolean circuits.
6 CONCLUSION
We have shown how the power of deeper ANNs can be quantified even for simple polynomials. We have proved that arbitrarily good approximations of polynomials are possible even with a fixed number of neurons and that there is an exponential gap between the width of shallow and deep networks required for approximating a given sparse polynomial. For n variables, a shallow network requires size exponential in n, while a deep network requires at most linearly many neurons. Networks with a constant number k > 1 of hidden layers appear to interpolate between these extremes, following a curve exponential in n1/k. This suggests a rough heuristic for the number of layers required for approximating simple functions with neural networks. For example, if we want no layers to have more than 210 neurons, say, then the minimum number of layers required grows only as log10 n. To further improve efficiency using the O(n) constructions we have presented, it suffices to increase the number of layers by a factor of log2 10  3, to log2 n.
7

Under review as a conference paper at ICLR 2018
It is worth noting that our constructions enjoy the property of locality mentioned in Cohen et al. (2016), which is also a feature of convolutional neural nets. That is, each neuron in a layer is assumed to be connected only to a small subset of neurons from the previous layer, rather than the entirety of them (or some large fraction). In fact, we showed (e.g. Prop. 4.5) that there exist natural functions that can be computed in a linear number of neurons, where each neuron is connected to at most two neurons in the preceding layer, which nonetheless cannot be computed with fewer than exponentially many neurons in a single layer, no matter how may connections are used. Our construction can also easily be framed with reference to the other properties mentioned in Cohen et al. (2016): those of sharing (in which weights are shared between neural connections) and pooling (in which layers are gradually collapsed, as our construction essentially does with recursive combination of inputs).
This paper has focused exclusively on the resources (notably neurons and synapses) required to compute a given function. An important complementary challenge is to quantify the resources (e.g. training steps) required to learn the computation, i.e., to converge to appropriate weights using training data -- possibly a fixed amount thereof, as suggested in Zhang et al. (2017). There are simple functions that can be computed with polynomial resources but require exponential resources to learn (Shalev-Shwartz et al., 2017). It is quite possible that architectures we have not considered increase the feasibility of learning. For example, residual networks (ResNets) (He et al., 2016) and unitary nets (see e.g. Arjovsky et al. (2016); Jing et al. (2016)) are no more powerful in representational ability than conventional networks of the same size, but by being less susceptible to the "vanishing/exploding gradient" problem, it is far easier to optimize them in practice. We look forward to future work that will help us understand the power of neural networks to learn.
REFERENCES
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning (ICML), pp. 1120­1128, 2016.
Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. IEEE transactions on neural networks and learning systems, 25(8):1553­1565, 2014.
Nadav Cohen and Amnon Shashua. Convolutional rectifier networks as generalized tensor decompositions. In International Conference on Machine Learning (ICML), 2016.
Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor analysis. Journal of Machine Learning Research (JMLR), 49, 2016.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2(4):303­314, 1989.
Amit Daniely. Depth separation for neural networks. In Conference On Learning Theory (COLT), 2017.
Olivier Delalleau and Yoshua Bengio. Shallow vs. deep sum-product networks. In Advances in Neural Information Processing Systems (NIPS), pp. 666­674, 2011.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Annual Conference on Learning Theory (COLT), pp. 907­940, 2016.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks. Neural networks, 2(3):183­192, 1989.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770­778, 2016.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Max Tegmark, and Marin Soljacic´. Tunable efficient unitary neural networks (EUNN) and their application to RNN. arXiv preprint arXiv:1612.05231, 2016.
8

Under review as a conference paper at ICLR 2018

Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well? Journal of Statistical Physics, 168(6):1223­1247, 2017.
James Martens, Arkadev Chattopadhya, Toni Pitassi, and Richard Zemel. On the representational efficiency of restricted Boltzmann machines. In Advances in Neural Information Processing Systems (NIPS), pp. 2877­2885, 2013.
Hrushikesh Mhaskar, Qianli Liao, and Tomaso Poggio. Learning functions: When is deep better than shallow. arXiv:1603.00988v4, 2016.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems (NIPS), pp. 2924­2932, 2014.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances In Neural Information Processing Systems (NIPS), pp. 3360­3368, 2016.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. Survey of expressivity in deep neural networks. arXiv:1611.08083, 2016.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of deep learning. arXiv:1703.07950, 2017.
Matus Telgarsky. Benefits of depth in neural networks. Journal of Machine Learning Research (JMLR), 49, 2016.
Matthew D Zeiler. ADADELTA: an adaptive learning rate method. arXiv:1212.5701, 2012.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations (ICLR), 2017.

APPENDIX

6.1 PROOF OF THEOREM 4.1.

Without loss of generality, suppose that ri > 0 for i = 1, . . . , n. Let X be the multiset in which xi occurs with multiplicity ri.

We first show that

n i=1

(ri

+

1)

neurons

are

sufficient

to

approximate

p(x).

Appendix A in Lin

et al. (2017) demonstrates that for variables y1, . . . , yN , the product y1 · · · · · yN can be Taylor-

approximated as a linear combination of the 2N functions (±y1 ± · · · ± yd).

Consider setting y1, . . . , yd equal to the elements of multiset X. Then, we conclude that we can approximate p(x) as a linear combination of the functions (±y1 ± · · · ± yd). However, these (·fui·gn·nc±otiroyinnNgs)p,aerperrmonvuoittnagtailotlhndasitsomtfintThcayte:losrti(hgpen)rse).aTreheinr=rie1f+(orrie1+, dthi1se)tri.enPcartroewpoaysniis=ti1too(nrai3s.+s3ig1imn) pd±liisetssiingtcnhtsaftutfonocrrtiihoocnmospoige(se±noyef1oxu±si polynomials p, we have m1uniform(p)  mT1aylor(p).

We now show that this number of neurons is also necessary for approximating p(x). Suppose that
N (x) is an -approximation to p(x) with depth 1, and let the Taylor series of N (x) be p(x)+E(x). Let Ek(x) be the degree-k homogeneous component of E(x), for 0  k  2d. By the definition of -approximation, supx E(x) goes to 0 as does, so by picking small enough, we can ensure that the coefficients of each Ek(x) go to 0.

9

Under review as a conference paper at ICLR 2018

Let m = mu1niform(p) and suppose that (x) has the Taylor expansion

 k=0

k xk .

Then,

by

group-

ing terms of each order, we conclude that there exist constants aij and wj such that

mn

d

d wj

aijxi = p(x) + Ed(x)

j=1

i=1

mn

k

k wj

aijxi = Ek(x) for k = d.

j=1

i=1

For each S  X, let us take the derivative of this equation by every variable that occurs in S, where we take multiple derivatives of variables that occur multiple times. This gives

d · d! |S|!

m

wj

ahj

j=1 hS

n

d-|S|





aij xi

=

p(x) + S

S Ed(x),

i=1

(7)

k · k! |S|!

m

wj

ahj

j=1 hS

n

k-|S|



aij xi

= S Ek(x).

i=1

(8)

Observe that there are r  in=1(ri + 1) choices for S, since each variable xi can be included

anywhere from 0 to ri times. Define A to be the r × m matrix with entries AS,j = hS ahj.

We claim that A has full row rank. This would show that the number of columns m is at least the

number of rows r =

n i=1

(ri

+

1),

proving

the

desired

lower

bound

on

m.

Suppose towards contradiction that the rows AS ,· admit a linear dependence:

r

c AS ,· = 0,

=1

where the coefficients c are all nonzero and the S denote distinct subsets of X. Let S be such that

|c| is maximized. Then, take the dot product of each side of the above equation by the vector with

entries (indexed by j) equal to wj (

n i=1

aij

xi)d-|S|:

rm

n d-|S|

0 = c wj ahj

aij xi

=1 j=1 hS

i=1

m
= c wj ahj
|(|S |=|S|) j=1 hS
m
+ c wj ahj
|(|S |=|S|) j=1 hS

n
aij xi
i=1

d-|S |

n
aij xi
i=1

(d+|S |-|S|)-|S |
.

We can use (7) to simplify the first term and (8) (with k = d + |S | - |S|) to simplify the second term, giving us:

0 = c · |S |! ·

|(|S |=|S|)

d · d!

 S p(x) + S Ed(x)

|S |! 

+

c
|(|S |=|S|)

·· d+|S |-|S| · (d + |S | - |S|)! S

Ed+|S |-|S|(x)

(9)

Consider

the

coefficient

of

the

monomial

 S

p(x),

which

appears

in

the

first

summand

with

coeffi-

cient

c

·

|S |! d ·d!

.

Since

the

S

are

distinct,

this

monomial

does

not

appear

in

any

other

term

 S

p(x),

but it could appear in

some of the terms

 S

Ek (x).

By definition, |c| is the largest of the values |c |, and by setting small enough, all coefficients

of

 S

Ek(x) can be made negligibly small for every k.

This implies that the coefficient of the

monomial

 S

p(x)

can

be

made

arbitrarily

close

to

c

·

|S |! d ·d!

,

which

is

nonzero

since

c

is

nonzero.

10

Under review as a conference paper at ICLR 2018

However, the left-hand side of equation (9) tells us that this coefficient should be zero - a contradiction. We conclude that A has full row rank, and therefore that m1uniform(p) = m  in=1(ri + 1). This completes the proof of part (i).

We now consider part (ii) of the theorem. It follows from Proposition 4.5, part (ii) that, for each i,

we we

can can

Taylor-approximate Taylor-approximate

xiri using all of the

x7ri ilougs2in(rgi

) a

neurons total of

arranged in a i 7 log2(ri)

deep network. Therefore, neurons. From Lin et al.

(2017), we know that these n terms can be multiplied using 4n additional neurons, giving us a total

of i(7 log2(ri) +4). Proposition 3.3 implies again that m1uniform(p)  mT1aylor(p). This completes the proof.

6.2 PROOF OF THEOREM 4.2.

As above, suppose that ri > 0 for i = 1, . . . , n, and let X be the multiset in which xi occurs with multiplicity ri.

It is shown in the proof of Theorem 4.1 that ni=1(ri + 1) neurons are sufficient to Taylorapproximate p(x). We now show that this number of neurons is also necessary for approximating

p(x). Let m = m1Taylor(p) and suppose that (x) has the Taylor expansion

 k=0

k

xk

.

Then,

by

grouping terms of each order, we conclude that there exist constants aij and wj such that

mn

d

d wj

aijxi = p(x)

(10)

j=1

i=1

mn

k

k wj

aijxi = 0 for 0  k  N - 1.

(11)

j=1

i=1

For each S  X, let us take the derivative of equations (10) and (11) by every variable that occurs in S, where we take multiple derivatives of variables that occur multiple times. This gives

d · d! |S|!

m

wj

ahj

j=1 hS

n

d-|S|



aij xi

= p(x), S

i=1

(12)

k · k! |S|!

m

wj

ahj

j=1 hS

n
aij xi
i=1

k-|S|
=0

(13)

for |S|  k  d - 1. Observe that there are r =

n i=1

(ri

+

1)

choices

for

S,

since

each

variable

xi can be included anywhere from 0 to ri times. Define A to be the r × m matrix with entries

AS,j = columns

m

hS
is at

ahj . least

We claim that A has full row rank. the number of rows r = in=1(ri + 1),

This would show that the number proving the desired lower bound on

of m.

Suppose towards contradiction that the rows AS ,· admit a linear dependence:
r

c AS ,· = 0,

=1

where the coefficients c are nonzero and the S denote distinct subsets of X. Set s = max |S |.

Then, take the dot product of each side of the above equation by the vector with entries (indexed by

j) equal to wj (

n i=1

aij

xi)d-s

:

rm

n d-s

0 = c wj ahj

aij xi

=1 j=1 hS

i=1

m n d-|S |

=

c wj ahj

aij xi

|(|S |=s) j=1 hS

i=1

m n (d+|S |-s)-|S |

+

c wj ahj

aij xi

.

|(|S |<s) j=1 hS

i=1

11

Under review as a conference paper at ICLR 2018

We can use (12) to simplify the first term and (13) (with k = d + |S | - s) to simplify the second term, giving us:

|S |! 

|S |!

0=

c
|(|S |=s)

·· d · d! S

p(x) +

c
|(|S |<s)

· d+|S |-s · (d + |S | - s)! · 0

= c · |S |! ·  p(x).

|(|S |=s)

d · d! S

Since the distinct monomials

 S

p(x) are linearly independent, this contradicts our assumption that

the c are nonzero.

n i=1

(ri

+

1).

This

We conclude that A has full row completes the proof of part (i).

rank,

and

therefore

that

mT1aylor(p)

=

m



Part (ii) of the theorem was demonstrated in the proof of Theorem 4.1. This completes the proof.

6.3 PROOF OF THEOREM 4.3.

Our proof in Theorem 4.1 relied upon the fact that all nonzero partial derivatives of a monomial are
linearly independent. This fact is not true for general polynomials p; however, an exactly similar argument shows that m1uniform(p) is at least the number of linearly independent partial derivatives of p, taken with respect to multisets of the input variables.

Consider x1r1 xr22 · ·

the · xrnn

monomial q of p . By Theorem 4.1,

such that mu1niform(q) m1uniform(q) is equal to

is the

maximized, and number in=1(ri

suppose that q(x) = + 1) of distinct mono-

mials that can be obtained by taking partial derivatives of q. Let Q be the set of such monomials,

and let D be the set of (iterated) partial derivatives corresponding to them, so that for d  D, we

have d(q)  Q.

Consider the set of polynomials P = {d(p) | d  D}. We claim that there exists a linearly independent subset of P with size at least |D|/c. Suppose to the contrary that P is a maximal linearly independent subset of P with |P | < |D|/c.

Since p has c monomials, every element of P has at most c monomials. Therefore, the total number of distinct monomials in elements of P is less than |D|. However, there are at least |D| distinct monomials contained in elements of P , since for d  D, the polynomial d(p) contains the monomial
d(q), and by definition all d(q) are distinct as d varies. We conclude that there is some polynomial p  P \P containing a monomial that does not appear in any element of P . But then p is linearly
independent of P , a contradiction since we assumed that P was maximal.

We conclude that some linearly independent subset of P has size at least |D|/c, and therefore that the space of partial derivatives of p has rank at least |D|/c = m1uniform(q)/c. This proves part (i) of the theorem. Part (ii) follows immediately from the definition of muniform(p).

Similar logic holds for mTaylor.

12

