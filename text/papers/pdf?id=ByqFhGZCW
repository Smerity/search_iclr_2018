Under review as a conference paper at ICLR 2018
MACHINE VS MACHINE: DEFENDING CLASSIFIERS AGAINST LEARNING-BASED ADVERSARIAL ATTACKS
Anonymous authors Paper under double-blind review
ABSTRACT
Recently, researchers have discovered that the state-of-the-art object classifiers can be fooled easily by small input perturbations unnoticeable to human eyes. Subsequent research has proposed several methods of crafting adversarial examples, as well as methods of robustifying the classifier against such examples. An attacker with the knowledge of the classifier parameters can generate strong adversarial patterns. In response, a classifier with the knowledge of such patterns can be trained to be robust to them. The cat-and-mouse game nature of the attacks and the defenses raises the question of the presence of an equilibrium in the dynamics. In this paper, we propose a game framework to formulate the interaction of attacks and defenses and present the natural notion of the best worst-case defense and attack. We propose simple algorithms to find those solutions motivated by sensitivity penalization. In addition, we show the potentials of learning-based attacks, and present the close relationship between the adversarial attack and the privacy attack problems. The results are demonstrated with MNIST and CIFAR10 datasets.
1 INTRODUCTION
Recently, researchers have made a somewhat surprising discovery that the state-of-the-art object classifiers can be fooled easily by small additive perturbations unnoticeable to human eyes (Szegedy et al., 2013; Goodfellow et al., 2014b). Studies followed which tried to understand the cause of the seeming failure of deep learning toward such adversarial samples. The vulnerability was ascribed to linearity (Szegedy et al., 2013), low flexibility (Fawzi et al., 2015), and flat or curved decision boundary (Moosavi-Dezfooli et al., 2017), but the underlying cause is still not well understood. This is troublesome since such vulnerability can result in a critical failure such as autonomous cars misreading traffic signs. Several methods of generating adversarial examples were proposed (Goodfellow et al., 2014b; Moosavi-Dezfooli et al., 2016; Carlini & Wagner, 2017), which use the knowledge of the classifier to craft examples. In response, a few defense methods were proposed: retraining target classifiers with adversarial examples called adversarial training (Szegedy et al., 2013; Goodfellow et al., 2014b); suppressing gradient by retraining with soft labels called defensive distillation (Papernot et al., 2016); hardening target classifiers by training with an ensemble of adversarial examples (Trame`r et al., 2017).
In this paper we focus on whitebox attacks, that is, the models and the parameters of the classifier are known to the attacker, which requires a more robust defense than simply relying on the secrecy of the parameters as a defense. When the classifier parameters are known to an attacker, existing attack methods are very successful at fooling the classifiers. Conversely, when the attack is known to the classifier, e.g., in the form of adversarial samples, the classifier can avert the attacks by retraining with adversarial training samples. This cat-and-mouse game of 1) attack based on the current classifier, and 2) retraining classifier using the current adversarial examples, can be repeated many times. To find a `hardened' classifier from such a procedure, we first propose a sensitivity-penalized optimization to find a solution for gradient-based attacks.
The attack methods described so far may be called rule- or formula-based, in that the recipe of generating adversarial examples is explicitly given. More recently, researchers started to consider training attacker neural networks to generate adversarial examples (Nguyen & Sinha, 2017; Baluja & Fischer, 2017). One advantage of such methods is that an adversarial example can be generated in
1

Under review as a conference paper at ICLR 2018
test time with a single feedforward pass, which makes it much faster than iterative or optimizationbased attack methods. More importantly, the attack networks have the flexibility to produce powerful adversarial patterns potentially beyond what rule-based attacks can do currently, giving rise to the machine-against-machine scenario of attack and defense.
To better understand the potentials and limits of the learning-based attacks and defenses, we introduce a game-theoretic framework for the adversarial attack problem. The game is played by an attacker and a defender/classifier 1, where the attacker tries to maximize the risk of the classification task by adding adversarial patterns under certain constraints (such as l0-norm bounds), and the defender/classifier try to adjust its parameters to minimize the same risk given the adversarial input. It is important to note that for adversarial attack problems, the performance of an attack or a defense cannot be measured in isolation, but only in pairs (attack, defense). This is because the effectiveness of an attack/defense depends on the defense/attack it is against. However, there is a natural notion of the best defense or attack in this scenario. Suppose one player moves first by choosing her parameters and the other player responds with the knowledge of the first player's move. This is an example of a leader-follower game (Bru¨ckner & Scheffer, 2011) for which there are two wellknown states, the minimax and the maximin solutions. We claim that the minimax and the maximin solutions provide the best worst-case defense and attack models, respectively, and we propose optimization procedures to empirically find those models. The contributions of this paper is summarized as follows.
· We propose a sensitivity-penalized optimization to defend against gradient-based attacks. · We demonstrate the effectiveness of learning-based attacks compared to rule-based attacks. · We provide a game framework to model the machine-against-machine scenario which is
more general than previous approaches assuming either a fixed attack or defense. · We present inherent properties of rational attacks and defenses (Lemma 1) that give us the
range of the possible loss values, which also can be measure empirically. · We provide a simple algorithm (Alg. 1) to find the minimax and the maximin solutions.
The proposed methods are demonstrated with MNIST and CIFAR-10 datasets. For readability, details about experimental settings and the results with CIFAR-10 are presented in the appendix. To facilitate the replication of the paper, all the codes used to generate all the results in this paper will appear in a public code repository.
2 RELATED WORK
The problem of making a classifier robust to test-time adversarial attacks has been studied before (Lanckriet et al., 2002; Dalvi et al., 2004; Globerson & Roweis, 2006) using classifiers such as linear (kernel) hyperplanes, naive Bayes and SVM, which also show the game-theoretic nature of the robust classification problems. Since the recent discovery of adversarial examples for deep neural networks, several methods of generating adversarial samples were proposed (Szegedy et al., 2013; Goodfellow et al., 2014b; Moosavi-Dezfooli et al., 2016; Carlini & Wagner, 2017), as well as several methods of defense (Szegedy et al., 2013; Goodfellow et al., 2014b; Papernot et al., 2016; Trame`r et al., 2017). These papers consider a static scenario, where the attack or the defense is constructed against a fixed opponent.
The idea of using neural networks to generate adversarial samples has appeared concurrently (Baluja & Fischer, 2017; Nguyen & Sinha, 2017). Similar to our paper, the two papers demonstrates that it is possible to generate strong adversarial samples by a learning approach. Baluja & Fischer (2017) explored different architectures for the "adversarial transformation networks" against several different classifiers. Nguyen & Sinha (2017) proposed "attack learning neural networks" to map clean samples to a region in the feature space where misclassification occurs and "defense learning neural networks" to map them back to the safe region. Instead of prepending the defense layers before the fixed classifier, we retrain the whole classifier as a defense method. However, the key difference of our work to the two papers is that we consider the dynamics of learning-based defense stacked with learning-based attacks, and the natural notion of an optimal defense/attack.
1The classifier and the defender are treated as the same in this paper.
2

Under review as a conference paper at ICLR 2018

The alternating gradient-descent method for finding an equilibrium of a game has gained renewed interest through the problem of Generative Adversarial Networks (GAN) (Goodfellow et al., 2014a). However, the instability of alternating descent has been known, and an "unrolling" method (Metz et al., 2016) was proposed to training GANs. Our optimization algorithm has a similarity with the unrolling method, but it is simpler (corresponding to a single-step unrolling) and involves a gradient-norm regularization which can be interpreted intuitively as sensitivity penalization. Lastly, the framework of minimax risks was also studied in Hamm (2016) for the purpose of privacy preservation. We propose a different algorithm in this paper, but we also show that the attack on classification and the attack on privacy are the two sides of the same optimization problem with the opposite cost.

3 CAT AND MOUSE GAME

A classifier whose parameters are known to an attacker is easy to attack. Conversely, an attacker whose methods are known to a classifier is easy to defend from. In this section, we demonstrate the cat-and-mouse nature of the interaction of the attacker and the classifier, using adversarial training (Adv train) as defense, and the fast gradient sign method (FGSM) (Goodfellow et al., 2014b)) or the iterative version (IFGSM) (Kurakin et al., 2016a)) as attack. Then we show that the convergence state, if any, can be found more efficiently by directly solving a sensitivity-penalized optimization problem.

3.1 A NAIVE APPROACH

Suppose g is a classifier g : X  Y f (ul(g(x), y) and l(g(x), y) is the loss function. The FGSM attack generates a perturbed sample z(x) given the clean sample x as follows:

z(x) = x +  xl(g(x), y) . xl(g(x), y) p

(1)

We use the l0 normalization in this paper, in which case z(x) = x +  sign(xl(g(x), y)). The clean input images are l0-normalized, that is, all pixel values are in [-1, 1]. It was argued that the use of true label y results in "label leaking" (Kurakin et al., 2016b), but we use the true label in the
paper.

For another example, the IFGSM attack iteratively refines the adversarial examples by the following

updates

zi+1 = clipx,(zi +  sign(xl(g(x), y)),

(2)

where the clipping used in this paper is clipx,(x ) = min{1, x + , max{-1, x - , x }}.

Existing attack methods such as FGSM and IFGSM are very effective at fooling the classifier. Ta-
ble 1 shows that all methods are able to perfectly fool the original undefended classifier in the range we tested ( = 0.3 - 0.6).

Defense\Attack

No attack

=0.3

FGSM =0.4 =0.5

=0.6

=0.3

IFGSM =0.4 =0.5

=0.6

No defense

0.006 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000

Table 1: Error rates of FGSM and IFGSM attacks on the original classifier for MNIST. These attacks can cause perfect misclassifications for the given range of .

On the other hand, these attacks, if known to the classifier, can be avoided successfully by defense methods such as retraining the classifier with the original dataset augmented by adversarial examples with ground-truth labels, known as adversarial training. We use the 1 : 1 mixture of the clean and the adversarial datatsets during training. Table 2 shows the results of adversarial training for different attacks. The test error rates for adversarial test examples after training become below 1% indicating near-perfect avoidance. This is in stark contrast with the near-perfect misclassification of the undefended classifier in Table 1.
A question arises regarding what would happen if the procedure of 1) attacking the current classifier, and 2) retraining the classifier using the current attack are repeated for a number of rounds. The

3

Under review as a conference paper at ICLR 2018

Defense\Attack

No attack

=0.3

FGSM =0.4 =0.5

=0.6

=0.3

IFGSM =0.4 =0.5

=0.6

Adv train

n/a 0.004 0.003 0.003 0.005 0.003 0.003 0.004 0.010

Table 2: Error rates of FGSM and IFGSM attacks on the adversarially-trained classifiers for MNIST. This defense can avert the attacks and achieve the error rates of the no-attack case.

answer to this cat-and-mouse game is easy to experiment although time-consuming. Let's denote the attack on the original classifier as FGSM1, and the corresponding retrained classifier as Adv FGSM1. Repeating the procedure above generates FGSM1  Adv FGSM1  FGSM2  Adv FGSM2, etc. Fig. 1 shows one such trial with 80 + 80 rounds of the procedure. Initially, the attacker achieves near-perfect attacks (i.e., error rate 1), and the defender achieves near-perfect defense (i.e., error rate 0). As the iteration increases, the attacker becomes weaker with error rate 0.4, but the defense is still very successful, and the rate seems to oscillate persistently. While we can run more iterations to see if it converges, this is not a very principled nor efficient approach to find an equilibrium.
000001......4608200 20 40 60 80 100 120 140 160

Figure 1: Cat and mouse game of FGSM attacks and adversarial training for MNIST. The upper red points are the error rates after adversarial training, and the lower green points are the error rates after FGSM attack. After 160 iterations ( = 0.3), the error rates are still oscillating between 0 and 0.5.

3.2 GRADIENT-BASED ATTACKS AND SENSITIVITY PENALTY

We can perform the cat-and-mouse simulation more efficiently by an optimization approach. To

emphasize the parameters u of the classifier/defender g(x; u), let's rewrite the loss of the perturbed

data as

f (u, z) = l(g(z(x); u), y),

(3)

where z(x) denote the FGSM attack method

l(g(z; u), y)

z(x) = x + 

.

z

(4)

With the abuse of the notation, here z means the perturbed input to the classifier/defender. In expectation of the FSGM output, the defender should choose u to minimize f (u, z(u)) where the dependence of the FGSM on the classifier u is expressed explicitly. If we use the gradient descent

u  u -  df (u, z) du

to

minimize

f (u, z(u)),

then

from

the

chain

rule,

the

total

derivative

df du

is

df f (u, z) z f (u, z) f (u, z) 2f (u, z) f (u, z)

=+

= +

,

du u u z u zu z

from (3) and (4).

(5) (6)

Interestingly, this total derivative (6) at the current state coincides with the derivative /u of the

following cost

 f (u, z) 2

fsens(u, z) = f (u, z) + 2

z

.

(7)

4

Under review as a conference paper at ICLR 2018

There are two implications. Interpretation-wise, this cost function is a sum of the original loss f and the `sensitivity' term f /z which penalizes the abrupt change of the loss w.r.t. the input. Therefore, u is chosen at each iteration to make the network insensitive to the small perturbation of inputs. The idea of minimizing the sensitivity to input is a familiar approach in robustifying the classifier (Lyu et al., 2015). Secondly, it can be implemented easily. The gradient descent update using the seeming complicated gradient (6) can be replaced by the gradient descent update of (7). The capability of automatic differentiation (Rall, 1981) in modern machine learning libraries can be used to compute the gradient of (7) efficiently. Using this direct approach, we can find the defense parameters u which will be robust to the FSGM attacks. Fig. 2 shows the decrease of test error during training using the this gradient descent approach for MNIST. It only takes a very small fraction of time to reach the finial state of the Fig. 2 compared to that of Fig. 1.

1.0 0.8 0.6 0.4 0.2 0.0 0

=0.3 1.0 0.8 0.6 0.4 0.2 2000 4000 6000 8000 10000 0.0 0

=0.4 1.0 0.8 0.6 0.4 0.2 2000 4000 6000 8000 10000 0.0 0

=0.5 1.0 0.8 0.6 0.4 0.2 2000 4000 6000 8000 10000 0.0 0

=0.6
2000 4000 6000 8000 10000

Figure 2: Convergence of test error rates during adversarial training against FSGM for MNIST, using the sensitivity-penalized optimization (7).

There is also an important difference between the solution of the cat-and-mouse game and the minimizer of (7). Table 3 shows that the adversarially trained classifier (Adv FGSM1) works well for clean data and FGSM1 attack, but is susceptible to FGSM2 attack, displaying the cat-and-mouse nature. The same holds for Adv FGSM2, Adv FGSM3, etc. After a number of the cat-and-mouse procedure, the classifier Adv FGSM80 is robust to FGSM80 as well as moderately robust to other attacks including FGSM81 (=FGSM-curr). However, the resultant classifier from the direct minimization of (7) (Sens FGSM) is even more robust than Adv FGSM80 and is overall the best.

=0.3 =0.4 =0.5 =0.6

Defense\Attack
No defense Adv FGSM1 Adv FGSM2 Adv FGSM80 Sens FGSM No defense Adv FGSM1 Adv FGSM2 Adv FGSM80 Sens FGSM No defense Adv FGSM1 Adv FGSM2 Adv FGSM80 Sens FGSM No defense: Adv FGSM1 Adv FGSM2 Adv FGSM80 Sens FGSM

No attack
0.006 0.012 0.012 0.009 0.011 0.006 0.012 0.011 0.009 0.011 0.006 0.012 0.010 0.008 0.010 0.006 0.014 0.011 0.009 0.009

FGSM1 1.000 0.004 0.999 0.335 0.038 1.000 0.003 0.983 0.509 0.087 1.000 0.003 0.947 0.653 0.058 1.000 0.005 0.784 0.439 0.094

FGSM FGSM2 · · ·
0.881 · · · 0.995 · · · 0.002 · · · 0.273 · · · 0.235 · · ·
0.953 · · · 0.987 · · · 0.002 · · · 0.523 · · · 0.217 · · ·
0.981 · · · 0.985 · · · 0.004 · · · 0.654 · · · 0.317 · · ·
0.980 · · · 0.966 · · · 0.004 · · · 0.521 · · · 0.248 · · ·

FGSM80 0.355 0.499 0.505 0.009 0.252 0.528 0.584 0.535 0.024 0.435 0.662 0.692 0.642 0.024 0.489 0.804 0.778 0.747 0.020 0.653

FGSM-curr
1.000 0.995 0.995 0.442 0.045 1.000 0.987 0.982 0.131 0.045 1.000 0.985 0.979 0.089 0.038 1.000 0.966 0.945 0.021 0.046

Table 3: Error rates of different attacks on various adversarially-trained classifiers for MNIST. FGSM-curr means the FGSM attack on the specific classifier on the left. Adv FGSM is the classifier adversally trained with FGSM attacks. Sens FGSM is the result of minimizing (7) by the gradient descent (5).

5

Under review as a conference paper at ICLR 2018

4 GAME FORMULATION

In this section, we extend the attacker to an arbitrary flexible class of functions such as deep neural networks, and present a game framework to define two equilibria ­ the minimax and the maximin solutions. We present algorithms that generalize the approach presented in the previous section.

4.1 LEARNING-BASED ATTACK

The attacker z(x) can be more general than a specific attacks with known recipes. We propose to consider a flexible class of functions z(x; v)  Z with its own parameters v such as network weights and biases, instead of being a known function of the classifier parameters u only. Suppose now f (u, v) is the empirical risk of the (classifer u, attackerv) pair. The attacker network, as we call it, can be trained by gradient descent as well. Given a classifier u, we can use the gradient descent

v  v +  f (u, v) v
to find an optimal attacker v that maximizes the loss f .

(8)

The attack network we consider here is a three-layer fully-connected network with 300 hiddens units per layer. Different from Nguyen & Sinha (2017) or Baluja & Fischer (2017), we feed the label y into the input of the network along with the features x. This is analogous to using the true label y in the original version of FGSM. While this label input is optional but it can make the training of the attacker network easier.

Table 4 compare the error rates of the FGSM attacks and the attack network (AttNet). The table shows that AttNet is better than or comparable to FGSM in all cases. In particular, FGSM attack is not effective anymore against the classifier `hardened' by the cat-and-mouse repetition (Adv FGSM80) or by the sensitivity minimization (Sens FGSM) methods incurring relatively a low error. In contrast, AttNet can incur significant error (> 0.9) for the hardened defenders. This demonstrates that the learning-based attacks have a potential to exploit the knowledge of the classifier and generate stronger adversarial examples than fixed-type attack methods.

Defense\Attack
No defense Adv FGSM1 Adv FGSM80 Sens FGSM
No defense Adv FGSM1 Adv FGSM80 Sens FGSM

No Attack
0.006 0.011 0.008 0.010
0.006 0.012 0.008 0.010

FGSM-curr AttNet-curr =0.3

1.000

1.000

0.996

1.000

0.473

0.911

0.044

0.999

=0.5

1.000

1.000

0.985

1.000

0.089

0.897

0.038

1.000

FGSM-curr AttNet-curr =0.4

1.000

1.000

0.984

1.000

0.131

0.903

0.045

0.902

=0.6

1.000

1.000

0.966

1.000

0.021

0.897

0.046

0.902

Table 4: Error rates of FGSM vs learning-based attack network (AttNet) on various adversariallytrained classifiers for MNIST. FGSM-curr/AttNet-curr means they are computed/trained for the specific classifier on the leftmost column. Note that FGSM fails to attack against the `hardened' networks (Adv FGSM80 and Sens FGSM), but AttNet can still attack them successfully.

4.2 MINIMAX GAME FOR LEARNING-BASED ATTACKS

Knowing the current classifier u, an optimal attacker is one that maximizes the loss f (u, v)

max f (u, v).
v

(9)

Consequently, the defender should choose the classifier parameters u such that the maximum loss is

minimized

u arg min max f (u, v) = arg min f (u, v(u)),

uv

u

(10)

6

Under review as a conference paper at ICLR 2018

where v(u) is the maximizer function

v(u) arg max f (u, v).
v

(11)

This solution to the continuous minimax problem has a natural interpretation as the best worst-case solution. Assuming the attacker is rational, i.e., it chooses the best attack from (11) given u, no other defense has a lower loss than the minimax defense u in (10). The minimax defense is also a conservative defense. If the attacker is not optimal, and/or if the attack does not know the defense u exactly (as in blackbox attacks), the actual loss can be lower than what the minimax solution f (u, v(u)) predicts. Before proceeding further, we point out that the claims above apply to the global minimizer u and the maximizer function v(·), but we can only find local solutions in practice for complex loss functions such as deep networks-based defenders and attackers.

To solve (10), we analyze the problem similarly to (5)-(7) from the previous section. At each iteration, the defender should choose u in expectation of the attack to minimize f (u, v(u)). We use the

gradient descent

u  u -  df (u, v) , du

(12)

where

the

total

derivative

df du

is

df f (u, v(u)) v(u) f (u, v)

=+

.

du u

u v

(13)

Since the maximizer v(u) is difficult to find exactly, v may also be updated incrementally by a few

steps of gradient ascent

v  v +  f (u, v) . v

(14)

The resulting formulation is closely related to the unrolled optimization (Metz et al., 2016) proposed

for training GANs, although the latter has a very different cost function f . Using the single update

(14), the total derivate is

df f (u, v(u)) 2f (u, v) f (u, v)

= +

.

du u

uv v

(15)

Similar to the sensitivity penalization for FGSM (7), the gradient update of u at each iteration can be done the gradient descent on the following function with automatic differentiation

 f (u, v) 2

fsens(u, v) = f (u, v) + 2

v

.

The algorithm is summarized in Alg. 1. Note that this algorithm is actually independent of the adversarial example problem, and may be used for other minimax problems as well.

Algorithm 1 Minimax Optimization by Sensitivity Minimization

Input: loss f (u, v), # of iterations N , learning rates (i) and (i) Output: (u, v(u)) Initialize u0, v0 Begin

for i = 1, ... , N do

Max step:

vi

= vi-1 + i

f (ui-1,vi-1) v

Min

step:

ui

=

ui-1

-

i

d du

f (ui-1, vi-1)

+

 2

.f (ui-1,vi-1) 2
v

end for Return (uN , vN ).

4.3 MINIMAX VS MAXIMIN PROBLEMS

In analogy with the minimax problem, we can also consider the maximin solution defined by

v arg max min f (u, v) = arg max f (u(v), v).

vu

v

(16)

7

Under review as a conference paper at ICLR 2018

where

u(v) arg min f (u, v)
u

(17)

is the minimizer function with the abuse of the notations for the values u, v and the functions u(·), v(·). Similar to the minimax solution, it has an intuitive meaning. The maximin solution is

the best worst-case solution for the attacker. Assuming the defender is rational, i.e., it chooses the

best defense from (17)that minimizes the loss f (u, v) given the attack v, no other attack can inflict a higher loss than the maximin attack v. It is also a conservative attack. If the defender is not

optimal, and/or if the defender does not know the attack v exactly, the actual loss can be higher than what the solution f (u(v), v) predicts.

To summarize, rational defenses and attacks have the following inherent properties. Lemma 1. Let u, v(u), v, u(v) be the solutions of (10),(11),(16),(17).

1. f (u, v(u))  f (u, v): For any given defense u, the max attack v(u) is the most effective attack.

2. f (u, v(u))  f (u, v(u)): Against the rational attack v(u), the minimax defense u is the most effective defense.

3. f (u(v), v)  f (u, v): For any given attack v, the min defense u(v) is the most effective defense.

4. f (u(v), v)  f (u(v), v): For the min defense u(v), the minimax attack v is the most effective attack.

5. maxv minu f (u, v)  minu maxv f (u, v): The loss of the best worst-case defense is higher than that of the best worst-case attack.

The proof follows from the definition and is omitted. The lemma helps us to better understand the dependence of defense and attack, and gives us the range of the possible loss values, which can be measure empirically.

To find a maximin solution, we can use the same algorithm (Alg. 1) except that the variables u and

v are switched and the sign of f is flipped before the algorithm is called. In addition to minimax

and maximin optimization, we also consider the alternating optimization used in Goodfellow et al.

(2014a):

u  u - f , v  v + f . u v

(18)

We show the difference of the three optimization algorithms minimax, maximin, alternating optimization applied to a common defense/attack network. Fig. 7) shows the test error over the course of optimization starting from random initializations. When  is small (top left), the three algorithms converge to a near-zero value. When  is larger, one can see that three solutions converge to different values.
After convergence, the three defense-attack pairs give us the three solutions: minimax (u, v(u)), alt (ualt, valt), and maximin (u(v), v). In Table 5 we compare how the three defense types fare against three attack types. The five properties in Lemma 1 can be verified from the table.

1. For any given defense u, the max attack v(u) is the most effective attack: For each , the first row shows f (1st row, 1st col)> f (1st row, 2nd or 3rd col) (e.g., 0.032 > 0.019 and 0.014 at  = 0.3.)
2. Against the rational attack v(u), the minimax defense u is the most effective defense: For each , the minimax error f (first row, first column) is smaller than any error incurred by a fixed defense against AttNet from Table 4 (e.g., 0.032 < 1.000, 1.000, 0.911 and 0.999 at  = 0.3).
3. For any given attack v, the min defense u(v) is the most effective defense: For each , the third row shows that f (3rd row, 3rd col) < f (1st or 2nd row, 3rd col) (e.g., 0.008 < 0.014 and 0.014 at  = 0.3.)

8

Under review as a conference paper at ICLR 2018

1.0 0.8 0.6 0.4 0.2 0.0 0 1.0 0.8 0.6 0.4 0.2 0.0 0

=0.3
minimax alt maximin
=0.510000 20000 30000 40000 50000 60000 70000
minimax alt maximin
10000 20000 30000 40000 50000 60000 70000

1.0 0.8 0.6 0.4 0.2 0.0 0 1.0 0.8 0.6 0.4 0.2 0.0 0

=0.4
minimax alt maximin
=0.610000 20000 30000 40000 50000 60000 70000
minimax alt maximin
10000 20000 30000 40000 50000 60000 70000

Figure 3: Convergence of the test error rates for minimax optimization (blue), alternating optimization (orange), and maximin optimization (green) for MNIST.

4. For the min defense u(v), the minimax attack v is the most effective attack: For each , the maximin error f (3rd row, 3rd column) is larger than any error incurred by FGSM or IFGSM against adversarially-trained defense from Table 2 (e.g., 0.008 > 0.004 (FSGM) and 0.003 (IFGSM) at  = 0.3.)
5. The loss of the best worst-case defense is higher than that of the best worst-case attack: For each , f (1st row, 1st col) > f (3rd row, 3rd col) (e.g., 0.032 > 0.008 at  = 0.3.)

Defense\Attack
minimax alt
maximin
minimax alt
maximin

minimax
0.032 0.037 0.477
0.544 0.466 0.843

alt =0.3
0.019 0.017 0.373 =0.5 0.387 0.331 0.767

maximin
0.014 0.014 0.008
0.105 0.116 0.003

minimax
0.078 0.065 0.713
0.555 0.468 0.792

alt =0.4
0.040 0.020 0.416 =0.6 0.387 0.406 0.656

maximin
0.033 0.023 0.005
0.114 0.121 0.003

Table 5: Error rates of the three attack methods { minimax (v(u)), alt (valt), maximin (v)} against the three defense methods { minimax (u), alt (ualt), maximin (u(v))} for MNIST.

Lastly, the adversarial examples generated by various attacks in the paper have diverse patterns and are shown in Fig. 4 of the appendix.

5 DISCUSSIONS
5.1 LIMITS OF LEARNING-BASED ATTACKS AND EXTENSIONS
We discuss possible misconceptions about the framework and also propose an extension. The minimax or the maximin solution is defined for given classes of defense and attack, e.g., a neural network of a fixed architecture, and therefore the minimax solution from a particular network class is no longer the solution for another network class, and the two minimax solutions are not directly comparable. Also, the three-layer attack network used in the paper outperforms FSGM for MNIST data but not always for CIFAR-10 data (Table 9 in the appendix), likely due to the increased complexity of training. It remains to perform more experiments with other classifier types and attacker types.
On the other hand, it is possible to build a better defense against such learning-based attacks and also other known attacks such as FGSM, IFGSM, or CW (Carlini & Wagner, 2017), simultaneously. Suppose z1(u), ..., zm(u) are m different types of fixed-rule attacks, e.g., z1=FGSM, z2=IFGSM,

9

Under review as a conference paper at ICLR 2018

z3=CW, etc. The minimax defense in this case is to solve the mixed continuous-discrete problem

min max{f (u, z1(u)), ..., f (u, zm(u))}.
u

(19)

Additionally, suppose zm+1(u, v), ..., zm+n(u, v) are n different types of learning-based attacks, e.g., zm+1=2-layer dense net, zm+2=5-layer convolutional nets, etc. The minimax defense against the mixture of multiple fixed-rule and learning-based attacks can be found by solving

min
u

max{f

(u,

z1(u)),

...

,

f

(u,

zm(u)),

max
v

f

(u,

zm+1

(u,

v)),

...

,

max
v

f

(u,

zm+n(u,

v)}.

(20)

Due to the huge computational demand to solve (20), we leave this experiment as future work.

5.2 ADVERSARIAL EXAMPLES AND PRIVACY ATTACKS
Lastly, we discuss a bigger picture of the game between adversarial players. The minimax optimization arises in the leader-follower game (Bru¨ckner & Scheffer, 2011) with the constant sum constraint. The leader-follower setting makes sense because the defense (=classifier parameters) is often public knowledge and the attacker exploits the knowledge. Interestingly, the problem of the attack on privacy (Hamm, 2016) has a very similar formulation as the adversarial attack problem, different only in that the classifier is an attacker and the data perturbator is a defender. In the problem of privacy preservation against inference, the defender is a data transformer z(x) (parameterized by u) which perturbs the raw data, and the attacker is a classifier (parameterized by v) who tries to extract sensitive information such as identity from the perturbed data such as online activity. The transformer is the leader, such as when the privacy mechanism is public knowledge, and the classifier is the follow as it attacks the given perturbed data. The risk for the defender is therefore the accuracy of the inference of sensitive information measured by -E[l(z(x; u), y; v)]. Solving the minimax risk problem (minu maxv -E[l(z(x; u), y; v)]) gives us the best worst-case defense when the classifier/attacker knows the transformer/defender parameters, which therefore gives us a robust data transformer to preserve the privacy against inference attacks. On the other hand, solving the maximin risk problem (maxv minu -E[l(z(x; u), y; v)]) gives us the best worst-case classifier/attacker when its parameters are known to the transformer. In fact, the problems of adversarial attack and privacy attack are two sides of the same coin which can be addressed by similar models and optimizers.

6 CONCLUSION
In this paper, we show that the game formulation of adversarial attacks and defenses are natural and perhaps necessary. As demonstrated with the MNIST dataset, it is easy to attack a fixed defense and also easy to defend against a fixed attack. The real problem is more difficult since one player can choose her action in expectation of the other player. The paper not only provides conceptual tool to model the interaction of attacks and defense, but also practical algorithms that can find feasible attacks and defenses, which are theoretically the best worst-case options given the class of attack/defense models.

REFERENCES
Shumeet Baluja and Ian Fischer. Adversarial transformation networks: Learning to generate adversarial examples. arXiv preprint arXiv:1703.09387, 2017.
Michael Bru¨ckner and Tobias Scheffer. Stackelberg games for adversarial prediction problems. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 547­555. ACM, 2011.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Security and Privacy (SP), 2017 IEEE Symposium on, pp. 39­57. IEEE, 2017.
Nilesh Dalvi, Pedro Domingos, Sumit Sanghai, Deepak Verma, et al. Adversarial classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 99­108. ACM, 2004.
Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Analysis of classifiers' robustness to adversarial perturbations. arXiv preprint arXiv:1502.02590, 2015.

10

Under review as a conference paper at ICLR 2018
Amir Globerson and Sam Roweis. Nightmare at test time: robust learning by feature deletion. In Proceedings of the 23rd international conference on Machine learning, pp. 353­360. ACM, 2006.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672­2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014b.
Jihun Hamm. Minimax filter: Learning to preserve privacy from inference attacks. arXiv preprint arXiv:1610.03577, 2016.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016a.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint arXiv:1611.01236, 2016b.
Gert RG Lanckriet, Laurent El Ghaoui, Chiranjib Bhattacharyya, and Michael I Jordan. A robust minimax approach to classification. Journal of Machine Learning Research, 3(Dec):555­582, 2002.
Chunchuan Lyu, Kaizhu Huang, and Hai-Ning Liang. A unified gradient regularization family for adversarial examples. In Data Mining (ICDM), 2015 IEEE International Conference on, pp. 301­309. IEEE, 2015.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. arXiv preprint arXiv:1610.08401, 2016.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard, and Stefano Soatto. Analysis of universal adversarial perturbations. arXiv preprint arXiv:1705.09554, 2017.
Linh Nguyen and Arunesh Sinha. A learning approach to secure learning. arXiv preprint arXiv:1709.04447, 2017.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Security and Privacy (SP), 2016 IEEE Symposium on, pp. 582­597. IEEE, 2016.
Louis B Rall. Automatic differentiation: Techniques and applications. 1981. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. Florian Trame`r, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble adversarial
training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.
11

Under review as a conference paper at ICLR 2018
A RESULTS WITH MNIST
The architecture of the MNIST classifier is similar to the Tensorflow model 2, and is trained with the following hyperparameters: Batch size = 128, learning rate = AdamOptimizer with 10-3, total # of iterations=50,000. The attack network has three hidden fully-connected layers of 300 units, trained with the following hyperparameters: Batch size = 128, dropout rate = 0.5, learning rate = AdamOptimizer with 10-4, total # of iterations=30,000. For minimax, alt, and maximin optimization, the total number of iteration was 70000.
a. b. c. d. e. f. g.
Figure 4: Adversarial samples generated from different attacks at  = 0.2. (a) Original data (b) FGSM1 (c) FSGM80 (d) IFGSM1 (e) Minimax (f) Alt (g) Maximin. Note the diversity of patterns.
B RESULTS WITH CIFAR-10
We preprocess the CIFAR-10 dataset by removing the mean and normalizing the pixel values with the standard deviation of all pixels in the image. It is followed by clipping the values to ±2 standard deviations and rescaling to [-1, 1]. The architecture of the CIFAR classifier is similar to the Tensorflow model 3 but is simplified further by removing the local response normalization layers. With the simple structure, we attained  79% accuracy with the test data. The classifier is trained with the following hyperparameters: Batch size = 128, learning rate = AdamOptimizer with 10-3, total # of iterations=100,000. The attack network has three hidden fully-connected layers of 300 units, trained with the following hyperparameters: Batch size = 128, dropout rate = 0.5, learning rate = AdamOptimizer with 10-4, total # of iterations=30,000. For minimax, alt, and maximin optimization, the total number of iteration was 70,000. In the rest of the appendix, we repeat all the experiments with the MNIST dataset using the CIFAR10 dataset.
2https://github.com/tensorflow/models/tree/master/tutorials/image/mnist 3https://github.com/tensorflow/models/tree/master/tutorials/image/ cifar10
12

Under review as a conference paper at ICLR 2018

Defense\Attack

No attack

=0.1

FGSM =0.2 =0.3

=0.4

=0.1

IFGSM =0.2 =0.3

=0.4

No defense

0.222 0.976 0.825 0.869 0.884 0.668 0.907 0.959 0.971

Table 6: Error rates of FGSM and IFGSM attacks on the original classifier for cifar10. These attacks can cause large misclassification for the given range of .

Defense\Attack

No attack

=0.1

FGSM =0.2 =0.3

=0.4

=0.1

IFGSM =0.2 =0.3

=0.4

Adv train

n/a 0.485 0.589 0.668 0.650 0.446 0.371 0.645 0.414

Table 7: Error rates of FGSM and IFGSM attacks on the adversarially-trained classifiers for CIFAR10. This defense can significantly lower the errors from the attacks, although not as well as the MNIST case.

1.0 0.5 0.0 0

20 40 60 80 100 120 140 160

Figure 5: Cat and mouse game of FGSM attacks and adversarial training for CIFAR-10. The upper green points are the error rates after adversarial training, and the lower orange points are the error rates after FGSM attack. After 160 iterations ( = 0.3), the error rates are still oscillating.

1.0 =0.1

1.0 =0.2

1.0 =0.3

1.0 =0.4

0.9 0.8

0.8

0.8

0.8

0.7 0.6 0.5

0.6 0.4

0.6 0.4

0.6 0.4

0.4 0.3

0.2

0.2

0.2

0.20 5000 10000 15000 20000 25000 30000 0.00 5000 10000 15000 20000 25000 30000 0.00 5000 10000 15000 20000 25000 30000 0.00 5000 10000 15000 20000 25000 30000

Figure 6: Convergence of test error rates during adversarial training against FSGM for CIFAR-10, using the sensitivity-penalized optimization (7).

1.0

=0.1
minimax

0.8 alt

0.6 maximin

0.4

0.2

0.0 0 1.0

=0.310000 20000 30000 40000 50000 60000 70000

0.8

0.6

0.4 minimax

0.2

alt maximin

0.0 0

10000 20000 30000 40000 50000 60000 70000

1.0

=0.2
minimax

0.8 alt

0.6 maximin

0.4

0.2

0.0 0 1.0

=0.410000 20000 30000 40000 50000 60000 70000

0.8

0.6

0.4 minimax

0.2

alt maximin

0.0 0

10000 20000 30000 40000 50000 60000 70000

Figure 7: Convergence of the test error rates for minimax optimization (blue), alternating optimization (orange), and maximin optimization (green) for CIFAR-10.

13

Under review as a conference paper at ICLR 2018

=0.1 =0.2 =0.3 =0.4

Defense\Attack
No defense Adv FGSM1 Adv FGSM2 Adv FGSM80 Sens FGSM No defense Adv FGSM1 Adv FGSM2 Adv FGSM80 Sens FGSM No defense Adv FGSM1 Adv FGSM2 Adv FGSM80 Sens FGSM No defense Adv FGSM1 Adv FGSM2 Adv FGSM80 Sens FGSM

No attack
0.222 0.310 0.258 0.228 0.227 0.222 0.220 0.305 0.218 0.213 0.222 0.211 0.205 0.223 0.204 0.222 0.211 0.206 0.225 0.204

FGSM-1 0.976 0.485 0.640 0.644 0.608 0.825 0.589 0.579 0.445 0.567 0.869 0.668 0.499 0.471 0.642 0.884 0.650 0.592 0.497 0.665

FGSM FGSM-2 · · ·
0.671 · · · 0.758 · · · 0.484 · · · 0.529 · · · 0.591 · · ·
0.692 · · · 0.739 · · · 0.290 · · · 0.502 · · · 0.633 · · ·
0.891 · · · 0.707 · · · 0.407 · · · 0.324 · · · 0.592 · · ·
0.899 · · · 0.691 · · · 0.546 · · · 0.385 · · · 0.627 · · ·

FGSM-80 0.595 0.510 0.612 0.087 0.560 0.818 0.623 0.599 0.078 0.551 0.877 0.707 0.514 0.081 0.651 0.891 0.700 0.618 0.121 0.680

FGSM-curr
0.976 0.967 0.708 0.086 0.465 0.969 0.699 0.557 0.078 0.417 0.955 0.720 0.389 0.084 0.582 0.941 0.694 0.545 0.124 0.604

Table 8: Error rates of different attacks on various adversarially-trained classifiers for CIFAR-10. FGSM-curr means the FGSM attack on the specific classifier on the leftmost column. Adv FGSM is the classifier adversally trained with FGSM attacks. Sens FGSM is the result of minimizing the sensitivity penalty (7).

Defense\Attack
No defense Adv FGSM1 Adv FGSM80 Sens FGSM
No defense Adv FGSM1 Adv FGSM80 Sens FGSM

No Attack
0.318 0.310 0.228 0.227
0.318 0.210 0.224 0.218

FGSM-curr AttNet-curr =0.1

0.985

0.763

0.967

0.461

0.086

1.000

0.465

0.985

=0.3

0.945

0.900

0.581

0.900

0.084

1.000

0.279

1.000

FGSM-curr AttNet-curr =0.2

0.967

1.000

0.699

0.995

0.078

1.000

0.417

1.000

=0.4

0.931

1.000

0.702

0.900

0.124

0.900

0.300

1.000

Table 9: Error rates of FGSM vs learning-based attack network (AttNet) on various adversariallytrained classifiers for CIFAR-10. FGSM-curr/AttNet-curr means they are computed/trained for the specific classifier on the leftmost column. Note that FGSM fails to attack against the `hardened' networks (Adv FGSM80 and Sens FGSM), but AttNet can still attack them successfully.

Defense\Attack
minimax alt
maximin
minimax alt
maximin

minimax
0.356 0.330 0.486
0.611 0.555 0.772

alt =0.1
0.336 0.331 0.475 =0.3 0.536 0.441 0.722

maximin
0.291 0.278 0.225
0.461 0.398 0.261

minimax
0.493 0.441 0.688
0.671 0.641 0.788

alt =0.2
0.459 0.411 0.645 =0.4 0.594 0.431 0.702

maximin
0.380 0.330 0.261
0.519 0.405 0.270

Table 10: Error rates of the three attack methods { minimax (v(u)), alt (valt), maximin (v)} against the three defense methods { minimax (u), alt (ualt), maximin (u(v))} for CIFAR-10.

14

