Under review as a conference paper at ICLR 2018
SAMPLE-EFFICIENT DEEP REINFORCEMENT LEARNING VIA EPISODIC BACKWARD UPDATE
Anonymous authors Paper under double-blind review
ABSTRACT
We propose Episodic Backward Update - a new algorithm to boost the performance of a deep reinforcement learning agent by fast reward propagation. In contrast to the conventional use of the replay memory with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state into its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate effectively throughout the sampled episode. We evaluate our algorithm on 2D MNIST Maze Environment and 49 games of the Atari 2600 Environment and show that our agent improves sample efficiency with a competitive computational cost.
1 INTRODUCTION
Recently, deep reinforcement learning (RL) has been very successful in many complex environments such as the Arcade Learning Environment (Bellemare et al., 2013) and Go (Silver et al., 2016). Deep Q-Network (DQN) algorithm (Mnih et al., 2015) with the help of experience replay (Lin, 1992) enjoys more stable and sample-efficient learning process, so is able to achieve super-human performance on many tasks. Unlike simple online reinforcement learning, the use of experience replay with random sampling breaks down the strong ties between correlated transitions and also allows the transitions to be reused multiple times throughout the training process.
Although DQN has shown impressive performances, it is still impractical in terms of data efficiency. To achieve a human-level performance in the Arcade Learning Environment, DQN requires 200 million frames of experience for training, which is approximately 39 days of game play in real time. Remind that it usually takes no more than a couple of hours for a skilled human player to get used to such games. So we notice that there is still a tremendous amount of gap between the learning process of humans and that of deep reinforcement learning agents. This problem is even more crucial in environments such as autonomous driving, where we cannot risk many trials and errors due to the high cost of samples.
One of the reasons why the DQN agent suffers from such low sample efficiency could be the sampling method over the replay memory. In many practical problems, the agent observes sparse and delayed reward signals. There are two problems when we sample one-step transitions uniformly at random from the replay memory. First, we have a very low chance of sampling the transitions with rewards for its sparsity. The transitions with rewards should always be updated, otherwise the agent cannot figure out which action maximizes its expected return in such situations. Second, there is no point in updating a one-step transition unless the future transitions have not been updated yet. Without the future reward signals propagated, the sampled transition will always be trained to return a zero value.
In this work, we propose Episodic Backward Update (EBU) to come up with solutions for such problems. Our idea originates from a naive human strategy to solve such RL problems. When we observe an event, we scan through our memory and seek for another event that has led to the former one. This is how humans normally recognize cause and effect relationship. We can take a similar approach to train an RL agent. We can solve the first problem above by sampling transitions in an episodic manner. Then, we can be assured that at least one transition with non-zero reward is being updated. We can solve the second problem by updating transitions in a backward way in which the transitions were made. By then, we can perform an efficient reward propagation without any meaningless updates. This method faithfully follows the principle of dynamic programing.
1

Under review as a conference paper at ICLR 2018

We evaluate our update algorithm on 2D MNIST Maze Environment and the Arcade Learning Environment. We observe that our algorithm outperforms other baselines in many of the environments with a notable amount of performance boosts.

2 RELATED WORKS

Reinforcement learning deals with environments where an agent can make a sequence of actions and receive corresponding reward signals, such as Markov decision processes (MDPs). At time t, the agent encounters a state st and takes an action at  A, observes the next state st+1 and receives reward rt  R. The agent's goal is to set up a policy  to take a sequence of actions so that the agent can maximize its expected return, which is the expected value of the discounted sum of rewards E[ t trt].
Q-learning (Watkins, 1989) is one of the most widely used methods to solve such RL tasks. The key idea of Q-learning is to estimate the state-action value function Q(s, a), generally called as the Qfunction. The state-action value function may be characterized by the Bellman optimality equation

Q(st, a) = E[rt +  max Q(st+1, a )].
a

(1)

Given the state-action value function Q(s, a), the agent may perform the best action a = argmaxa Q(s, a) at each time step to maximize the expected return.
In practice, the state space is extremely large, so it is impractical to tabularize Q-values of all stateaction pairs. Deep Q-Network (Mnih et al., 2015) overcomes this issue by using deep neural networks to approximate the Q-function. Deep Q-Network (DQN) takes 2D representation of the state st as input. Then the information of the state st passes through a number of convolutional neural networks (CNNs) and fully connected networks, finally returns the Q-values of each action at at state st. In order to use the agent's training experience more efficiently, DQN adopts experience replay (Lin, 1992). After observing a transition (st, at, rt, st+1), the agent stores the transition into its replay buffer. In order to set up the target values, the agent samples transitions from the replay memory uniformly at random. This process breaks the temporal correlation between transitions and guarantees each transition being used multiple times during the training process.

There have been a variety of methods proposed to improve the performance of DQN in terms of stability, sample efficiency and runtime. Some methods propose new network architectures. The dueling network architecture (Wang et al., 2015) contains two streams of separate Q-networks to estimate the value functions and the advantage functions. Neural episodic control (Pritzel et al., 2017) and model free episodic control (Blundell et al., 2016) use episodic memory modules to estimate the state-action values.

Some methods tackle the uniform random sampling replay strategy of DQN. Prioritized experience replay (Schaul et al., 2016) assigns non-uniform probability for sampling transitions, where the probability is proportional to the temporal difference errors of each transition.

Another wave of methods tries to aggregate TD values with Monte-Carlo returns. Q() (Harutyunyan et al., 2016) and Retrace() (Munos et al., 2016) modify the target values to allow the on-policy samples to be used interchangeably for on-policy and off-policy learning, which ensures safe and efficient reward propagation. Count-Based exploration method combined with intrinsic motivation (Bellemare et al., 2016) takes a mixture of one-step return and Monte-Carlo return to set up the target value.

Optimality Tightening (He et al., 2017) applies constraints on the target using the values of several neighboring transitions. Simply by adding a few penalty terms into the loss, it efficiently propagates reliable values to achieve faster convergence. Our work is similar to optimality tightening in a sense that we just modified the target value without a single change done on the network structure of the original DQN. However, instead of using a limited number of neighboring transitions, our method samples a whole episode from the replay memory and propagates values sequentially throughout the entire sampled episode in a backward way. In this way, all rewards gained from the episode are guaranteed to propagate through all transitions of the sampled episode, no matter how sparse and delayed the reward signal is.

2

Under review as a conference paper at ICLR 2018

Probability of learning the optimal path

1.0
1 0.8
0.6
2 1 3 4
 = 1 0.4
2 3 4
0.2 uniform sample episodic backward
0.0 0 5 10 15 20 25 30 35 40 # of sample transitions
Figure 1: A motivating exmple for episodic backward update. Left: Simple navigation domain with 4 states and a single rewarded transition. Right: The probability of learning the optimal path (s1  s3  s4) after updating the Q-values with sample transitions.

3 EPISODIC BACKWARD UPDATE

We start with a simple motivating toy example to describe the effectiveness of episodic backward update. Then we generalize the idea into deep learning architectures and propose the full algorithm.

3.1 MOTIVATION

The idea of backward update starts from a simple toy example. Let us imagine a simple graph
environment with a sparse reward (illustrated in Figure 1, left). In this example, s1 is the initial state and s4 is the terminal state. A reward of 1 is gained only when the agent moves to the terminal state and a reward of 0 is gained from any other transitions. To make it simple, assume that we only have one episode stored in the experience memory: (s1  s2  s1  s3  s4). When sampling transitions uniformly at random as Nature DQN, the important transitions (s1  s3) and (s3  s4) may not be sampled for updates. Even when those transitions are sampled, there is no guarantee that the update of the transition (s3  s4) would be done before the update of (s1  s3). So by updating all transitions within the episode in a backward manner, we can speed up the reward
propagation, and due to the recursive update, it is also computationally efficient.

We can calculate the probability of learning the optimal path (s1  s3  s4) for the number of sample transitions trained. With the simple episodic backward update stated in Algorithm 1, the agent can come up with the optimal policy just after 4 updates of Q-values. However, we see that the uniform sampling method requires more than 30 transitions to assure the agent has learned the optimal path (plot in Figure 1, right).

Note that this method is different to the standard n-step Q-learning (Watkins, 1989).

Q(st,

at)



(1

-

)Q(st,

at)

+

(rt

+

rt+1

+

.

.

.

+

 n-1 rt+n-1

+

max
a

nQ(st+n,

a)).

(2)

In n-step Q-learning, the number of future steps for target generation is fixed as n. However, our method takes T future values in consideration, which is the length of the sampled episode. Also, n-step Q-learning takes max operator at the n-th step only, whereas we take max operator at every iterative backward steps which can propagate high values faster. To avoid exponential decay of the Q-value, we set the learning rate  = 1 within the single episode update.

3.2 EPISODIC BACKWARD UPDATE ALGORITHM
The fundamental idea of tabular version of backward update algorithm may be applied to its deep version with just a few modifications. In practice, due to the large state-action space, we cannot update all the Q-values of all state-action pairs using a single Q-table. Instead, we use a function

3

Under review as a conference paper at ICLR 2018
Algorithm 1 Simple Episodic Backward Update (single episode, tabular)
1: Initialize the Q- table Q  RS×A with zero matrix. Q(s, a) = 0 for all state action pairs (s, a)  S × A.
2: Experience an episode E = {(s1, a1, r1, s2), . . . , (sT , aT , rT , sT +1)} 3: for t = T to 1 do 4: Q(st, at)  rt +  maxa Q(st+1, a ) 5: end for
Algorithm 2 Episodic Backward Update
1: Initialize replay memory D to capacity N 2: Initialize on-line action-value function Q with random weights  3: Initialize target action-value function Q^ with random weights - 4: for episode = 1 to M do 5: for t = 1 to Terminal do 6: With probability select a random action at 7: Otherwise select at = argmaxa Q(st, a; ) 8: Execute action at, observe reward rt and next state st+1 9: Store transition (st, at, rt, st+1) in D 10: Sample a random episode E = {S, A, R, S } from D, set T = length(E) 11: Generate temporary target Q table, Q~ = Q^(S , ·; -) 12: Initialize target vector y = zeros(T ) 13: yT = RT 14: for k = T - 1 to 1 do 15: Q~[Ak+1, k]  yk+1 + (1 - )Q~[Ak+1, k] 16: yk  Rk +  maxa Q~[a, k] 17: end for 18: Perform a gradient descent step on (y - Q(S, A; ))2 with respect to  19: Every C steps reset Q^ = Q 20: end for 21: end for
approximator to estimate the Q-values and generate a temporary Q-table Q~ of the sampled episode for the recursive backward update. The full algorithm is introduced in Algorithm 2.
The algorithm is almost the same as that of Nature DQN (Mnih et al., 2015). Our contributions are the episodic sampling method and the recursive backward target generation with a diffusion coefficient  (from line number 10 to line number 17 of Algorithm 2).
The use of separate on-line and target Q-networks and -greedy exploration is identical to Nature DQN. Our algorithm has its novelty starting from the sampling stage. Instead of sampling transitions at uniformly random, we make use of all transitions within the sampled episode E = {S, A, R, S }. For simplicity, let the sampled episode start with s1 and contain T transitions. Then E can be denoted as a set of 1 × n vectors, i.e. S = {s1, s2, . . . sT }, A = {a1, a2, . . . aT }, R = {r1, r2, . . . rT } and S = {s2, s3, . . . sT +1}. The temporary target Q-table Q~, is initialized to store all the target Q-values of S for all valid actions. Q~ is a |A| × T matrix which stores the target Q-values of all states S for all valid actions. Therefore the j-th column of Q~ is a column vector containing Q^(sj+1, a; -) for all valid actions a from j = 1 to T .
Our goal is to estimate the target vector y and train the network to minimize the loss between each Q(sj, aj) and yj for all j from 1 to T . After the initialization of the temporary Q-table, we perform a recursive backward update. Adopting the backward update idea, one element Q~[Ak+1, k], the k-th column of the Q~ is replaced by the next transition's target yk+1. Then yk is estimated using the maximum of the newly modified k-th column of Q~. Repeating this procedure in a recursive manner until the start of the episode, we can successfully apply the backward update algorithm in a deep Q-network. The process is described in detail with a supplementary diagram in Appendix D.
4

Under review as a conference paper at ICLR 2018

relative length relative length

60

Wall density: 20% EBU

60

Wall density: 50% EBU

50

Uniform N-step

50

Uniform N-step

40 40

30 30

20 20

10 10

00 20000 40000step60000 80000 100000

00 20000 40000step60000 80000 100000

Figure 3: Relative lengths of EBU and other baseline algorithms. As the wall density increases, EBU outperforms other baselines with a significant amount of difference. The filled regions indicate the standard deviation of results from 50 random mazes.

When  = 1, the proposed algorithm is identical to the tabular backward algorithm stated in Algorithm 1. But unlike the tabular situation, now we are using a function approximator and updating correlated states in a sequence. As a result, we observe unreliable values with errors being propagated and compounded through sequential updates, which degrades the performance of the agent. This problem, caused by the correlation between neighboring transitions have been observed by many other works including Nature DQN. We solve this problem by introducing the diffusion coefficient . By setting   (0, 1), we can take a weighted sum of the newly learnt value and the pre-existing value. This process stabilizes the learning process by exponentially decreasing the error terms and preventing the compounded error from propagating. Note that when  = 0, the algorithm is identical to episodic one-step DQN.
We train the network to minimize the squared-loss between the Q-values of sampled states Q(S, A; ) and the backward target y. In general, the length of an episode is much longer than the minibatch size. So we divide the loss vector y - Q(S, A; ) into segments with size equal to the minibatch size. At each step, the network is trained using a single segment. A new episode is sampled only when all of the loss segments are trained.

4 EXPERIMENTS
Our experiments are designed to verify the following two hypotheses: 1) EBU agent can propagate reward signals fast and efficiently in environments with sparse and delayed reward signals. 2) EBU algorithm is sample-efficient in complex domains and does not suffer from stability issues despite its sequential updates of correlated states. To investigate these hypotheses, we performed experiments on 2D MNIST Maze Environment and on 49 games of the Arcade Learning Environment (Bellemare et al., 2013).

4.1 2D MNIST MAZE ENVIRONMENT

We test our algorithm in 2D maze environment with sparse and delayed rewards. Starting from the initial position, the agent has to navigate through the maze to discover the goal position. The agent has 4 valid actions: up, down, left and right. When the agent bumps into a wall, then the agent returns to its previous state. To show effectiveness in complex domains, we use the MNIST dataset (LeCun et al., 1998) for state representation (illustrated in Figure 2). When the agent arrives at each state, it receives the coordinates of the position in two MNIST images as the state representation.

 

Figure 2: 2D MNIST maze

5

Under review as a conference paper at ICLR 2018

Table 1: Relative length on MNIST Maze Environment

Wall density 20% 30% 40% 50%

EBU (ours) 5.44 8.14 8.61 5.51

Uniform 14.40 25.63 25.45 22.36

N-step 3.26 8.88 8.96 11.32

We compare the performance of EBU to uniform one-step Q-learning and n-step Q-learning. For the n-step Q-learning, we set n as the length of the episode. We use 10 by 10 mazes with randomly placed walls. The agent starts at (0,0) and has to reach the goal position at (9,9) as soon as possible. Wall density indicates the probability of having a wall at each position. We assign a reward of 1000 for reaching the goal and a reward of -1 for bumping into a wall. For each wall density, we generate 50 random mazes with different wall locations, and train for 200,000 steps. The MNIST images for state representation are randomly selected every time the agent visits each state. The relative length is defined as lrel = lagent/loracle, which is the ratio between the length of the agent's path lagent and the length of the ground truth shortest path loracle. Figure 3 and Table 1 report the mean of the relative lengths from the 50 random mazes. For this example, we set the diffusion coefficient  = 1. The details of hyperparameters and the network structure are described in Appendix C.
The result shows that EBU agent outperforms other baselines in most of the situations. Uniform sampling DQN shows the worst performance in all configurations, implying the inefficiency of uniform sampling update in environments with sparse and delayed rewards. As the wall density increases, valid paths to the goal become more complicated. In other words, the oracle length loracle increases, so it is important for the agent to make correct decisions at bottleneck positions. N-step Q-learning shows the best performance with a low wall density, but as the wall density increases, EBU shows better performance than n-step Q. Especially when the wall density is 50%, EBU finds paths twice shorter than those of n-step Q. This performance gap originates from the difference between the target generation methods of the two algorithms. EBU performs recursive max operators at each positions, so the optimal Q-values at bottlenecks are learned faster.

4.2 ARCADE LEARNING ENVIRONMENT

The Arcade Learning Environment (Bellemare et al., 2013) is one of the most popular RL benchmarks for its diverse set of challenging tasks. The agent receives high-dimensional raw observations from exponentially large state space. Even more, observations and objectives of the games are completely different over different games, so the strategy for high score should also vary from game to game. Therefore it is very hard to create a robust agent with a single set of network and parameters that can learn to play all games. We use the same set of 49 Atari 2600 games which was evaluated in Nature DQN paper (Mnih et al., 2015).

We compare our algorithm with two baselines: Nature DQN and Optimality Tightening (He et al., 2017). We train EBU agent and baselines for 10 million frames on 49 Atari games with the same network structure, hyperparameters and evaluation methods used by Nature DQN. We divide the training steps into 40 epochs of 250,000 frames. At the end of each epoch, we evaluate the agent for 30 episodes using -greedy policy with = 0.05. Transitions of the Arcade Learning Environment are fully deterministic. In order to give diversity in experience, both train and test episodes start with at most 30 no-op actions. We train each game for 8 times with different random seeds. For each agent with a different random seed, the best evaluation score during the training is taken as its result. Then we report the mean score of 8 agents as the result of the game.

We observe that the choice of  = 1 degrades the performance in most of the games. Instead, we

use 

=

1 2

,

which

shows

the

best

performance

among

{

1 3

,

1 2

,

2 3

,

3 4

,

1}

that

we

tried.

Further fine

tuning of  may lead to a better result.

First, we show the improvements of EBU over Nature DQN for all 49 games in Figure 4. To compare the performance of an agent over its baseline, we use the following measure (Wang et al., 2015).

ScoreAgent - ScoreBaseline max{ScoreHuman - ScoreBaseline} - ScoreRandom

(3)

6

Under review as a conference paper at ICLR 2018

4335.4

281.9 486.4

MonCtheNozaKpuSuppDCFmmWiUrneRaiVeDesairSpagzcJ'oIMoTPtB-hBdzmTsarecasiBauCuRaaCaiFT.eeryoiKhRitSIretAnadBFobienvuRFmaodtvlnaosrraeHnPgnlGVsBrCnadAmet.ZeAeHeRGElAGrnmoebAnavtatkBtPeeoTmoeiQMrAsiveesliEaofsouunsaDgckoaGaqeot.*eDnreramtstmebPpzDxtwahAnctvimnipntdkeaKahPlxdulRoonRBsiabieaioioeinomxrruakEwenrbia.dbnohtnudutWilaeiearetnunnninnewaodmnrgtyroeibacnnuadiriaeeeeeoeaslrsuoOlld.etrtgnkrkrnlnyttrsddgstgkxeolsrotenrrreeyyreesm -7-------------3---321.111196541100029......94...44.21......4180647610453354 000011222349111111.........2..222.2300257783377533057811057767.....6..77....0.56034.232..26732470..22532

Figure 4: Relative performance (Eq.(3)) of EBU over Nature DQN in percents (%) after 10 million frames of training.

This measure shows how well the agent performs the task compared to its level of difficulty. Out of the 49 games, our agent shows better performance in 32 games. Not only that, for games such as `Atlantis', `Breakout' and `Video Pinball', our agent shows significant amount of improvements. In order to compare the overall performance of an algorithm, we use Eq.(4) to calculate the human normalized score (van Hasselt et al., 2015).

ScoreAgent - ScoreRandom |ScoreHuman - ScoreRandom|

(4)

Then we report the mean and median of the human normalized scores of 49 games in Table 2.

Table 2: Summary of training time and human normalized performance

EBU (10M) Nature DQN (10M) Optimality Tightening (10M)

Training Time (all 49 games, 1 GPU) 152 hours 138 hours 407 hours

Mean 255.19% 133.95% 162.66%

Median 53.65% 40.42% 49.42%

The result shows that our algorithm outperforms the two baselines in both mean and median of the human normalized scores. EBU algorithm does show poor performance in some games such as `Krull' and `Robotank'. But for the games such as `Video Pinball' and `Breakout', the improvements are significant, resulting EBU to outperform Nature DQN in both mean and median sense. Furthermore, our method requires only about 37% of computation time used by Optimality Tightening1. Since Optimality Tightening has to calculate the Q-values of neighboring states and compare them to generate the penalty term, it requires about 3 times more training time than Nature DQN. Since EBU performs iterative episodic updates using the temporary Q-table that is shared by all transitions in the episode, its computational cost is almost the same as that of Nature DQN. Scores for each game after 10 million frames of training are summarized in Appendix A.
1We used NVIDIA Titan X for all experiments.

7

Score Score Score Score

Under review as a conference paper at ICLR 2018

Atlantis

80000

EBU ( = 0.5) EBU ( = 1.0)

70000 Nature DQN

60000

Optimality Tightening

50000

40000

30000

20000

10000

Breakout

175

EBU ( = 0.5) EBU ( = 1.0)

150

Nature DQN Optimality Tightening

125

100

75

50

25

00 2 4Million Frames6 8 10 00 2 4Million Frames6 8 10

Gopher

3500

EBU ( = 0.5) EBU ( = 1.0)

3000

Nature DQN Optimality Tightening

2500

Pong

10

EBU ( = 0.5) EBU ( = 1.0)

5

Nature DQN Optimality Tightening

0

2000 5

1500 10

1000 15

500 20

00 2 4Million Frames6 8 10 250 2 4Million Frames6 8 10

Figure 5: Scores of EBU and baselines on 4 games: `Atlantis', `Breakout', `Gopher' and `Pong'. Moving average test scores of 40 epochs with window size 4 are plotted. The filled regions indicate the standard deviation of 8 results.

We show the performance of EBU and the baselines for 4 games `Atlantis', `Breakout', `Gopher' and `Pong' in Figure 5. EBU with a diffusion coefficient  = 0.5 shows the best performance in all 4 games, reflecting that our algorithm does not suffer from the stability issue due to the sequential update of correlated states. Note that naive backward algorithm with  = 1.0 fails in most games.
5 CONCLUSION
We propose Episodic Backward Update, which samples transitions episode by episode and updates values recursively in a backward manner. Our algorithm achieves fast and stable learning due to the efficient value propagation. We show that our algorithm outperforms other baselines in many complex domains without much increase in computational cost. Since we did not change any network structures, hyperparameters and exploration methods, we hope that there is plenty of room left for further improvement.

8

Under review as a conference paper at ICLR 2018
ACKNOWLEDGEMENTS
We used the code2 uploaded by the authors of Optimality Tightening (He et al., 2017) to evaluate the baseline. We implemented EBU algorithm upon the same code with slight modifications.
REFERENCES M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 06 2013. M. G. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying countbased exploration and intrinsic motivation. arXiv preprintarXiv:1606.01868, 2016. D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996. C. Blundell, B. Uria, A. Pritzel, Y. Li, A. Ruderman, J. Z. Leibo, J. Rae, D. Wierstra, and D. Hassabis. Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016. A. Harutyunyan, M. G. Bellemare, T. Stepleton, and R.Munos. Q () with off-policy corrections. In International Conference on Algorithmic Learning Theory, pp. 305-320. Springer, 2016. F. S. He, Y. Liu, A. G. Schwing, and J. Peng. Learning to play in a day: Faster deep reinforcement learning by optimality tightening. In International Conference on Learning Representations (ICLR), 2017. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In Institute of Electrical and Electronics Engineers (IEEE), 86, 2278-2324, 1998. L-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine Learning, 1992. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D.Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 2015. R. Munos, T. Stepleton, A. Harutyunyan, and M. G. Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pp. 1046-1054, 2016. A. Pritzel, B. Uria, S. Srinivasan, A. Puig-'domenech, O. Vinyals, D. Hassabis, D. Wierstra, and C. Blundell. Neural Episodic Control. In International Conference on Machine Learning (ICML), 2017. T. Schaul, J. Quan, I. Antonoglou, and D. Silver. Prioritized Experience Replay. In International Conference on Learning Representations (ICLR), 2016. D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 2016. R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998. H. van Hasselt, A. Guez, and D. Silver. Deep Reinforcement Learning with Double Q-learning. arXiv preprint arXiv:1509.06461, 2015. Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, and N. de Freitas. Dueling Network Architectures for Deep Reinforcement Learning. In arXiv preprint arXiv:1511.06581 , 2015. C. J. C. H. Watkins. Learning from delayed rewards. PhD thesis, University of Cambridge England, 1989.
2https://github.com/ShibiHe/Q-Optimality-Tightening
9

Under review as a conference paper at ICLR 2018

APPENDIX A

SCORES ACROSS ALL 49 GAMES

Table 3: Summary of raw scores after 10 million frames of trainig by the 30 no-op evaluation method. Mean and standard deviation from 8 random seeds are reported.

Alien Amidar Assault Asterix Asteroids Atlantis Bank Heist Battle Zone Beam Rider Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar Hero Ice hockey Jamesbond Kangaroo Krull Kung-Fu Master Montezuma's Revenge Ms. Pacman Name This Game Pong Private Eye Q*Bert River Raid Road Runner Robotank Seaquest Space Invaders Star Gunner Tennis Time Pilot Tutankham Up and Down Venture Video Pinball Wizard of Wor Zaxxon

EBU (± std)
708.08 (125.48) 129.73 (9.99) 4109.18 (1011.77) 1898.12 (206.74) 1002.17 (77.11) 66271.67 (11445.91) 359.62 (41.08) 26002.44 (2764.24) 5628.99 (660.76) 78.80 (11.87) 55.95 (8.70) 174.76 (8.44) 6052.38 (804.16) 1287.08 (114.38) 65329.63 (10692.70) 7924.14 (580.27) -16.19 (1.28) 415.59 (62.99) -39.13 (2.32) 19.07 (2.95) 437.92 (183.64) 3318.50 (331.15) 294.58 (50.10) 3089.90 (132.37) -4.71 (1.42) 391.67 (46.70) 535.83 (123.65) 7587.24 (154.88) 20578.33 (1769.84) 1.04 (0.00) 1249.79 (100.35) 6960.46 (939.79) 5.53 (2.71) 953.58 (669.18) 785.00 (137.06) 3460.62 (231.38) 10086.74 (1817.24) 11.65 (2.26) 1380.67 (179.08) 797.29 (91.60) 2737.08 (314.14) -3.41 (7.48) 3505.42 (202.64) 204.83 (13.27) 6841.83 (651.99) 105.10 (23.24) 84859.24 (19151.87) 1249.89 (130.53) 3221.67 (203.69)

Nature DQN (± std)
690.32 (74.50) 125.42 (9.09) 2426.94 (370.05) 2936.54 (112.89) 654.99 (99.20) 20666.84 (3787.50) 234.70 (54.51) 22468.75 (6975.89) 3682.92 (1248.83) 65.23 (21.58) 37.28 (7.82) 28.36 (6.64) 6207.30 (965.85) 1168.67 (134.76) 74410.74 (7294.75) 7772.39 (1191.70) -17.94 (0.65) 516.10 (59.39) -65.53 (7.90) 16.24 (9.39) 466.02 (115.84) 1726.52 (271.66) 193.55 (20.30) 2767.97 (551.02) -4.79 (0.56) 183.35 (30.98) 709.88 (78.74) 24109.14 (33400.52) 21951.72 (3586.71) 3.95 (5.55) 1861.80 (108.69) 7560.33 (512.98) -2.68 (6.04) 1388.45 (1360.39) 2037.21 (830.05) 3636.72 (250.29) 8978.17 (881.04) 16.11 (1.22) 762.10 (33.99) 755.95 (97.25) 708.66 (161.22) 0.00 (0.00) 3076.98 (233.83) 165.27 (12.12) 9468.04 (2311.51) 96.70 (81.43) 17803.69 (5174.13) 529.85 (92.74) 685.84 (541.40)

Optimality Tightening (± std) 1078.67 (66.37) 220.00 (20.23) 2499.23 (668.28) 2592.50 (67.29) 985.88 (31.45) 57520.00 (8646.15) 407.42 (43.47) 20400.48 (2292.29) 5889.54 (317.66) 53.45 (6.33) 60.89 (11.45) 75.00 (9.65) 5277.79 (907.83) 1615.00 (262.68) 92972.08 (4387.57) 6872.04 (458.67) -15.92 (0.69) 615.05 (58.73) -69.66 (3.03) 14.63 (11.33) 2452.75 (190.50) 2869.08 (241.57) 263.54 (40.27) 10698.25 (1034.97) -5.79 (1.09) 325.21 (27.98) 708.33 (184.57) 7468.70 (250.54) 22211.25 (2539.89) 0.00 (0.00) 1849.00 (84.32) 7358.25 (454.64) 2.60 (4.35) 1277.53 (1434.49) 3955.10 (327.67) 4643.62 (398.79) 19081.55 (1876.88) 12.17 (1.46) 2170.33 (396.49) 869.83 (36.61) 1710.83 (324.04) -6.37 (7.47) 4012.50 (212.23) 247.81 (24.05) 6706.83 (630.28) 106.67 (22.42) 38528.58 (10725.68) 1177.08 (69.99) 2467.92 (506.10)

10

Under review as a conference paper at ICLR 2018

APPENDIX B

AVERAGED PERFORMANCE OVER ALL GAMES

Human Normalized Score

1.0 EBU ( = 0.5) Nature DQN
0.8 Optimality Tightening 0.6 0.4 0.2 0.0 0 2 4 6
Million Frames

8 10

Human Normalized Score

4.0 EBU ( = 0.5)

3.5

Nature DQN Optimality Tightening

3.0

2.5

2.0

1.5

1.0

0.5

0.0 0 2 4 6 Million Frames

8 10

Figure 6: Learning curves of EBU and baselines on 49 games of the Arcade Learning Environment. Up: median over 49 games. Down: mean over 49 games. We averaged the 30 no-op scores of 8 agents with different random seeds. Use of different evaluation methods and seeds may output different results.

11

Under review as a conference paper at ICLR 2018

APPENDIX C

NETWORK STRUCTURE AND HYPERPARAMETERS

2D MNIST Maze Environment Each state is given as a grey scale 28 × 28 image. We apply 2 convolutional neural networks (CNNs) and one fully connected layer to get the output Q- values for 4 actions: up, down, left and right. The first CNN uses 64 channels with 4 × 4 kernels and stride of 3. The next CNN uses 64 channels with 3 × 3 kernels and stride of 1. Then the layer is fully connected into size of 512. Then we fully connect the layer into size of the action space 4. After each layer, we apply rectified linear unit (ReLU).
We train the agent for a total of 200,000 steps. The agent performs -greedy exploration. starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner. We use RMSProp optimizer with learning rate of 0.001. The online-network is updated every 50 steps, the target network is updated every 2000 steps. The replay memory size is 30000 and we use minibatch size of 350. The discount factor  = 0.9 and diffusion coefficient  = 1.0 are used.
Arcade Learning Environment We use exactly the same network structure and hyperparameters of Nature DQN (Mnih et al., 2015). The raw observation is preprocessed into gray scale image of 84 × 84. Then it passes through three convolutional layers: 32 channels with 8 × 8 kernels with stride of 4; 64 channels with 4 × 4 kernels with stride of 2; 64 channels with 3 × 3 kernels with stride of 1. Then it is fully connected into size of 512. Then it is again fully connected into the size of the action space.
We train agents for a total of 10 million frames, which is equivalent to 2.5 million steps with frame skip of 4. The agent performs -greedy exploration. starts from 1 and is linearly annealed to reach the final value 0.1 at 4 million frames of training. To give randomness in experience, we select a number k from 1 to 30 uniform randomly at the start of each train and test episode. We start the episode with k no-op actions. The network is trained by RMSProp optimizer with a learning rate of 0.00025. At each step, we update transitions in minibatch with size 32. The replay buffer size is 1 million steps (4 million frames). The target network is updated every 10000 steps. The discount factor  = 0.99 and diffusion coefficient  = 0.5 are used.
We divide the training process into 40 epochs of 250,000 frames each. At the end of each epoch, the agent is tested for 30 episodes, with = 0.05. The agent plays the game until it runs out of lives or time (18,000 frames, 5 minutes in real time).

12

Under review as a conference paper at ICLR 2018

APPENDIX D

SUPPLEMENTARY FIGURE: BACKWARD UPDATE ALGORITHM

Sample episode E
 1  1  1  2

... ... ... ...

-2 -2 -2 -1



-1 -1 -1


   +1

Initialization


(2, 1) (2, 2)


(1, ||)

... ...  ...

(-1, 1) (-1, 2)
 (-1, ||)

(, 1) (, 2)
 (, ||)

0 0  0

 0 ...

0

0 

||

Recursive updates (2, 1)
(2, 2)



...

 -1 + 1 -  (, 2)

(, 1)

...

(-1, 2)

  + 1 -  (, 2)





(1, ||)

...

(-1, ||)

(, ||)



1 +  max  [:, 1]

...

-2 +

-1 +

 max  [:, T-2]  max  [:, T-1]

0 0  0


Figure 7: Target generation process from the sampled episode E

||

13

