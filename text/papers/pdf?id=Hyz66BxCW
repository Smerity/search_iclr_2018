Under review as a conference paper at ICLR 2018
EMPIRICAL INVESTIGATION ON MODEL CAPACITY AND GENERALIZATION OF NEURAL NETWORKS FOR TEXT
Anonymous authors Paper under double-blind review
ABSTRACT
Recently, deep neural network models have shown promising opportunities for many natural language processing (NLP) tasks. In practice, the number of parameters of deep neural models is often significantly larger than the size of the training set, and its generalization behavior cannot be explained by the classic generalization theory. In this paper, with extensive experiments, we empirically investigate the model capacity and generalization of neural models for text. The experiments show that deep neural models can find patterns better than brute-force memorization. Therefore, a large-capacity model with early-stopping stochastic gradient descent (SGD) as implicit regularizer seems to be the best choice, as it has better generalization ability and higher convergence speed.
1 INTRODUCTION
Although neural network models are non-convex and overly-expressive, they have still achieved great success in many fields, such as in computer vision, natural language processing, etc. (Krizhevsky et al. (2012); He et al. (2016); Bahdanau et al. (2015); Seo et al. (2017)). Currently, most neural models are trained by SGD optimization. Although SGD is simple, it is able to find a "good" local optimal by minimizing the training error. More surprisingly, the learned models also exhibit good generalization behavior under the over-parameterized setting (Neyshabur et al. (2017); Zhang et al. (2017)).
Therefore, there is rising interest in studying the capacity and generalization of neural models from both theoretic (Neyshabur et al. (2017)) and practical (Zhang et al. (2017)) views. However, the theoretic upper bounds of generalization error cannot give satisfactory interpretations of the practical observations of the neural models.
There are some empirical investigations on the generalization of neural models, Zhang et al. (2017) showed the capability of neural models by memorizing the samples with random labels. They also empirically observed that explicit regularization, such as weight decay, seemed to be unnecessary to obtain small test errors, which is different from the generalization of linear models.
Although these empirical investigations are inspiring, they are limited to image data. There is naturally a question on how they perform on text, particularly on natural language. What are the effective capability and generalization of the popular neural architectures on text? What is the difference between these architectures? Which architectures should be chosen?
In this paper, we investigate the capacity and generalization of widely used neural architectures on text, including continuous bag-of-words (CBOW), long short-term memory network (LSTM) (Hochreiter & Schmidhuber (1997)) and convolutional neural network (CNN). We first evaluate the effective capacity of the three different architectures, and measure their generalization ability with several regularization methods, including weight decay, dropout (Srivastava et al. (2014)) and early stopping. Specifically, we study the model capacity and generalization from the following two views.
Effective Capacity Following Zhang et al. (2017), we evaluate the effective capacity of different models on samples with random labels, without considering their generalization error. We found
1

Under review as a conference paper at ICLR 2018
all the models are able to fit the randomly-labeled data with zero error, provided there are enough parameters. CNN has the largest effective capacity with the same number of parameters, and the attention mechanism can also effectively increase the capacity of LSTM. The capacity also depends on the length and linguistic regularities of the text.
Generalization Although generalization theory states that the generalization error is low when the model complexity is small, low model complexity is not a necessary condition of low generalization error (Kawaguchi et al. (2017)). Following the experience on linear models, weight decay is often used as regularization in deep neural models. Besides weight decay, there are some other regularization methods, such as dropout, early stopping, and batch normalization (Ioffe & Szegedy (2015)).
We evaluate the generalization of different neural architectures with different regularization methods on the corrupted data. Although the large-capacity model prefers to overfit the training data, stopping SGD early is an effective regularization strategy, which can effectively prevent it from overfitting. Similar to Zhang et al. (2017), we also find that weight decay seems to be non-essential, compared to early-stopping.
Optimization We also investigate three commonly used optimizers: vanilla SGD, Adagrad (Duchi et al. (2011)) and Adam (Kingma & Ba (2015)). The experiments show that Adam works well on text and outperforms other optimizers. With the Adam optimizer, we could achieve the best results without the need of adjusting the learning rate.
2 RELATED WORK
For the capacity, Bartlett et al. (1998) showed if the number of layers of neural networks is fixed, the Vapnik Chervonenkis (VC) dimension grows as W log W , where W is the number of parameters in the network. Following this, Bartlett et al. (2017) showed new upper and lower bounds on the VC dimension of deep neural networks with the rectified linear unit (ReLU) activation function. The VC dimension is O(W L log(W )), where W is the number of weights and L is the number of layers. For the generalization ability of neural network, Neyshabur et al. (2017) studied how normbased control, sharpness and robustness can ensure generalization and connected the sharpness to PAC-Bayes theory. In spite of the very large capacity and instability, non-robustness and sharp minima, Kawaguchi et al. (2017) gave us an explanation for the strong generalization ability of the neural network. Zhang et al. (2017) found that deep neural networks can easily fit random labels and showed that the traditional solutions can not be applied to the explanation of the generalization ability of neural networks.
Pascanu et al. (2014) provided a framework for comparing deep and shallow models in the family of piecewise linear functions. Livni et al. (2014) revisited the computational complexity of training neural networks and offered practical methods for training specific neural networks. Poggio et al. (2017) showed deep networks perform better than shallow networks and the weight sharing is not the main reason for the better performance. The parameters of the neural network are in the order of one million, and in the case of over parameters (i.e., the number of parameters is very large), the optimization method has a great influence on finding the local optimum. Neyshabur et al. (2015) introduced Path-SGD method and showed better generalization ability.
However, for the measurement of capacity, there is no empirical results. When the sample size reaches a certain level, the model with a certain limit of the parameters may not be able to fit the random labels. This prompts us to explore the empirical relationship between model capacity and generalization ability.
3 DATASET AND MODEL ARCHITECTURE
3.1 OVERVIEW OF DEEP LEARNING METHODS
Learning theory of classification. Given training data (X, Y ) = {(xi, yi), i = 1, 2, ..., n}, (xi  Rd, yi  {1, 2, ..., K}), xi is independent identically distributed. We want to learn a classifier f that can predict the output given the input so that y  f (x). We parameterize the set of learned functions
2

Under review as a conference paper at ICLR 2018

f (x,  :   ) and the loss function is L(y, f (x, )). The process of learning is to pick an optimal

one from these function sets. The evaluation criteria of optimality is expected risk minimization,

where expected risk R() = L(y, f (x, ))dF (x, y), and F (x, y) is the true distribution function

of X, Y . Since we do not know F (x, y), the usual practice is to use the arithmetic average to replace

the mathematical expectation, namely, empirical risk minimization, with empirical risk Remp() =

1 n

n i=1

L(yi,

f

(xi,

)).

Consistency means that the optimal value of Remp() converges to the

optimal value of R() when the size of samples tends to infinity. If empirical risk minimization can

provide a function sequence f (x, ) such that Remp() and R(w) converge to the smallest possible risk value R(0), the learning process of this empirical risk minimization learning is consistent.

The necessary and sufficient condition for the consistency of empirical risk minimization is that the

empirical risk converges to the expected risk on the set of functions:

lim
n

P

[sup


|R()

-

Remp()|

>

] = 0, 

>0

where P [·] is the probability. There is no learning method to determine whether the set of functions can satisfy the condition of consistency. Thus, some norms are defined to measure the performance of the set of functions and the most important one is VC dimension. VC dimension reflects the learning ability of the set of functions. The greater the VC dimension, the more complex the learning machine. At present, the VC dimension of some special function sets can be accurately understood, but for some complex learning machines (such as neural networks), the VC dimension is not only related to the choice of the set of functions, but also the optimization algorithm and it is hard to determine.
Role of regularization. Generalized regularization is a strategy that is added to the training process to improve the generalization of the model without necessarily reducing the training error. The most common way is parameter norm penalty, namely:

L(y, f (x, )) = L(y, f (x, )) + g().

For the choice of g, the usual way is the L2 norm (also called weight decay or ridge regression), namely:
L(y, f (x, )) = L(y, f (x, )) + T .

Another commonly used regularization is dropout. In dropout, during the training process some inputs or hidden states are randomly chosen and involved in the calculation. Parameter norm penalty and dropout are explicit regularizations. For large models, large training sets are advantageous, so data augmentation is also an explicit regularization and it can improve the generalization ability. For implicit regularization, early stop is commonly used. When training large models, usually the validation error will increase when the training error is reduced. Early stop is a strategy which may be used to deal with this problem. In the process of training, we save the parameter value of the minimum validation error in the training history and when the validation error is larger than the best saved value for many times, we stop the training and return to the saved value.
Different kinds of neural networks Feed-forward neural network is the most common network topology, in which the neurons receive the inputs of the previous layer and output to the next layer. In practice, it is often used as the fully connected layer. Because feed-forward neural network has so many parameters and it can not be used for high-dimensional space, there is the parameter sharing mechanism and the most common is CNN and RNN. CNN differs from the ordinary neural network in that CNN contains a feature extractor composed of convolution and sub-sampling layers. In the convolution layer of CNN, a neuron is connected only to a portion of the adjacent neurons. Sub-sampling is also called pooling and usually in the form of mean pooling and max pooling. Sub-sampling can be seen as a special convolution process. Convolution and sub-sampling greatly simplify the model complexity and reduce the number of parameters of the model. The purpose of RNN is to process the sequential data and the network will remember the previous information and apply it to the calculation of the current output. Theoretically, RNN can process the data of any length. In the past few years, researchers have proposed complex RNN to improve the shortcomings of the vanilla RNN model. The most common RNN is LSTM, which is essentially the same as the vanilla RNN structure, except that different functions are used to calculate the hidden state.

3

Under review as a conference paper at ICLR 2018
3.2 DATASET We performed an experiment on the task of text classification (Kim (2014); Yogatama et al. (2017)) using the data set from the Yelp Dataset Challenge (2015)1 which is sentiment analysis with five categories. The total number of samples selected is 650000, split into training set (645000), development set (5000) and test set (50000). Figure 1 shows the statistics of the data set.

Figure 1: Statistics of data set. Most of the sentences are of length 0 - 600.

3.3 MODEL ARCHITECTURE

The architecture of the model consists of three basic layers:

Embedding layer. Given a context c consisting of t tokens {c1, c2, ..., ct}, we use a pre-trained
word embedding to map each word to a word vector space to form the word embedding space {p1, p2, ..., pt}. The pre-trained word embedding we use here is Glove.6B.300d 2 (Pennington et al.
(2014)). Then, the embedding space is passed to the processing layer.

Processing layer.

For LSTM-based classification, we use word embedding space as the input of LSTM, and gen-

erate hidden states {h1, h2, ..., ht}. Finally, we average the hidden state and get the vector

m

=

h1

+h2

+...+ht t

.

For CNN-based classification, the input layer gets a number of feature maps by convolution operation. The size of the convolution window is t × d, where t is the number of words and d is the dimension of the word vector. Through such a large convolution window, we can get a number of feature maps. The filter size we use here is 3, 4 and 5. The number of filters is 68. After 3 layers of convolution operation, we use a method called max-over-time pooling in the next pooling layer. This method simply retrieves the maximum value from the previous feature maps, thus solving the problem of variable input length. The output of the final pool layer is the maximum value of each feature map, that is, a one-dimensional vector m.

For CBOW-based classification, we average the embedding space and get the vector m =

p1

+p2

+...+pt t

.

Output layer. The output of the one-dimensional vector m is connected to a softmax layer by means of a fully connected layer, and the softmax layer reflects the probability distribution on the final classes.

1https://www.yelp.com/dataset/challenge 2https://nlp.stanford.edu/projects/glove/
4

Under review as a conference paper at ICLR 2018
4 EXPERIMENTS
4.1 CAPACITY OF NEURAL NETWORKS
Relation between capacity and parameters. The capacity of neural networks is approximately equal to the maximum number of samples that can be remembered (i.e. training error is very small) under the condition of random labels (Neyshabur et al. (2017)). For the neural network with the order of 10k parameters, we want to know the capacity of the neural network. We believe that one of the factors that affects the capacity of neural networks is the number of parameters. We want to explore the exact relationship between capacity and the number of parameters. We use text classification for this exploration. For the determination of capacity, we will use a random label for each sample and treat the number of samples remembered by the neural network as the capacity of the model. We choose D  {50k, 100k, 150k, 200k, 250k, 300k, 350k, 400k, 450k, 500k, 550k} samples from the training set, and then measure the accuracy of each training set under different number of parameters after the model converges. Then, we interpolate between two adjacent points to calculate the number of samples with the four levels of accuracy 90%, 80%, 70% and 60%. The results of the measurement are shown in Figure 2. From the experiments we can see that the capacity and parameters of neural networks show logarithmic linear relationship. This means that the marginal effect of the capacity on the number of parameters will be reduced as the number of parameters increases.
Impact of different architectures. Another factor which affects the model capacity is the architecture of the model. We compare the capacity of CBOW and LSTM with the same order of number of parameters. The capacity of LSTM is significantly larger than that of CBOW. From Table 1 we can see that when the sample size changes from 50k to 350k, the accuracy of LSTM declines slowly, and that of CBOW drops dramatically. Although when the sample size is 50k and 100k, CBOW can almost completely remember the sample size, from Table 2 we can see that the number of training steps which CBOW requires reaches the order of 20M, and the convergence is extremely slow. From the experimental results, in the task of text classification, CNN and LSTM with attention can increase the capacity of the model, and they also converge readily in the training process.

Figure 2: Impact of number of parameters. As Figure 3: Impact of dropout. As the keep rate in-

the number of parameters increases, the marginal creases, the capacity of the model gradually in-

effect of the capacity of models decreases.

creases.

Model
LSTM64d CBOW LSTM256d CNN LSTM with attention

Number of parameters 97,925 91,805 637,445 641,345 636,340

50k 100% 100% 100% 100% 100%

100k 71% 100% 100% 100% 100%

150k 56% 23% 100% 100% 100%

200k 48% 22% 100% 98% 100%

250k 42% 22% 100% 98% 99%

300k 39% 21% 86% 90% 92%

350k 37% 21% 73% 91% 89%

Table 1: Capacity of different architectures. Overall, the capacity of LSTM changes gradually, but CBOW does not. CNN and LSTM with attention mechanism can effectively increase the capacity of models.

Impact of regularization. In statistical learning theory, VC dimension is a measurement of the complexity of the model. As can be seen from our experiments (Figure 3, Table 3), the effect of dropout on model capacity is far weaker than the effect of weight decay. Weight decay greatly limits

5

Under review as a conference paper at ICLR 2018

Model LSTM64d
CBOW

Number of parameters 97,925
91,805

Number of samples 100k 50k 100k

Convergence steps 584,000
23,400,000 46,800,000

Table 2: Steps of convergence. LSTM converges more readily and faster than CBOW.

the complexity of the model. From the experiments, we can see that when the weight decay is over a certain range, the model will completely forget the training set, even on a small data set. The effect of weight decay on model capacity is a hard effect, that is, when the weight decay rate is less than some specific value, the model will completely remember the training set, and when it is greater than this value, the training process of the model will be completely disrupted. On the other hand, the effect of dropout on model capacity is a soft effect. As the keep rate of dropout rises, the number of samples that the model can remember will gradually increase.

Model Number of parameters Number of samples Weight decay rate Training accuracy

LSTM512d

1,930,245

50k

 10-4  10-3

 99%  20%(fail to converge)

Table 3: Impact of weight decay. The effect of weight decay on model capacity is very large.

Impact of random context and short context. We shuffle the context of the data set and give each sample a random label to see if the neural network of different architectures can remember the samples. From the experiment shown in Table 4, we observe that LSTM, CNN and LSTM with attention are all able to remember the samples. Except for LSTM, the number of samples which are remembered by CNN and LSTM with attention is almost the same as when the context is not shuffled. In addition to being able to capture the distribution of the language model, LSTM can capture the distribution of sequences that do not conform to the language model. We also reduce the length of the text from 400 to 100, observing that LSTM can remember more samples.

Mode of context Random context
Short context

Model LSTM256d
CNN LSTM with attention
LSTM256d

Number of parameters 637,445 641,345 636,340 637,445

50k 100% 100% 100% 100%

100k 100% 100% 100% 100%

150k 100% 100% 100% 100%

200k 99% 99% 100% 100%

250k 94% 96% 99% 100%

300k 77% 91% 95% 96%

350k 72% 94% 90% 87%

Table 4: Impact of random and short contexts. LSTM, CNN and LSTM with attention do not fail to fit the random labels. For short context, LSTM performs better.

4.2 GENERALIZATION OF NEURAL NETWORKS
Dataset and training details. The dataset is split into a training set (645000), a development set (5000) and a test set (50000). The labels of the development and test sets are correct. The labels of the training set are randomized from 0 to 1.0. 0.5 means that the labels of 50% of the training set are randomized and in general, approximately 60% (50% + 50% × 0.2) of the labels which are correct in the training set. For training, we use a mini-batch size of 512 and the largest length of text is set to 400. We use the Adam optimizer to train the models with the initial learning rate 0.001 and we decay it with the rate of 0.9 every 2000 steps. The validation step is set to 100 steps and early stop is set to 10 validation steps.
Generalization ability. Figure 4a shows that even if the training set is mixed with the data of wrong labels, the model which is trained with the training set also has a strong generalization ability. From the experiment, we can see that there is a jump in the test accuracy when the label corruption changes from 1.0 from 0.9. More specifically, when all the training set is contaminated (in fact approximately 20% (100% × 0.2) of the labels of the training set are correct) and each category has almost the same number of samples, we can see that the model trained with these data just guesses randomly. However, when 90% of the training set is contaminated (in fact about 28% (10% + 90% × 0.2) of the labels of the training set are correct), we can see that the generalization ability of the model trained with the data is greatly enhanced. The reason may be that SGD has the
6

Under review as a conference paper at ICLR 2018

(a) Test Accuracy

(b) Mode Selection

Figure 4: Generalization of corrupted training set. Although the training set only has a small number of samples with correct labels, the model can generalize very well.

ability to perform implicit regularization, which prefers to select the generalizable models over their counterparts. Experiments (Figure 4a) also show that SGD training finds the generalizable models before memorizing the random noise.
Linked to the mode selection of distribution. There are many patterns of distribution in the data set, and the training process of the neural network is to find one or more distribution patterns. Given supervised information, supervised learning is used to guide the neural network to find the pattern of distribution corresponding to it and to fit the pattern of this distribution or the mixed pattern of multiple distributions in the course of training. From the experiment, we can see that when the label corruption is 1.0, the trained model is in a random guessing state. The distribution of the training set is in a level of equilibrium, so the neural network cannot choose the correct pattern of distribution corresponding to the supervised information, and cannot achieve generalization ability. When the label corruption is 0.9, although the number of correct labels is only increased slightly, the equilibrium of supervised information is broken, and with the change of the degree of inclination, the neural network is guided to choose the correct distribution mode. Once the correct distribution mode is chosen, optimization becomes easier, so there will be a jump when the label corruption changes from 1.0 to 0.9. Figure 4b shows the process, and the small supervised information will make the neural network select the distribution pattern that conforms to the supervised information. Assuming there are two distribution modes in the training data, the Model 1 is the distribution mode of the supervised information, and the supervised information will cause the neural network to find its distribution.

4.3 OPTIMIZATION OF NEURAL NETWORKS
The function represented by neural network is a high dimensional non-convex function space. Therefore, the choice of optimization methods greatly impacts the performance of the network. Here, we compare several commonly used optimization methods, namely, vanilla gradient descent, Adagrad and Adam. Table 5 shows the results. Adam's convergence is very fast and it can also converge for the samples of random labels. However, vanilla gradient descent and Adagrad fail to converge for the samples of random labels, even for the small dataset.

Model LSTM512d

Number of parameters 1,930,245

Number of samples 50k(True label)
50k(Random label)

Optimizer Adam Adagrad
GradientDescent Adam Adagrad
GradientDescent

Training accuracy
100% 62% 56% 100%  20%(fail to converge)  20%(fail to converge)

Table 5: Impact of optimizer. Adagrad and vanilla gradient descent fail to converge for samples of random labels.

7

Under review as a conference paper at ICLR 2018
4.4 MODEL SELECTION
We want to provide guidelines on the choice of a model from the perspective of the capacity of neural network. The main problem is whether we choose a large-capacity model or a small-capacity model. Figure 5a shows the convergence trend of the training loss vs. the training steps under different capacity of the same architecture. From Figure 5a we can see that the convergence of large-capacity models for random samples is fast. Figure 5b and Figure 5c show the loss trend for the training set and validation set under the training set of true labels. For the experiments shown in Figure 5, we use the dataset of 150k samples. Although the lowest loss that each model achieves is almost the same, the large-capacity model is easier to fit and in the condition of implicit regularization of early stop, the large-capacity model can stop earlier. Table 6 shows the generalization ability of models of different capacities on the test set. We can get the intuition that for the large dataset, the model of large capacity can generalize better than the model of small capacity. However, for the small dataset, the model of large capacity and small capacity have almost the same generalization ability if the regularization is chosen properly. Overall, weight decay is not an effective way for generalization. Early stop performs much better than weight decay.

(a) Training Loss (random label) (b) Training Loss (true label)

(c) Validation Loss

Figure 5: Curves of loss. (a) shows that LSTM with 512-dimension and 256-dimension hidden size can fit the samples of random labels completely. (b) and (c) show the training and validation losses of the training process with random labels and early stop.

Amount of data Large data(645k) Small data(150k)

Weight decay(10-4)
Yes No Yes No

LSTM32d 43,845 60.56% 61.08% 58.60% 58.96%

LSTM64d 97,925 62.52% 63.18% 59.15% 59.04%

LSTM128d 236,805 63.69% 63.59% 59.50% 59.65%

LSTM256d 637,445 63.53% 64.10% 59.84% 59.75%

LSTM512d 1,930,245 63.62% 64.23% 59.57% 59.21%

Table 6: Test accuracy. Large data can increase the generalization ability over small data. A model with more parameters can generalize better than one with less parameters with early stop.

5 CONCLUSION
We performed an empirical investigation of the capacity and generalization ability of neural networks on text. We give the definition of model capacity and the exact relationship between the capacity of neural networks commonly used for text and the number of parameters from an empirical point of view. We compare the capacity of different architectures and find that CNN and attention mechanism will effectively increase the capacity of the model, and the structured model converges more readily and is more robust in the process of training. In the experiment of shuffled context and short text, we conclude that although neural networks can fit any distribution, it is easier to fit simple distribution than complex distribution. For the generalization ability of the model and the model selection, we conclude that in the case of over-parameterized neural networks, although the model of large capacity is more likely to fit, if we use appropriate regularization mechanism, such as early stop, the model of large capacity will have a greater advantage than that of small capacity.
REFERENCES
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015.
8

Under review as a conference paper at ICLR 2018
Peter L. Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise polynomial networks. Neural Computation, 10(8):2159­2173, 1998.
Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piecewise linear neural networks. arXiv preprint arXiv:1703.02930, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735­1780, 1997.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, 2015.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv preprint arXiv:1710.05468, 2017.
Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2014.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advanced in Neural Information Processing Systems, 2012.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In Advanced in Neural Information Processing Systems, 2014.
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In Advanced in Neural Information Processing Systems, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. arXiv preprint arXiv:1706.08947, 2017.
Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of deep feed forward networks with piece-wise linear activations. In International Conference on Learning Representations, 2014.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Empirical Methods on Natural Language Processing, 2014.
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli Liao. Why and when can deep ­ but not shallow ­ networks avoid the curse of dimensionality: a review. International Journal of Automation and Computing, 2017.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hananneh Hajishirzi. Bidirectional attention flow for machine comprehension. In International Conference on Learning Representations, 2017.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(Jun:1929­1958, 2014.
Dani Yogatama, Chris Dyer, Wang Ling, and Phil Blunsom. Generative and discriminative text classification with recurrent neural networks. arXiv preprint arXiv:1703.01898, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.
9

