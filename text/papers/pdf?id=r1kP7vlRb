Under review as a conference paper at ICLR 2018
TOWARD LEARNING BETTER METRICS FOR SEQUENCE
GENERATION TRAINING WITH POLICY GRADIENT
Anonymous authors Paper under double-blind review
ABSTRACT
Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is essentially difficult. In a such situation, learning a metric of a sequence from data is one possible solution. The previous study, SeqGAN, proposed the framework for unsupervised sequence generation, in which a metric is learned from data, and a generator is optimized with regard to the learned metric with policy gradient, inspired by generative adversarial nets (GANs) and reinforcement learning. In this paper, we make two proposals to learn better metric than SeqGAN's: partial reward function and expert-based reward function training. The partial reward function is a reward function for a partial sequence of a certain length. SeqGAN employs a reward function for completed sequence only. By combining long-scale and short-scale partial reward functions, we expect a learned metric to be able to evaluate a partial correctness as well as a coherence of a sequence, as a whole. In expert-based reward function training, a reward function is trained to discriminate between an expert (or true) sequence and a fake sequence that is produced by editing an expert sequence. Expert-based reward function training is not a kind of GAN frameworks. This makes the optimization of the generator easier. We examine the effect of the partial reward function and expert-based reward function training on synthetic data and real text data, and show improvements over SeqGAN and the model trained with MLE. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.
1 INTRODUCTION
Generating sequential data is one of the main areas of research in machine learning. Recently, sequential generative model with recurrent neural networks (RNNs) have shown great success in several sequence generation tasks (Graves, 2013; Sutskever et al., 2011). The most common training method of RNNs is maximum log likelihood estimation (MLE). Although MLE provides stable training, a trained RNN generator suffers from the discrepancy of training mode and inference mode, called exposure bias (Bengio et al., 2015). Exposure bias occurs because, at the inference, the generator predicts the next token given the tokens that the generator itself has generated so far, though the generator is trained to predict the next token given previous true tokens. To alleviate exposure bias, sequence training methods with reinforcement learning (RL) have been proposed (Ranzato et al., 2016; Bahdanau et al., 2017). Owing to the methods of RL, the RNN generator can be optimized w.r.t. a task specific metric such as BLEU (Papineni et al., 2002), rather than the log likelihood. The application of RL methods to sequence generation tasks is becoming important in recent (Wu et al., 2016; Li et al., 2016). Throughout this paper, we use the term "metric" as a total reward for a sequence.
If we have a good metric of a sequence, we can expect that a good sequence generative model would be obtained by using a method of RL. However, as Abbeel & Ng (2004) pointed out, it is generally difficult to manually specify a task specific metric for RL. It is especially difficult to manually design a proper metric for unsupervised sequence generation tasks, such as text generation or music generation (imagine how hard it is to manually design the metric of the naturalness of a sentence, or the beauty of music). One of the solutions to designing a metric for those tasks is to learn a metric from data.
1

Under review as a conference paper at ICLR 2018
Yu et al. (2017) proposed SeqGAN, which a metric of sequence is learned from data, and a generative model is optimized w.r.t. the metric. Inspired by generative adversarial nets (GANs) (Goodfellow et al., 2014) and RL, SeqGAN employs a discriminator which is trained to discriminate between a true sequence and a generated sequence, and a generator is trained with policy gradient (Sutton et al., 2000) by treating the discriminator as the reward function. Because SeqGAN learns a metric from data and optimizes a generator in RL manner, we can see SeqGAN as the study of inverse reinforcement learning (IRL).
In this study, we also consider unsupervised sequence generation as a task of IRL, and we aim to learn the better metric than SeqGAN's. We state two proposals for this purpose: partial reward function and expert-based reward function training.
The partial reward function is the reward function for a partial sequence of a certain length. SeqGAN only uses a reward function for completed sequence. As a background of its proposal, we have an assumption that it is too much of a burden on a reward function employed in SeqGAN to evaluate a coherence of sequence as well as a partial correctness comprehensively. By employing the partial reward function, we aim to make a metric that can evaluate both a coherence and a partial correctness of a sequence. Empirically, we show that the partial reward function can correctly evaluate a partial mistake of a sequence which a reward function for a completed sequence can not evaluate.
In expert-based reward function training, we train the reward function without the generator's samples. The reward function is trained to discriminate between an expert sequence and a fake sequence that is produced by editing expert one. Unlike SeqGAN, expert-based reward function is not a kind of GAN frameworks. Although GAN framework has an advantage that a reward function is simultaneously trained with a generator's performance, the training of the generator frequently fails because of an instability of the GAN framework. Expert-based reward function training prioritizes executing stable training of the generator over taking an advantage of GAN framework.
We conducted experiments based on synthetic data and real text data to investigate the effectiveness of partial reward function and expert-based reward function training. As an evaluation method, we employ oracle negative log likelihood (NLL) in synthetic data, and BLEU (Papineni et al., 2002) in text data. We show that the models with our proposals outperform SeqGAN in both experiments. Specifically, whereas SeqGAN gains 0.42 improvement of NLL over MLE on synthetic data, our best model gains 3.02 improvement, and whereas SeqGAN gains 0.029 improvement of BLEU over MLE, our best model gains 0.250 improvement.
2 BACKGROUND
2.1 RELATED WORK
Recent studies have attempted to use RL for the training of a sequence generator. Ranzato et al. (2016) introduced policy gradient to sequence generation training. This study uses BLEU score as a task specific reward, and trains generator to maximize BLEU. Bahdanau et al. (2017) applied the actor-critic method to a translation task and also uses BLEU as a task specific reward. Although they show an applicability of RL to sequence generation training, they assume that the task specific reward is given. Sequence tutor (Jaques et al., 2017) is the study that utilizes the reward function learned from data. Sequence tutor treats RNN trained with MLE as a reward function, called reward RNN. When a generator is trained with RL, sequence tutor uses both the log likelihood of the reward RNN and the task specific reward as the metric. We also employ RL to the sequence generation training, but we assume that the task specific reward is totally unavailable.
TextGAN is the very recent proposed study of the GAN-based text generation (Zhang et al., 2017). It assumes that the task specific reward is unavailable, and optimizes the generator by using the gradient of the loss from the discriminator w.r.t. the outputs by the generator, as original GANs have done (Goodfellow et al., 2014). As our study argues the importance of training of the metric in the context of IRL, textGAN argues the importance of the training of the discriminator in the context of GAN. To prove the effectiveness of our proposals, we focus on comparing our model with SeqGAN in the experiment. We mention the applicability of our proposals to GAN-based sequence generation, such as textGAN, in the discussion.
There are several studies that attempted to train a neural network reward function in IRL (Finn et al., 2016; Ho & Ermon, 2016). However, there is no precedent to use edited expert trajectories for the
2

Under review as a conference paper at ICLR 2018

training of a neural network reward function. We assume it is because a dynamics has to be known to produce a fake trajectory from an expert one. In sequence generation task, we have a lot of expert trajectories and the dynamics is known. This situation enables the expert-based reward function training.

2.2 SEQGAN
In SeqGAN, the metric of a sequence is learned from data, and the generator is optimized w.r.t. the learned metric. SeqGAN employs a GAN framework and RL to achieve it. SeqGAN is the most relevant study to ours in that both studies learn the metric purely from data, and optimize the generator in the RL manner.
Given a dataset of real-world sequences, parameterized generator G is trained to produce a sequence Y1:T = {y1, ..., yt, ..., yT }, yt  Y, where Y is the vocabulary of candidate tokens. G is trained to produce a sequence that is similar to real data. SeqGAN considers this problem as RL, considering G to produce action (next token yt) given state (previously generated tokens Y1:t-1).
SeqGAN trains parameterized discriminator D as well as generator G. Like GAN training, D is trained to discriminate between real data and generated data from G. At the same time, G is trained via policy gradient by seeing D as a reward function. SeqGAN iteratively updates the discriminator and the generator until the convergence.

2.3 POLICY GRADIENT

Policy gradient is the theorem for directly maximizing the reward by ascending the gradient w.r.t. policy parameters.
The objective of RL can be stated as:

J() = E [r] = d(s) (s, a)Q(s, a)

sS

aA

(1)

where s is state, a is action, r is reward, d(s) is the probability to encounter the state,  is the policy, and Q(s, a) is the action-state value. The gradient of Eq.(1) can be defined as:

J () =

d(s) (s, a)log(s, a)Q(s, a)

sS

aA

= E [log(s, a)Q(s, a)].

(2)

In a sequence generation setting, state s denotes the tokens that policy has ever generated so far, and action a is the next token. In this paper, the term "generator" is identical to policy. In practical situations of sequence generation, such as text generation, it is important to ensure the variety of samples that the generator generates. To prevent a generator from becoming deterministic, it is common to add an entropy regularizer (O'Donoghue et al., 2016) to Eq.(2), that is,

  E [log(s, a)Q(s, a)] + E [H(s)]

(3)

where H(s) = - a (s, a)log(s, a) denotes the entropy, and  is the hyper parameter.

3 PARTIAL REWARD FUNCTION

The partial reward function returns a reward for a partial sequence of a certain length. The partial reward function is trained to discriminate between real partial sequence data and fake data. Figure 1 shows the overview of the partial reward function. SeqGAN can be viewed as having only yellow reward function.

3.1 PARTIAL REWARD FUNCTION SPECIFICATION
We choose the convolutional neural network (CNN) of Kim (2014) as the partial reward function. The same CNN is employed as the discriminator in SeqGAN and textGAN. Let wt denote the one-hot vector representation of the t-th token in the sequence, yt. Each wt is embedded into a k-dimensional vector yt = We ·wt, where We  Rk×V is an embedding matrix, and

3

Under review as a conference paper at ICLR 2018

BC:D BC:E ...

BD:F

...

B

I have ' 4
scale=2

a pen ) .
scale=3

and I am ... EOS

5 6 7

0

':0 = {', ... 0}

scale=sequence length

Figure 1: The overview of partial reward functions. SeqGAN has only the yellow reward function.

( = ':),  = .)
Averege
   

... rollout

':0 = {', ... 0}

I have a

pen and

I

am

a

...

' 4 ) . 5 6 7 8

Figure 2: How to calculate action-state value with partial reward function. The red token is the action.

V is the number of unique tokens. A sequence of embedded tokens of length T , {y1, ..., yt, ..., yT }, is represented as a matrix YT  Rk×T by concatenating a sequence of y over the timesteps.

A partial reward function Di only takes a filters and each filter can be represented as

WseqcuhenceRokf×che,rwtahinerleenhgitshaLwDiindaoswinspiuzet..

Di has several The maximum

size of window size is LDi . A filter of window size h is applied to YLDi to produce a feature map

vcehct=or,a(nYdLDdienoWteschth+e cbo)nvoluRtiLoDnia-l ho+pe1r,awtohr.erMe ax-iosvaern-otinmlienepaoroalicntgivoaptieornatfiuonnc(tCioonl,lobbeisrt

a bias et al.,

2011) is then applied to the feature map to get the maximum value of ch over the timesteps, i.e., c^h = max{ch}. We get as many c^h as the number of filters with varying window sizes. Those c^h are concatenated to produce ^c = [..., c^h, ...], and finally the output Di(YLDi ) = r^  [0, 1] is produced by the fully connected layer. To enhance the performance, we add the highway architecture

(Srivastava et al., 2015) before the final fully connected layer. We specifically describe how r^ is

produced from ^c in appendix.

3.2 POLICY GRADIENT WITH PARTIAL REWARD FUNCTION

Given the state s = Y1:t = {y1, ..., yt} and the action a = yt+1, we want to update the generator by the policy gradient given in Eq.(2). However, we do not know Q(st, yt+1), so we need to estimate it. Figure 2 shows the overview of how to calculate the action-state value for the partial reward function.
We estimate the action-state value in the same manner as SeqGAN, that is, we generate the complete sequence after the generation of yt+1 following the current generator, and observe the actual reward the generated sequence will receive. It is known as REINFORCE (Williams, 1992). The process to complete sequence is called rollout. Given the partial reward function Di, the action-state value can be derived as:

1 QDi (s = Y1:t, a = yt+1) = N

N

T

1 -

t

T -t
Di kDi(Ytn+k+1-LDi :t+k)

n=1

k=1

(4)

where N is the number of roll-out trajectories, Di is the reward scaler for Di, and  is the discount value. Practically, we ignore the future partial sequences that do not contain yt+1, and set the

4

Under review as a conference paper at ICLR 2018

expert

I

have

a

pen and you have

a

...

edited expert

I '

talk

a

pen view you

so

a

...

4 ) .  ':. = exp(- 1)

5 6 7 8 ':0 = {', ... 0}
 .:7 = exp( - 2)

Figure 3: Overview of expert-based reward function training. An edited expert is produced from expert data by editing some tokens to another, and we treat it as fake data. A pseudo reward function is the negative hamming distance, so it counts the number of tokens that are edited. Then quality function can be calculated as above.

discount value at 1.0. Therefore, Eq.(4) can be re-written as:

QDi (s

=

Y1:t, a

=

yt+1)

=

1 N

N n=1

1 LDi

LDi
Di Di(Ytn+k+1-LDi :t+k).
k=1

(5)

We performed this simplification to reduce the calculation costs. It should not harm the estimation
of QDi (s = Y1:t, a = yt+1) much, because the future partial sequence that does not contain yt+1 would be only slightly influenced by yt+1. Note that at the beginning or the end of the sequence where we can not take the partial sequence as Eq.(5), we ignore such partial sequences, and the
denominator LDi is subtracted. For example, when we estimate QDi (s = Y1:2, a = y3) and LDi = 4, we can not take the partial sequence Y0:3, so we ignore this partial sequence, and we calculate the action value from other partial sequences with the denominator LDi - 1 instead of LDi .

4 EXPERT-BASED REWARD FUNCTION TRAINING

SeqGAN employs a GAN framework. Although GAN has shown great success, it has also been reported that its training is difficult (Arjovsky & Bottou, 2017). A simple way to alleviate its problem is not to employ a GAN framework. Although GAN framework is attractive, it is not necessary when we can compose a good reward function in another way. In expert-based reward function training, reward functions are trained by discriminating between expert (or true) data and fake data that are produced by editing expert data. Expert-based reward function training does not use the generator's samples at all; therefore, it is not a kind of GAN frameworks.

There are several ways to produce a fake sequence from an expert one. We demonstrate a very simple approach in this paper. We get the expert sequence from the training dataset and randomly select some tokens and change them to another. These samples are then used as the fake data to train a reward function. Figure 3 shows the example of samplings of fake data from expert data. Although it is a very simple approach, we can expect the reward function not to get overfitted to certain samples, which frequently occurs when the samples of the trained generator are used, because the reward function is trained with various fake data.

We further utilize the advantage of expert-based reward function training that we know which part of the fake sequence is edited from the expert. When we train the generator with a policy gradient, a smooth reward function is desirable rather than a binary-like peaky reward function, especially when the reward function is for long-term sequence and generating a plausible long sequence is difficult. To compose a smooth reward function, we consider the goodness of the fake sequence, and use it to add modification to the objective of the reward function.

To measure the goodness of edited sequences, we introduce the quality function

q(x) = exp(r (x)/ ),

(6)

where r (x)  0 is the pseudo reward function and  is the temperature parameter. The pseudo
reward function roughly measures how good the edited sequence is, and can be any function as long as r (x)  0 is satisfied. In this paper, we chose the negative hamming distance as the pseudo
function. The hamming distance of sequence can be calculated by counting the number of tokens

5

Under review as a conference paper at ICLR 2018

that are changed to another token. Figure 3 shows the example of the calculation of the quality function. By using the quality function, we formulate the objective of the reward function as:

LD

=

E[logD(x)]xPexp

+

E[ 1 1

- +

q(x q(x

) log(1 )

-

D(x

))]x

Pexp

(7)

where Pexp is the distribution of expert data and Pexp is the distribution of edited expert data. The quality function in Eq.(6) has a value to some extent when the sequence is little edited from the expert one. Then, Eq.(7) decreases the weight for such samples. Especially, when the sequence is not edited from the expert one at all, Eq.(7) does not take such a sample as fake data. The quality function becomes close to 0 when the sequence is heavily edited from the expert one. Then, Eq.(7) becomes close to conventional binary cross-entropy loss. The closer  is to 0, the steeper the quality function becomes. This means that the quality function even returns a value close to 0 for the sample that is little edited from the expert one, resulting in composing a strict (binary-like) reward function. The bigger  is, the gentler the quality function becomes. This means that quality function returns the value to some extent for the sample that is changed from the expert one, resulting in composing a smooth reward function.

5 EXPERIMENTS
We examine the effect of the partial reward function and expert-based reward function training in synthetic data and real text data. For synthetic data experiments, we conduct the oracle test, which was also conducted in Yu et al. (2017). For real text data experiments, we conduct the text generation with BBC news articles.

5.1 SYNTHETIC SEQUENCE GENERATION

5.1.1 EXPERIMENTAL SETTING

In synthetic sequence generation, we use RNN with long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) whose parameters are randomly initialized and fixed, as the real data generator. This model is called the oracle. The oracle model provides the true data distribution poracle(yt|y1, ..., yt-1), and we train the generator to fit to the oracle.

Let G and Y denote the generator and the generated completed sequence. In the test, the per-

formance of generator can be evaluated as N LLoracle = -EY1:T G [

T t=1

logporacle(yt|Y1:t-1

)].

The oracle test can evaluate the exact performance of the generative models, which is not possible

with real data. The best way to evaluate the performance of the generator is to show the generator's

samples and let humans review them, but it takes too much time and effort to review a sufficient

number of samples. Now, if we assume that human has the natural distribution phuman(x) and the generator's distribution is q(x), we can realize such an evaluation by the negative log likelihood of

the human natural distribution -Exq[logphuman(x)] (Husza´r, 2015). In the oracle test, we use the oracle data distribution poracle instead of phuman.

As the oracle, we provide RNN with LSTM whose parameters are initialized by the normal distribution N (0, 1). Then, we generate 10,000 sequences of length 20 from the oracle as the training set S. For the reward function, we provide two partial reward functions: the short-term reward function, which treats the sequence of length 4, and the long-term reward function, which treats the sequence of length 20. In this experiments, the length of the completed sentences is always 20; therefore, the latter reward function can be considered as the reward function for the completed sequence, which is exactly the same as SeqGAN's reward function. Note that when we employ only a long-term reward function trained with adversarial training, this experimental setting is exactly the same as Yu et al. (2017). Window size and kernel numbers for reward function are shown in appendix. We employ L2 regularization to the reward function only when it is trained with adversarial training, and the hyper parameter for the regularization term  is set to be 0.01. The number of units of the hidden layer and embedding layer in the generator and oracle are all set to be 32. Batch size is set to be 40. The reward scaler  is set to be always 1.0. We do not employ the entropy regularization term for the objective of the policy gradient in this experiment. The number of rollout trajectories is set to be 10. We use Adam (Kingma & Ba, 2014) as the optimizer for both generator and reward functions. At the test, G generates 10,000 samples by stochastically choosing tokens according to the generator's output distribution and calculate the average N LLoracle over the samples.

6

Under review as a conference paper at ICLR 2018

Model Name PG Short R Long R Adversarial or Expert-based Oracle NLL

MLE

N

PG L(SeqGAN) Y

N

Y

Adversarial

9.03 8.61 (8.73)

PG S PG SL PG L exp PG S exp PG SL exp

YY YY YN YY YY

N Y Y N Y

Adversarial Adversarial Expert-based Expert-based Expert-based

8.33 8.58 7.50 6.01 6.69

Table 1: The result of oracle test. PG, R denote the policy gradient and reward function. The top is the model trained with only MLE. The second top is the original SeqGAN model. The score in parentheses is the one SeqGAN originally reported. The temperature  of the proposed training method is set to be 1.5 for the long-term reward function (Long R), and 0.001 for the short-term reward function (Short R).

Reward function
Long R Long R Short R Short R Short R Short R Short R

Given sequence
Y1:20
Y1:20 Y2:5 Y3:6 Y4:7 Y5:8 Y6:9

Reward
0.970 0.945 0.585 0.452 0.480 0.638 0.874

Table 2: The output of long-term reward function (Long R) and short-term reward function(Short R). Fake sequence Y is produced from Y by changing y5 to a random token. The reward is the average reward of 100 samples.

Model Name
PG S exp PG S exp PG S exp PG L exp PG L exp PG L exp PG L exp


0.001 0.3 1.0 0.01 1.0 1.5 2.0

Oracle NLL
6.01 6.55 6.59 7.99 7.85 7.50 7.75

Table 3: The performance of PG S exp and PG L exp with different  value.

We first pretrain the generator G by MLE with S, then train G by policy gradient. The pretraining of the generator by MLE is conducted because the generator produces very random sequences at first, and the training with the policy gradient is difficult in such a situation.
In adversarial training, the reward function is first pretrained with G trained with MLE. Then, the reward function is iteratively trained with G. In expert-based reward function training, reward functions are trained with dataset S and the edited expert dataset S by discriminating them until the convergence. Then, the generator G is trained with policy gradient until its convergence. The reward function is fixed during the training of the generator. When we make the edited expert dataset S , we change each token of the expert one to another random token with a 25-percent chance.
5.1.2 RESULT OF SYNTHETIC SEQUENCE GENERATION
Table 1 presents the result of the oracle test. Note that the top is the model trained with only MLE, and the second top is the same as SeqGAN. We use a word "exp" for the model trained with expertbased reward function training. The models with our proposals outperform SeqGAN and MLE, and PG S exp is the best model in all models.
PG S and PG SL outperform PG L, and PG S exp and PG SL exp outperform PG L exp, indicating that introducing the partial reward function is effective. To see the actual benefit of the partial reward function, we conducted further analysis. Table 2 presents the output of the long-term reward function and short-term reward function when they are given expert sequence Y or edited sequence Y . These reward functions are trained with expert-based reward function training. Y is produced from Y by changing y5 to a random token. Reward is the average reward of 100 samples. When we see the output of the long-term reward function, we can observe that it gives a high reward to both Y and Y . However, when we see the output of the short-term reward function, we can observe that it gives a low reward to fake partial sequence which contains y5. From those observations, we can say that the short-term reward function can give correct reward to a sequence, which the longterm reward function can not, and it is the reason that introducing the short-term reward function benefits sequence generation. It is noteworthy that using only the short-term partial reward function outperforms the use of both the long-term and the short-term partial reward function. It indicates

7

Under review as a conference paper at ICLR 2018

that the partial optimization of the sequence actually causes the optimization of the whole sequence in the oracle test. We assume this is because the sequence of the oracle model is not structured. In real data, such as text, the sequence is more structured, and the partial optimization usually does not cause the whole optimization of a sequence.

PG L exp, PG S exp, and PG SL exp outper-

form PG L, PG S, and PG SL respectively, indicating that expert-based reward function training is effective. We can see significant improvements over models trained with adversarial train-

1.000 0.975 0.950 0.925

Reward NLL

9.0 8.8 8.6

Reward NLL

ing (PG L exp, PG S exp, and PG SL exp make 1.11, 2.32, and 1.89 improvements over PG L, PG S, and PG SL respectively). We assume that an instability of adversarial training causes serious damage to the training of the generator. We found that the performance of expert-based reward func-

0.900 0.875 0.850 0.825 0.800
0

8.4
8.2
8.0
7.8 20 40 60 80 100
epoch

tion training depends on a temperature  . Table 3 Figure 4: Plots of reward and NLL during

shows the oracle score and  values in proposed the generator's training of PG L exp. We can

training method. For short-term partial reward see that the generator is properly optimized

function,  0 gives good performance, indicat- w.r.t. the reward function, and as the returned

ing that the generator prefers strict short-term re- reward increases, NLL decreases.

ward function. For long-term partial reward func-

tion,  = 1.5 gives good performance, indicating that the generators prefer a smooth reward func-

tion, which gives reward to some extent when the partial sequence is different from real data. In

expert-based reward training, the reward function is fixed during the training of the generator, so

we can visualize a return of the reward function to see if the policy gradient successfully works.

As we can see in Figure 4, the generator is properly optimized w.r.t. the reward function, and NLL

decreases as a returned reward increases, indicating that this metric is proper and easy to optimize

for the generator. Note that NLL is the average negative log likelihood of 10,000 samples from the

generator, and the reward is the average reward of 10,000 samples from the generator.

5.2 TEXT GENERATION
5.2.1 EXPERIMENTAL SETTING
As the dataset for text generation, we use BBC news articles. We use a corpus 1 which is a collection of articles of two topics, technology and business. The dataset consistes of sentences of length 20. When a sentence is shorter than 20, we add first some words of the next sentence. When a sentence is longer than 20, we remove last words. We get an 11,163 training set and a 1,500 test set. Each sequence is lower-cased. The size of the vocabulary is 18,004.

We compare the model MLE, PG L, PG L exp, PG S exp, and PG SL exp in this experiment. We use a partial reward function of length 4 and length 20, and they are trained in the same manner as the oracle test. We note again that the partial reward function of length 20 can be considered as the reward function for the completed sentence, because we only consider the sequence whose length is 20. In this experiments, we change the scaler parameter for short-term reward S in PG SL exp. The scaler parameter for long-term reward L is set to be always 1.0. The temperature parameters of the quality function are set to be 0.001 and 1.5 for the short-term reward function and the longterm reward function, respectively. The number of units of the hidden layer and embedding layer in the generator are set to be 200. When we make an edited sequences dataset S for expert-based reward function training, we change each token of the expert with a 15-percent chance. Moreover, when a token is changed to another token, it is sampled from the distribution of word occurence in the training dataset, rather than sampled randomly. We did this because, unlike with synthetic data, the occurence frequency is different for each word. If we train reward function with the same strategy as synthetic data, the learned reward function gives a low reward to a sentence that has a rare word, because a rare word appears more often in fake sequences. In this experiment, we add an entropy regularization term to the objective of the policy gradient to prevent the policy from becoming deterministic. The hyper parameter  in Eq.(3) is first set to be 0.02, and after 15 epochs,  is set to be 0.05. This is because, when  is high at the beginning of the training of the generator, an optimization w.r.t. reward does not occur. Other training settings are the same as the oracle test.

1http://mlg.ucd.ie/datasets/bbc.html

8

Under review as a conference paper at ICLR 2018

Model name
MLE PG L(SeqGAN) PG L exp PG S exp PG SL exp (S = 0.3) PG SL exp (S = 1.0)

BLEU-3
0.094 0.123 0.201 0.344 0.240 0.272

Generated sentence
the mill might lead that mobiles are continuing ," said frank stone in the west , down from 4 . about 120 ,000 people are strategic huge libraries to then they are as a virtual face report . the firm the machine is the year in the next step toward combining two of the software is about it to be according to the industry is part of the xbox consoles will be used to prevent conflicts . they will be it is still a special broadcast receiver . "we have a way of support for the consumer electronics , with time warner's fourth quarter and its profits for the study of the two companies could be done for their own

Table 4: The result of text generation. Although PG S exp scores the best BLEU-3, its output sentence lacks coherence. We can see that models of PG SL exp balance the coherence as well as partial correctness.

To evaluate the performance of the generator, we calculate the BLEU score of the generated sequence. As previous studies of text generation have done (Zhang et al., 2017; Yu et al., 2017), we use all the test set as the reference for BLEU evaluation. We generate 1,000 samples, and calculate an average BLEU-3 score.
5.2.2 RESULT OF TEXT GENERATION
Table 4 demonstrates the result of the text generation experiments. The models with our proposals outperform SeqGAN and MLE. PG S exp scores the best in BLEU-3. It is apparent that the PG L exp generates more comprehensible sentence than SeqGAN and MLE, indicating that expertbased reward training is effective. It is reasonable that PG S exp gives good scores in BLEU 3, because it prioritizes to generate the sequence, which is partially correct. This optimization fits to the n-gram-based evaluation. PG S exp, however, fails to generate a coherent sequence as we can see in Table 4. Unlike the experiment with synthetic data, partial optimization does not cause whole optimization because text data are well structured. Although the BLEU score of PG L exp is the fourth best in all models, a coherence of sequence seems to be maintained. The models of PG SL exp ensure both the partial correctness of sequence and coherence of sequence. By decreasing the short-term reward scaler S, we can generate more coherent sentences. Additional generated samples are in the appendix. They show that a variety of samples is ensured to some extent.
6 DISCUSSION
We stated two proposals for a learning better metric: partial reward function and expert-based reward function training. We showed that the partial reward function returns an accurate reward for a partial sequence, and benefits sequence generation training. By using partial functions of different scales (such as long term and short term), one can compose a reward function that can evaluate both coherence and the partial correctness of a sequence. We also showed that expert-based reward function training is effective compared to adversarial training. We demonstrated that a generator is well optimized w.r.t. the metric that is trained with expert-based reward function training.
The balance of short-term reward and long-term reward is a difficult problem. When prioritizing short-term reward too much, the generator produces a sequence that is partially correct but not coherent, and vice versa. One possible solution is that we first produce a coherent sequence by using only long-term reward, and then use a short-term reward function to make a modification to partial mistakes of the produced sequence.
As the generator is improving, it is desirable to update the reward function to a more strict one. A GAN framework is a good method in this sense, but as experimental results showed, it is difficult to train. One idea to update the reward function is to decrease a probability to change a token of expert in expert-based reward function training. If we decrease a probability, the reward function would become more strict. By decreasing a probability as the generator is improving, the generator might generate a more sophisticated sequence.
We believe that our proposals in this paper can be applied to GAN-based sequence generation. The partial reward function can be applied to GAN-based text generator directly. In fact, Shrivastava et al. (2016) used similar technique in the image generation with GAN. Expert-based reward function might make GAN training stable. The edited expert sequences have a lot of variety. There is a technique that uses the past generator's samples to ensure the variety of the samples for the training of the discriminator to stabilize GAN training, as we can see in Shrivastava et al. (2016), and the edited expert can be also applied for this purpose.

9

Under review as a conference paper at ICLR 2018
REFERENCES
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st international conference on Machine learning, pp. 1. ACM, 2004.
Martin Arjovsky and Le´on Bottou. Towards principled methods for training generative adversarial networks. In Proceedings of the 5th international conference on Learning representations, 2017.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In Proceedings of the 5th international conference on Learning representations, 2017.
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 1171­1179, 2015.
Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493­2537, 2011.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In Proceedings of the 33rd international conference on Machine learning, pp. 49­58, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2672­2680, 2014.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems, pp. 4565­4573, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735­1780, 1997.
Ferenc Husza´r. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? arXiv preprint arXiv:1511.05101, 2015.
Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose´ Miguel Herna´ndez-Lobato, Richard E Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In Proceedings of the 34th international conference on Machine learning, pp. 1645­1654, 2017.
Yoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.
Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy gradient and q-learning. In Proceedings of the 5th international conference on Learning representations, 2016.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311­318. Association for Computational Linguistics, 2002.
Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In Proceedings of the 4th international conference on Learning representations, 2016.
10

Under review as a conference paper at ICLR 2018
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb. Learning from simulated and unsupervised images through adversarial training. arXiv preprint arXiv:1612.07828, 2016.
Rupesh Kumar Srivastava, Klaus Greff, and Ju¨rgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015.
Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In Proceedings of the 28th international conference on Machine learning, pp. 1017­1024, 2011.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, pp. 1057­1063, 2000.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In Proceedings of association for the advancement of artificial intelligence, pp. 2852­2858, 2017.
Y. Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao, D. Shen, and L. Carin. Adversarial feature matching for text generation. In Proceedings of the 34th international conference on Machine learning, 2017.
11

Under review as a conference paper at ICLR 2018

A GENERATED SENTENCES

Model name MLE

Generated sentence
us light sweet crude futures to enter the [crossover] segment ," said nick ross , daimlerchrysler and damaged property finance the reserve chairman united said the us sought bankruptcy protection in the us on monday , kill bill: volume 2 his previous two companies holes has been picking up in the past year . "we are bringing a small third but the uk has been on hold of many of users to get their hands at least . icstis , "if you work i were struggling . "people's spending offer have to bring across the uk opened a 5bn) earlier " the intel researchers have leveraged the company's "most affordable media use , reaching the firm , the operating system unless voluntary fit will fund the animation prices , should remain also in novel ways that p2p is a human according to the stability pact to rises and confirm that thrives in succession . tourism said this tactic of creating vedomosti newspaper said mr irish and mark all the caribbean said the ability of the $2 , amid allegations of the virus-fighting program , updated monthly or just also brings me that had been compressed into recession - are a

PG L(SeqGAN)

these are pooled in london , draw about its future while it is the richest part of the digital images identity theft are being people to jump in sound to the neat business which can walk along at the show a apple of financial mail messages that has already has got given the chicago data , or or their semi-conductor after leaving weather , video games in over the third . stuff has telling parliament: level time checking for gadget there are combining automatic syncing worldwide , and you will give people the state pension age of of large and just been wiped out . the federal reserve is struggling with treasury and organisations have said you play online in a hydroelectric-power generator the airline said in the election , you would then its broadcasting attraction ," said research , a number of stagnation and more than 1 million copies of the personal firewalls ," he said . "it is us lawyers claimed that the mac mini and is being piloted by the royal national hi-tech crime unit yuganskneftegas (yugansk) "it will be disruptive to music , employment for mobile firm , almost three-quarters of job creation , the airline

PG L exp

"a literate and qualified turkish population ," insisted the year to meet he has been security to be . however the game on be done in the us in the us - it will play games on the net , the success is set to the company's and court ," he said , it plans the market by the uk an ex-chief financial advice , that is the biggest category said a problem in europe was not enough . people yukos claims that it would the banning of a market . it will make up of work recently about security there were originally had to be seen in the way . it were also falling demand at 20% of the however , which will continue to make 50gb of high-quality data , which is one of its investment can come the game maker you can be done in the year . "skype's success at spreading on the launch of sony's russian newspapers has been done - its own fuelled by lower prices than up to 100% . "we will be mr ghosn will devote 40% of the directive will put up to google's funds . but on the network is

PG S exp

they would retaliate by seeking injunctions in the company , said they had been seen as they will be used they will be able to add to be used to spot in the world's largest car maker , said they they will be able to be recycled at 1 . 4% in october . 3bn . however , he said however , prices fell as part of the service . they need to invest in the cost of more than however , he said he would be to raise awareness of the 14 . but it is part of the this is likely to be seen in the us government , you go from the deal . but he said however , the company , which is part of the euro last week after new york times on the mobile they will be able to prevent conflicts to take their office . but they are looking for bargains , which more than 1 . 4% in the company , which is part of the industry will be able to invest the deal has been seen as they will be able to prevent conflicts . but they had been sidelined in

PG SL exp (S = 0.3)

at a mere £20 , metal slug 3 is as cheap , but it is not the second time when the global entertainment industry was more than two to the uk exported , according to the uk-based journal screen that it is not the firm of england is expected to go on a broadband connection , with a single threat according to figures to come to meet , it said it would also reduce its customers , according to prevail if the end of the year , microsoft , which is expected by to $4 . 35bn , said it two of the most important costs . " spanish , it would be failed to do a new record for it is expected to make an advisor to work , said it would allow broadband connections by the trading national users navigated around the dollar of 572 ,900 points to build the risks , and it is so far , it is not about stealing to the growing efforts in new york in the south following an apple ipod , "it's for the most important for us crude oil company in early february , according to the report . at

PG SL exp (S = 1.0)

according to the report , the company has not been being announced it will go from the decision to discuss one of the two companies will be able to go with other digital entertainment , with other companies like the it will be able to be part of its efforts . "we're on the outlook for its core businesses , "we want to go on the technology ," he said . "we're in december , in a europe - on yukos has been made in december to graphics out of its efforts in 2005 , and paramount will go for their aim is to launch a new rental subscription service , this proves , the company will be able to more than 50% of the economy is part of its efforts to transfer files as a threat of the russian meanwhile , the decision for bt is available in december , the largest us giant earned $630m (£481 . 5m) the company announced it will see the study of the decision for digital images and technology from two companies , people will have to think of the report that it has been working with other carmakers . 5bn in january

Table 5: The examples of generated sentences.

12

Under review as a conference paper at ICLR 2018

B THE WINDOW SIZE AND KERNEL NUMBERS OF REWARD FUNCTIONS

Sequence length 4
20

(window size, kernel numbers) (1,100), (2,200), (3,200), (4,200)
(1,100), (2,200), (3,200), (4,200), (5,200), (6,100), (7,100), (8,100), (9,100), (10,100), (15, 160), (20,160)

Table 6: Window size and kernel numbers for reward functions

We set window size and kernel numbers as Table 6 in both experiments.

C DETAIL OF PARTIAL REWARD FUNCTION
We describe how to get a feature map containing each filter's output ^c in section 3.1. We, then, add highway architecture as below,
 = (WT · ^c + bT ), C^ =  · H(^c, WH ) + (1 -  ) · ^c,
where WT , bT , and WH are highway layer weights, H denotes an affine transform followed by a nonlinear activation function such as ReLU, and  is the "transform gate" with the same dimensionality as H(^c, WH ) and ^c. Finally, we get an output of partial reward function as
r^ = (Wo · C^ + bo)
where Wo and bo are the output layer weight and bias, respectively.

13

