Under review as a conference paper at ICLR 2018
DISCOVERING THE MECHANICS OF HIDDEN NEURONS
Anonymous authors Paper under double-blind review
ABSTRACT
Neural networks trained through stochastic gradient descent (SGD) have been around for more than 30 years, but they still escape our understanding. This paper takes an experimental approach, with a divide-and-conquer strategy in mind: we start by studying what happens in single neurons. While being the core building block of deep neural networks, the way they encode information about the inputs and how such encodings emerge is still unknown. We report experiments providing strong evidence that hidden neurons behave like binary classifiers during training and testing. During training, analysis of the gradients reveals that a neuron separates two categories of inputs, which are impressively constant across training. During testing, we show that the fuzzy, binary partition described above embeds the core information used by the network for its prediction. These observations bring to light some of the core internal mechanics of deep neural networks, and have the potential to guide the next theoretical and practical developments.
1 INTRODUCTION
Deep neural networks are methods full of good surprises. Today, to perform image classification, one can train a 100M parameters convolutional neural network (CNN) with 1M training examples. Beyond raising questions about generalization (Zhang et al., 2017), it appears that the classification models derived from those CNNs offer object detectors for free, simply by thresholding activation maps (Yosinski et al., 2015; Zhou et al., 2015; Bau et al., 2017). The learned representations also appear to be universal enough to be re-used on new tasks even in an entirely different domain (e.g. from natural to medical images in Gulshan et al. (2016)). If memory or computation are bottlenecks, no problem, networks with binary weights and binary activations work just as well (Rastegari et al., 2016). What characteristics of SGD trained neural networks allow these intriguing behaviour to emerge?
Deep neural networks also have their limitations. They currently pose lots of difficulties with respect to continuous learning (Kemker et al., 2017), robustness (Szegedy et al., 2014; Nguyen et al., 2015), or unsupervised learning (Bojanowski & Joulin, 2017). Are there other good surprises to expect in those fields, or do those difficulties correspond to fundamental limitations of SGD trained deep neural networks?
In order to answer both questions, a better understanding of deep neural networks is definitely needed. Since the intricate nature of the network hinders theoretical developments, we believe experiments offer a valuable alternative path to offer an insight into the key mechanisms supporting the success of neural networks, thereby paving the way both for future theoretical and practical developments. In other words: analysing how something works helps understanding why it works, and gives ideas to make it work better.
In particular, the workings of hidden neurons, while being the core building block of deep neural networks, are still a mystery. It is tempting to associate hidden neurons to the detection of semantically relevant concepts. Accordingly, many works studying neurons have focused on their interpretability. A common and generally admitted conception consists in considering that they represent concepts with a level of abstraction that grows with the layer depth (LeCun et al., 2015). This conception has been supported by several works showing that intermediate feature maps in convolutional neural networks can be used to detect higher level objects through simple thresholding ((Yosinski et al., 2015; Zhou et al., 2015; Bau et al., 2017). However, it is not clear
1

Under review as a conference paper at ICLR 2018
if these observations reflect the entire relevant information captured by that feature map, or, on the contrary, if this interpretation is ignoring important aspects of it. Moreover, investigating further the emergence of concepts into neurons is also motivated by the observation that the object detection technique only works on a subset of the feature maps, leaving the understanding of the other feature maps as an open question.
While previous works provide hints about it, the complete characterization of the way a neuron encodes information about the input remains unknown. Moreover, the dynamics of training that lead to the encoding of information used by a neuron is -to our knowledge- unexplored. This paper uses an experimental approach that makes a big step forward in the understanding of both these aspects of neurons. The main observation is the following: the encodings and dynamics of a neuron can approximately be characterized by the behaviour of a binary classifier. More precisely:
1. During training, we observe that the sign of the partial derivative of the loss with respect to an activation is impressively constant. We observe experimentally that this leads a neuron to push activations either up, or down, partitioning the inputs in two categories of nearly equal size.
2. During testing, quantization and binarization experiments show that the fuzzy, binary partition observed in point 1. embeds the core information used by the network for its predictions.
This surprisingly simple behaviour has been observed across layers, networks and at different problem scales (MNIST, cifar10 and ImageNet). It seems like hidden neurons have a clearly defined behaviour that naturally emerges in neural networks trained with stochastic gradient descent, that -to our knowledge- has remained undiscovered until now.
While further validation is needed, this paper also presents a discussion of the implications of such claim. First of all, it shows that the results about neuron interpretabiliy do not reflect the complete behaviour of a neuron. Moreover, we illustrate that our claim provides helpful guidelines for the design of activation functions with a brief analysis of ReLu. We also point out another future direction of research: we argue that the global dynamics regulating which samples are the most clearly partitioned in a single neuron are, with respect to generalization, as important as the partition itself.
2 RELATED WORK
Previous work trying to understand the function of a neuron focuses on its interpretability in terms of semantically relevant concepts. In the context of convolutional neural networks for image classification, number of recent works have investigated how the activation of a single neuron is related to the input image, by developing methods to visualize the image structures that activate a neuron the most. Those methods include the training of a deconvolution network to project the feature activations back to the input pixel space (Zeiler & Fergus, 2014), and the analysis of how a neuron activation decreases when occluding portions of the input image, revealing which parts of the scene are important regarding this neuron activation (Zeiler & Fergus, 2014; Zhou et al., 2015). Inverse problem formulations have also been considered to reconstruct an image by inverting a representation obtained inside the network, using a gradient-descent approach regularized by different kinds of image models (Mahendran & Vedaldi, 2015; Yosinski et al., 2015). More recently, Bau et al. (2017) went a step further by developping methods to quantify the interpretability of the signal extracted through the previously described visualization methods. All those work conclude that (some of) the individual neurons have the capability to capture visually consistent structures. The fact that object detection emerges when considering units with highest activation inside a CNN trained to recognize scenes (Zhou et al., 2015) supports the idea that a binary form of encoding is embedded within the trained network. However, it is not clear if these observations reflect the entire relevant information captured by the studied feature map. Moreover, investigating further the emergence of concepts into neurons is also motivated by the observation that the object detection technique only works on a subset of the feature maps, leaving the understanding of the others as an
2

Under review as a conference paper at ICLR 2018
open question. Our paper leaves interpretability behind, but provides experiments for the validation of a complete description of the encoding of information in any neuron.
Since the idea of binary encoding is central to our work, it is also related to works considering network binarization in a power consumption context, to mitigate the computational and memory requirements of convolutional network. In Courbariaux & David (2015), only the weights are constrained to only two possible values while, in Rastegari et al. (2016) and Hubara et al. (2016), both the filters and the inputs to convolutional layers are approximated with binary values. The fact that those methods only induce a negligible loss in accuracy reveals that the conventional continuous definition of activations is certainly redundant. Motivated by those previous observations, our work further challenges the binary nature of individual neurons. It does not force binary activations during training, but instead reveals that a bimodal activation pattern naturally emerges from a conventional training procedure.
An important part of our work relies on the observation that the gradients used by the learning algorithm follow some consistent, predictable patterns. This observation has already been highlighted by Shwartz-Ziv & Tishby (2017) and Sinha et al. (2017). However, while these works focus on the gradients with respect to parameters on a batch of samples, we analyse the gradients with respect to activations on single samples. This difference of perspective is crucial for the understanding of the representation learned by a neuron, and is a key aspect of our paper.
3 PRELIMINARIES
Our goal is to describe the behaviour of neurons in a neural network. Given the growing complexity of neural networks, it is useful to define which part of the architecture we denote as a neuron. We associate neurons to activation functions: each application of a non-linear function to a single value defines one neuron. Following the literature, we will refer to the value preceding the application of the activation function as the pre-activation, and the result of it as the activation. In order to reflect the spatial structure of convolutional layers, we consider the different pixels of a feature map as different activations from a same neuron when studying statistical distributions.
We experiment with three different models: a 2-layer MLP with 0.5 dropout (Srivastava et al., 2014) trained on MNIST (LeCun et al., 1998), A 12-layer CNN with batchnorm (Ioffe & Szegedy, 2015) trained on CIFAR-10 (Krizhevsky & Hinton, 2009) and a 50-layer ResNet (He et al., 2016) trained on ImageNet (Deng et al., 2009). All networks use ReLu activation (Nair & Hinton, 2010). Through the paper, we will repeatedly refer to specific layers of these networks. For readability, the naming of the layers is specified only once. For the MLP, we simply refer to the two fully-connected layers as dense1 and dense2. The cifar CNN is divided in 4 stages of three layers. Layers from a stage have the same spatial dimensions and stages are separated by max-pooling layers. We refer to each layer through the index of its stage and the position of the layer inside the stage, starting at 0. Stage2layer0 refers thus to the first layer of the third stage. We use the ResNet50 network as provided by the Keras applications. We re-use their notations and refer to layers through their stage (in numbers) and block index (in letters). We only study the neurons after combination of the block outputs and the skip connections. The very first layer does not belong to a standard ResNet block, and is denoted as conv1. More information about the models and their training procedure can be found in appendix. Our experiments were implemented using the Keras (Chollet & others, 2015) and Tensorflow (Agarwal et al., 2016) libraries.
4 NEURONS BEHAVE LIKE BINARY CLASSIFIERS DURING TRAINING
We start our quest of understanding a neuron by watching the gradients flowing through it. Most of the works analysing training dynamics of neural networks have focused on analysing gradients of the loss with respect to parameters, since these are directly used by the learning method. However, gradients with respect to the activations can also give us precious insights, since they directly reveal how the representation of a single sample is constructed.
3

Under review as a conference paper at ICLR 2018

4.1 THE REGULARITY OF ACTIVATION GRADIENTS
We proceed to a standard training of the cifar CNN and the MNIST MLP networks until convergence. During training, but in a separate process, we record the activation gradients of a layer on a regular basis (every 100 batches for cifar and every 10 batches for MNIST), leading to 1600 and 2350 recordings respectively. Measures were only performed on a random subset of neurons and samples due to memory limitations (see appendix for more details).
While training of neural networks could potentially be a very noisy procedure, we observed a remarkably clear and regular signal in the activation gradients. The regularity of training has already been observed for weights in Shwartz-Ziv & Tishby (2017) and Sinha et al. (2017), we observe it now through the lens of activations. In particular, we observe that an activation is pushed in the same direction throughout nearly all the training: either up or down. Figure 1 shows, for five randomly selected neurons from different layers, the histograms of the average sign of activation partial derivatives over all the recordings. Zero partial derivatives are ignored to make the signal more clear. As one can see, the average sign is either 1 or -1 for most of the activations, which indicates that the derivative sign doesn't change at all through the training. This is exactly the behaviour you would expect in the output of a binary classifier trying to separate two categories. Since around half of the activations have positive derivatives and the other half negative ones, a neuron seemingly tries to partition the input distribution in two distinct and nearly equally-sized categories.
This behaviour is much less apparent in early layers. Indeed, the histogram corresponding to a neuron from stage2layer2 shows more sign changes than the one from stage3layer2. Stage0Layer0 is even worse: the majority of the activation gradients constantly change signs during training. It has been observed that noise in gradients increases exponentially with depth (Balduzzi et al., 2017). The observed noise seems to be an inconvenience emerging from the architecture, rather than a key aspect of learning. We suspect that this noise hides the behaviour observed in the last layers and that further inspection could actually reveal the same signal, or, alternatively, that this noise disables learning in the first layers while not altering final performance. We leave this analysis as future work.

# of samples

1e3 6 4 2 0 -1.0

neuron from dense1

-0.5 0.0

0.5

Average gradient sign

1.0

# of samples

1e4 1.0 0.8 0.6 0.4 0.2 0.0 -1.0

neuron from dense2

-0.5 0.0

0.5

Average gradient sign

1.0

1e4 neuron from stage0layer0

1.25 1.00 0.75 0.50

0.25 0.00 -1.0

-0.5 0.0

0.5

Average gradient sign

1.0

# of samples # of samples

1e4 neuron from stage2layer2
3

2

1

0 -1.0

-0.5 0.0

0.5

Average gradient sign

1.0

1e4 5 4 3 2 1 0 -1.0

neuron from stage3layer2

-0.5 0.0

0.5

Average gradient sign

1.0

# of samples

Figure 1: The figures show the histograms of the average sign of partial derivatives of the loss with respect to activations, as collected over training for a random neuron in five different layers. An average derivative sign of 1 means that the derivative of this activation was positive in all the recordings performed during training. We observe two distinct categories: some activations should always go up, others always down. This reveals that the neuron receives consistent information about how to affect the activation, allowing it to act as a binary classifier. The trend is however not apparent in early layers. We suspect this is due to the inconvenient gradient noise appearing in very deep networks (Balduzzi et al., 2017). As detailed in section 3, the layers from the first row are part of a network trained on MNIST, the second row on CIFAR-10.

4

Under review as a conference paper at ICLR 2018
4.2 WATCHING NEURONS LEARN The gradients strongly indicate that a neuron tries to separate two categories of inputs. Does this effectively happen during training? We assign each sample to a category based on its average activation partial derivative sign, and see how both categories' pre-activations evolve across the recordings. Categories are named 'low' and 'high' for negative and positive derivatives respectively. Figure 2 shows the results for a neuron in dense2, and another one in stage3layer2. The dynamics of more neurons can be found in appendix and in video format on the following link: https://www.youtube.com/channel/UC5VC20umb8r55sOkbNExB4A. This visualization unveils a seemingly endless struggle to separate both categories. While very slow, the signal is effectively there: both categories are distinguished through the training procedure. However, training stops before both categories are completely separated. As will be discussed in section 6, this raises a question that we believe is crucial: what mechanism regulates which samples are well partitioned in a neuron? To illustrate that the dynamics are not a simple translation, the final highest pre-activations are highlighted in yellow in the visualizations.
Figure 2: Evolution of the pre-activation distributions across training of a neuron from dense2 (above, trained on MNIST) and stage3layer2 (below, trained on cifar10). Pre-activations are separated in two categories, high and low, based on the average partial derivative sign over training of their corresponding activation. We can see that both categories are being separated during training. The final highest pre-activations of the high category are highlighted to show that it is not a simple translation. Supplementary images from other neurons can be found in appendix and in video format on https://www.youtube.com/channel/UC5VC20umb8r55sOkbNExB4A.
5 THE BINARY BEHAVIOUR OF ACTIVATIONS
We have shown that neurons operate like binary classifiers during training. Does this also reflect the way a neuron encodes information about the input during testing? Even though the categories are not completely separated, does this partition provide the necessary information for the next layer? In this section, we test if all the information a neuron transmits is encoded in the binary partition observed in the previous section. We do this by studying how the performance of neural networks changes when activations of a trained layer are modified through specifically designed quantization and binarization strategies. The strategies are designed not only to highlight the hypothetical binary aspect of the encodings, but also to reveal structural components of it: how fuzzy is the binary rule and can we locate the thresholds? Moreover, this section also studies ResNet50 since computational limitations are less of a problem.
5

Under review as a conference paper at ICLR 2018

5.1 A STUDY OF ROBUSTNESS TO PRE-ACTIVATION QUANTIZATION
The first experiment aims at testing if a neural network trained in a standard way is robust to quantization of pre-activations. Instead of accepting a continuous range of values, only two distinct values per neuron can be provided. Are these two values per neuron enough for transmitting the relevant information to the next layers? The quantization is based on the percentile rank of a pre-activations with respect to the pre-activation distribution of the neuron. For each neuron, percentiles are computed based on a subset of the data (training or test). The percentile corresponding to a chosen rank is then used as a threshold, separating the pre-activations in two distinct sets. A pre-activation will be quantized to the average value of the set it belongs to. Eleven thresholds equally spaced between 0 and 100 are tried out for the experiment. While the percentile is computed for each neuron specifically, the percentile rank used as a threshold is the same for all of them.
Figure 3 shows how accuracy on the test set is affected when quantization is performed on different layers. No form of training to adapt to this new pre-activation distribution is applied. The first and last layers of each network are studied, as well as one intermediate layer for the cifar10 CNN and ResNet50. The signal is clear: neural networks are astonishingly robust to quantization of their pre-activations, although not explicitly designed to be so. Performance is quite robust to the chosen threshold, with a preference for higher percentile ranks. Amongst the 8 layers tested, only the conv1 layer from ResNet50 shows significant decrease in accuracy when its pre-activations are quantized. We believe this is due to poor the quality of the gradients in early layers, as discussed in section 4.1.

Accuracy

Accuracy Accuracy

1.0 dense1
original 0.8 quantized 0.6 relu treshold
0.4
0.2
0.0 0 20 40 60 80 100 Percentile rank used as treshold

1.0 dense2
0.8 0.6 0.4 0.2 0.0 0 20 40 60 80 100
Percentile rank used as treshold

1.0 stage0layer0
0.8
0.6
0.4 original 0.2 quantized
relu treshold 0.0 0 20 40 60 80 100
Percentile rank used as treshold

Accuracy

1.0 stage2layer0
0.8 0.6 0.4 0.2 0.0 0 20 40 60 80 100
Percentile rank used as treshold

Accuracy

1.0 stage3layer2
0.8 0.6 0.4 0.2 0.0 0 20 40 60 80 100
Percentile rank used as treshold

1.0 conv1
0.8
0.6 original quantized
0.4 relu treshold
0.2
0.0 0 20 40 60 80 100 Percentile rank used as treshold

Top k accuracy

1.0 3d 0.8 0.6 0.4 0.2 0.0 0 20 40 60 80 100
Percentile rank used as treshold

Top k accuracy

1.0 0.8 0.6 0.4 0.2 0.0 0

5c
20 40 60 80 100 Percentile rank used as treshold

Top k accuracy

Figure 3: Quantization experiment: measuring test accuracy when pre-activations of a layer are quantized to two values per neuron, based on their percentile rank. Quantization is performed on a single layer at a time, using a range of percentile ranks as quantization thresholds. Except for conv1 (very first layer of ResNet50), the networks are astonishingly robust to quantization, suggesting that neurons provide a binary signal to the next layers. The average percentile rank of the zero pre-activation (which corresponds to ReLu's threshold) is also provided, and is discussed in section 5.3.

6

Under review as a conference paper at ICLR 2018
5.2 A SLIDING WINDOW BINARIZATION EXPERIMENT
The quantization experiment suggests that each neuron transmits a binary signal to the next layer, which is a first step for confirming our hypothesis. But we still don't have a clear view on how the signal is encoded. Is there a clear threshold or a fuzzy rule? When can we be confident that a pre-activation should be considered as a member of the low category or the high one? What is the size of both categories? This section presents the design and results of a sliding window binarization experiment whose purpose is to provide insights around these questions.
In this experiment, instead of separating the pre-activations in two groups using a single percentile rank as threshold, we use two thresholds, forming a window. Activations between the two thresholds are mapped to 1, and activations outside of it are mapped to 0. The experiment is performed using a window with a width of 10 percentile ranks and a center that slides from rank 5 to rank 95. Thus, only 10% of all the pre-activations of a neuron are mapped to 1. Which 10% is fixed by the center of the window: if the center is at rank 35, only the activations between the 30 and 40 percentiles are mapped to 1. Similarly to the quantization experiment, the percentiles are computed using a randomly selected subset of the data (train or test).
With such a binarization method, the only information from the original signal that remains is if the activation was inside or outside the window. The usefulness of this information for a particular window depends on the coding scheme used by the neuron. The results can thus potentially provide insights about the organization of the representation and allow us to indirectly observe the binary partition used to encode information. To measure the usefulness of the transformed pre-activations, we monitor the test accuracy after reinitialization and retraining of the layers that follow the layer where the binarization has been performed. For computational reasons, linear classifier probes (Alain & Bengio, 2016) are used for analysing ResNet50 layers instead of retraining all the subsequent layers 1. With this approach, we can verify if a network is able to make good use of the information contained in the binarized pre-activations and learn useful patterns that generalize to the test set. Since a neuron hypothetically transmits information through a binary partition of the inputs, performance of the network should be better when the pre-activations inside the window correspond to the same category. The performance should decrease when the window is located in fuzzy regions, where both categories are equally present. This experiment thus provide a tool to indirectly measure the presence of the two categories used by the coding scheme.
The results presented in Figure 4 show a clear signal across all layers and networks: the further away the center of the window is from rank 50, the better the performance of the network. Moreover, the symmetry around percentile rank 50 is striking. Given the binarization strategy, these results indicate a fuzzy partition of two categories, with a threshold around percentile rank 50, and a confidence in the category that increases the higher (or lower) the activation. While the partitions separate the inputs in equally sized categories on average, the size of the categories varies across neurons and is not exactly 50%, which explains the fact that a window center at the 50th percentile rank does not induce random predictions. Our results confirm thus the behaviour observed when analysing the training dynamics: a neuron partitions the inputs in two distinct but overlapping categories of quasi equal size. Moreover, these new experiments tell us that this partition also characterizes how neurons encode information about the inputs.
5.3 A BRIEF ANALYSIS OF THE RELU ACTIVATION
Having strong evidence that a neuron encodes information through a fuzzy, binary partition of the inputs, let's analyse the role of the ReLu activation. Having a thresholding behaviour built in, we can analyse where this threshold is positioned compared to the partition threshold which lies around the 50th percentile rank. Table 1 shows the average and standard deviation of the ReLu threshold positions across all neurons of a layer. We observe that in the last layers of the network, the ReLu threshold positions converge to very specific percentile ranks with high precision in all their neurons.
1The original paper presenting linear probes mentions a problem emerging from the very high dimensionality of convolutional layers which is not suited for a linear classifier. To address this issue, we apply the linear probes on the global averages of the feature maps.
7

Under review as a conference paper at ICLR 2018

Accuracy

Accuracy Accuracy

1.0 dense1
0.8
0.6
0.4 0.2 original
binarized 0.0 0 20 40 60 80 100
Percentile rank used as window center

1.0 dense2
0.8 0.6 0.4 0.2 0.0 0 20 40 60 80 100
Percentile rank used as window center

1.0 stage0layer0
0.8
0.6
0.4 0.2 original
binarized 0.0 0 20 40 60 80 100
Percentile rank used as window center

Accuracy

1.0 stage2layer0
0.8 0.6 0.4 0.2 0.0 0 20 40 60 80 100
Percentile rank used as window center

Accuracy

1.0 stage3layer2
0.8 0.6 0.4 0.2 0.0 0 20 40 60 80 100
Percentile rank used as window center

1.0 3d original
0.8 binarized
0.6
0.4
0.2
0.0 0 20 40 60 80 100 Percentile rank used as window center

Top k probe accuracy

1.0 4d 0.8 0.6 0.4 0.2 0.0 0 20 40 60 80 100
Percentile rank used as window center

Top k probe accuracy

1.0 5c

0.8

0.6

0.4

0.2

0.0 0

20 40 60 80 100

Percentile rank used as window center

Top k probe accuracy

Figure 4: Sliding window binarization experiment: pre-activations inside a window with a width of percentile rank 10 are mapped to 1, pre-activations outside of it to 0. Information that remains in the signal is only the fact that the pre-activation was inside or outside the window. Observing if a new network can use this information for classification reveals structure about the encoding: which window positions provide the most important information for a classifier? The results show a clear pattern across all layers and networks that confirms an encoding based on a fuzzy, binary partition of the inputs in two categories of nearly equal size.

Table 1: Average and standard deviation of the percentile rank of the ReLu threshold (0 value) across neurons of a layer. The percentile rank is based on the pre-activation distributions of each neuron. We observe that in the last layers, the position of the ReLu threshold is nearly the same for all neurons, suggesting convergence to a very precise position in the pre-activation distribution.

Layer dense1 dense2
stage0layer0 stage2layer0 stage3layer2
conv0 3d 5c

Average 80 50
76 65 53
31 31 84

Standard Deviation 8.9 12
38 16 2.2
37 20 2.8

In particular, the threshold converges to the 52th percentile rank in stage3layer2 and to the 84th rank for ResNet50 with a standard deviation smaller than 3 ranks. This is very helpful given the coding scheme: ReLu seems to only keep activations that have sufficiently high probability of being in the high partition, and adapts to the fuzzy nature of the coding. The more fuzzy, the higher the percentile rank of the threshold. While further analysis is necessary, this behaviour adjusts well to our hypothesis and will hopefully give us new insights about the behaviour of activation functions.
8

Under review as a conference paper at ICLR 2018
6 DISCUSSION AND FUTURE WORK
In this paper, we try to validate an ambitious hypothesis describing the behaviour of a neuron in a neural network during training and testing. Our hypothesis is surprisingly simple: a neuron behaves like a binary classifier, separating two categories of inputs. The categories, of nearly equal size, are provided by the backpropagated gradients and are impressively consistent during training. While stronger validation is needed, our current experiments all validate this behaviour. An important next step will be to repeat our experiments on other data modalities and networks, and to explain our observations using theoretical tools.
Our results have direct implications on the interpretability of neurons. Studies analysing interpretability focused on the highest activations, e.g. above the 99.5 percentile in Bau et al. (2017). While these activations are the ones who are the best partitioned by the neuron, we show that they do not reflect the complete behaviour of the neuron at all. While the majority of concepts we use appear in a limited number of samples (e.g. objects), neurons tend to consistently learn concepts that distinguish half of the observed samples, which is fundamentally different.
We believe our work also provides precious insights for the design of activation functions. As we've started to show in section 5.3, discovering the way a neuron encodes information seems crucial to understand the role of activation functions. In particular, the ReLu activation seems to have a very convenient behaviour in the last layers: it only keeps activations for which the belonging to the high category is sufficiently certain.
Finally, we hope that our work will help the community to understand the bigger picture: explaining the behaviour and highlighting the limits of deep neural networks. In particular, we believe that our work provides a new angle of attack for the puzzle of the generalization gap observed in Zhang et al. (2017). Indeed the works on neuron interpretability tell us that a neuron, while not able to finish its partitioning before convergence, seems to prioritize samples with common patterns. This prioritization effect during training has already been observed indirectly in Arpit et al. (2017), and we are now able to localize and study it in depth. The dynamics behind this prioritization between samples of a same category should provide insights about the generalization puzzle. While most of previous work has focused on the width of local minima (Keskar et al., 2017), the regularity of the gradients and the prioritization effect suggest that the slope leading to it also matters: local minima with good generalization abilities are stronger attractors and are reached more rapidly. Further investigation of this hypothesis is our next objective.
7 CONCLUSION
Our paper presents an experimental approach towards the understanding of deep neural networks. With a divide-and-conquer strategy in mind, we start by studying the behaviour of single neurons. Our observations are quite unexpected: neurons seem to have a simple and regular behaviour during training and testing. Specifically, we provide experiments validating that the behaviour of a neuron can be characterized by the one of a binary classifier.
First, an analysis of the gradients of the loss with respect to activations indicates that a neuron tries to separate samples in two categories. Indeed, the sign of the partial derivatives of an activation is, across training, either mostly positive or negative. Visualizations enable us to see how a fuzzy, binary partition of the activations emerges from this behaviour. Both categories, moreover, are approximately of equal size.
Our next step consists in testing if the observed partitioning reflects the way a neuron encodes information about the inputs, when the network makes a prediction. We apply quantization and binarization strategies, specifically designed for highlighting the structure of the coding scheme used by neurons. We first observe that neural networks are very robust to quantization of their activations -even without retraining, suggesting a form of binary encoding. Next, a sliding window experiment allows us to observe how the binary encoding is structured: how fuzzy is it and where is the threshold? Our observations supplement our claim: neurons encode information through a
9

Under review as a conference paper at ICLR 2018
fuzzy, binary partition of the inputs in two categories of nearly equal size.
Our work thus tries to validate an ambitious claim: characterizing the behaviour of neurons as binary classifiers during training and testing. While a theoretical study is still lacking, and experiments on other modalities and networks are needed, we believe our results are sufficiently convincing to already talk about the impact of this claim. A direct consequence of it is that previous work on neuron interpretability are not representative of the entire function of neurons. Moreover, understanding the encodings used by neurons is also essential to understand and design activation functions, as we briefly show through an analysis of the ReLu activation. Finally, we believe our insights provide a new angle of attack for solving the generalization puzzle. Indeed, we unveil a prioritizing effect already observed indirectly, which, we argue, is essential for the generalization abilities of neural networks.
ACKNOWLEDGMENTS
To be filled in.
REFERENCES
Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv preprint arXiv:1603.04467, 2016.
Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A Closer Look at Memorization in Deep Networks. In ICML, 2017.
David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, and Brian Ma, Kurt Wan-Duo McWilliams. The Shattered Gradients Problem: If resnets are the answer, then what is the question? In ICML, 2017.
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network Dissection: Quantifying Interpretability of Deep Visual Representations. In CVPR, 2017.
Piotr Bojanowski and Armand Joulin. Unsupervised Learning by Predicting Noise. arXiv preprint arXiv:1704.05310, 2017.
Chollet and Franc¸ois others. Keras, 2015. URL https://github.com/fchollet/keras.
Matthieu Courbariaux and Jean-pierre David. BinaryConnect : Training Deep Neural Networks with binary weights during propagations. In NIPS, pp. 3123---3131, 2015.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, pp. 248­255, 2009.
Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, Ramasamy Kim, Rajiv Raman, Philip C Nelson, Jessica L Mega, and Dale R Webster. Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs. Jama, 316(22):2402­2410, 2016. doi: 10.1001/jama.2016.17216.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In CVPR, pp. 770­778, 2016.
10

Under review as a conference paper at ICLR 2018
I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized Neural Networks. In NIPS, 2016.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. ICML, pp. 448---456, 2015. ISSN 0717-6163. doi: 10.1007/s13398-014-0173-7.2. URL http://arxiv.org/abs/1502.03167.
Ronald Kemker, Angelina Abitino, Marc Mcclure, and Christopher Kanan. Measuring Catastrophic Forgetting in Neural Networks. arXiv preprint arXiv:1708.02072, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. In ICLR, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning Multiple Layers of Features from Tiny Images. Technical report, University of Toronto, 2009.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2323, 1998. ISSN 00189219. doi: 10.1109/5.726791.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, 2015. ISSN 0028-0836. doi: 10.1038/nature14539. URL http://dx.doi.org/10.1038/ nature14539{%}5Cn10.1038/nature14539.
Aravindh Mahendran and Andrea Vedaldi. Understanding Deep Image Representations by Inverting Them. In CVPR, 2015.
Vinod Nair and Geoffrey E Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In ICML, pp. 807---814, 2010.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep Neural Networks are Easily Fooled : High Confidence Predictions for Unrecognizable Images. In CVPR, pp. 427­436, 2015.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In ECCV, pp. 525­542. Springer, 2016.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the Black Box of Deep Neural Networks via Information. arXiv preprint arXiv:1703.00810, 2017.
Abhishek Sinha, Mausoom Sarkar, Aahitagni Mukherjee, and Balaji Krishnamurthy. Introspection: Accelerating Neural Network Training By Learning Weight Evolution. In ICLR, 2017.
N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout : A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15 (1):1929­1958, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskeveer, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.
Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding Neural Networks Through Deep Visualization. In Deep Learning Workshop at ICML, 2015.
Matthew D Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks. ECCV, pp. 818­833, 2014.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires re-thinking generalization. In ICLR, 2017.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors emerge in Deep Scene CNNs. In ICLR, 2015.
11

Under review as a conference paper at ICLR 2018
APPENDIX
A INFORMATION ABOUT THE NETWORKS AND THEIR TRAINING PROCEDURE
MNIST MLP Architecture: Dense (512) - ReLu - Dropout (0.5) - Dense (512) - ReLu - Dropout (0.5) - Dense (10) Training information:
· Learning rate: 1e-1 · Batch size: 128 · Number of epochs: 50
CIFAR10 CNN One layer is composed of a convolution, a ReLu activation and BatchNormalization. We denote a layer with L(n) where n is the number of filters in the convolution. We denote maxpooling as MP and global average pooling as GP. Architecture: L(64) *3 - MP - L(128) *3 - MP - L(256)*3 - MP - L(512) *3 - GP - Dense(10) Training information:
· Learning rate: 1e-2, divided by 5 after epoch 60 · Batch size: 32 · Number of epochs: 100
RESNET50 This network is directly taken from the keras applications. Information about the architecture can be found at: https://github.com/fchollet/keras/blob/master/keras/applications/ resnet50.py
Training information is not provided.
B EXPERIMENT DETAILS
In section 4, gradients and pre-activations are recorded for · 30.000 samples for dense1 and dense2 · 10.000 for stage3layer2 · 5.000 for stage2layer2 · 500 for stage0layer0
The samples were randomly selected. Computing percentiles is also performed on randomly selected samples for computation efficiency. More precisely, we limit the number of samples used to 100.000. The probes used on ResNet50 used for figure 4 used only 100.000 training samples from the
12

Under review as a conference paper at ICLR 2018 ImageNet dataset. The test error, however, is computed on the complete ImageNet validation set.
C SUPPLEMENTARY IMAGES
Figure 5: Evolution of the pre-activation distributions across training of different neurons from dense2 (trained on MNIST). Each line corresponds to the dynamics of a different neuron. Preactivations are separated in two categories, high and low, based on the average partial derivative sign over training of their corresponding activation. We can see that both categories are being separated during training. The final highest pre-activations of the high category are highlighted to show that it is not a simple translation. These illustrations can be seen in video format on https: //www.youtube.com/channel/UC5VC20umb8r55sOkbNExB4A.
13

Under review as a conference paper at ICLR 2018
Figure 6: Evolution of the pre-activation distributions across training of different neurons from stage3layer2 (trained on cifar10). Each line corresponds to the dynamics of a different neuron. Preactivations are separated in two categories, high and low, based on the average partial derivative sign over training of their corresponding activation. We can see that both categories are being separated during training. The final highest pre-activations of the high category are highlighted to show that it is not a simple translation. These illustrations can be seen in video format on https: //www.youtube.com/channel/UC5VC20umb8r55sOkbNExB4A.
14

