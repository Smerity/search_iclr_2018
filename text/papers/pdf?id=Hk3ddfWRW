IMITATION LEARNING FROM VISUAL DATA WITH MULTIPLE INTENTIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled learning complex robot skills that involve high dimensional perception such as raw image inputs. LfD algorithms generally assume learning from single task demonstrations. In practice, however, it is more efficient for a teacher to demonstrate a multitude of tasks without careful task set up, labeling, and engineering. Unfortunately in such cases, traditional imitation learning techniques fail to represent the multi-modal nature of the data, and often result in sub-optimal behavior. In this paper we present an LfD approach for learning multiple modes of behavior from visual data. Our approach is based on a stochastic deep neural network (SNN), which represents the underlying intention in the demonstration as a stochastic activation in the network. We present an efficient algorithm for training SNNs, and for learning with vision inputs, we also propose an architecture that associates the intention with a stochastic attention module. We demonstrate our method on real robot visual object reaching tasks, and show that it can reliably learn the multiple behavior modes in the demonstration data. Video results are available at https://vimeo.com/240212286/fd401241b9.
1 INTRODUCTION
A key problem in robotic control is to simplify the problem of programming a complex behavior. Traditional control engineering approaches, which rely on accurate manual modeling of the system environment, are very challenging to apply in modern robotic applications where most sensory inputs come from images and other high-dimensional signals such as tactile feedback.
In contrast, imitation learning, or learning from demonstration (LfD) approaches (Schaal et al., 2003) aim to directly learn a control policy from mentor or expert demonstrations. The key advantages of LfD are simplicity and data-efficiency, and indeed, LfD has been successfully used for learning complex robot skills such as locomotion (Schaal et al., 2005), driving (Pomerleau, 1989; Ross et al., 2011), flying (Abbeel & Ng, 2004), and manipulation (Mu¨lling et al., 2013; Chebotar et al., 2016; Pastor et al., 2009). Recently, advances in deep representation learning (Goodfellow et al., 2016) have facilitated LfD methods with high dimensional perception, such as mapping raw images directly to controls (Giusti et al., 2016). These advances are capable of learning generalizable skills (Levine et al., 2015), and offer a promising approach for modern industrial challenges such as pick and place tasks (Correll et al., 2016).
One challenge in LfD, however, is learning different modes of the same task. For example, consider learning to pick up an object from a pile. The demonstrator can choose to pick up a different object each time, yet we expect LfD to understand that these are similar demonstrations of the same pickup skill, only with a different intention in mind. Moreover, we want the learned robot behavior to display a similar multi-modal1 nature.
Standard approaches for LfD with image inputs, such as learning with deep neural networks (NNs) (Pomerleau, 1989; Giusti et al., 2016; Levine et al., 2015), are not suitable for learning multimodal behaviors. In their essence, NNs learn a deterministic mapping from observation to control, which cannot represent the inherently multi-modal latent intention in the demonstrations. In prac-
1In this paper, multi-modal refers to a distribution that contains multiple modes. In robotics literature, multimodal can also refer to policies that act on different input modalities such as vision and sound. We emphasize that this is not the setting in this paper.

tice, this manifests as an `averaging' of the different modes in the data (Bishop, 1994), leading to an undesirable policy.
A straightforward approach for tackling the multi-modal problem in LfD is to add a label for each mode in the data. Thus, in the pick-up task above, the demonstrator would also explicitly specify the object she intends to pick-up beforehand. Such an approach has several practical shortcomings: it requires the demonstrator to record more data, and requires the possible intentions to be specified in advance, making it difficult to use the same recorded data for different tasks. More importantly, such a solution is conceptually flawed ­ it solves an algorithmic challenge by placing additional burden on the client.
In this work, we propose an approach for LfD with multi-modal demonstrations that does not require any additional data labels. Our method is based on a stochastic neural network model, which represents the latent intention as a random activation in the network. We propose a novel and efficient learning algorithm for training stochastic networks, and present a network architecture suitable for LfD with raw image inputs, where the intention takes the form of a stochastic attention over features in the image.
We show that our method can reliably reproduce behavior with multiple intentions in real-robot object reaching tasks. Moreover, in scenarios where multiple intentions exist in the demonstration data, the stochastic neural networks perform better than their deterministic counterparts.
2 RELATED WORK
In this work we focus on a direct imitation learning approach, known as behavioral cloning (Pomerleau, 1989). This approach does not require any model of the task dynamics, and does not require additional queries of the mentor or robot execution rollouts beyond the collected demonstrations (though such can be used to improve performance (Ross et al., 2011)). An alternative approach is inverse reinforcement learning (IRL) (Ng & Russell, 2000; Abbeel & Ng, 2004; Ziebart et al., 2008), where a reward model that explains the demonstrated behavior is sought. Recently, model free IRL approaches that can learn complex behavior policies from high-dimensional data were proposed (Finn et al., 2016; Ho & Ermon, 2016). These approaches, however, rely on taking additional policy rollouts as a fundamental step of the method, which, in realistic robot applications, requires substantial resources.
Multi-task IRL learns from unlabeled demonstrations generated by varying intentions or objectives (Babes et al., 2011; Dimitrakakis & Rothkopf, 2012). Dimitrakakis & Rothkopf (2012) propose a Bayesian approach for inferring the intention of an agent performing a series of tasks in a dynamic environment. Babes et al. (2011) propose an EM for clustering the unlabeled demonstrations and then application of IRL for inferring the intention of a given cluster. Both approaches have been shown promising results on relatively simple low dimensional problems. Several recent works on multi-task IRL (Hausman et al., 2017; Wang et al., 2017; Li et al., 2017) extended the generative adversarial imitation learning (GAIL) algorithm (Ho & Ermon, 2016) to high dimensional multi-modal demonstrations. Our approach, in comparison, does not require taking additional robot rollouts.
Recently, Rahmatizadeh et al. (2017) proposed a method for learning multi-modal policies from raw visual inputs, using mixture density networks (Bishop, 1994) for generating outputs from a mixture of Gaussians distribution. Their training method requires labeling each task with a specific signal. While the modes, or intentions in our work can be seen as different tasks, we do not require any labeling of the intention in the demonstrations.
To our knowledge, this is the first LfD approach that can handle multiple modes in the demonstrations and: (1) does not require additional robot rollouts, (2) does not require a label for the mode, and (3) can work with raw image inputs.
The stochastic neural network model we use here is related to recently proposed generative models such as (conditional) variational autoencoders (VAEs) (Kingma & Welling, 2014; Sohn et al., 2015b), and generative adversarial nets (GANs) (Goodfellow et al., 2014; Mirza & Osindero, 2014). We opted for stochastic neural networks since GANs are known to have problems learning multimodal distributions (Arora & Zhang, 2017), and conditional VAEs (Pillai & Leonard, 2017) require training an additional encoding network, which proved to be difficult in our experimental domain.

3 PRELIMINARIES

We first introduce some preliminary concepts for presenting our methods, and then present our problem formulation of imitation learning with multi-modal behaviors in expert demonstrations.

Learning from Demonstration To explicitly formulate the problem of imitation learning, let X

and U denote the observation and action spaces of the robot, and let xt  X and ut  U denote an observation and a control command for the robot at time t. Given a data-set D of N

trajectories T i with length T (for simplifying notations we drop the subscript i in Ti), where a demonstrated task is recorded in the form of sequential pairs of observations and actions T i =

x1i , ui1 , . . . , xTi , uTi Ni=1, LfD aims to learn a policy P : X  P(U ) that is parametrized by feature weight vector   , such that it reliably performs the task. Here P(U) represents the space of probability distributions defined on the action space U. Since each observation is associated with

an action label, the imitation learning policy can be found by solving the maximum-likelihood (ML)

objective: actions as

  u1:T

=.argu1m, .a.x.,uT

1 N

N i=1

log

P

(u1i :T

, and the sequence

o|xf i1o:Tbs,er)v,awtiohnesreaws ex1a:bTbr=.evxia1te, d. .

the sequence of . , xT . This ob-

jective function is the empirical average of the conditional log likelihood, which is a consistent

estimator of the expected conditional log likelihood: E [log P (u1:T |x1:T , )]. If, for example, the policy is Gaussian with parametrized mean vector f (x; ) and an identity co-variance matrix, then

the above supervised learning problem is equivalent to an 2 regression problem with objective

function

1 N

N i=1

T t=1

uit - f (xti; ) 22.

Stochastic Neural Networks Multilayer perceptrons (MLPs) are general purpose function ap-
proximators for nonlinear regression in feedforward neural networks (NNs) (Goodfellow et al.,
2016). Parametrized by the NN weights , the output of the MLP, f (x; ), is often interpreted as the sufficient statistics of the conditional probability P (u|x; ), if the conditional probability belongs to the exponential family (conditioned on the input x). For example, if P (u|x; ) is parametrized as an isotropic Gaussian distribution, it can be represented by N (u|f (x; ), I). The parameters 
are typically learned by maximizing the expected log likelihood function. However, since the MLP activation functions are all deterministic, by nature the model P (u|x; ) is a unimodal distribution.

For many structured prediction problems, we are interested in a conditional distribution that is multimodal. To satisfy the multi-modality requirement, a common approach is to make the hidden variables in the NN stochastic. Sigmoid belief nets (SBNs) (Neal, 1992) are an early example of this idea, using binary stochastic hidden variables. However, inference in SBNs is generally intractable, and costly to compute in practice. Recently, Tang & Salakhutdinov (2013) introduced the stochastic feedforward neural network (SNN) for modeling the multi-modal conditional distribution P (u|x; ). Unlike SBNs, SNNs add to the deterministic NN latent features a stochastic latent variable z, and decompose the conditional distribution as: P (u|x; ) = z P (u|x, z; )P (z). It is also assumed that P (u|x, z, ) and P (z) can be easily computed, for example, as in (Tang & Salakhutdinov, 2013), where z is represented by Bernoulli random variable nodes in the network. For learning the parameters , Tang & Salakhutdinov (2013) proposed a generalized EM algorithm, where importance sampling is used in the E-step, and error back-propagation is used in the M-step.

Problem Formulation In this work, we consider an imitation learning setting, where the mentor demonstrations of a particular task consist of multiple behaviors. In particular, we assume that the demonstrator can perform the task in several different strategies, which we term as intentions. As a concrete example, consider the task of picking up an object from a pile of different objects in the scene (see Figure 1(a) for an example). In this case, the data-set of mentor demonstrations consists of a list of trajectories, where the target object in each trajectory is inherently decided by the demonstrator, and not explicitly labeled. Our goal is to learn a stochastic policy that accurately mimics the mentor's policy, and accurately displays the multiple intentions demonstrated in the data.

4 MULTI MODAL LFD WITH SNNS
In this section, we first present our LfD formulation based on the SNN model for learning multiple intentions, and then propose a sampling based algorithm for learning the SNN parameters. We then present a particular SNN architecture that is suitable for vision-based inputs, where the stochastic intention takes the form of an attention over image features.

4.1 THE SNN FORMULATION

We model the intention of the demonstrator using a random vector z  RM with probability

distribution P(z). For example, P(z) could be a unit normal distribution N (0, I), or a vector

of independent multinomial probabilities. Here we assume that throughout a single trajectory,

the intention does not change, and the intention is independent of the observations2. Therefore,

the conditional data likelihood is obtained by marginalizing out the random variable of intention:

P (u1:T |x1:T ; ) = z P (u1:T |x1:T , z; )P (z). SNNs can be viewed as directed graphical models (see Figure 4 in the appendix for a diagram) where at each time t  {1, . . . , T }, the generative pro-

cess starts from an observation xt, combines with a latent intention z, which is the same throughout the trajectory, and then generates an action ut. We also make the standard assumption that, given

the intention z at each trajectory, the demonstrator policy is memory-less (a.k.a. Markov), which

implies the following equality: P (u1:T |x1:T , z; ) =

T t=1

P

(ut

|xt,

z;

).

Given an intention z, we model the action probability as log P (u|z, x; )  -d(f (x, z; ), u), where
f is a deterministic NN that takes as input both the observation x and the intention z, and d is some distance metric. One immediate example is when d(a, b) = a - b 2, one obtains P (u|z, x; ) as a normal distribution N (u|f (x, z; ), 2) for some MLP mean predictor f (x, z; ) and constant variance term 2. In our experiments, we found the 1 distance function to work well. Note that when the intention variable z is fixed, the output action follows a unimodal distribution. However,
since z is a random vector that is input to a nonlinear NN computation, the distribution of f (x, z; ), and thereby the output distribution P (u1:T |x1:T ; ), can take a multi-modal form.

4.2 THE MONTE CARLO LEARNING ALGORITHM
We first describe a basic Monte Carlo (MC) sampling algorithm for learning the parameters    of the SNN model in Section 4.1. Let z1, . . . , zN denote N samples of z, where zi  P (z). For each given parameter , sequence of observations x1:T , and sequence of actions u1:T , let r(z; x1:T , u1:T , ) = P (u1:T |x1:T , z; ) be the reward function, which associates an intention with the data likelihood given it. The reason we use this terminology is to later connect the likelihood maximization problem with risk-sensitive optimization concepts that will be key in our approach. A Monte Carlo approximation of the likelihood is given by,

P (u1:T |x1:T ; )

=

EzP [r(z; x1:T , u1:T , )]



1 N

N

r(zi; x1:T , u1:T , ).

i=1

(1)

A

direct

approach

for

computing



would

be

to

directly

maximize

1 N

N i=1

log

r(zi;

x1:T

,

u1:T

,

),

which is a MC estimate of EzP [log r(z; x1:T , u1:T , )], with gradient-based optimization. By

Jensen's inequality, the above term is a lower bound of the data log likelihood log P (u1:T |x1:T ; ),

corresponding to a maximum-likelihood approach. While the estimator of the gradient is unbiased

and consistent to EzP [ log r(z; x1:T , u1:T , )], in practice, such an approach suffers from ex-

tremely high variance (with respect to intention z). To justify this observation, consider the sum

of probabilities in (1). Since each sampled intention zi is given an equal weight in explaining the observed sequence of actions, even sampled intentions that are very different from the ones that

generated the data (i.e., have high cost) are expected to produce a high likelihood.

4.3 INTENTION-DRIVEN LEARNING IN SNNS

To reduce variance in training SNNs, in this section we introduce a sampling strategy whose gradient updates focuses only on the most correct underlying intentions ­ the intentions that have the lowest cost. We analyze the bias and variance trade-offs of this new sampling gradient estimate, and show that the variance of our proposed approximation is lower than that of a naive MC approach.

For any given threshold   [0, 1], let q() denote the (upper) -quantile of the reward func-

tion q() =. maxw

z:r(z;x1:T ,u1:T ,)w P (z)   . This associates with the -quantile of the

underlying intention with highest likelihood probability. We define an intention-driven probability

2By assuming an input-independent intention, our generation model P (u|x) is similar to the generation model in a conditional VAE (Sohn et al., 2015a), or conditional GAN (Mirza & Osindero, 2014), although our training method is different. We found the input-independent model to be sufficient for our experiments. Extending our method to observation-dependent intention is possible, along the lines of (Tang & Salakhutdinov, 2013), and deferred to future work.

distribution as follows:

Q(z|u1:T , x1:T ; )

=

P

(z 

)

1

{r(z;

x1:T

,

u1:T

,

)



q()} .

This quantity can be interpreted as a weighted distribution that only samples from the % of the most correct underlying intentions (i.e., the intentions that best explain the mentor demonstrations). The expected reward induced by the intention-driven distribution is given by:

EzQ [r(z; x1:T , u1:T , )] = E [r(z; x1:T , u1:T , ) |r(z; x1:T , u1:T , )  q() ] ,

(2)

which is equal to the conditional likelihood function of the % most correct intentions. In the financial risk literature, this metric is known as the expected shortfall (Rockafellar & Uryasev, 2000), and is typically used to evaluate the risky tail distribution of financial assets. While our setting is completely different, we will use tools developed for expected shortfall estimation in our approach.

By definition of Q, one has the following inequality. 1
P (u1:T |x1:T ; ) = E[r(z; x1:T , u1:T , )]  EQ [r(z; x1:T , u1:T , )]   P (u1:T |x1:T ; ).

(3)

We propose to maximize EzQ [r(z; x1:T , u1:T , )] using Monte Carlo sampling techniques. Intuitively, since the support of Q is limited to the most likely z values, estimating it using sampling has lower variance than estimating the original likelihood. We will further elaborate on
this point technically later in the section. However, this comes at the cost of adding a bias z:Q(z)=0 P (u1:T |x1:T , z; )P (z). Empirically, we have found this procedure to work well.

To sample from Q, we use empirical quantile estimation (Glynn, 1996). Let z1ord, . . . , zNord denote the MC samples z1, . . . , zN sorted in descending order, according to the reward func-

tion r(z; x1:T , u1:T , ). Let N = N be the number of samples corresponding to the

-quantile. Then we have the following empirical estimate: EzQ [r(z; x1:T , u1:T , )] 

1 N

N i=1

r(ziord;

x1:T

,

u1:T

,

).

It

has

been

shown

in

Theorem

1

of

Glynn

(1996)

that

under

stan-

dard assumptions, the above expression is a consistent estimator of EzQ [r(z; x1:T , u1:T , )] with

order O(N -1/2), which we have shown above to be a lower bound to the likelihood function. For

the special case of N = 1 (when  = 1/N ), we can replace the sorting operation with a simple

min operation, yielding a simple and intuitive algorithm ­ we choose the sampled z with the lowest

error for updating the parameters .

In practice, maximizing the log-likelihood of the data is known to work well (Goodfellow et al.,

2016). In our case, we correspondingly maximize EzQ [log r(z; x1:T , u1:T , )], which, by the

Jensen inequality3, is a lower bound on log EzQ [r(z; x1:T , u1:T , )]. We therefore obtain the

following

gradient

estimate

GN,()

:=

1 N

N i=1



log

r(ziord;

x1:T

,

u1:T

,

).

We

term

this

sam-

pling technique as intention-driven sampling (IDS). Pseudocode is given in Algorithm 1.

Optimistic Sampling: To predict actions at test time, we first sample z in the beginning of the episode and fix it, and then use the NN to predict ut = f (xt, z) at every time step. While we could sample z from P (z), this might result in a z value that has a low likelihood to reproduce the demonstrations, i.e., a z that has low reward. We observed this to be problematic in practice, and
therefore devise an alternative approach, which we term optimistic sampling. We propose to store a set of the K most recent z values that obtained the highest reward during training, and at inference time sample a z uniformly from this set. This corresponds to sampling z from Q(z), averaged over the training data. Optimistic sampling dramatically improves the prediction performance in practice.

Analysis: By Theorem 4.2 of Hong & Liu (2009), GN,() is a consistent gradient estimator of the lower bound with asymptotic bias of O(N -1/2). In Appendix B, we deduce the following expression for the variance of GN,():

VI DS



1 2N

Var((log r(z1; x1:T , u1:T , )-q())1{log

r(z1; x1:T , u1:T , )  q()}).4

(4)

3Since log is a monotonic function, all the order statistics of r(z; x1:T , u1:T , ), including the quantile
q(), will not be affected. 4Here q()) is the -quantile of the log reward function.

When N = N , i.e.,  = 1, we obtain the variance of standard MC sampling , i.e.,

VMC

=

1 N

Var(

log

r(z

;

x1:T

,

u1:T

,

)).

On

the

other

hand,

when





1/N the variance

is bounded in O(1/N 2). This result is due to the fact that (i) |(log r(z1; x1:T , u1:T , ) -

q())|1{log r(z1; x1:T , u1:T , )  q()} is bounded by a constant that is O(),5 (ii) Var(1{log r(z1; x1:T , u1:T , )  q()}) =  · (1 - ), and (iii) therefore VIDS = O( · (1 - )/(2N ) · 2) = O(N -2). Therefore, one can treat  as a nob to trade-off bias (see (3)) and

variance (see (4)) in IDS.

Algorithm 1: IDS
Input: A minibatch of K samples {ut, xt, . . . , ut+K , xt+K } from the same demonstration trajectory T i
Output: An update direction for , and a sample from Q 1 Sample z1, . . . , zN  P (z) 2 Set z = arg maxzi P (ut:t+K |xt:t+K , zi; ) 3 return  log P (ut:t+K |xt:t+K , z; ), and z

Comparison to SNNs: Broadly speaking, generalized EM algorithms (Tang & Salakhutdinov,

2013) can be seen as designing an importance sampling weight to reshape the sampling distri-

bution, in order to lower the variance of the gradient estimate, by using entropy maximization

or posterior distribution matching (more details can be found in Appendix A). For the specific

SNN algorithm of Tang & Salakhutdinov (2013) applied to our NN architecture, the importance

weights correspond to a soft-max over the reward defined above. In IDS the importance weight is

w(z)

=

1 

1{r(z;

x1:T

,

u1:T

,

)



q()},

which for N

=

1 amounts to replacing the soft-max

with a hard max. Interestingly, these two similar algorithms were developed from very different

first principles. In practice, however, the IDS algorithm integrates naturally with optimistic sam-

pling, which leads to significantly better performance, as we show in our experiments.

4.4 INTENTION DRIVEN SNN ARCHITECTURE FOR STOCHASTIC VISUAL ATTENTION
In this section we present Intention-SNN (henceforth I-SNN), an architecture that implements the stochastic intention as an attention over particular features in the image. This architecture is suitable for LfD domains where the visual observation contains information about multiple possible intentions, as in the object pick up task in Section 5.
Our I-SNN architecture is presented in Figure 1, and is comprised of three modules. The first module is a standard multi-layer (fully) convolutional neural network (CNN) feature extractor (Goodfellow et al., 2016), followed by a spatial softmax layer. The CNN maps the input image onto C feature maps. The spatial softmax, introduced by Levine et al. (2015), calculates for each feature map in its input the corresponding (x, y) position in the image where this feature is most active. Let c,i,j denote the activation of feature map c at coordinate (i, j). The spatial softmax output for that feature is (fc,x, fc,y), where fc,x = i,j exp(c,i,j ) · i/ i ,j exp(c,i ,j ), and fc,y = i,j exp(c,i,j ) · j/ i ,j exp(c,i ,j ). Thus, the output of the spatial softmax is of dimensions C × 2.
The second module applies a stochastic soft attention over the spatial softmax features. We use a MLP to map the M -dimensional random intention vector z onto a C-dimensional vector w. We then apply a softmax to obtain the attention weight vector a  RC , ac = exp(wc)/ c exp(wc ) (Xu et al., 2015). The attention weight is multiplied with the spatial softmax output to obtain the attention-modulated feature activations: fc,x = fc,x · ac, and fc,y = fc,y · ac, which are input to the control network along with the robot's d-dimensional vector of current pose. The control network is a standard MLP mapping RC×2+d to P (u).
The intuition behind this architecture is that the stochastic intention can `select' which features are relevant for making a control decision, by giving them higher weights in the attention. When
5Intuitively, when the threshold  approaches 1/N , the quantile q() increases (up to the maximum value of log reward) at rate O(), leaving less and less room for log rewards above the quantile. Concretely, the random variable | log r(z1; x1:T , u1:T , ) - q()|1{log r(z1; x1:T , u1:T , )  q()} converges to 0 at rate O(). Consequently, this random variable is bounded by O(). Analogous arguments can be applied for the case with gradients as well, more technical details can be found in Section 4 of Hong & Liu (2009).

Feature Extraction xt Spatial Softmax
FCNN
z

end-effector Cartesian pose
Control Network
ut
Motion Control

Random Intention

Soft Attention MLP

Stochastic Intention

(a) (b)

Figure 1: (a) 6-DOF IRB-120 robot and an example of a task configuration; (b) A schematic diagram of the I-SNN architecture.

multiple objects are in the scene, each object would naturally be represented by different features, therefore the intention in this architecture can correspond to attending to a particular object6.

5 EXPERIMENTS
We demonstrate the effectiveness of our approach on learning a reaching task with the IRB-120 robot (Figure 1b), where mentor trajectories are collected in the form of sequential image-action pairs. The main questions we seek to answer are: (1) Can our IDS learn to effectively reproduce multiple intentions in the demonstrations? (2) How does IDS approach compare to a standard deterministic NN approach for LfD? (3) How does training an I-SNN using IDS compare with training it using the SNN algorithm7 of Tang & Salakhutdinov (2013)?
To maintain a fair comparison, we evaluated deterministic NN policies with identical structure as I-SNN except for the stochastic intention module (cf. Section 4.4, and Figure 1).
5.1 SNN IMPLEMENTATION DETAILS
In all our experiments we used the following parameters for the SNN training, which we found to work well consistently across different domains:(1) Loss function: we represent the output distribution as log P (u|h, x; )  - f (x, h; ) - u 1, where f (x, h; ) is the output of the control network, as described in Section 4.4. This corresponds to an L1 regression loss, which we found to perform better than the popular L2 loss in our experiments. (2) Monte Carlo samples: we chose N = 5, which we found to work well. Higher values resulted in degraded performance at test time, due to the higher bias (see Section 4). (3) Intention variable dimension: we chose z  R5 for all experiments. We did not observe this parameter to be sensitive, and dimensions from 2 to 10 performed similarly. (4) P(z): a 5-dimensional vector of independent uniform multinomials in {0 : 4}.
5.2 REAL WORLD ROBOT REACHING TASK
In this task, depicted in Figure 3(a), the objective is to navigate the end-effector of robot to reach a point above one of 3 objects in the scene ­ a soap box, a blue electronic box and a measuring cup. We used a 6-DOF IRB-120 robot where the control is a 3-dimensional Cartesian vector applied to
6We note that this architecture is not directly applicable for cases where two objects have exactly the same appearance, and therefore the same feature activation function. Such cases can be handled by adding spatial information to the features or the attention module, which will be investigated in future work.
7We also investigated using a conditional VAE (CVAE), however, despite extensive experimentation, we could not get the CVAE to work. We attribute this to the fact that the recognition network in a CVAE needs to map the image to a latent variable distribution that `explains' the observed action. This mapping is complex, as it needs to understand from the image what goal the demonstration is aiming towards. Our approach, on the other hand, does not require a recognition module for reducing variance during training.

the the end effector ut = (dxt, dyt, dzt). The observations consists of (1) a 480 × 640 RGB image of the scene (further cropped and resized to 64 × 64 resolution) using a point-grey camera mounted
at a fixed angle; (2) the 3 dimensional end-effector Cartesian pose (see Figure 1 for more details).

Data Collection: We denote a specific scene arrangement of object placements and initial endeffector pose as a task configuration. Task configurations were randomly generated by arbitrarily placing the three objects on the work bench and randomly initializing the end-effector pose. Once a task configuration is generated, the position of the objects remain fixed for the entire episode. For each task configuration we collect 3 demonstrations, each generated by a human navigating the end-effector using a 3DConnexion space-mouse to reach one of the objects. At each time step t, the observation ot together with the Cartesian control signal ut (see above) are recorded. We collected demonstrations from 468 different task configurations, for a total of 1404 demonstration trajectories.

Training: We compare using IDS and the SNN algorithm of Tang & Salakhutdinov (2013) (henceforth SNN) for training the I-SNN architecture. To reduce the training time, we pre-trained the weights of the convolutional layers in the Feature Extraction module reusing the weights learned in the deterministic NN model. For optimization, we also used Adam (Kingma & Ba, 2014), with the default parameters using 90% of the data set for training and 10% for validation. .

Evaluation: We evaluate each model on 10 randomly generated task configurations, with 20 trials on each task configuration, for a total of 200 trials. We run the model until it either succeeds to reach an object or fails. For the IDS algorithm, we used optimistic sampling. For SNN, we experimented with both optimistic and uniform sampling, however the latter resulted in a better overall performance.
Table 1 shows the overall success rate for every model across all 200 trials. The deterministic NN model succeeded in reaching one of the objects only in 3 task configurations (and kept on reaching that same object for the 20 evaluations in each), and failed on the other 7 task configurations due to the averaging problem. The stochastic algorithms performed significantly better by learning multiple modes of the problem. IDS significantly out-performed the SNN algorithm, as we explain below.
We evaluate the mode learning ability by counting, for each task configuration, how many different objects were reached, as depicted in Table 1 (last four columns). As expected, the deterministic NN could not reach more than one object. I-SNN trained by SNN algorithm, on the other hand, could reach two different (but fixed) objects for 6 task configurations, and all three objects for the rest. The best performance is achieved by I-SNN trained by IDS, reaching all three objects in all the tasks, thereby demonstrating a strong ability to learn all the modes in the data. In the appendix (Figure 3), we show a histogram of reaching the different objects that demonstrates that IDS learned a near-uniform distribution over the modes. Additionally, in the appendix (Figure 5) we visualize the features that I-SNN attends to, showing that in each episode the model consistently attends to the same object throughout the execution.
In Figure 2 we explain the superior performance of IDS over SNN. Since IDS focuses only on the best samples (through the max) compared to SNN which gives weight also to non best samples (through the soft-max), IDS better `tunes' the network for the best samples. Combined with optimistic sampling, which draws these best samples during execution, this leads to better results.

Table 1: The first column shows the overall success rate for every model calculated over 10 different task configurations, and 20 trials each, for a total of 200 trials. The last four columns show the unnormalized distribution of modes learned across 10 task configurations.

deterministic NN SNN IDS

Overall Success Rate 30% 60.5% 98.5%

1 object 3 0 0

2 objects 0 6 0

3 objects 0 4 10

Fail 7 0 0

6 CONCLUSION
We presented an approach for learning from demonstrations that contain multiple modes of performing the same task. Our method is based on stochastic neural networks, and represents the mode

Loss

SNN IDS

Loss

SNN IDS

Loss

SNN IDS

Epoch
(a)

Epoch
(b)

Epoch
(c)

Figure 2: Comparison of IDS and SNN algorithms. We plot three different errors during training (on the training data), for the same model trained using IDS and SNN algorithm. Left: the respective training loss for each method. Since the max in IDS upper bounds the softmax in SNN, the loss plot for IDS lower bounds SNN. Middle: the IDS loss on the training data, for both models. Since the SNN is trained on a different loss function (softmax), its performance is worse. This shows an important point: if, at test time, we use optimistic sampling to sample z from best samples during training, we should expect IDS to perform better than SNN. Right: the average log-likelihood loss during training. The SNN wins here, since the softmax encourages to increase the likelihood of `incorrect' z values. This provides additional motivation for using optimistic sampling.

of performing the task by a stochastic vector ­ the intention, which is given as input to a feedforward neural network. We presented a simple and efficient algorithm for training our models, and a particular implementation suitable for vision-based inputs. As we demonstrated in real-robot experiments, our method can reliably learn to reproduce the different modes in the demonstration data, and outperforms standard approaches in cases where such different modes exist.
In future work we intend to investigate the extension of this approach to more complex manipulation tasks such as grasping and assembly, and domains with a very large number of objects in the scene. An interesting point in our model is tying the features to the intention by an attention mechanism, and we intend to further investigate recurrent attention mechanisms (Xu et al., 2015) that could offer better generalization at inference time.
REFERENCES
P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML, pp. 1. ACM, 2004.
S. Arora and Y. Zhang. Do GANs actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224, 2017.
M. Babes, V. Marivate, K. Subramanian, and M. L. Littman. Apprenticeship learning about multiple intentions. In ICML, pp. 897­904. ACM, 2011.
C. M. Bishop. Mixture density networks. Technical report, 1994.
Y. Chebotar, K. Hausman, O. Kroemer, G. S. Sukhatme, and S. Schaal. Generalizing regrasping with supervised policy learning. In International Symposium on Experimental Robotics, pp. 622­632. Springer, 2016.
N. Correll, K. Bekris, D. Berenson, O. Brock, A. Causo, K. Hauser, K. Okada, A. Rodriguez, J. Romano, and P. Wurman. Lessons from the amazon picking challenge. arXiv preprint arXiv:1601.05484, 2016.
C. Dimitrakakis and C. A. Rothkopf. Bayesian multitask inverse reinforcement learning. In EWRL, EWRL'11, pp. 273­284, Berlin, Heidelberg, 2012. Springer-Verlag.
C. Finn, S. Levine, and P. Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In ICML, pp. 49­58, 2016.
A. Giusti, J. Guzzi, D. Ciresan, F. He, J. P. Rodriguez, F. Fontana, M. Faessler, C. Forster, J. Schmidhuber, G. Di Caro, D. Scaramuzza, and L. Gambardella. A machine learning approach to visual perception of forest trails for mobile robots. IEEE Robotics and Automation Letters, 2016.
P. W. Glynn. Importance sampling for Monte Carlo estimation of quantiles. In Mathematical Methods in Stochastic Simulation and Experimental Design, pp. 180­185, 1996.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pp. 2672­2680, 2014.
I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.

K. Hausman, Y. Chebotar, S. Schaal, G. S. Sukhatme, and J. Lim. Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets. CoRR, abs/1705.10479, 2017.
J. Ho and S. Ermon. Generative adversarial imitation learning. In NIPS, pp. 4565­4573, 2016.
L Jeff Hong and Guangwu Liu. Simulating sensitivities of conditional value at risk. Management Science, 55 (2):281­293, 2009.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014.
S. Levine, N. Wagener, and P. Abbeel. Learning contact-rich manipulation skills with guided policy search. In ICRA, 2015.
Y. Li, J. Song, and S. Ermon. Inferring the latent structure of human decision-making from raw visual inputs. arXiv preprint arXiv:1703.08840, 2017.
M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.
K. Mu¨lling, J. Kober, O. Kroemer, and J. Peters. Learning to select and generalize striking movements in robot table tennis. The International Journal of Robotics Research, 32(3):263­279, 2013.
R. Neal. Connectionist learning of belief networks. Artificial intelligence, 56(1):71­113, 1992.
R. M. Neal and G. E. Hinton. A view of the EM algorithm that justifies incremental, sparse, and other variants. In Learning in graphical models, pp. 355­368. Springer, 1998.
A. Y. Ng and S. J. Russell. Algorithms for inverse reinforcement learning. In ICML, pp. 663­670, 2000. ISBN 1-55860-707-2.
P. Pastor, H. Hoffmann, T. Asfour, and S. Schaal. Learning and generalization of motor skills by learning from demonstration. In Robotics and Automation, 2009. ICRA'09. IEEE International Conference on, pp. 763­768. IEEE, 2009.
S. Pillai and J. J. Leonard. Towards visual ego-motion learning in robots. arXiv preprint arXiv:1705.10279, 2017.
D. A. Pomerleau. ALVINN: An autonomous land vehicle in a neural network. In NIPS, pp. 305­313, 1989.
R. Rahmatizadeh, P. Abolghasemi, L. Bo¨lo¨ni, and S. Levine. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. CoRR, abs/1707.02920, 2017.
R Tyrrell Rockafellar and Stanislav Uryasev. Optimization of conditional value-at-risk. Journal of risk, 2: 21­42, 2000.
S. Ross, G. J. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011.
S. Schaal, A. Ijspeert, and A. Billard. Computational approaches to motor learning by imitation. Phil. Trans. of the Royal Society of London: Biological Sciences, 358(1431):537­547, 2003.
S. Schaal, J. Peters, J. Nakanishi, and A. Ijspeert. Learning movement primitives. Robotics Research, pp. 561­572, 2005.
K. Sohn, H. Lee, and X. Yan. Learning structured output representation using deep conditional generative models. In NIPS, pp. 3483­3491. 2015a.
K. Sohn, H. Lee, and X. Yan. Learning structured output representation using deep conditional generative models. In NIPS, pp. 3483­3491. 2015b.
Y. Tang and R. R. Salakhutdinov. Learning stochastic feedforward neural networks. In NIPS, pp. 530­538, 2013.
Z. Wang, J. Merel, S. Reed, G. Wayne, N. de Freitas, and N. Heess. Robust imitation of diverse behaviors. arXiv preprint arXiv:1707.02747, 2017.
K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.
B. Ziebart, A. Maas, A. Bagnell, and A. Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pp. 1433­1438. Chicago, IL, USA, 2008.

A GENERALIZED EM ALGORITHM FOR LEARNING SNNS VIA IMPORTANCE SAMPLING

We review the work of Tang & Salakhutdinov (2013) as relevant for our setting. Following their notation, we use h for the intention variable (z in our formulation). For the importance sampling based approach, instead of maximizing the log likelihood function, consider maximizing the following evidence lower bound (ELBO):

F (Q, ) =EQ

log

P (u1:T , h|x1:T ; Q(h|x1:T , u1:T

) )

EP (·|x1:T ,u1:T ;) [log P (y1:T |x1:T ,)]

 log P (y1:T |x1:T ,)

F is related to the Kullback Liebler divergence between P (u1:T |h, x1:T ; ) and Q(h|u1:T , x1:T ) as follows:
F (Q, ) = -DKL(Q||P (·|x1:T , u1:T ; )) + log P (y1:T |x1:T ,).

Most importantly, it has also been shown in Theorem 2 of Neal & Hinton (1998) that if Q and  form a pair of local maximizer to F , then  is also a local maximum of the original likelihood maximization problem. To maximize F w.r.t Q, one has the closed form solution based on Bayes theorem:

Q(h|u1:T , x1:T ; old) =P (h|y1:T , x1:T , )

=

P

(y1:T |h, x1:T , P (y1:T |x1:T

old)P , old)

(h)



1 N

P

(y1:T |h, x1:T , old)P (h)

N i=1

P

(y1:T

|hj ,

x1:T

,

old)

.

Here, {h1, . . . , hN } is a sequence of latent random variables sampled i.i.d. from the distribution P (h).

Tang & Salakhutdinov (2013) present a generalized EM algorithm to train a SNN. In the E-step, the following approximate posterior distribution is used:

Q^(h|u1:T , x1:T ; old) := r¯(h; x1:T ,y1:T , old)P (h),

where

r¯(h; x1:T ,y1:T , old) = 1
N

r(h; x1:T , u1:T , old)

N i=1

r(hi;

x1:T

,

u1:T

,

old)

is the the importance sampling weight. Recall that for our distribution model,

r(h; x1:T , u1:T , old)  exp(-d(f (x, z; ), u)), therefore we obtain that the importance weights correspond to a soft-max over the prediction error.

In the M-step, the  parameters are updated with the gradient vector with respect to the following optimization:   arg max ^(, old), where

^(, old)

=

1 N

N

r¯(hi; x1:T ,y1:T , old) log P (y1:T , hi|x1:T , )

i=1

is the empirical expected log likelihood, and Q^ is the posterior distribution from the E-step. Here we drop the last term in F because in our case Q that does not depend on . Correspondingly, the gradient estimate is given by:

 ^(, old)

=

1 N

N

r¯(hi) log r(hi; x1:T , u1:T , ),

i=1

the equality is due to the facts that

log P (y1:T , h|x1:T , ) = log r(h; x1:T ,y1:T , ) + log P (h) and distribution P (h) is independent of .

B TECHNICAL PROOF OF THE VARIANCE IN INTENTION-DRIVEN SAMPLING

In this section, we study the variance of the gradient estimate of CVaR(R(z; )). This proof follows analogously from the analysis in Section 4.3 of Hong & Liu (2009) for the case of estimating asymptotic variance in gradient. Here we use R(z; ) as the shorthand notation of the log reward function log r(z; x1:T , y1:T , ). For any given cut-off level  and sample size n, consider the following update formula of the CVaR gradient estimate

1 GN = N

N

R(zi; )S(zi),

i=1

where S(zi) = 1{R(zi; )  q()}, i, and {z1, . . . , zN } is sampled in an i.i.d. fashion from P (h). Notice that

2Var(GN

)

=

1 N

Var( R(z1 ;

)S(z1)

+

1 = N Var(R(z1; )S(z1) +

1- 1 N
1- 1 N

· Cov(R(z1; )S(z1), R(z2; )S(z2)) · E[R(z1; )R(z2; )S(z1)S(z2)]

- E[R(z1; )S(z1)]2 .

The correlation comes from the fact that quantile is defined based on the order statistics of the reward. Now notice the following property:

S(z1) · S(z2) =1{R(z1; )  q(), R(z2; )  q()} =1{R(z1; ) > N N -1:N , R(z2; ) > N N -1:N } =1{R(z1; ) > N N -1:N-2}1{R(z2; ) > N N -1:N-2}

where Ni:N is the i-th order statistic from the N observations of reward {R(zi; )}iN=1. This implies that N N -1:N-2 is independent of R(z1; ) and R(z2; ). Furthermore, the above condition implies,

E[R(z1; )R(z2; )S(z1)S(z2)] =E[f 2(N N -1:N-2)] =Var(f (N N -1:N-2)) + E[f (N N -1:N-2)]2,

where f (N N -1:N-2) := E[R(z1; )S(z1)]. Following the same lines of arguments, one can also show that E[R(z1; )S(z1)]2 = E[f (N N -1:N-1)]2.
Combining all the above analysis together, one has the following expression:

2Var(GN

)

=

1 N

Var( R(z1 ;

)S(z1)

+

1- 1 N

· Var(f (N N -1:N-2)) + E[f (N N -1:N-2)]2]

- E[f (N N -1:N-1)]2 .

From the results in Proposition 4.3 to 4.4 of Hong & Liu (2009), with q() = q(), we have that

Var(f (N

N

-1:N -2 ))

=

(1

- )(q())2 N -1

+

O(N -1/2),

and

E[f (N N -1:N-2)]2 - E[f (N N -1:N-1)]2

=O(N -1/2)

-

2(1 N

- -

) 1 q()E[R(z1;

)1{R(z1; )



q()}].

Figure 3: Raw histogram of the learned modes and failure cases.
xt xt 
ut z ut
TT
(a) (b)
Figure 4: (a) Most LfD approaches often learn multimodal representations as a function of current state (or history of states captured by a recurrent neural network). In such models, task level intention is not guaranteed to be consistently inferred at every step throughout the task execution; (b) in contrast I-SNN samples and uniformly commits to the same mode at the task level throughout the task execution.

Therefore, the variance of GN can be expressed as:

2Var(GN

)

=

1 N

Var( R(z1 ;

)S(z1))

+

1- 1 N

· (1 - )(q())2 N -1

-

2(1 N

- -

) 1

q

()E[

R(z1;

)S(z1)]

+ O(N -1/2)

1 =
N

E[(R(z1; ))2S(z1)] + (q())2

-2q()E[(R(z1; ))S(z1)] - E[(R(z1; ))2S(z1)]

+ 2(q())2 -2q()E[(R(z1; ))S(z1)] +O(N -1/2)

1 =
N

E[(R(z1; ) - q())2S(z1)] - E[(R(z1; ) - q())S(z1)]2

+ O(N -1/2)

=

1 N

Var([

R(z1;

)

-

q

()]1{R(z1

;

)



q()})

+

O(N

-1/2).

This provides the variance of intention-driven sampling gradient estimate.

C ADDITIONAL MATERIAL

Figure 5: Visualization of the stochastic Intention network: every row shows 5 snapshots of a trajectory generated by running the I-SNN trained by the IDS algorithm. Each run was generating by randomly sampling an intention at the beginning and using it throughout the run. Smaller green circles show the 32 coordinates outputted by the spatial softmax layer. The larger red circle shows the top spatial softmax feature that received the highest weight from the soft attention generated by the Stochastic Intention Network. Note that for each run, the model consistently attends to the same mode that it randomly selected at the beginning of the run.

