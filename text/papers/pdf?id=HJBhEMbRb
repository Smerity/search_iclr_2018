Under review as a conference paper at ICLR 2018
A SPECTRAL APPROACH TO GENERALIZATION AND OPTIMIZATION IN NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
The recent success of deep neural networks stems from their ability to generalize well on real data; however, Zhang et al. (Zhang et al., 2016) have observed that neural networks can easily overfit random labels. This observation demonstrates that with the existing theory, we cannot adequately explain why stochastic gradient methods can find generalizable solutions for neural networks. In this work, we use a Fourier-based approach to study the generalization properties of gradient-based methods over 2-layer neural networks with sinusoidal activation functions. We prove that if the underlying distribution of data has nice spectral properties such as bandlimitedness, then gradient-based methods will converge to generalizable local minima. We also establish a Fourier-based generalization bound for bandlimited spaces, which generalizes to other activation functions. Our generalization bound motivates a grouped version of path norms for measuring the complexity of 2-layer neural networks with ReLU activation functions. We demonstrate numerically that regularization of this group path norm results in neural network solutions that can fit true labels without losing test accuracy while not overfitting random labels.
1 INTRODUCTION
Deep neural networks (DNNs) have achieved state-of-the-art performance on a wide array of diverse tasks (LeCun et al., 2015). A given DNN architecture represents a highly rich space of hypotheses. However, numerous empirical results have demonstrated that a simple stochastic gradient descent (SGD) learner can efficiently search over this space to find a solution that achieves high performance on both training and test data. Despite many successful applications of DNNs to practical tasks such as computer vision (Krizhevsky et al., 2012), natural language processing (Collobert & Weston, 2008) and speech recognition (Hinton et al., 2012), our basic understanding of the factors that drive DNN generalization is still lacking.
Addressing generalization for DNNs is hard for two fundamental reasons: 1) Empirical risk minimization for neural networks is a non-convex optimization problem with possibly many local minima, and 2) Two different local minima with the same training performance can achieve significantly different performance on test data. For these reasons, the neural network optimization method plays an important role in the generalizability of the local minima found. For example, SGD has been empirically shown to outperform large-batch gradient descent (Keskar et al., 2016). Also, SGD's performance can be improved upon by incorporating the geometry of observed data (Duchi et al., 2011; Neyshabur et al., 2015a).
For DNNs, however, a good optimization method is not sufficient for guaranteeing good generalization. Zhang et al. (Zhang et al., 2016) empirically demonstrate that a neural network trained by SGD can easily overfit random labels on the CIFAR-10 (Krizhevsky & Hinton, 2009) data. Yet, the same neural network fitted by the same SGD algorithm achieves good generalization performance for the original CIFAR-10 labels. This observation challenges the ability of traditional learning theory to explain why SGD learns generalizable hypotheses over neural networks. To shed light on this phenomenon, two recent works have developed generalization bounds and complexity measures for neural networks which can distinguish the local minima found for true and random labels. (Bartlett et al., 2017) proves a margin-based generalization bound and shows how it correlates with the generalization risk of DNNs when fitting true and random labels. (Neyshabur et al., 2017) explores different complexity scores for DNNs and how they behave differently for true and random labels. The complexity measures investigated in these works can effectively distinguish generalizable from poorly-generalizable local
1

Under review as a conference paper at ICLR 2018
a. b1.

b2.

Inputlayer Hiddenlayer

Output

b3.

b4.

Figure 1: (a) A 2-layer neural network with activation function , (b) Training and test accuracy on CIFAR10 with true and random labels on a 2-layer neural network with 512 ReLU hidden units, regularized with an additive penalty: (b1) no penalty, (b2) 2-norm, (b3) 2-group path norm, (b4) 1-path norm. The 2-group path norm and 1-path norm were successful to close the generalization gap for both true and random labels.
minima. They do not explain, however, why SGD converges to generalizable local minima when there exist poorly-generalizable local minima which can also perfectly fit the training set.
To approach this question, one needs to understand the key characteristic of CIFAR-10's original labeling which differentiates it from random labels and how it is exploited by SGD to achieve good generalization performance. In this work, we approach this problem in the Fourier domain where non-random labeling schemes behave completely differently from random labeling schemes. While signals recoverable from few measurements possess nice spectral properties such as bandlimitedness, fully random stochastic processes are not bandlimited and not recoverable from any finite number of measurements.
Using spectral analysis, we focus on characterizing spectral properties of an underlying distribution which can be exploited by gradient-based methods to converge to generalizable local minima. We address this problem for 2-layer neural networks (see Figure 1a) with sinusoidal activation functions, where we show that if the underlying labeling scheme has limited bandwidth and Fourier 1-norm (i.e. "nice" Fourier properties), we expect a gradient-based method to achieve good generalization performance. To arrive at this result, we first develop a Fourier-based generalization bound for 2-layer neural networks in terms of bandwidth and Fourier 1-norm. Next, we prove that the local minima found by gradient-based methods over a 2-layer neural network with sine activation are expected to have bandwidth and Fourier 1-norm bounded in terms of the spectral properties of the underlying labeling scheme.
As a byproduct of our Fourier analysis, we derive generalization bounds for 2-layer neural networks with general activation functions. For bandlimited activation functions with finite Fourier 1-norm, such as sinusoidal or Gaussian activation, our bound is tighter than the generalization bound obtained using only the Lipschitz constant of the activation function. For ReLU-type activation functions, our generalization bound is comparable to Lipschitz-based bounds; however, it leads to a grouped version of the path norms developed in (Neyshabur et al., 2015a). We therefore call this capacity norm group path norm which can be used as an additive penalty to regularize 2-layer neural networks with ReLU activation. Our numerical experiments suggest that the generalization gap can be effectively tightened by regularizing the group path norm. Figure 1b demonstrates how group path norm regularization can help close the generalization gap for both true and random labels.
2 RELATED WORK
Generalization has been a topic of central interest in statistical learning theory (Vapnik, 1999; ShalevShwartz & Ben-David, 2014). Generalization bounds have been derived using the stability of a learning algorithm (Bousquet & Elisseeff, 2002) and various complexity measures of a function space

2

Under review as a conference paper at ICLR 2018

such as VC-dimension (Vapnik, 2013) and Rademacher complexity (Bartlett & Mendelson, 2002). (Hardt et al., 2015) develops a stability-based generalization result for SGD as the learning algorithm, which holds for both convex and non-convex loss functions.
We note that spectral analysis has provided a powerful framework for studying neural networks. (Barron, 1993) uses a Fourier-based approach to prove the universal approximation theorem for 2-layer neural networks. Similarly, (Lee et al., 2017) applies Fourier analysis to extend Barron's result to a general feedforward neural network. (Rippel et al., 2015) uses a spectral approach to model and analyze convolutional neural networks (CNNs) and introduce the spectral pooling scheme for CNNs. Also, our Fourier-based approach to analyze SGD's performance for 2-layer neural networks follows the same prinicples as the analysis performed in (Shamir, 2016) to prove the hardness of fitting periodic labeling schemes via gradient-based methods. We should note that in this work we use only periodic activation functions and not periodic labeling schemes. Therefore, the hardness result shown in (Shamir, 2016) does not affect our numerical experiments.
In general, theoretical studies of neural networks can be categorized into three main categories: 1) Approximation: Neural networks have been proven to be powerful in expressing a very rich class of functions (Cybenko, 1989) and in general deeper networks need fewer neurons to express the same class of functions (Eldan & Shamir, 2016; Liang & Srikant, 2016). 2) Generalization: Tight bounds have been shown on the VC dimesnion of feedforward neural networks (Anthony & Bartlett, 2009; Harvey et al., 2017). Also, norm-based Rademacher complexity bounds have been developed at (Bartlett & Mendelson, 2002; Neyshabur et al., 2015b). Sharpness of local minima and its connection to their generalizibility have been the focus of several recent works (Keskar et al., 2016; Dinh et al., 2017; Neyshabur et al., 2017) 3) Optimization: theoretical studies have shown both positive (Andoni et al., 2014; Daniely, 2017) and negative (Shalev-Shwartz et al., 2017) results about the performance of gradient-based methods in training neural networks.

3 PRELIMINARIES

3.1 SUPERVISED LEARNING AND GENERALIZATION

Suppose that we are given n samples (xi, yi)ni=1 drawn i.i.d. from a population distribution PX,Y . Here X denotes the random vector of features and Y denotes the target variable. Using these n samples, the goal of a supervised learner is to find a prediction rule f from a function space F which
can predict Y for an unseen test sample X. Therefore, given loss function the supervised learner wants to find f   F minimizing the population risk, defined as E f (X), Y averaged under the
population distribution.

However, the supervised learner does not know the population distribution PX,Y and has only

access to the n training samples. The supervised learner can minimize the empirical risk, defined

as 1/n

n i=1

f (xi), yi and find fnemp. Since we only observe a limited number of samples, the

empirical risk would be different from the population risk. The generalization risk, defined for f  F

as E[ (f risk for

(X),

Y

)]

-

1 n

f . Studying the

n i=1

(f (xi),

behavior of

yi), is the difference among the population risk and empirical fnemp's generalization risk for different function spaces and

learning algorithms is a topic of central interest in statistical learning theory.

3.2 FOURIER TRANSFORM AND BANDLIMITED FUNCTIONS

Consider a real-valued function f : Rk  R. The Fourier transform of this function, which we denote by f , is defined as

f () = f (x) exp -2iT x d.

(1)

Some important examples of Fourier transform are:
· Sinusoidal function: f (x) = exp(2iT x), then f () = ( - ) where  denotes the Dirac delta function, which also implies ­ f (x) = cos(2T x), then f () = 1/2 ( + ) + ( - ) . ­ f (x) = sin(2T x), then f () = i/2 ( + ) - ( - ) .

3

Under review as a conference paper at ICLR 2018



· Gaussian function: f (x) = (

2)k exp

-

x

2 2

/22

, then f () = exp

-2



22/2

.

Thus, the Fourier transform of a Gaussian function preserves the Gaussian shape.

A function f is called B-bandlimited if f () = 0 for every  where  2 > B. The smallest B for which this property holds is called the bandwidth of f . We use B(f ) to denote the bandwidth of
function f . We also use f 1 to denote the 1-norm of f 's Fourier transform,

f 1 = |f ()| d

(2)

which we call the Fourier 1-norm of f . Fourier 1-norm can be interpreted as the absolute volume under f 's Fourier transform, and is an approximate measure of f 's sparsity. Fourier 1-norm is both scale and shift invariant, i.e. if we define g(x) = f (Wx + b) for a real-valued f and W  Rr×k and b  Rr for some r  k, then g 1 = f 1. Some other useful properties of Fourier transform are:
· Synthesis: f (x) = f () exp 2iT x d, which also implies f 1 = f (0) if f is real and non-negative.
· Shift: fb() = exp(2ibT )f () where fb(x) := f (x - b), which implies fb 1 = f 1 and B(fb) = B(f ).
· Derivative: f () = 2i f () , where f denotes the gradient of f .
· Isometry: f (x)g(x) dx = f ()g() d where z denotes the complex conjugate of z.
· Convolution: f g = f g where denotes the convolution operator i.e. f g() := f ()g( - ) d. Therefore, B(f g)  B(f ) + B(g) and f g 1  f 1 g 1.

4 A FOURIER-BASED GENERALIZATION BOUND

Consider a supervised learning task with n training samples

xi, yi

n i=1

and

function

space

F.

We

are interested in uniform convergence bounds on the generalization risk. A standard approach to

bound the generalization risk is based on the notion of Rademacher complexity. Given samples xi, yi in=1, the empirical Rademacher complexity of F is defined as

Renmp(F ) := E

1n

sup
f F

n

if (xi)
i=1

(3)

where i's are i.i.d. random variables uniformly distributed over {-1, +1}. In fact, the Rademacher complexity of F measures how well F can fit some random labels over input xi's. The following result shows how to bound the generalization risk over F through its Rademacher complexity.

Theorem 1 (Bartlett & Mendelson (2002)). Consider a -Lipschitz loss function (f (x), y) bounded as | (z, y)|  c. Then, for any  > 0, with probability at least 1 - 

f  F :

E

(f (X), Y ) - 1 n n

(f (xi), yi)  2Renmp(F ) + 4c

2 log(4/) .
n

i=1

(4)

Since the Rademacher complexity of norm-bounded linear functions can be appropriately bounded (Kakade et al., 2009), one can effectively apply Theorem 1 to bound generalization risk over normbounded linear functions. To use Theorem 1 in the Fourier domain, here we provide a Rademacher complexity bound for bandlimited functions with bounded Fourier 1-norm. We apply the following Rademacher complexity bound to bound generalization risk for 2-layer neural networks in Section 5, and also to analyze the performance of gradient-based methods with sinusoidal activation functions in Section 6.
Theorem 2. Consider function space F = f : Rk  R s.t. B(f )  B, f 1  V of Bbandlimited functions with V -bounded Fourier 1-norm. Then, the empirical Rademacher complexity for samples (xi, yi)in=1 is bounded as

Renmp(F )  V

4k log 64 nB maxi xi 2 . n

(5)

4

Under review as a conference paper at ICLR 2018

Proof. We defer the proof to the Appendix.

Corollary 1. Assume that X 2  C holds almost surely and the loss function is -Lipschitz. Then, for any  > 0 with probability at least 1 -  the following generalization bound holds for any

B-bandlimited function f with V -bounded Fourier 1-norm:

1 n k log(nBC/)

E

(f (X), Y ) - n

(f (xi), yi)  O V

. n

i=1

(6)

Proof. The corrollary is a direct result of applying the bound in Theorem 2 to Theorem 1.

The above corollary bounds the generalization risk uniformly over all bandlimited f 's such that B(f )  B and f 1  V . Next, we apply the above results to 2-layer neural networks.
5 APPLICATION OF THEOREM 2 TO 2-LAYER NEURAL NETWORKS

Consider a 2-layer neural network including d neurons with activation function  in the hidden layer (See Figure 1a). The output of this neural network is

fa,W,b(x) = aT (Wx + b).

(7)

If  has bounded bandwidth and Fourier 1-norm, we can apply Theorem 2 to bound the Rademacher complexity and hence generalization risk over the 2-layer neural network. Here, we use W 2, to denote the maximum 2-norm wi 2 among all rows of W.
Corollary 2. Let F = f (x) = aT (Wx + b) : W 2,  W, a 1  A be the class
of 2-layer neural networks where B() = B and  1 = V . Then, the empirical Rademacher complexity of F for samples (xi, yi)in=1 is bounded as follows

Rnemp(F)  O AV

k log nBW max xi 2 n

.

(8)

Proof. We defer the proof to the Appendix.

Notice that for bandlimited activation functions with bounded Fourier 1-norm, the above generalization bound is increasing logarithmically with W 2,. For example, for the sinusoidal activation
function (x) = sin(2x) we have  1 = 1, B() = 1. By exploiting the spectral properties of , Corollary 2 results in a tighter generalization bound than the bounds using only the Lipschitz constant of .

However, an unbounded activation function such as ReLU activation (x) = max(x, 0) has an infinite Fourier 1-norm. Therefore, Corollary 2 does not directly apply to these activation functions. The following theorem uses a boundedness assumption on input X to apply Theorem 2 to ReLU-type activation functions.

Theorem 3. Suppose that (x) = max{x, x} where   [0, 1] is an arbitrary constant. Consider

the pair of dual norms ( · p, · q) where 1  p, q   and 1/p + 1/q = 1. Assume that

xi p  C holds for all xi's. Then, for F = fa,W(x) = aT (Wx) :

d i=1

|ai|

wi

q V

Rnemp(F )  O V C

k log(nkC) .
n

(9)

Proof. We relegate the proof to the Appendix.

The above bound uses the complexity score

d i=1

|ai|

wi

q for each fa,W(x) = aT (Wx). We

can rewrite this complexity score in the following way, which is an 1,q-group norm on the product

of weights for each path from the input nodes to the output node of the 2-layer neural network,

d
q fa,W =

k 1/q

|ai||wi,j | q

.

i=1 j=1

(10)

5

Under review as a conference paper at ICLR 2018

Here wi,j denotes the weight on the link from the jth node of the input layer to the ith node of the hidden layer. Based on the path-norm function defined at (Neyshabur et al., 2015a), we call
q fa,W the group path norm. For q = 1, 1-group path norm leads to the 1-path norm for 2-layer neural networks. We can use group path norms as an additive regularization penalty to learn over
2-layer neural networks. In our numerical experiments, we test the performance of 2-group path norm and 1-path norm regularization to control the generalization risk over 2-layer neural networks.

6 FOURIER ANALYSIS OF GRADIENT-BASED METHODS FOR 2-LAYER NEURAL NETWORKS WITH SINE ACTIVATION

In this section, we apply Fourier analysis for a 2-layer neural network with sinusoidal activation. We aim to understand the connection between generalizibility of local minima found by gradient-based methods and spectral properties of the population distribution PX,Y . As a simplifying assumption, let's assume that target variable Y is a deterministic function Y (x) of input X, which we call the labeling scheme. In our analysis, we consider the squared-error loss (y, y ) = (y - y )2.
We specifically ask this question: how can spectral properties of labeling scheme Y (x) and population density function PX(x) affect the generalization performance of a gradient-based method? To address this question, we use a similar strategy to the analysis performed in (Mei et al., 2016) by establishing generalization results for both the empirical risk and the gradient of empirical risk. First, we show that the bandwidth and Fourier 1-norm for the local minima of the population risk can be bounded in terms of the bandwidth and Fourier 1-norm of Y (x) and PX(x). Next, we establish a generalization result for the gradient of the empirical risk, proving that the gradient of empirical risk would stay close to the gradient of population risk given that Y (x) has limited bandwidth and Fourier 1-norm. These two results show that by assuming a labeling scheme with limited bandwidth and Fourier 1-norm, the local minima found by a gradient-based method are expected to generalize well.

6.1 POPULATION RISK WITH SINUSOIDAL ACTIVATION

Consider fa,W,b(x) =

d j=1

aj

sin(2wjT

x

+

bj )

coming

from

a

2-layer

neural

network

with

d

sinusoidal hidden units. Given the labeling scheme Y (x) the population risk will be

d
EPX fa,W,b(x) , Y (x) = EPX Y (x) - aj sin(2wjT x + bj ) 2 ,
j=1

(11)

where the expectation is according to the population density function PX(x).

Lemma 1. Consider the population risk in (11). Assume wj satisfies  i = j : min wi -
wj 2, wi+wj 2 > B(PX). Then, if (a, W, b) is assumed to be a local minimum of the population risk,

|aj|  2 Y PX(wj) .

(12)

Proof. We defer the proof to the Appendix.

Lemma 1 says that if the component aj sin(2wjT x) becomes isolated for a local minimum, by which we mean there are no other component ai sin(2wiT x) with min{ wi - wj 2, wi + wj 2} less than PX's bandwidth, then the value of aj for that local minimum is nicely bounded in terms of the population distribution. This result leads to the following Theorem which describes the Fourier properties of the local minima of the population risk.
Theorem 4. Consider the minimization problem of the population risk (11). If a local minimum (a, W, b) satisfies the isolated components condition, i.e. for any two different i, j we have min wi - wj 2, wi + wj 2 > 2 B(PX), then for the local minimum function fa,W,b
· B(fa,W,b )  B(Y ) + B(PX),
· fa,W,b 1  2 Y 1.

6

Under review as a conference paper at ICLR 2018

Proof. We defer the proof to the Appendix. We also note that the isolated components condition is just a technical condition to find the exact value instead of approximating a convolution integral. Even if the isolated components condition does not hold, this result remains valid within an approximation error term, which is discussed in section 8.6.

The above result implies that the bandwidth of the local minima of the population risk is less than the
sum of bandwidths for Y and PX. Also, the Fourier 1-norm for the local minima of the population distribution is bounded by twice the Fourier 1-norm of Y .

6.2 GENERALIZATION TO THE EMPIRICAL RISK

Theorem 4 characterizes the Fourier properties of the local minima for the population risk. However,
we want to investigate the generalization performance of the local minima of the empirical risk defined for training samples (xi, Y (xi))ni=1 as

1n n
i=1

1n

fa,W,b(xi) , Y (xi)

= n

i=1

d2
Y (xi) - aj sin(2wjT xi + bj) .
j=1

(13)

To address this question, note that the bandwidth and Fourier 1-norm of the loss's gradient with respect to each aj are bounded in terms of the bandwidth and Fourier 1-norm of Y (x) as

aj fa,W,b(x),Y (x) 1  Y 1 + a 1,

(14)

B aj fa,W,b(x), Y (x)  B(Y ) + 2 W 2,.

(15)

We can apply Corollary 1 to show that not only the empirical risk uniformly converges to the population risk but also the gradient of the empirical risk will stay close to the gradient of the population risk.

Corollary 3. Consider fa,W,b(x) =

d j=1

aj

sin(wjT

x

+

bj

)

and

squared

error

loss

. Then, given

that X 2  C, for any  > 0 with probability at least 1 -  we have

 j, a, W, b s.t. a 1 + Y 1  V, 2 W 2, + B(Y )  B :

(16)

E aj fa,W,b(X), Y (X)

1n

- n

aj fa,W,b(xi), Y (xi)

i=1

k log nBC/

O V

.

n

Proof. The corollary is a direct result of Corollary (1) given (14) and (15). Note that the generalization bound holds with probability 1 -  for the derivative with respect to all aj's, since the bounds in (14) and (15) hold for all j's.

Therefore, if a gradient-based method such as stochastic gradient descent starts learning from fa,W,b with low a 1 and W 2, and also the bandwidth and the Fourier 1-norm for Y (x) are limited, we expect that the result of Theorem 4 to hold for the local minima of the empirical risk found by the gradient-based method. Hence, we expect that the bandwidth and Fourier 1-norm for the local minima of the empirical risk to be also bounded as stated in Theorem 4. We emphasize that to prove Theorem 4 we need to analyze only the derivatives with respect to aj's and not with respect to W (See Lemma 1). In conclusion, due to Corollary (1), our discussion suggests that the neural network function found by a gradient-based method should possess good generalization properties if the underlying labeling scheme Y (x) has limited bandwidth and Fourier 1-norm.
7 NUMERICAL EXPERIMENTS
For all experiments described in this section, we implemented and trained the two-layer neural network described in Figure 1a using TensorFlow 1.3.0. We used SGD to train the model for 2000 epochs with an initial learning rate of 0.01. The learning rate decayed slightly each epoch at a rate of 0.95 every 390 epochs. We used h = 512 hidden units and a batch size of 128. When working with CIFAR10 data, we preprocessed the data as described in (Zhang et al., 2016), resulting in each training sample having dimension d = 2352. Initial weights from the first layer were sampled from N (0, 0.01/d) and initial weights from the second layer were sampled from N (0, 0.01/h).

7

Under review as a conference paper at ICLR 2018
a. b.

c.

d. e.

Figure 2: Training an test performance on cat and airplane CIFAR10 images with true and random labels. Sine activation and mean-squared-error loss were used.

a. b. c.

d.

Figure 3: Training and test performance on cat and airplane CIFAR10 images with true and random labels. ReLU activation and cross-entropy loss were used.
7.1 SGD GRADUALLY LEARNS HIGHER FOURIER 1-NORM, BANDWIDTH HYPOTHESES
We first numerically demonstrate that how Fourier 1-norm and bandwidth both increases during training via SGD. Motivated by the analysis from Section 6, we use the squared-error as our loss function and sine as our activation function. Our samples consist of cats and airplanes from the CIFAR10 dataset with the labels mapped to -1 and 1. We use 5000 and 2000 samples from each category for training and test, respectively. We arbitrarily chose two of the ten classes to accommodate our choice of loss function. We evaluate the network's performance for both random and true labels.
Figure 2a shows that without regularization, SGD learns to perfectly fit both the true and random labels, which is consistent with the results from Zhang et al. (2016). Additionally, the random labels are harder to learn, requiring more epochs before achieving a perfect fit. Figures 2b and 2c confirm that both Fourier 1-norm and bandwidth consistently increase with training, highlighting how SGD gradually finds more complex hypotheses in order to fit the data. Finally, we see in figures 2d and 2e how both Fourier 1-norm and bandwidth increase with generalization risk (the difference between test mean squared-error (MSE) and training MSE) with almost perfect correlation. This suggests that, as implied by the theory above, regularizing Fourier 1-norm and bandwidth could improve generalizability of the final learned model.
7.2 GROUP PATH NORM REGULARIZATION FOR RELU ACTIVATION
We regularize group path norm for ReLU activation as motivated by Theorem 3. Although 2-group path norm is not convex, it is differentiable and we can use it as an additive penalty and find a local minimum via SGD. Using the same experimental setup as from section 7.1, we swap sine for ReLU and test the network's performance for both random and true labels.
8

Under review as a conference paper at ICLR 2018
Figure 3a confirms that, like before, the network can fit both true and random labels. The generalization gap, however, remains large for random labels. By regularizing the 2-norm of all the weights, we see that the generalization gap closes for both the true labels and the random labels without compromising test accuracy significantly (Figure 3b). This result is further improved when we use the 2-group path norm and 1-path norm (Figure 3c and 3d), demonstrating that direct regularization of Fourier 1-norm leads to better generalization. We note that while we tested multiple values of  for each regularization technique, we always chose the  that resulted in the smallest generalization gap with comparable test performance.
We repeated the experiment using all 50000 CIFAR10 training samples (and 10000 test samples). We included all 10 classes and switched to cross-entropy loss. The results are shown in Figure 1b. Again, we see that while all regularization techniques give similar test performance, the generalization gap is closed significantly for the 2-group path norm and 1-path norm.
REFERENCES
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural networks. In International Conference on Machine Learning, pp. 1908­1916, 2014.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge university press, 2009.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930­945, 1993.
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1706.08498, 2017.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Olivier Bousquet and André Elisseeff. Stability and generalization. Journal of Machine Learning Research, 2(Mar):499­526, 2002.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pp. 160­167, 2008.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2(4):303­314, 1989.
Amit Daniely. Sgd learns the conjugate kernel class of the network. arXiv preprint arXiv:1702.08503, 2017.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. arXiv preprint arXiv:1703.04933, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference on Learning Theory, pp. 907­940, 2016.
Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. arXiv preprint arXiv:1509.01240, 2015.
Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piecewise linear neural networks. arXiv preprint arXiv:1703.02930, 2017.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82­97, 2012.
9

Under review as a conference paper at ICLR 2018
Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pp. 793­800, 2009.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
Holden Lee, Rong Ge, Andrej Risteski, Tengyu Ma, and Sanjeev Arora. On the ability of neural nets to express distributions. arXiv preprint arXiv:1702.07028, 2017.
Shiyu Liang and R Srikant. Why deep neural networks? arXiv preprint arXiv:1610.04161, 2016.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv preprint arXiv:1607.06534, 2016.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2413­2421, 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In COLT, pp. 1376­1401, 2015b.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring generalization in deep learning. arXiv preprint arXiv:1706.08947, 2017.
Oren Rippel, Jasper Snoek, and Ryan P Adams. Spectral representations for convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 2449­2457, 2015.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning. In International Conference on Machine Learning, pp. 3067­3075, 2017.
Ohad Shamir. Distribution-specific hardness of learning neural networks. arXiv preprint arXiv:1609.01037, 2016.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):988­999, 1999.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
8 APPENDIX
8.1 PROOF OF THEOREM 2
We use a high-dimensional grid in the Fourier domain to approximate the Fourier transform of a B-bandlimited function. Consider the ball  :  2  B . Using the bounds on the covering number for 2-norm, for any 0 < < B we can find a set of points {j : 1  j  (3B/ )k} such that for any  with  2  B, there exists some j with  - j 2  .
10

Under review as a conference paper at ICLR 2018

Let Sj = { :  - j 2  } for each 1  j  (3B/ )k. Note that { :  2  B}  jSj. We then define Sj = Sj \ tj=-11St to have a group of disjoint sets Sj covering { :  2  B}. Since any f  F is assumed to be B-bandlimited, for f  F

f (x) = f () exp(2iT x) d

(3B/ )k
= f () exp(2iT x) d.
j=1 Sj

Then, for any f  F = {f : B(f )  B, f 1  V } we have

(3B/ )k

f (x) -

exp(2ijT x)

f () d

j=1

Sj

(3B/ )k
(=a) f () exp(2iT x) - exp(2ijT x) d
j=1 Sj

(3B/ )k
 f () exp(2iT x) - exp(2ijT x) d
j=1 Sj

(b) (3B/ )k
 f () 2 x 2  - j 2 d
j=1 Sj

(3B/ )k
 2 x 2
j=1

f ()
Sj

 - j 2 d

(c)
 2

(3B/ )k
x2
j=1

f () d
Sj

(17)

= 2 x 2 f () d

 2 x 2V.
Here, (a) is a direct application of (17). (b) holds as exp(ibz) = cos(bz) + i sin(bz) is b-Lipschitz as a function of z  R for any real number b > 0. (c) holds because according to our definitions Sj  Sj and Sj = { :  - j 2  }.

Therefore, the following function space F can approximate any f  F = f : B(f )  B, f 1 
V within 2 CV accuracy for any x 2  C. Here a is, in general, a vector of complex numbers, and a 1 := j |aj| where |z| denotes the absolute value of complex number z,

F=

(3B/ )k

f (x) =

aj exp(2iTj x) :

j=1

a 1V .

(18)

Then, F is the space of 1-norm bounded linear functions in terms of the input vector exp(2iTj x) j. Now, we can apply a well-known bound (Shalev-Shwartz & Ben-David, 2014) on the Rademacher complexity of 1-norm bounded linear space Flin,1 = f : Rk  R s.t. f (x) =
aT x, a 1  A as

Rnemp(Flin,1)

A

max
i

xi



2 log(2k) .
n

Applying the above bound, we can bound the Rademacher complexity of F as

(19)

Renmp(F )  V

2k log(6B/ ) .
n

(20)

11

Under review as a conference paper at ICLR 2018

Since for each f  F there exists f~  F such that  x 2  C : |f (x) - f~(x)|  2 CV ,

Rnemp(F ) = E

1n

sup
f F

n if (xi)
i=1

 E

sup
f~F

1 n

n

if~(xi)

i=1

+ 2 V max xi 2
i

=

Rnemp(F

) + 2

V

max
i

xi

2.

Finally, combining (20) and (21) we obtain:

(21)

 > 0 : Rnemp(F )  V

2k log(6B/ )

n

+ 2 V max xi 2.
i

If we choose the value

=

1 2n maxi

xi

, then we get
2

(22)

Rnemp(F )  V

2k log 12nB maxi

xi 2

1 +

nn

 V 4k log 12nB maxi xi 2 + 2/n n

(23)

 V 4k log 64 nB maxi xi 2 , n
where the last inequality follows from the fact that 1  k, n. Therefore, the proof is complete.

8.2 PROOF OF COROLLARY 2

First, we prove the following lemma. Lemma 2. Given function f : Rk  R and matrix W  Rk×k, we define g(x) = f (Wx). Then,
· B(g)  W 2 B(f ) with W 2 denoting the spectral norm of W, · g 1 = g 1.

Proof. From the properties of the Fourier transform we know

g() =

1 | det(W)| f

W-T 

.

(24)

Therefore, g(WT  ) =

|

1 det(W)|

f



and if  2  B(f ), then WT  2  W 2B(f ) gives

an upperbound on B(g). Also,

g 1 = g() d

= 1 f W-T  ) d | det(W)|

1 =

f W-T  d

| det(W)|

11 = | det(W)| f  ) | det(W-T )| d

= f ( ) d

(25)

= f 1.

12

Under review as a conference paper at ICLR 2018

It can be seen that this result remains valid even if W is not an invertible matrix, which will complete the proof for Corollary 2. However, we continue proving Corollary 2 without using this fact.

As shown in the above lemma, Fourier 1-norm and bandwidth are invariant to an orthonormal transformation W. Given fi(x) = ai(wiT x), we define gi(x) = fi(Aix) where Ai is an orthonor-

mal matrix with wi as an eigenvector. Note that fi 1 = gi 1 and B(fi) = B(gi). However,

gi(x) is a function of only one of the coordinates, which we can assume, without loss of generality,

to be the first coordinate. Hence, gi(x) = ai( wi 2x1) for the first coordinate x1, implying

gi() =

ai wi

(
2

1 wi

2 ).2(2) . . . k(k) where j

is the Dirac delta function across the jth dimen-

sion. Hence, we can use the above lemma in the 1-dimensional case to show gi 1 = |ai|  1 and

B(gi) = wi 2B(). As a result,

fi 1 = |ai|  1, B(fi)  wi 2B().

Hence, for f (x) = aT (Wx + b) =

d i=1

ai(wiT

x

+

bi)

we

have

(26)

f 1  a 1  1, B(f )  W 2,B(). The corollary is then a direct application of Theorem 2.

(27)

8.3 PROOF OF THEOREM 3

Given a ReLU-type activation function (z) = max{z, z},

 wT x

=

w q (

w )T x . wq

(28)

Since

w wq

q = 1, if

x p  C, then

(

w w

q

)T

x

 C and hence the input to  in the R.H.S. of

(28) is always between -C and C.

Suppose that function  satisfies (z) = (z) for z  [-C, C]. Then, based on the above
discussion, we can bound the Rademacher complexity of F by finding a bound on the Rademacher complexity of F = fv,U(x) = vT (Ux) : v 1  V, i : ui q = 1 .

To find a good candidate for , we use a symmetrization trick to define

 -C



(z)

=

 


(z) (2C - z)



 -C

if z < -C, if - C  z < C, if C  z < 3C, if 3C  z.

(29)

Note

that

(z)

=

(1

-

)C

h(

z-C C

)

+

2C

h(

z-C 2C

)

-

C

where

h(z)

=

max{0,

1

-

|z|}.

It

can

be seen that h() =

sin() 

2 which is real and positive everywhere. Therefore,

h 1 = h(0) = 1

which means that  1  C(1 + 2)  3C.

Since

|h()|



1 2

,

we

have

()



1 2

.

For

B

>

0,

we

let

the

B-filtered

,B

be

a

function

with

the following Fourier transform:

,B() =

() if ||  B 0 otherwise.

(30)

Then, since

()



1 2

we

have

z  R :

(z) - ,B(z) 

||B

()

d



2 .
B

(31)

Thus,

for

any

B

>

0

the

defined

,B

approximates



with

a

maximum

error

of

2 B

uniformly

over

[-C, C]. ,B also satisfies ,B 1  3C and B(,B) = B. Applying Corollary 2, we get

B > 0 :

Renmp F  O V C

k log nB max xi 2 n

2 +.
B

(32)

13

Under review as a conference paper at ICLR 2018

 Here we can bound maxi xi 2  k maxi xi   kC, and choose B = n to get

Rnemp F  O V C

k log nkC 1 +,
nn

which completes the proof.

(33)

8.4 PROOF OF LEMMA 1

Note that aj EPX

fa,W,b(X), Y (X)

= EPX aj fa,W,b(X), Y (X) = EPX aj fa,W,b(X) - Y (X) 2
d
= EPX 2 sin(2wjT X + bj) at sin(2wtT X + bt) - Y (X)
t=1
= EPX 2aj sin2(2wjT X + bj) - EPX 2 sin(2wjT X + bj)Y (X)
+ EPX 2at sin(2wtT X + bt) sin(2wjT X + bj)
t=j
= aj - 2 cos(bj) Im Y PX(wj) + sin(bj) Re Y PX(wj) .
(34)

To show the last equality, we use the isolatedness assumption for wj, i.e. t = j : min wt - wj 2, wt + wj 2 > B(PX), and also wj 2  B(PX)/2. Then, for each t

EPX 2 sin(2wtT X + bt) sin(2wjT X + bj) = 2 PX(x) sin(2wtT x + bt) sin(2wjT x + bj) dx

= PX(x) cos 2(wt - wj)T x + bt - bj

- cos 2(wt + wj)T x + bt + bj dx

= 0.5 exp(j(bt - bj))PX(wt - wj)

+ 0.5 exp(j(bj - bt))PX(wj - wt)

- 0.5 exp(j(bt + bj))PX(wt + wj)

- 0.5 exp(-j(bt + bj))PX(-wt - wj)

=

0 if t = j, 1 if t = j.

Also, by applying the convolution property of Fourier transform we can show

(35)

EPX sin(2wjT X + bj)Y (X) = PX(x)Y (x) sin(2wjT x + bj) dx

= (PX × Y )(x) cos(bj) sin(2wjT X) + sin(bj) cos(2wjT X) dx
= cos(bj) Im Y PX(wj) + sin(bj) Re Y PX(wj) .
Finally if (a, W, b) is a local minimum for the population risk, for all t's we have at EPX fa,W,b(X), Y (X) = 0. Therefore, due to the isolatedness assumption of wj we have

|aj| = 2 cos(bj) Im Y PX(wj) + sin(bj) Re Y PX(wj)  2 Y PX(wj) . (36)

14

Under review as a conference paper at ICLR 2018

8.5 PROOF OF THEOREM 4

Since the isolatedness assumption holds for all j's, by Lemma 1,

 j : |aj |  2 Y PX(wj) .

(37)

If wt 2 > B(Y ) + B(PX) holds for some t , (37) implies

|at|  2 Y PX(wt) = 0.

(38)

Hence, at will be 0, implying there will be no component in fa,W,b with wt 2 > B(Y )+B(PX). This discussion proves the first part of Theorem, i.e. B(fa,W,b )  B(Y ) + B(PX).

To show the second part, note that

fa,W,b 1 = a 1

(a) d
2 Y
t=1

PX(wt)

d
=2
t=1

Y ()PX(wt - ) d

d
2
t=1

Y ()PX(wt - ) d

d
=2
t=1

Y () PX(wt - ) d

d

= 2 Y ()

PX(wt - ) d

t=1

(b) d
2

Y () d

t=1

(39)

= 2 Y 1.

Here, (a) comes from Lemma 1. Also, since min wt - wr 2, wt + wr 2 > 2 B(PX) is

assumed for any t = r, for any  at most one element in

PX(wt

- )

d t=1

can

be

nonzero.

Because

if both PX(wt - ) and PX(wr - ) are nonzero for r = t, then wt -   B(PX) and also wr -   B(PX) which results in wt - wr  2B(PX) which is a contradiction. Hence,

d

PX(wt - )

 max


PX( )



t=1

PX(x) dx = 1,

(40)

which proves (b) and completes the proof.

8.6 PROOF OF THEOREM 4 WITHOUT THE ISOLATED COMPONENTS ASSUMPTION

What happens if a component wi is not isolated from the other components which has been assumed in Theorem 4? As a simplifying assumption, we assume that b = 0 and PX is real. We can write

 i : ai EPX

fa,W(x), Y (x)

d
= ajPX(wi - wj) - Im Y PX(wi)
j=1

d

j=1

aj -

Im{Y ()} d PX(wi - wj)

Swj

(41)

15

Under review as a conference paper at ICLR 2018

Here Swj 's, which are centered around wj's, are disjoint sets covering the bandwidth region for Y , i.e. { :  2  B(Y )}  j Swj . Note that in (41) we have approximated the convolution integral as

Y PX(wi) =

PX(wi - )Y () d

:  2B(Y )

d

= PX(wi - )Y () d
j=1 Swj

d

 PX(wi - wj)

Y () d.

j=1

Swj

Letting the gradient element in (41) be zero for all ai's at a local minimum (a, W) of the population risk, the following approximation holds in general case:

 j : aj 

Im{Y ()} d,

Swj

(42)

Here, the matrix PX(wi - wj) 1i,jd is positive-definite and hence invertible, because PX is the Fourier transform of PX and due to Bochner's theorem a positive-definite kernel function. Therefore, the system of linear equations PX(wi - wj) aj - Swj Im{Y ()} d  0 would imply (42). This discussion indicates that the result of Theorem 4 would remain valid even if the isolated
components condition does not hold.

16

