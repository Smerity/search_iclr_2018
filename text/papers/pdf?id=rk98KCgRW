Under review as a conference paper at ICLR 2018
IMPROVED LEARNING IN CONVOLUTIONAL NEURAL NETWORKS WITH SHIFTED EXPONENTIAL LINEAR UNITS (SHELUS)
Anonymous authors Paper under double-blind review
ABSTRACT
The Exponential Linear Unit (ELU) has been proven to speed up learning and improve the classification performance over activation functions such as ReLU and Leaky ReLU for convolutional neural networks. The reasons behind the improved behavior are that ELU reduces the bias shift, it saturates for large negative inputs and it is continuously differentiable. However, it remains open whether ELU has the optimal shape and we address the quest for a superior activation function. We use a new formulation to tune a piecewise linear activation function during training, to investigate the above question, and learn the shape of the locally optimal activation function. With this tuned activation function, the classification performance is improved and the resulting, learned activation function shows to be ELU-shaped irrespective if it is initialized as a RELU, LReLU or ELU. Interestingly, the learned activation function does not exactly pass through the origin indicating that a shifted ELU-shaped activation function is preferable. This observation leads us to introduce the Shifted Exponential Linear Unit (ShELU) as a new activation function. Experiments on Cifar-100 show that the classification performance is further improved when using the ShELU activation function in comparison with ELU. The improvement is achieved when learning an individual bias shift for each neuron.
1 INTRODUCTION
The classification accuracy of Convolutional Neural Networks (CNNs) has improved remarkably over the last years. The reason for the improvement is manifold: more sophisticated layer designs (Lin et al., 2013; He et al., 2016), effective regularization techniques reducing overfitting such as dropout (Srivastava et al., 2014) and batch normalization (Ioffe & Szegedy, 2015), new nonlinear activation functions (Clevert et al., 2015; Trottier et al., 2016), improved weight initialization methods (Glorot & Bengio, 2010; He et al., 2016), data augmentation and large scale data as ImageNet (Deng et al., 2009).
In this work, we focus on the nonlinear activation function and its effect on the network learning behavior. Since the introduction of the Rectified Linear Unit (ReLU) (Glorot et al., 2011), it is generally accepted that the activation should be noncontractive to avoid the vanishing gradient problem. The vanishing gradient hampered the learning for the sigmoid and tanh activations. As ReLU is not symmetric, its mean response will be non-negative and will introduce a bias shift for the units in the next layer. The Leaky Rectified Linear Unit (LReLU) (Maas et al., 2013) was proposed to alleviate this bias shift. The LReLU introduces a small linear activation for negative inputs controlled by a constant hyperparameter .
Centering the activation, i.e. reducing the bias shift, is claimed to speed up learning (Le Cun et al., 1991). When the Exponential Linear Unit (ELU) was introduced by Clevert et al. (2015), one of the reason for its success and fast learning capability was claimed to be that the activation saturates for large negative inputs. ELU is also controlled by a hyperparameter that determines the saturation level. Another activation that is saturated for negative inputs is the Shifted ReLU (SReLU). Its shape is similar to ReLU, but the "kink" is at -1 instead of 0. This reduces the bias shift while being saturated for negative inputs. ELU learns both faster and better than SReLU (Clevert et al.,
1

Under review as a conference paper at ICLR 2018

Table 1: Activation functions.

Activation x > 0

x0

ReLU

x

0

LReLU

x

x

SReLU

x max(x,-1)

ELU

x (exp(x)-1)

PELU

x (exp( x)-1)

2015), but it is not obvious which properties of ELU that actually create this improvement. It may be the smooth exponential decay for small negative inputs and/or the fact that it is continuously differentiable. The question also remains whether the shape of ELU is truly the optimal activation function or if there are other shapes, not yet found, that would further speed up and improve learning. And if they exist, how are they to be found. These were the type of issues that we wanted to explore when starting this work.
To improve the learning capabilities for the above mentioned activation functions, tuneable variants of them have been published where the control parameters are tuneable and learned instead of being set as a constant parameter according to their original publications. The Parametric ReLU (PReLU) was introduced by He et al. (2016) where the single control parameter  for LReLU is now learned during training. The Parametric ELU (PELU) was introduced by Trottier et al. (2016), also tuning the control parameters for ELU. Classification results were shown to improve with parameter tuning in both papers. The Scaled ELU (SELU)1 was defined by Klambauer et al. (2017) and is essentially a more simple variant of PELU. The activation functions mentioned so far are defined in Table 1.
Piecewise linear activation functions have previously been used by Agostinelli et al. (2014) to improve the performance compared to LReLU. In this work, we use the same concept with tuneable piecewise linear activation functions, but now with the additional objective to investigate the shape of the learned activation function. We apply this approach for nonlinear regression of the optimal activation function. We initialized the activation as linearized versions of ReLU, LReLU and ELU, and they all resulted in the same shape after tuning the network. The ReLU and LReLU activation functions are tuned into an ELU-shaped function whereas the ELU activation function retains its shape. However, we also noted that the tuned ELU-shaped activation function does not exactly pass through the origin. There is a small shift introduced around the origin while retaining the overall shape of the activation function.
Based on this observation, we introduce a shifted variant of the ELU activation function. In our experiments, we found that a horizontal shift is favorable and we call this new activation function Shifted Exponential Linear Unit (ShELU). The shift is tuneable during training and the shift is individual for each neuron. Experiments show that the classification performance is improved when allowing this shift in the activation function.
Our main contribution is the introduction of the shifted activation function ShELU. The second contribution is a new formulation of a tuneable piecewise linear activation function with constraints to make it continuous. This formulation can be used to explore for other, up to now unseen, shapes of activation functions. The third contribution is experimental support that an ELU-shaped activation function is favorable for learning; the tuneable piecewise linear activation function adapts to an ELU-shape during training, but with a small shift around the origin.
2 PIECEWISE LINEAR ACTIVATION FUNCTIONS
Piecewise linear activation functions were first introduced by Agostinelli et al. (2014). In this work, we use the same idea but now with the additional objective to investigate the shape of the learned activation function. If we initialize the activation function as a linearized version of ReLU, LReLU or ELU, how will the shape be changed during training?
Our formulation of a piecewise linear and continuous activation function is different from the one in Agostinelli et al. (2014) and consists of two steps: first a soft histogram is formed as in Felsberg
1SELU is a variant of PELU and to avoid confusion we name the Shifted ELU ShELU.
2

Under review as a conference paper at ICLR 2018

& Granlund (2006), second, a weighted sum of the histogram outputs is computed. The piecewise linear activation functions are learned individually for each neuron.

2.1 SOFT HISTOGRAM

As it has been shown by Felsberg & Granlund (2006), a soft histogram can be represented by two

components; one offset component and one histogram component. We use N bins for positive input

values and another N bins for negative input values. All bins in the histogram have constant and

unity width. The bin limits take integer values and the bin centers are at -0.5, 0.5, 1.5 etc. The

concept of the soft histogram is illustrated in Figure 1. In the figure, N is equal to 4, but in the

experiments we also used more bins like N = 8 and 16. Within a certain activation layer, we extract

the maximum and the minimum input over a minibatch. We then linearly scale the positive input

values to lie in the range [0 N] and the negative input values to lie in the range [-N 0]. Now, as an

example, consider two units with scaled input values of 2.68 and -1.80, respectively. For the first

unit, the histogram component will be 1 for the bin centered at 2.5 and 0 for all other bins. The

corresponding offset component will be 0.18, i.e. the signed distance from the bin center. For the

second unit, the histogram component will be 1 for the bin centered at -1.5 and 0 for all other bins.

The corresponding offset component will be -0.30. The output from the soft histogram for the two

units will be

y(2.68) =

0 0

0 0

0 0

0 0

0 0

0 0

0.18 1

0 0

(1a)

y(-1.80) =

0 0

0 0

-0.30 1

0 0

0 0

0 0

0 0

0 0

An analytical formulation of the soft histogram can be given as

(1b)

y(x) =

(x - floor(x) - 0.5)m(x) m (x)

=
2×2N

o0 h0

o1 h1

... ...

o2N-1 h2N-1

,

(2)

where m(x) denotes the membership of the respective bins. The membership is 1 if the unit belongs to that bin and 0 for all other bins. The soft histogram output can alternatively be expressed with the
offset and histogram components as in the right hand side of (2).

For backpropagation we need to compute the derivative of the soft histogram. In our formulation,

we consider the membership m(x) to be locally constant. This leads to a particular choice of

subgradient that is also used in most implementations of max pooling, and it works well in practice.

The derivative is computed as

dy =
dx

m (x) 0
2×2N

(3)

The output from the soft histogram is independent of the activation function, but any activation function can be represented or approximated by a weighted sum of the histogram output.

2.2 WEIGHTED SUM
Different piecewise linear activation functions can now be realized by varying the weights applied to the soft histogram outputs. For each activation layer, we define a matrix W with weights for the

Figure 1: Soft histogram decomposed into rectangular and linear basis functions. 3

Under review as a conference paper at ICLR 2018

Figure 2: Weighted sum examples, ReLU activation (left) and LReLU activation (right).

offset components and the histogram components

W=

wo0 wh0

wo1 wh1

... ...

wo2N-1 wh2N-1

.

(4)

To obtain a ReLU activation function, we set all weights to 1 for the offset components on the positive side and 0 for all offset components on the negative side. The weights for the offset components correspond to the slope of the activation function for each linear piece. Further, for a ReLU, we set the weights for the histogram components to 0.5, 1.5, 2.5, etc. on the positive side and to 0 on the negative side. The weights for the histogram components correspond to the bias level at the bin centers of the activation function for each linear piece.

To obtain a LReLU activation function, the weights (slopes) for the negative offset components are set to the value for the hyperparameter , e.g. to 0.1. The weights for the offset components on the positive side are the same as for ReLU. For  = 0.1, the weights for the histogram components on the negative side are set to -0.35, -0.25, -0.15 and -0.05, i.e. the bias level at the bin centers.

To summarize, the weight matrices for the ReLU and LReLU activation functions are defined as

WReLU =

0 0

0 0

0 0

0 0

1 0.5

1 1.5

1 2.5

1 3.5

(5a)

WLReLU =

0.1 -0.35

0.1 -0.25

0.1 0.1 -0.15 -0.05

1111 0.5 1.5 2.5 3.5

.

(5b)

The output for the weighted sum is the sum of an elementwise multiplication of the soft histogram with the weight matrix

y = wo o + wh h .


(6)

The weighted sum output for the two example units is illustrated in Figure 2. For the ReLU activation, the outputs will be 2.5 × 1 + 1 × 0.18 = 2.68 and 0 × 1 + 0 × (-0.30) = 0, respectively. For the LReLU activation, the output for the unit with input value -1.80 will be -0.15 × 1 + 0.1 × (-0.30) =
-0.18, as desired.

Our formulation will generate a piecewise linear activation function for any values chosen as weights for the offset and histogram components. However, it is obvious that constraints need to be put on the weights if a continuous activation function is to be obtained. The weights (slopes) for the offset components can be set independently but only one weight for the histogram component is independent from the other weights. Assume that wh0 is set as desired. To obtain a continuous linear function, the remaining histogram weights then need to be set as

wh1 =
wh2 = ...

wh0 + 0.5(wo0 + wo1 ) wh0 + 0.5(wo0 + 2wo1 + wo2 )

wh2N-1 = wh0 + 0.5(wo0 + 2wo1 + · · · + 2wo2N-2 + wo2N-1 ) .

(7)

The constraints in (7) must be enforced when updating the weights in the backpropagation step.

3 SHIFTED ACTIVATION FUNCTIONS
The results presented in section 4.1 show that an ELU-shaped activation function which is shifted around the origin seems favorable to improve learning. Hence, we introduce the ShELU activation

4

Under review as a conference paper at ICLR 2018

Activation ShELU SvELU
PShELU

Table 2: Shifted activation functions.

Value Region 1

Value

x +  x +  > 0 (exp(x + ) - 1)

x+

x > 0 (exp(x) - 1) + 

 

(x

+

)

x+ > 0

(exp(

x+ 

)

-

1)

Region 2 x+ 0
x0
x+ 0

function with horizontal shift and the SvELU activation function with vertical shift and define them as in Table 2. The hyperparameter  is considered to be a pre-set constant and it is not tuned during training. In our experiments, we set  = 1. We also define PShELU, a variant of PELU with horizontal shift. The parameters  and  for PShELU are learned during training. In the experiments, they were initialized as  =  = 1.0, i.e. as an original ELU activation.
Note that the introduced shifts  in Table 2 are individual for all neurons. As an example, consider the first layers in the Lenet network (LeCun et al., 1998) shown in Figure 3. The input is an image 32 × 32 × 3. In the first convolutional layer, there are 192 filters with size 5 × 5 × 3. The output from the convolutional layer consists of 192 feature maps with size 32 × 32. The output includes a bias level for each feature map (each large square in the convolutional output), i.e. a total of 192 bias levels. The activation function is applied to the individual neurons resulting in an output with size 32 × 32 × 192. When we say that we introduce individual shifts for all neurons, it means that there is one tuneable shift for each of the 32 × 32 × 192 neurons (all small squares in the activation output) where the activation function is applied.
In Goodfellow et al. (2016), chapter 9.5, it is stated that for CNNs it is natural to have shared biases with the same tiling pattern as the convolutional kernels, but that individual biases for each neuron "would allow the model to correct for differences in the image statistics at different locations". By introducing the activation function ShELU, with individual shifts for each neuron, we have indirectly created individual biases for the convolutional layer feature map output. Note that a convolutional layer with a shared bias level for each feature map output followed by a ShELU activation is equivalent with a convolutional layer with individual bias levels for each feature map output followed by an ELU activation. This equivalence was verified with experiments presented in section 4.2.1. However, frameworks as Caffe (Jia et al., 2014) and MatConvNet do not allow for individual biases in a convolutional layer but is restricted to shared biases.
4 EXPERIMENTS
4.1 EXPERIMENTS WITH PIECEWISE LINEAR ACTIVATION FUNCTIONS
To investigate the behavior of the piecewise linear activation function we made some experiments with the Lenet network and the Cifar-100 dataset (Krizhevsky & Hinton, 2009). We used the implementation of Lenet as provided when downloading the MatConvNet framework. We ran the Lenet network with the ReLU activation function, and also replaced all activation layers with LReLU and

Figure 3: Bias levels for convolutional layers and shifts for activation function. Biases/shifts can either be shared (large squares) or individual for each neuron (small squares).
5

Under review as a conference paper at ICLR 2018

Activation ReLU
Tuned ReLU

Table 3: Top1error on Cifar-100 with Lenet network.

Top1error (%) Activation Top1error (%) Activation

46.58

LReLU

45.41

ELU

45.92

Tuned LReLU

45.18

Tuned ELU

Top1error (%) 44.96 44.51

ELU. We then exchanged the activation layers with the piecewise linear activation layer. We initialized the layers as a linear version of ReLU, LReLU and ELU respectively. We consistently noticed a slight improvement (a few tenths of a percent) in classification performance when using the tuneable piecewise linear activation function compared to its corresponding fixed activation function, see Table 3. Besides the slight classification improvement, it is also interesting to analyse the shape of the activation functions after tuning, see Figure 4. All three activation functions remain linear and with almost unity slope on the positive side. All three tuned activation functions exhibit a smooth exponential decay for small negative inputs and then remain fairly constant for larger negative inputs. The resulting shape after tuning for all three initializations is close to the ELU shape. However, notice that all tuned activation functions tend to return a variable but negative output for zero input and that they do not pass through the origin. These results suggest that we introduce the Shifted Exponential Linear Unit (ShELU) as an activation function. From the results it is not obvious whether the introduced shift around the origin should be vertical or horizontal. For a horizontal shift, the saturation level remains constant for large negative inputs which may seem more intuitive. For a vertical shift, the saturation level will vary depending on the shift which better matches the achieved results on the Lenet network.
4.2 EXPERIMENTS WITH SHIFTED ACTIVATION FUNCTIONS
4.2.1 EXPERIMENTS ON CIFAR-100 WITH LENET NETWORK
We now want to evaluate if the classification performance improves with the new activation functions ShELU and SvELU compared to ELU. We start with the Lenet network and replace all ELU activations with either the ShELU or the SvELU activation. The learning rate was set to 0.005 for the first 40 epochs, then lowered by a factor of 10 every 20 epochs, running a total of 80 epochs. The learning rate momentum was set to 0.9 and the weight decay to 0.0005. Image data was preprocessed with global contrast normalization and whitening (Coates et al., 2011). Note that the complete dataset was divided by a factor of 10 (compared to the preprocessing provided with the MatConvNet download) to better match the variance with Xavier initialization. During training the dataset was augmented with random horizontal flipping and by randomly cropping images from the original images zero padded with a frame of width four.
The classification errors for the training and test sets shown in Figure 5 are the average over 8 runs for each activation function. The top1errors in Table 4 are the average over the last 15 epochs for the lowest learning rate. The learning rate for the ShELU and SvELU activation layer weights was set to 2% of the base learning rate. The results show that there is a small improvement on the top1error using the ShELU and SvELU activation functions compared with the original ELU. Futhermore, the shifted activation function PShELU achieves a slightly better test results than both ELU and PELU.

Output Output Output

Cifar100 Lenet Pchannel Relu 8bins 8
80% 7 50%
20% 6 init 5
4
3
2
1
0
-1
-2 -8 -6 -4 -2 0 2 4 6 Input

8

Cifar100 Lenet Pchannel Lrelu 8bins 8
80% 7 50%
20% 6 init 5
4
3
2
1
0
-1
-2 -8 -6 -4 -2 0 2 4 6 Input

8

Cifar100 Lenet Pchannel Elu 8bins 8
80% 7 50%
20% 6 init 5
4
3
2
1
0
-1
-2 -8 -6 -4 -2 0 2 4 6 8 Input

Figure 4: Initialization (red) and 20, 50 and 80 percentiles for tuned activation functions in last layer; ReLU (left), LReLU (middle) and ELU (right).

6

Under review as a conference paper at ICLR 2018

0.6 0.55
0.5 0.45
0.4 0.35
0

Top1Error Cifar100 Lenet

ELU SvELU ShELU PELU PShELU

20 40 60 Epochs

80

0.46 0.458 0.456 0.454 0.452
0.45 0.448 0.446 0.444 0.442
0.44 40

Top1 Test Error Cifar100 Lenet

ELU SvELU ShELU

50 60 70 Epochs

80

0.46 0.458 0.456 0.454 0.452
0.45 0.448 0.446 0.444 0.442
0.44 40

Top1 Test Error Cifar100 Lenet

ELU PELU PShELU

50 60 70 Epochs

80

Figure 5: Training (dashed) and test (solid) errors on Cifar-100 with network Lenet (left). Test errors (final part) for ELU, SvELU and ShELU (middle), and ELU, PELU and PShELU (right).

Table 4: Top1 test errors on Cifar-100 with Lenet and Clevert-11 networks.

Activation

Lenet network Clevert-11 network

ELU

44.96

28.76

SvELU

44.70

28.85

ShELU

44.77

28.57

PELU

45.03

28.78

PShELU

44.76

28.74

ConvIndBias + ELU

44.78

-

The training behavior is very similar for all activation functions but the shifted activation functions exhibit a slightly better generalization behavior. However, the significance of these results is limited as Lenet is a rather shallow network.
We also created a network layer named "ConvIndBias", which is an identity mapping but it also adds an individually learned bias shift for each neuron. The results in Table 4 confirm that a ShELU activation is equivalent to the combination of a ConvIndBias layer and an ELU activation as was stated in section 3.
4.2.2 EXPERIMENTS ON CIFAR-100 WITH CLEVERT-11
To further evaluate the shifted activation functions in comparison with ELU, we built the 11-layer network used by Clevert et al. (2015) to replicate the experiments when ELU was introduced. We denote the network Clevert-11. Parameter settings and weight initializations were as in Clevert et al. (2015). Our results are the average over 9 runs for each activation function. Our classification results with the network Clevert-11 on the Cifar-100 dataset for the activation functions ELU, SvELU, ShELU, PELU and PShELU are presented in Figure 6 and summarized in Table 4. The results in the table are the average top1error over the last 20 epochs for each activation function. The results show that the test error for the ShELU activation function is significantly better than for ELU, whereas the error for SvELU is slightly inferior. The results suggest that a horizontal shift for the activation function is preferable to a vertical shift. The training behavior is almost identical for ELU and

0.6 0.5 0.4 0.3 0.2 0.1
0 0

Top1Error Cifar100 Clevert-11

ELU SvELU ShELU PELU PShELU

50 100 150 200 250 300 Epochs

0.3 0.298 0.296 0.294 0.292
0.29 0.288 0.286 0.284 0.282
0.28 200

Top1 Test Error Cifar100 Clevert-11

ELU SvELU ShELU

220 240 260 280 300 320 Epochs

0.3 0.298 0.296 0.294 0.292
0.29 0.288 0.286 0.284 0.282
0.28 200

Top1 Test Error Cifar100 Clevert-11

ELU PELU PShELU

220 240 260 280 300 320 Epochs

Figure 6: Training (dashed) and test (solid) errors on Cifar-100 with network Clevert-11 (left). Test errors (final part) for ELU, SvELU and ShELU (middle), and ELU, PELU and PShELU (right).

7

Under review as a conference paper at ICLR 2018

Layer 1 0.1

Layer 2 0.1

Layer 3 0.1

Layer 4 0.1

Layer 5 0.1

0.05

0.05

0.05

0.05

0.05

00000 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4

10 -3

10 -3

10 -3

10 -3

10 -3

Layer 6 0.1

Layer 7 0.1

Layer 8 0.1

Layer 9 0.1

Layer 10 0.1

0.05

0.05

0.05

0.05

0.05

00000 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -4 -2 0 2 4 -0.2 0 0.2

10 -3

10 -3

10 -3

10 -3

Kurtosis ShELU shift

3.3 3.25
3.2 3.15
3.1 3.05
3 2.95
0

Cifar100 Clevert11 ShELU
2468 Layer

10

Spatial shift ShELU layer1

Figure 7: Learned shifts for ShELU activation function, relative frequency (left), kurtosis (middle) and spatial variation (right).

ShELU. We believe that the improved test result can be attributed to that ShELU adaptively learns where to set the reference level between the linear and exponential parts of the activation function.
ELU, PELU and PShELU all show very similar test errors. Note, however, that the training error is by far lower for PELU indicating pronounced overfitting compared to ELU. The training error is lower for PShELU than for ShELU but the test error is inferior. This suggests that PShELU suffers from overfitting when allowed to tune the hyperparameters  and . Note that we were able to almost exactly reproduce the results for ELU achieved in Clevert et al. (2015) who report a top1error of 28.75%.
4.3 LEARNED SHIFTS FOR SHELU
In all experments, we initialized the individual shifts for the ShELU activation from a Gaussian distribution with standard deviation 0.001. The learned shifts after training in the 10 activation layers of the Clevert-11 network are shown as normalized frequency histograms in Figure 7, together with the kurtosis and the spatial variation for the shifts.
The shape of the learned shifts is almost a perfect Gaussian distribution for all layers. This is supported by the computed kurtosis which is close to 3.0. The kurtosis increases slightly for the last three layers where the distribution tends to be somewhat skewed towards the negative side. The standard deviation for the shift is relatively constant for the first nine layers but grows considerably for the last layer.
Figure 7 shows the learned shifts for the first ShELU activation layer where the shifts for the 192 feature maps have been placed as 12 × 16 tiles side by side. Interestingly, the spatial variation for the learned shift seems to be completely random. Any statistical difference spatially over the image cannot be perceived.
5 CONCLUSIONS
We use a new formulation to tune a continuous piecewise linear activation function during training and learn the shape of the locally optimal activation function. With this tuned activation function, the classification performance for convolutional neural networks is improved and the resulting, learned activation function shows to be ELU-shaped irrespective whether it is initialized as a RELU, LReLU or ELU activation function. The learned activation function exhibits a variable shift around the origin for each neuron, indicating that a shifted ELU-shaped activation function is preferable. This observation leads us to introduce the Shifted Exponential Linear Unit (ShELU) as a new activation function.
Experiments on Cifar-100 show that the classification performance is further improved when using the ShELU activation function in comparison with ELU. Normally in a convolutional network layer, one shared bias shift is learned for each feature map output. The improvement for the ShELU activation is achieved when learning an individual bias shift for each neuron. The equivalent to the ShELU activation function would be to learn an individual bias shift for each neuron in the convolutional layer output and then apply an ELU activation, which however is not supported by commonly used deep learning frameworks. The implementation of individual biases in the activation function is therefore preferable and leads to the ShELU activation function.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Forest Agostinelli, Matthew Hoffman, Peter Sadowski, and Pierre Baldi. Learning activation functions to improve deep neural networks. arXiv preprint arXiv:1412.6830, 2014.
Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 215­223, 2011.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.
Michael Felsberg and Go¨sta Granlund. P-channels: Robust multivariate m-estimation of large datasets. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, volume 3, pp. 262­267. IEEE, 2006.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249­256, 2010.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 315­323, 2011.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 448­456, Lille, France, 07­09 Jul 2015. PMLR. URL http://proceedings. mlr.press/v37/ioffe15.html.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.
Gu¨nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. arXiv preprint arXiv:1706.02515, 2017.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices: Application to neural-network learning. Physical Review Letters, 66(18):2396, 1991.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In Proc. ICML, volume 30, 2013.
MatConvNet. http://www.vlfeat.org/matconvnet/,v.beta-20.
Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of machine learning research, 15(1):1929­1958, 2014.
9

Under review as a conference paper at ICLR 2018

Ludovic Trottier, Philippe Gigue`re, and Brahim Chaib-draa. Parametric exponential linear unit for deep convolutional neural networks. arXiv preprint arXiv:1605.09332, 2016.

APPENDIX

DERIVATIVES OF SHELU AND SVELU

For backpropagation, the derivates of ShELU and SvELU with respect to the input x and the shift  are computed as

dShELU =
dx

1, if x +  > 0 (exp(x + )), if x +   0

dShELU =
d

1, if x +  > 0 (exp(x + )), if x +   0

dSvELU =
dx

1, if x > 0 (exp(x)), if x  0

dSvELU =1
d

(8a) (8b) (8c) (8d)

DERIVATIVES OF PSHELU

For backpropagation, the derivates of PShELU with respect to the input x, the hyperparameters  and , and the shift  are computed as

dPShELU =
dx dPShELU
= d dPShELU
= d dPShELU
= d

 

,

if

x+ > 0

 

(exp(

x+ 

)),

if

x+  0

x+ 

,

if

x+ > 0

exp(

x+ 

)

-

1.0,

if

x+  0

-

 2

(x

+

),

if

x+ > 0

-

 2

(exp(

x+ 

)),

if

x+  0

 

,

if

x+ > 0

 

(exp(

x+ 

)),

if

x+  0

(9a) (9b) (9c) (9d)

10

