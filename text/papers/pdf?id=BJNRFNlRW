Under review as a conference paper at ICLR 2018

TRAINING GENERATIVE ADVERSARIAL NETWORKS VIA PRIMAL-DUAL SUBGRADIENT METHODS
Anonymous authors Paper under double-blind review

ABSTRACT
We relate the minimax game of generative adversarial networks (GANs) to finding the saddle points of the Lagrangian function for a convex optimization problem, where the discriminator outputs and the distribution of generator outputs play the roles of primal variables and dual variables, respectively. This formulation shows the connection between the standard GAN training process and the primal-dual subgradient methods for convex optimization. The inherent connection does not only provide a theoretical convergence proof for training GANs in the function space, but also inspires a novel objective function for training. The modified objective function forces the distribution of generator outputs to be updated along the direction according to the primal-dual subgradient methods. A toy example shows that the proposed method is able to resolve mode collapse, which in this case cannot be avoided by the standard GAN or Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and real-world image datasets demonstrate the performance of the proposed method on generating diverse samples.

1 INTRODUCTION

Generative adversarial networks (GANs) are a class of game theoretical methods for learning data distributions. It trains the generative model by maintaining two deep neural networks, namely the discriminator network D and the generator network G. The generator aims to produce samples resembling real data samples, while the discriminator aims to distinguish the generated samples and real data samples.

The standard GAN training procedure is formulated as the following minimax game:

min
G

max
D

Expd

(x)

{log

D(x)}

+

Ezpz

(z)

{log(1

-

D(G(z

)))},

(1)

where pd(x) is the data distribution and pz(z) is the noise distribution. The generated samples G(z) induces a generated distribution pg(x). Theoretically, the optimal solution to (1) is pg = pd and D(x) = 1/2 for all x in the support of data.

In practice, the discriminator network and the generator network are parameterized by d and g, respectively. The neural network parameters are updated iteratively for d and g according to gradient descent. In particular, the discriminator is first updated either with multiple gradient descent steps until convergence or with a single gradient descent step, then the generator is updated with a single descent step. However, the GAN training process does not have theoretical convergence guarantee, as noted by Ian Goodfellow in (Goodfellow, 2016), "For GANs, there is no theoretical prediction as to whether simultaneous gradient descent should converge or not. Settling this theoretical question, and developing algorithms guaranteed to converge, remain important open research problems.".

There have been some recent studies on the convergence behaviours of GAN training(Heusel et al., 2017; Li et al., 2017; Mescheder et al., 2017). One notable inconvergence issue is referred to as mode collapse, where the generator characterizes only a few modes of the true data distribution (Goodfellow et al., 2014; Li et al., 2017). Various methods have been proposed to alleviate the mode collapse problem. Feature matching for intermediate layers of the discriminator has been proposed in (Salimans et al., 2016). In (Metz et al., 2016), the generator is updated based on a sequence of previous unrolled discriminators. A mixture of neural networks are used to generate diverse samples (Tolstikhin et al., 2017; Hoang et al., 2017; Arora et al., 2017). In (Arjovsky & Bottou, 2017), it was proposed that

1

Under review as a conference paper at ICLR 2018

adding noise perturbation on the inputs to the discriminator can alleviate the mode collapse problem. The Wasserstein divergence is proposed to resolve the problem of incontinuous divergence when the generated distribution and the data distribution have disjoint supports (Arjovsky et al., 2017; Gulrajani et al., 2017). Mode regularization is used in the loss function to penalize the missing modes (Che et al., 2016; Srivastava et al., 2017). The regularization is usually based on heuristics, which tries to minimize the distance between the data samples and the generated samples, but lacks theoretical convergence guarantee.
In this paper, we formulate the minimax optimization for GAN training (1) as finding the saddle points of the Lagrangian function for a convex optimization problem. In the convex optimization problem, the discriminator function D(·) and the probabilities of generator outputs pg(·) play the roles of the primal variables and dual variables, respectively. This connection not only provides important insights in understanding the convergence of GAN training, but also enables us to leverage the primal-dual subgradient methods to design a novel objective function that helps to alleviate mode collapse. A toy example reveals that for some cases when standard GAN or WGAN inevitably leads to mode collapse, our proposed method can effectively avoid mode collapse and converge to the optimal point.
In this paper, we do not aim at achieving superior performance over other GANs, but rather provide a new perspective of understanding GANs, and propose an improved training technique that can be applied on top of existing GANs. The contributions of the paper are as follows:
· The standard training of GANs in the function space is formulated as primal-dual subgradient methods for solving convex optimizations.
· This formulation enables us to show that with a proper gradient descent step size, updating the discriminator and generator probabilities according to the primal-dual algorithms will provably converge to the optimal point.
· This formulation results in a novel training objective for the generator. With the proposed objective function, the generator is updated such that the probabilities of generator outputs are pushed to the optimal update direction derived by the primal-dual algorithms. Experiments have shown that this simple objective function can effectively alleviate mode collapse in GAN training.
· The convex optimization framework incorporates different variants of GANs including the family of f -GAN (Nowozin et al., 2016) and an approximate variant of WGAN. For all these varaints, the training objective can be improved by including the optimal update direction of the generated probabilities.

2 PRIMAL-DUAL SUBGRADIENT METHODS FOR CONVEX OPTIMIZATION

In this section, we first describe the primal-dual subgradient methods for convex optimization. Later, we explicitly construct a convex optimization and relate the subgradient methods to standard GAN training. Consider the following convex optimization problem:

maximize f0(x) subject to fi(x)  0, i = 1, · · · ,
x  X,

(2a) (2b) (2c)

where x  Rk is a length-k vector, X is a convex set, and fi(x), i = 0 · · · , , are concave functions mapping from Rk to R. The Lagrangian function is calculated as

L(x, ) = f0(x) + ifi(x).
i=1

(3)

In the optimization and dual variables,

problem, the respectively.

Tvhareiapbrilmesaxl-duaRl kpaainrd(x, R)+isaraesraedfdelrere-pdotiontasofptrhime aLlavgarrainagbilaens

fuction, if it satisfies:

L(x, ) = min max L(x, ).
0 xX

(4)

2

Under review as a conference paper at ICLR 2018

Primal-dual subgradient methods have been widely used to solve the convex optimization problems, where the primal and dual variables are updated iteratively, and converge to a saddle point (Nedic´ & Ozdaglar, 2009; Komodakis & Pesquet, 2015).
There are two forms of algorithms, namely dual-driven algorithm and primal-dual-driven algorithm. For both approaches, the dual variables are updated according to the subgradient of L(x(t), (t)) with respect to (t) at each iteration t. For the dual-driven algorithm, the primal variables x(t) are updated to achieve maximum of L(x(t), (t)). For the primal-dual-driven algorithm, the primal variables are updated according to the subgradient of L(x(t), (t)) with respect to x(t). The iterative update process is summarized as follows:

x(t) =

arg maxxX L (x, (t))

(dual-driven algorithm)

PX [x(t) + (t)xL (x(t), (t))] (primal-dual-driven algorithm)

(5)

(t + 1) = [(t) - (t)L (x(t), (t))]+ ,

(6)

where PX (·) denotes the projection on set X and (x)+ = max(x, 0).

The following theorem proves that the primal-dual subgradient methods will make the primal and dual variables converge to the optimal solution of the convex optimization problem.

Theorem 1 Consider the convex optimization (2). Assume the set of saddle points is compact. Suppose f0(x) is a strictly concave function over x  X and the subgradient at each step is bounded. There exists some step size (t) such that both the dual-driven algorithm and the primal-dual-driven algorithm yield x(t)  x and (t)  , where x is the solution to (2), and  satisfies

L(x, ) = max L (x, ) .
xX

(7)

Proof: See Appendix 7.1.

3 TRAINING GAN VIA PRIMAL-DUAL SUBGRADIENT METHODS

3.1 GAN AS A CONVEX OPTIMIZATION

We explicitly construct a convex optimization problem and relate it to the minimax game of GANs. We assume that the source data and generated samples belong to a finite set {x1, · · · , xn} of arbitrary size n. The extension to uncountable sets can be derived in a similar manner (Luenberger, 1997).
The finite case is of particular interest, because any real-world data has a finite size, albeit the size
could be arbitrarily large.

We construct the following convex optimization problem:
n
maximize pd(xi) log(Di)
i=1
subject to log(1 - Di)  log(1/2), i = 1, · · · , n D  D,

(8a)
(8b) (8c)

where D is some convex set. The primal variables are D = (D1, · · · , Dn). Let pg = (pg(x1), · · · , pg(xn)), where pg(xi) is the Lagrangian dual associated with the i-th constraint. The Lagrangian function is thus

nn
L(D, pg) = pd(xi) log(Di) + pg(xi) log(2(1 - Di)), D  D.
i=1 i=1

(9)

When D = {D : 0  Di  1, i}, finding the saddle points for the Lagrangian function is exactly equivalent to solving the GAN minimax problem(1). This inherent connection enables us to utilize the
primal-dual subgradient methods to design update rules for D(x) and pg(x) such that they converge to the saddle points. The following theorem provides a theoretical guideline for the training of GANs.

3

Under review as a conference paper at ICLR 2018

Theorem 2 Consider the Lagrangian function given by (9) with D = {D :  Di  1 - , i}, where 0 < < 1/2. If the discriminator and generator have enough capacity, and the discriminator
output and the generated distribution are updated according to the primal-dual update rules (5) and (6) with (x, ) = (D, pg), then pg(·) converges to pd(·).

Proof: The

n i=1

pd(xi)

optimization log(Di), fi(·)

problem = log(1

(8) is a - Di) and

particularized form X = [ , 1 - ]n. The

of (2), objective

where function

f0(·) = is strictly

concave. Moreover, since D is projected onto the compact set [ , 1 - ] at each iteration t, the

subgradients fi(D(t)) is bounded. The assumptions of Theorem 1 are satisfied.

Since the constraint (8b) gives an upper bound of Di  1/2, the solution to the above convex

optimization is obviously Di = 1/2, for all i = 1, · · · , n. Since the problem is convex, the optimal

primal solution is the primal saddle point of the Lagrangian function (Bertsekas, 1999, Chapter 5).

Moreover, any primal-dual saddle point (D, pg) satisfies L(D, pg) = maxDD L D, pg . Since

D is strictly we have pg =

inside D, we have pd, and the saddle

DL(D, pg) = 0. Since Di L(D, pg) = 2pd(xi) - 2pg(xi), point is unique. By Theorem 1, the primal-dual update rules will

guarantee convergence of D(t), pg(t) to the primal-dual saddle point (D, pg).

It can be seen that the standard training of GAN corresponds to either dual-driven algorithm (Nowozin et al., 2016) or primal-dual-driven algorithm (Arjovsky et al., 2017; Goodfellow et al., 2014). A natural question arises: Why does the standard training fail to converge and lead to mode collapse? As will be shown later, the underlying reason is that standard training of GANs in some cases do not update the generated distribution according to (6). Theorem 2 inspires us to propose a training algorithm to tackle this issue.

3.2 ALGORITHM DESCRIPTION

Algorithm 1 Training GAN via Primal-Dual Subgradient Methods

Initialization: Choose the objective function f0(·) and constraint function f1(·) according to the
GAN realization. For the original GAN based on Jensen-Shannon divergence, f0(D) = log (D) and f1(D) = log(2(1 - D)).
while the stopping criterion is not met do
Sample minibatch m1 data samples x1, · · · , xm1 . Sample minibatch m2 noise samples z1, · · · , zm2 . for k = 1, · · · , k0 do
Update the discriminator parameters with gradient ascent:

1 m1

1 m2

d

m1

i=1

f0(D(xi)) +

m2

f1 (D (G (zj)))
j=1

.

(10)

end for Update the generated distribution as:

pg(xi) = pg(xi) - f1(D(xi)), i = 1, · · · , m1, where  is some step size and

(11)

pg (xi )

=

1 m2

m2
k(G(zj) - xi).
j=1

(12)

With pg(xi) fixed, update the generator parameters with gradient descent:

g

1 m2

1 m1

m2

f1 (D (G (zj))) +
j=1

m1

i=1

pg (xi )

-

1 m2

m2
k(G(zj )
j=1

-

xi)

2

.

end while

(13)

4

Under review as a conference paper at ICLR 2018

First, we present our training algorithm. Later, we will use a toy example to give intuitions of why our algorithm is effective to avoid mode collapse.

The algorithm is described in Algorithm 1. The maximum step of discriminator update is k0. In the context of primal-dual-driven algorithms, k0 = 1. In the context of dual-driven algorithms, k0 is some large constant, such that the discriminator is updated till convergence at each training epoch. The update of the discriminator is the same as standard GAN training. The main difference is the modified loss function for the generator update (13). The intuition is that when the generated samples have disjoint support from the data, the generated distribution at the data support may not be updated. This is exactly the source of mode collapse. Ideally, the modified loss function will always update the generated probabilities at the data support along the optimal direction.

The

generated

probability

mass

at

x

is

pg (x)

=

1 m

m i=1

1{G(zi)

=

x},

where

1{·}

is

the

indicator

function. The indicator function is not differentiable, so we use the Gaussian kernel to approximate it.

Define

k (x)

=

e-

||x||2 2

,

(14)

where  is some positive constant. The constant  is also called bandwidth for kernel density estimation. The empirical generated distribution is thus approximately calculated as (12). There are different bandwidth selection methods (Botev et al., 2010; Hall et al., 1991). It can be seen that as   0, k(x - y) tends to the indicator function, but it will not give gradients to far areas that experience mode collapse. A larger  implies a coarser quantization of the space in approximating the distribution. In practical training, the kernel bandwidth can be set larger at first and gradually decreases as the iteration continues.

By the dual update rule (6) , the generated distribution should be updated as

pg (x)

=

pg (x)

-



L(D, pg)  pg (x)

.

(15)

This motivates us to add the second term of (13) in the loss function, such that the generated distribution is pushed towards the desirable updated distribution (15).

3.3 INTUITION OF AVOIDING MODE COLLAPSE

Mode collapse occurs when the generated samples have a very small probability to overlap with some families of the data samples, and the discriminator D(·) is locally constant around the region of the generated samples. We use a toy example to show that the standard training of GAN and Wasserstein may fail to avoid mode collapse, while our proposed method can succeed.

Claim 1 Suppose the data distribution is pd(x) = 1{x = 1}, and the initial generated distribution is pg(x) = 1{x = 0}. The discriminator output D(x) is some function that is equal to zero for |x - 0|   and is equal to one for |x - 1|  , where 0 <  < 1/2. Standard training of GAN and
WGAN leads to mode collapse.

Proof: We first show that the discriminator is not updated, and then show that the generator is not updated during the standard training process.

In standard training of GAN and WGAN, the discriminator is updated according to the gradient of (10). For GAN, since 0  D(x)  1,

Epd log (D (x)) + Epg log (1 - D (x)) = log(D(1)) + log(1 - D(0))  0, which is achieved by the current D(x) by assumption.

(16)

For WGAN, the optimal discrminator output D(x) is some 1-Lipschitz function such that Epd {D(x)} - Epg {D(x)} is maximized. Since

Epd {D(x)} - Epg {D(x)} = D(1) - D(0)  1,

(17)

where (17) is due to the Lipschitz condition |D(1) - D(0)|  1. The current D(x) is obviously optimal. Thus, for both GAN and WGAN, the gradient of the loss function with respect to d is zero and the discriminator parameters are not updated.

5

Under review as a conference paper at ICLR 2018

Table 1: Variants of GANs under the convex optimization framework.

Divergence metric Kullback-Leibler
Reverse KL Pearson 2 Squared Hellinger 2 Jensen-Shannon Approximate WGAN Other metric

f0(Di) log(Di)
-Di

Di

1 - Di

log(Di)

Di - Di2

-

1 2

Di2

+

Di

f1(Di) 1 - Di

log Di

-

1 4

Di2

-

Di

1 - 1/Di

log(1 - Di) - log(1/2)

-Di Di - 2

Di
pd (xi ) pg (xi) pg (xi)
pd (xi ) 2(pd(xi)-pg (xi))
pg (xi)
pg (xi)
pd (xi ) pd (xi ) pd(xi)+pg (xi) pd(xi)-pg (xi)
2 pd(xi)+pg (xi)
pd (xi )

On the other hand, in standard training, the generator parameters g are updated with only the first term of (13). By the chain rule,

1

 g

log

(1

-

D

(G

(zi)))

=

- 1

-

D

(G

(zi)) xD(x)

x=G(zi)g G(zi)

= 0,

(18) (19)

where (19) is due to the assumption that D(x) is locally constant for x = 0. Therefore, the generator and the discriminator reach a local optimum point. The generated samples are all zeros.

In our proposed training method, when x = 1, the optimal update direction is given by (11), where
pg is a large value because D(1) = 1. Therefore, by (13), the second term in the loss function is very large, which forces the generator to generate samples at G(z) = 1. As the iteration continues, the
generated distribution gradually converges to data distribution, and D(x) gradually converges to 1/2, which makes pg(x)L(D(x), pg(x)) = log(2(1 - D(x))) become zero. The experiment in Section 5 demonstrates this training dynamic.

4 VARIANTS OF GANS

Consider the following optimization problem:

n

maximize pd(xi)f0(Di)

(20a)

i=1

subject to f1(Di)  0, i = 1, · · · , n,

(20b)

where f0(·) and f1(·) are concave functions. Compared with the generic convex optimization problem (2), the number of constraint functions is set to be the variable alphabet size, and the constraint functions are fi(D) = f1(Di), i = 1, · · · , n.

The objective and constraint functions in (20) can be tailored to produce different GAN variants. For example, Table 1 shows the large family of f -GAN (Nowozin et al., 2016). The last row of Table 1 gives a new realization of GAN with a unique saddle point of D(x) = 2 and pg(x) = pd(x).

We also derive a GAN variant similar to WGAN, which is named "Approximate WGAN". As shown

in Table 1, the objective and constraint functions yield the following minimax problem:

min
G

max
D

Expd

(x)

D(x) - D2(x)

- Expg(x) {D(x)} ,

(21)

where is an arbitrary positive constant. The augmented term D2(x) is to make the objective

function strictly concave, without changing the original solution. It can be seen that this problem has a unique saddle point pg(x) = pd(x). As tends to 0, the training objective function becomes
identical to WGAN. The optimal D(x) for WGAN is some Lipschitz function that maximizes Expd(x) {D(x)} - Expg(x) {D(x)}, while for our problem is D(x) = 0. Weight clipping can still be applied, but serves as a regularizer to make the training more robust (Merolla et al., 2016).

The training algorithms for these variants of GANs follow by simply changing the objective function f0(·) and constraint function f1(·) accordingly in Algorithm 1.

6

Under review as a conference paper at ICLR 2018

Generated samples

5 GAN - proposed method
4 GAN WGAN
3

2

1

0

-1

-2

-3

-4

-5

0

1000

2000

3000

4000

5000

6000

7000

8000

Iteration number

(a)
1 Iteration 0
0.9 Iteration 4000 Iteration 8000
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 -5 -4 -3 -2 -1 0 1 2 3 4 5
x
(c)

D(x) D(x)

1 Iteration 0
0.9 Iteration 4000 Iteration 8000
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 -5 -4 -3 -2 -1 0 1 2 3 4 5
x
(b)
200 Iteration 0
180 Iteration 4000 Iteration 8000
160
140
120
100
80
60
40
20
0 -5 -4 -3 -2 -1 0 1 2 3 4 5
x
(d)

D(x)

Figure 1: Performance of a toy example. Figure (a) shows the generated samples for different GANs. Figure (b) shows the discriminator output D(x) for GANs trained using the proposed method. Figure (c) and (d) show the discriminator output D(x) for standard GANs and WGANs.

Iteration 0

Iteration 10k

Iteration 20k

Iteration 30k

Iteration 40k

Figure 2: Performance of 2D mixture of Gaussian data. The data samples are marked in blue and the generated samples are marked in orange.

5 EXPERIMENTS

5.1 SYNTHETIC DATA

Fig. 1 shows the training performance for a toy example. The data distribution is pg(x) = 1{x = 0}. The inital generated samples are concentrated around x = -3.0. The details of the neural network parameters can be seen in Appendix 7.2. Fig. 1a shows the generated samples in the 90 quantile as the training iterates. After 8000 iterations, the generated samples from standard training of GAN and WGAN are still concentrated around x = -3.0. As shown in Fig. 1c and 1d, the discrminators hardly have any updates throughout the training process. Using the proposed training approach, the generated samples gradually converge to the data distribution and the discriminator output converges to the optimal solution with D(1) = 1/2.

Fig. 2 shows the performance of the proposed method for a mixture of 8 Gaussain data on a circle.

While the original GANs experience mode collapse (Nguyen et al., 2017; Metz et al., 2016), our

proposed method is able to generate samples over all 8 modes. In the training process, the bandwidth

of

the

Gaussian

kernel

(14)

is

inialized

to

be

2

=

0.1

and

decreases

at

a

rate

of

0.8

t 2000

,

where

t

is

the iteration number. The generated samples are dispersed initially, and then gradually converge to

the Gaussian data samples. Note that our proposed method involves a low complexity with a simple

regularization term added in the loss function for the generator update.

5.2 REAL-WORLD DATASETS
We also evaluate the performance of the proposed method on two real-world datasets: MNIST and CIFAR-10. Please refer to the appendix for detailed architectures. Inception score (Salimans et al.,

7

Under review as a conference paper at ICLR 2018

Method
Real data WGAN (Arjovsky et al., 2017) MIX + WGAN (Arora et al., 2017) Improved-GAN (Salimans et al., 2016)
ALI (Dumoulin et al., 2016) DCGAN (Radford et al., 2015)
Proposed method

Score
11.24 ± 0.16 3.82 ± 0.06 4.04 ± 0.07 4.36 ± 0.04 5.34 ± 0.05 6.40 ± 0.05
4.53 ± 0.04

Table 2: Inception scores on CIFAR-10 dataset.

2016) is employed to evaluate the proposed method. It applies a pretrained inception model to every generated image to get the conditional label distribution p(y|x). The Inception score is calculated as exp (Ex {KL(p(y|x) p(y)}). It measures the quality and diversity of the generated images.
5.2.1 MNIST
The MNIST dataset contains 60000 labeled images of 28 × 28 grayscale digits. We train a simple LeNet-5 convolutional neural network classifier on MNIST dataset that achieves 98.9% test accuracy, and use it to compute the inception score. The proposed method achieves an inception score of 9.8, while the baseline method achieves an inception score of 8.8. The examples of generated images are shown in Fig. 3. The generated images are almost indistinguishable from real images.

MNIST

CIFAR

Figure 3: Examples of generated images using MNIST and CIFAR dataset.

5.2.2 CIFAR-10
CIFAR is a natural scene dataset of 32 × 32. We use this dataset to evaluate the visual quality of the generated samples. Table 2 shows the inception scores of different GAN models on CIFAR-10 dataset. The inception score of the proposed model is much better than the baseline method WGAN that uses similar network architecture and training method. Note that although DCGGAN achieves a better score, it uses a more complex network architecture. Examples of the generated images are shown in Fig. 3.
6 CONCLUSION
In this paper, we propose a primal-dual formulation for generative adversarial learning. This formulation interprets GANs from the perspective of convex optimization, and gives the optimal update of the discriminator and the generated distribution with convergence guarantee. By framing different variants of GANs under the convex optimization framework, the corresponding training algorithms can all be improved by pushing the generated distribution along the optimal direction.

8

Under review as a conference paper at ICLR 2018
Experiments on two synthetic datasets demonstrate that the proposed formulation can effectively avoid mode collapse. It also achieves competitive quantitative evaluation scores on two benchmark real-world image datasets.
REFERENCES
Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial networks. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214­223, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (GANs). arXiv preprint arXiv:1703.00573, 2017.
Dimitri P Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
Dimitri P Bertsekas and John N Tsitsiklis. Parallel and distributed computation: numerical methods. Prentice-Hall, Inc., 1989.
Zdravko I Botev, Joseph F Grotowski, Dirk P Kroese, et al. Kernel density estimation via diffusion. The Annals of Statistics, 38(5):2916­2957, 2010.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Diego Feijer and Fernando Paganini. Stability of primal­dual gradient dynamics and applications to network optimization. Automatica, 46(12):1974­1981, 2010.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249­256, 2010.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of Wasserstein GANs. arXiv preprint arXiv:1704.00028, 2017.
Peter Hall, Simon J Sheather, MC Jones, and James Stephen Marron. On optimal data-based bandwidth selection in kernel density estimation. Biometrika, 78(2):263­269, 1991.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Günter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. Multi-generator gernerative adversarial nets. arXiv preprint arXiv:1708.02556, 2017.
Nikos Komodakis and Jean-Christophe Pesquet. Playing with duality: An overview of recent primal dual approaches for solving large-scale optimization problems. IEEE Signal Processing Magazine, 32(6):31­54, 2015.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. Towards understanding the dynamics of generative adversarial networks. arXiv preprint arXiv:1706.09884, 2017.
David G Luenberger. Optimization by vector space methods. John Wiley & Sons, 1997.
9

Under review as a conference paper at ICLR 2018

Paul Merolla, Rathinakumar Appuswamy, John Arthur, Steve K Esser, and Dharmendra Modha. Deep neural networks are robust to weight binarization and other non-linear distortions. arXiv preprint arXiv:1606.01981, 2016.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. arXiv preprint arXiv:1705.10461, 2017.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016.
Angelia Nedic´ and Asuman Ozdaglar. Subgradient methods for saddle-point problems. Journal of optimization theory and applications, 142(1):205­228, 2009.
Tu Dinh Nguyen, Trung Le, Hung Vu, and Dinh Phung. Dual discriminator generative adversarial nets. arXiv preprint arXiv:1709.03831, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
Akash Srivastava, Lazar Valkov, Chris Russell, Michael Gutmann, and Charles Sutton. VEEGAN: Reducing mode collapse in GANs using implicit variational learning. arXiv preprint arXiv:1705.07761, 2017.
Ilya Tolstikhin, Sylvain Gelly, Olivier Bousquet, Carl-Johann Simon-Gabriel, and Bernhard Schölkopf. Adagan: Boosting generative models. arXiv preprint arXiv:1701.02386, 2017.

7 APPENDIX

7.1 PROOF OF THEOREM 1

The proof of convergence for dual-driven algorithms can be found in (Bertsekas & Tsitsiklis, 1989, Chapter 3).

The primal-dual-driven algorithm for continuous time update has been studied in (Feijer & Paganini, 2010). Here, we show the convergence for the discrete-time case.

We choose a step size (t) that satisfies

(t) > 0, (t) = , 2(t) < .
t=1 t=1

(22)

Let z(t) = [x(t), (t)]T be a vector consisting of the primal and dual variables at the t-th iteration. The primal-dual-driven update can be expressed as:

z(t + 1) = z(t) + (t)T (t),

(23)

where

T (t) =

xL(x(t), (t)) -L(x(t), (t))

=

f0(x(t)) + i=1 ifi (x(t)) -F (x(t))

,

(24)

and

 f1(x) 

F (x) =  

...

. 

(25)

f (x)

10

Under review as a conference paper at ICLR 2018

Since the subgradient is bounded by assumption, there exists M > 0 such that ||T (·)||22 < M , where ||.||2 stands for the L2 norm. Let x be the unique saddle point and  be the set of saddle points of . For any   , it satisfies

L(x, )  L(x, )  L(x, ).

(26)

for all x and . For any saddle point (x, ), we have

||x(t + 1) - x||22 = ||PX [x(t) + (t)xL(x(t), (t))] - x||22  ||x(t) + (t)xL(x(t), (t)) - x||22 = ||x(t) - x||22 + 2(t)xL(x(t), (t))(x(t) - x)

(27) (28)

+ 2(t)||xL(x(t), (t))||22  ||x(t) - x||22 + 2(t)xL(x(t), (t))(x(t) - x) + 2(t)M

(29) (30)

where (28) is due to the nonexpansive projection lemma for any X that contains x (Bertsekas &

Tsitsiklis, 1989, Chapter 3), and (30) is due to the assumption that the subgradients are upper bounded

by M .

Similarly, we have

||(t + 1) - ||22  ||(t) - ||22 - 2(t)F (x)T ((t) - ) + 2(t)M.

(31)

Let (t) = any   ,

arg min we have

||(t)

-

||2.

Define

z(t)

=

[x, (t)].

Since

(30)

and

(31)

hold

for

||z(t + 1) - z(t + 1)||22 = ||x(t + 1) - x||22 + ||(t + 1) - (t + 1)||22  ||x(t + 1) - x||22 + ||x(t + 1) - (t)||22
 ||z(t) - z(t)||22 + 2(t)T T (t)(z(t) - z(t)) + 22(t)M.

(32) (33) (34)

Next we will show that (x(t), (t)) converges to a saddle point. The intuition is that for large t, the second term (34) is less than zero and dominates over the third term, thus z(t) will be driven to the set of saddle points.

Since L(x, ) is concave in x and convex in , we have
xL(x(t), (t))(x(t) - x)  L(x(t), (t)) - L(x, (t)) L(x(t), (t))((t) - )  L(x(t), (t)) - L(x(t), ).

(35) (36)

Therefore,
T T (t)(z(t) - z(t))  L(x(t), (t)) - L(x, (t)) + L(x(t), (t)) - L(x(t), (t)) = L(x, (t)) - L(x, (t)) + L(x(t), (t)) - L(x, (t))  0,

(37) (38) (39)

where the last step is due the definition of saddle point (26). By (34), we have ||z(t + 1) - z(t + 1)||22  ||z(t) - z(t)||22 + 22(t)M.
Summing (40) over 1  t  n - 1, we have

(40)

n-1
||z(n) - z(n)||22  ||z(1) - z(1)||22 + 22(t)M.
t=1

(41)

Since the saddle points are bounded by assumption,

 t=1

2(t)

is

bounded,

||z(n)||2

must

be

bounded.

the

initial

point

||z(1)||2

is

bounded

and

Give any > 0, define a neighbor of the saddle points as

A = {z : ||z - (x, )||22 < ,   }.

(42)

11

Under review as a conference paper at ICLR 2018

We first show that there must be infinitely many points of z(t) that are in A . Suppose this does
not hold, then there exists some N0 > 0, such that for every t  N0, zt / A . Therefore, there exists some  > 0 such that for every t  N0, T (x(t), (t))(z(t) - z(t)) < -. In this case, by summing (34) over N0  t  n - 1, we have

n-1

n-1

||z(n) - z(n)||22  ||z(N0) - z(N0)||22 - 2

(t) +

22(t)M.

t=N0

t=N0

(43)

Note that ||z(N0) - z(N0)||22 is bounded. By the choice of the step size, we have ||z(n) - z(n)||22 tends to -, which is contradicted with the fact that ||z(n) - z(n)||22  0. Therefore, there are infinitely many z(t) that are in A .

Consequently, we can find a large enough N1 such that 2M z(N1)||  . Summing (40) over N1  t  n - 1 we have

 t=N1

2(t)



and ||z(N1) -

n-1

||z(n) - z(n)||22  ||z(N1) - z(N1)||22 + 2M

2(t)

t=N1

2 .

(44) (45)

In other words, for any > 0, there exists N1 > 0 such that for all t  N1, ||z(t) - z(t)||2  2 . Since it holds for all , it implies that there are infinitely many z(t) that belong to the saddle points.

The set of saddle points is compact by assumption. By the Bolzano­Weierstrass theorem, there must

exist a subsequence of {z(tn)} that converges to a saddle point z0. For such subsequence, there

exists some large (40) holds for any

esnadodulgehpno0instuzch(tth),awt tenr0eplaNce1

and ||z(tn) z(t) by z0

-z and

0||22 sum

, (40)

for every n over tn0 

 n0. tn

Since - 1 to

obtain

n-1

||z(n) - z0||22  ||z(tn0 ) - z0||22 + 2M

2(t)

t=tn0

n-1

 ||z(tn0 ) - z0||22 + 2M

2(t)

t=N1

2 .

(46)
(47) (48)

This means for any , there exists N2 = tn0 such that for every t  N2, ||z(t) - z0||22  . That concludes the proof that z(t) converges to a saddle point.

7.2 TOY EXAMPLE TRAINING DETAILS
In the toy example, both the generator and the discriminator has only one ReLU hidden layer with 64 neurons. The output activation is sigmoid function for GAN and ReLU for WGAN. For WGAN, the parameters are clipped in between [-1, 1], and the networks are trained with Root Mean Square Propagation (RMSProp) with a learning rate of 1e-4. For GAN, the networks are trained with Adam with a learning rate of 1e-4. The minibatch size is 32. The bandwidth parameter for the Gaussian kernel is initialized to be  = 0.5 and then is changed to 0.1 after 2000 iterations.

7.3 2D MIXTURE GAUSSIAN DATA TRAINING DETAILS
We use the network structure in (Metz et al., 2016) to evaluate the performance of our proposed method. The data is sampled from a mixture of 8 Gaussians of standard deviation of 0.02 uniformly located on a circle of radius 2. The noise samples are a vector of 256 independent and identically distributed (i.i.d.) Gaussian variables with mean zero and standard deviation of 1.
The generator has two hidden layers of size 128 with ReLU activation. The last layer is a linear projection to two dimensions. The discriminator has one hidden layer of size 128 with ReLU activation followed by a fully connected network to a sigmoid activation. All the biases are initialized to be zeros and the weights are initalilzed via the "Xavier" initialization (Glorot & Bengio, 2010).

12

Under review as a conference paper at ICLR 2018
The training follows the primal-dual-driven algorithm, where both the generator and the discriminator are updated once at each iteration. The Adam optimizer is used to train the discriminator with 8e-4 learning rate and the generator with 4e-4 learning rate. The minibatch sample number is 64. 7.4 MNIST TRAINING DETAILS For MNIST dataset, the generator network is a deconvolutional neural network. It has two fully connected layer with hidden size 1024 and 7 × ×7 × 128, two deconvolutional layers with number of units 64, 32, stride 2 and deconvolutional kernel size 4 × 4 for each layer, respectively, and a final convolutional layer with number of hidden unit 1 and convolutional kernel 4 × 4.. The discriminator network is a two layer convolutional neural network with number of units 64, 32 followed by two fully connected layer of hidden size 1024 and 1. The input noise dimension is 64. We employ ADAM optimization algorithm with initial learning rate 0.01 and  = 0.5. 7.5 CIFAR TRAING DETAILS For CIFAR dataset, the generator is a 4 layer deconvolutional neural network, and the discriminator is a 4 layer convolutional neural network. The number of units for discriminator is [64, 128, 256, 1024], and the number of units for generator is [1024, 256, 128, 64]. The stride for each deconvolutional and convolutional layer is two. We employ RMSProp optimization algorithm with initial learning rate of 0.0001, decay rate 0.95, and momentum 0.1.
13

