Under review as a conference paper at ICLR 2018
UNDERSTANDING GANS: THE LQG SETTING
Anonymous authors Paper under double-blind review
ABSTRACT
Generative Adversarial Networks (GANs) have become a popular method to learn a probability model from data. Many GAN architectures with different optimization metrics have been introduced recently. Instead of proposing yet another architecture, this paper aims to provide an understanding of some of the basic issues surrounding GANs. First, we propose a natural way of specifying the loss function for GANs by drawing a connection with supervised learning. Second, we shed light on the statistical performance of GANs through the analysis of a simple LQG setting: the generator is linear, the loss function is quadratic and the data is drawn from a Gaussian distribution. We show that in this setting: 1) the optimal GAN solution converges to population Principal Component Analysis (PCA) as the number of training samples increases; 2) the number of samples required scales exponentially with the dimension of the data; 3) the number of samples scales almost linearly if the discriminator is constrained to be quadratic. Moreover, under this quadratic constraint on the discriminator, the optimal finitesample GAN performs simply empirical PCA.
1 INTRODUCTION
Learning a probability model from data is a fundamental problem in statistics and machine learning. Building off the success of deep learning methods, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have given this age-old problem a face-lift. In contrast to traditional methods of parameter fitting like maximum likelihood estimation, the GAN approach views the problem as a game between a generator whose goal is to generate fake samples that are close to the real data traaining samples and a discriminator whose goal is to distinguish between the real and fake samples. The generator and the discriminators are typically implemented by deep neural networks. GANs have achieved impressive performance in several domains (e.g., (Ledig et al., 2016; Reed et al., 2016)). Since (Goodfellow et al., 2014), many variations of GANs have been developed, including f -GAN (Nowozin et al., 2016), MMD-GAN (Dziugaite et al., 2015; Li et al., 2015), WGAN (Arjovsky et al., 2017), improved WGAN (Gulrajani et al., 2017), relaxed WGAN (Guo et al., 2017), Least-Squares GAN (Mao et al., 2016), Boundary equilibrium GAN (Berthelot et al., 2017), etc. These GANs use different metrics in the optimization for training the generator and discriminator networks (Liu et al., 2017).
The game theoretic formulation in GANs can be viewed as the dual of an optimization that minimizes a distance measure between the empirical distributions of the fake and real samples. In the population limit where there are infinite number of samples, this optimization minimizes the distance between the generated distribution and the true distribution from which the data is drawn. In the original GAN framework, this distance measure is the Jenson Shannon-divergence. However, Arjovsky et al (Arjovsky et al., 2017) noted that this distance does not depend on the generated distribution whenever its dimension is smaller than that of the true distribution. In this typical case, the Jenson Shannon divergence does not serve as a useful criterion in choosing the appropriate generated distribution. To resolve this issue, (Arjovsky et al., 2017) proposed the Wasserstein GAN (WGAN) which uses the first-order Wasserstein distance instead of Jensen-Shannon divergence. This distance is meaningful even when the dimension of the generated distribution is less than the true distribution. Nevertheless there are many other distance measures that satisfy this criterion and it is not clear how to choose among them. This is responsible in part for the fact that there are so many different GAN architectures. In fact, there is currently some confusion in the literature even on the basic issue of how to specify the loss function for GANs. For example, while the "Wasserstein" in Wasserstein
1

Under review as a conference paper at ICLR 2018

empirical

Unconstrained Discriminator Solution
slow (Result 2)
Unconstrained Discriminator Solution
(= PCA, Result 1)

Quadratic Discriminator Solution
(empirial PCA)
fast (Result 3)
Quadratic Discriminator Solution
(PCA)

population

Figure 1: Summary of main results. In the LQG setting, the population optimal GAN solution is PCA when the discriminator is unconstrained and when the discriminator is constrained to be quadratic. But the convergence to the population optimal is exponentially faster under the quadratic constraint on the discriminator.

GAN refers to the use of Wasserstein distance in the distance measure between the generated and true distributions. the "Least Squares" in Least-Squares GAN (Mao et al., 2016) refers to the use of squared error in the discriminator optimization objective. These are two totally different types of objects. This is in contrast to the situation in supervised learning, where how the loss function is specified in the formulation is clear.
A central issue in any learning problem is generalization: how close a model learnt from a finite amount of data is to the true distribution. Or, in statistical terms, how fast is the rate of convergence of the learnt model to the true model as a function of number of samples? Arora et al (Arora et al., 2017) have recently studied this problem for GANs. They observed that for Wasserstein GAN, if there are no constraints on the generator or the discriminator, the number of samples required to converge scales exponentially in the data dimension. They then showed that if the discriminator is constrained to be in a parametric family, then one can achieve convergence almost linearly in the number of parameters in that family (Theorem 3.1 in (Arora et al., 2017)). However, the convergence is no longer measured in the Wasserstein distance but in a new distance measure they defined (the neural network distance). The result is interesting as it highlights the role of the discriminator in generalization, but it is somewhat unsatisfactory in that the distance measure needs to be modified to tailor to the specific constraints on the discriminator. Moreover, the result requires the invention of (yet) another family of distance measures for GANs.
In this paper, we first argue that there is a natural way to specify the loss function for GANs, in an analogous way as in the supervised learning setup. The resulting optimal GAN minimizes a generalized loss-function dependent Wasserstein distance between the generated distribution and the true distribution, and the dual formulation of this Wasserstein distance leads to a loss-function dependent discriminator architecture. To study the impact of the generator and the discriminator on the generalization performance in this distance measure, we focus on a basic Linear-QuadraticGaussian setting: the generator is constrained to be linear of a given rank k, the loss function is quadratic, and the true distribution is Gaussian. In this setting, the optimal GAN minimizes the second-order Wasserstein distance between the generated distribution and the empirical data distribution among all linear generators of a given rank. We show the following results:
1 in the population limit as the number of data samples grow, the optimal generated distribution is the rank k Gaussian distribution retaining exactly the top k principal components of the true distribution, i.e. GAN performs PCA in the population limit;
2 The number of samples required for convergence in (second-order) Wasserstein distance however scales exponentially with the dimension of the data distribution.
3 Under a further constraint that the discriminator is quadratic, GAN converges to the same population-optimal PCA limit, but with the number of samples scaling almost linearly with the dimension. The constrained GAN simply performs empirical PCA, and in the case
2

Under review as a conference paper at ICLR 2018

when the rank k of the generator is the same as the dimension of the data distribution, GAN is equivalent to maximum likelihood estimation of the underlying Gaussian model.
These results are summarized in Figure 1.
(Arora et al., 2017) observed that the number of samples required to generalize for GAN is exponential in the number of samples when there are no constraints on either the generator or the discriminator. (They proved the result for first-order Wasserstein distance but a similar result holds for second-order Wasserstein distance as well, see Lemma 2 in Section 3.) Result 2 above says that even constraining the generator drastically to be linear cannot escape this exponential scaling. Result 3 says that this exponential scaling is not due to statistical limitation, but much better inference can be obtained by constraining the discriminator appropriately. Similar to Theorem 3.1 in (Arora et al., 2017), Result 3 points to the importance of constraining the discriminator. But there are two key differences. First, the convergence in Result 3 is with respect to the original (second-order) Wasserstein distance, not another distance measure tailored to the constraint on the discriminator as in Theorem 3.1 in (Arora et al., 2017). Thus, the original loss function is respected. Second, the population limit is the same as the PCA limit achieved without constraints on the discriminator. Thus, by imposing a discriminator constraint, the rate of convergence is drastically improved without sacrificing the limiting performance. There is no such guarantee in (Arora et al., 2017). On the other hand, our results are obtained in a specific setting.
The LQG setting, dating back to at least Gauss, has been widely used across many fields, including statistics, machine learning, control, signal processing and communication. It has resulted in celebrated successes such as linear regression, the Wiener filter, the Kalman filter, PCA, etc., and is often used to establish a baseline to understand more complex models. We believe it serves a similar role here for GANs. Indeed it allows us to make a clear connection between GAN and PCA, perhaps the most basic of unsupervised learning methods.
When the discriminator is not constrained, the number of samples scale exponentially with the dimension. It turns out that a simple characterization of the performance of GAN in that regime can be obtained using rate-distortion theory (Cover & Thomas, 2012). In the population limit, quadratic GAN minimizes the Wasserstein distance between the generated distribution and the true data distribution. This distance is the solution of an unconstrained optimal transport problem. It turns out that when there are a finite number of samples, the performance is again characterized by an optimal transport between the generated and true distributions, but with a constraint on the mutual information of the transport to be less than the logarithm of the number of samples. This constraint reflects the fact that only empirical samples from the true distribution are seen and therefore the generated distribution cannot be too strongly coupled to the true distribution. We discuss this story in the appendix.
The rest of the paper is organized as follows. In Section 2, we discuss a formulation of the GAN problem for general loss functions. In Section 3, we specialize to the LQG setting and analyze the generalization performance of GAN. In Section 4, we analyze the performance of GAN under a quadratic constraint on the discriminator. In Section 5, we present some experimental results.

2 A GENERAL FORMULATION FOR GANS

Let {yi}ni=1 be n observed data points in Rd drawn i.i.d. from the distribution PY . Let QYn be

the empirical distribution of these observed samples. Moreover, let PX be a normal distribution

N (0, Ik). GANs can be viewed as an optimization that minimizes a distance between the observed

erempplaicriecsalQdYnistwriibthutPioYn.QTYnheanqduetshteiognewneeraatsekd

distribution Pg(X). The population GAN in this section is: what is a natural way of

optimization specifying a

loss function for GANs and how it determines the distance?

2.1 WGAN REVISITED Let us start with the WGAN optimization (Arjovsky et al., 2017):
inf W1(PY , Pg(X)),
g(.)G

(1)

3

Under review as a conference paper at ICLR 2018

where G is the set of generator functions, and the p-th order Wasserstein distance between distributions PZ1 and PZ2 is defined as (Villani, 2008)

Wpp(PZ1 , PZ2 ) := inf E [ Z1 - Z2 p] ,
PZ1 ,Z2

(2)

where the minimization is over all joint distributions with marginals fixed. Replacing (2) in (1), the WGAN optimization can be re-written as

inf inf E [ Y - g(X) ] .
g(.)G Pg(X),Y

(3)

or equivalently:

inf inf E [ Y - g(X) ] ,
g(.)G PX,Y

(4)

where the second minimization is over all joint distributions PX,Y with fixed marginals PX and PY .
We now connect (4) to the supervised learning setup. In supervised learning, the joint distribution PX,Y is fixed and the goal is to learn a relationship between the feature variable represented by X  Rk, and the target variable represented by Y  Rd, according to the following optimization:

inf E [ (Y, g(X))] ,
g(.)G

(5)

where is the loss function. Assuming the marginal distribution of X is the same in both optimizations (4) and (5), we can connect the two optimization problems by setting (y, y ) = y - y in optimization (5). Note that for every fixed PX,Y , the solution of the supervised learning problem (5) yields a predictor g which is a feasible solution to the WGAN optimization problem 4. Therefore, the WGAN optimization (3) can be re-interpreted as solving the easiest such supervised learning, over all possible joint distributions PX,Y with fixed PX and PY .

2.2 FROM SUPERVISED TO UNSUPERVISED LEARNING

GAN is a solution to an unsupervised learning problem. What we are establishing above is a general connection between supervised and unsupervised learning problems: a good predictor g learnt in a supervised learning problem can be used to generate samples of the target variable Y. Hence, to solve an unsupervised learning problem for Y with distribution PY , one should solve the easiest supervised learning problem PX,Y with given marginal PY (and PX , the randomness generating distribution). This is in contrast to the traditional view of the unsupervised learning problem as observing the feature variable X without the label Y . (Thus in this paper we break with tradition and use Y to denote the unknown distribution and X as randomness for the generator in stating the GAN problem.)
This connection between supervised and unsupervised learning leads to a natural way of specifying the loss function in GANs: we simply replace the l2 Euclidean norm in (3) with a general loss function :

inf inf E [l (Y, g(X))] .
g(.)G Pg(X),Y

(6)

The inner optimization is the optimal transport problem between distributions of g(X) and Y (Villani, 2008) with general cost . This is a linear programming problem for general cost, so there is always a dual formulation (the Kantorovich dual). The dual formulation can be interpreted as a generalized discriminator optimization problem for the cost . (For example, in the case of being the Euclidean norm, we get the WGAN architecture; see Figure 2(a).) Hence, we propose (6) as a formulation of GANs for general loss functions.

2.3 QUADRATIC LOSS
The most widely used loss function in supervised learning is the quadratic loss: l(y, y ) = y - y 2 (squared Euclidean norm). Across many fields its use had led to many important discoveries. With the connection between supervised and unsupervised learning in mind, this loss function should be

4

Under review as a conference paper at ICLR 2018

(a) Y Rd
X Rk

WGAN

(Y)

:1-Lip

g(.)

Y^  Rk

-(Y^ )

(b)
Y Rd
+
X Rk

Quadratic GAN

-(Y)

:convex

g(.)

Y^  Rk

-*(Y^ )

+

Figure 2: Dual (min-max) formulations of (a) WGAN, and (b) Quadratic GAN.

a prime choice to consider in GANs as well. This choice of the loss function leads to the quadratic GAN optimization:

inf
g(.)G

W22(PY , Pg(X)).

(7)

Since Wasserstein distances are weakly continuous measures in the probability space (Villani, 2008), similar to WGAN, the optimization of the quadratic GAN is well-defined even if k < d. The dual formulation (discriminator) for W2 is shown in Figure 2. Note that in this dual, the discriminator applies  to the real data and the convex conjugate  to the generated (fake) data.

The empirical quadratic GAN optimization can be formulated by replacing PY with the empirical distribution QnY of the data as follows:

inf
g(.)G

W22(QnY , Pg(X)).

(8)

Note that while in practice one generates fake samples from X, we will keep the notations simpler in this paper by assuming we can generate the exact distribution g(X), i.e. we can generate as many fake samples as we wish. Almost all our results can be extended to the case when we have finite number of samples from X.

For the rest of the paper, we will focus on the problem (8) for the particular case of Y Gaussian of dimension d, and g linear of rank k  d. This is the LQG setting for GANs.

3 GANS UNDER THE LQG SETUP

3.1 THE POPULATION GAN OPTIMIZATION

First, we analyze the population GAN optimization under the LQG setup. We have the following lemma:

Lemma 1 Let S be a k dimensional subspace in Rd. Let Y^ be a random variable whose support lies in S. Then, Y^ , the optimal solution of the optimization

inf W22(PY , PY^ ),
PY^

(9)

is the projection of Y to S.

Proof 1 See Section C.2.

This Lemma holds even if PY is a non-Gaussian distribution. However, the optimal solution of GAN may not be generated as g(X) when PX  N (0, Ik) and g(.) is restricted to be linear.
Using Lemma 1 and under the LQG setup, we show that the optimal solution for the population GAN optimization is the same as the PCA solution. PCA is the most standard unsupervised dimensionality reduction approach (Jolliffe, 2002). PCA computes an optimal linear mapping from Y to Y^ under the rank constraint on the covariance matrix of Y^ (KY^ ). We say Y^ is the k-PCA solution of Y if KY^ is a rank k matrix whose top k eigenvalues and eigenvectors are the same as top k eigenvalues and eigenvectors of the covariance matrix of Y (KY ).

5

Under review as a conference paper at ICLR 2018

Theorem 1 Let Y  N (0, KY ) where KY is full-rank. Let X  N (0, Ik) where k  d. The optimal population GAN solution of optimization (7) under linear G is the k-PCA solution of Y .
Proof 2 See Section C.3.
Lemma 1 holds if we replace W2 with W1. However, the conclusion of Theorem 1 is tied to the W2 distance because the PCA optimization also considers the quadratic projection loss.

3.2 THE EMPIRICAL GAN OPTIMIZATION

In practice, one solves the GAN optimization over the empirical distribution of the data QnY , not the population distribution PY . Thus, it is important to analyze how close optimal empirical and population GAN solutions are in a given sample size n. This notion is captured in the generalization
error of the GAN optimization, defined as follows:

Definition 1 (Generalization of GANs) Let n be the number of observed samples from Y . Let g^(.) and g(.) be optimal solutions for empirical and population GAN optimizations. Then,

dG (PY , QnY ) := W22(PY , Pg^(X)) - W22(PY , Pg(X)),

(10)

is a random variable representing the excess error of the empirical solution.

dG(PY , QYn ) can be viewed as a distance between PY and QnY which depends on G. To have a proper generalization property, one needs to have dG(PY , QY )  0 quickly as n  . Before analyzing the convergence rate of dG(PY , QnY ) for linear G, we characterize this rate for an unconstrained G. For an unconstrained G, the second term of (10) is zero (this can be seen using a space filling
generator function (Cannon & Thurston, 1987)). Moreover, Pg^(X) can be arbitrarily close to QYn . Thus, we have

Lemma 2 If G is unconstrained, we have dG (PY , QYn ) = W22(PY , QnY ),
which goes to zero with the rate of O(n-2/d).

(11)

The approach described for the unconstrained G corresponds to the memorization of the empirical distribution QnY using the trained model. Note that one can write

n = 2 .-

2 d

-

2

log(n) d

Thus, to have a small W22(PY , QnY ), the number of samples n should be exponentially large in d (Canas & Rosasco, 2012). It is possible that for some distributions PY , the convergence rate of
W22(PY , QnY ) is much faster than O(n-2/d). For example, reference (Weed & Bach, 2017) shows that if PY is clusterable (i.e., Y lies in a fixed number of separate balls with fixed radii), then the convergence of W22(PY , QYn ) is fast. However, even in this case, one optimal strategy would be to memorize observed samples which is not the mechanism used in GANs.

A natural question is whether constraining the family of generator functions G can improve the
generalization of GANs. Below we study this question in the LQG setup as follows. To simplify calculations, we assume that PY  N (0, Id) and d = k. Under these assumptions, the GAN optimization (8) can be re-written as

min
µ,K

W22(N (µ, ) , QYn ),

(12)

where K is the covariance matrix with the eigen decomposition K = UUt. The optimal population solution of this optimization is µpop = 0 and Kpop = I, which provides a zero Wasserstein loss.

Theorem 2 Let µn and Kn be optimal solutions for optimization (12). Then, µn  0 with the rate of O(d/n) and T r(n )  d with the rate of O(n-2/d).

Proof 3 See Section C.4.

6

Under review as a conference paper at ICLR 2018

Theorem 2 indicates that there is a bias in GAN's estimation of the true distribution which translates to the slow convergence of the generalization error. Note that in the Wasserstein space, the empirical distribution QnY and the population distribution PY are far from each other (the distance between them concentrates around n-2/d (Canas & Rosasco, 2012)). Thus, if there exists another Gaussian distribution within the sphere around QnY with the radius of n-2/d, the Wasserstein-based learning method will converge to the wrong Gaussian distribution. This phenomenon causes a bias term in estimating the true distribution. In Section A, we characterize this bias term explicitly in an asymptotic regime by drawing a connection between GANs and the rate-distortion optimization in information theory.
Theorem 2 considers the regime where k = d. In practice, the dimension of the generated distribution is often much smaller than that of the true one (i.e., k d). In this case, GAN's convergence rate can be increased from O(n-2/d) to O(n-2/k). However, this faster convergence comes at the expense of the increased bias term of the excess error (the second term of (10)). The trade off is favorable if Y is near low rank.
Theorem 2 suggests that GAN's convergence rate is slow. In practice, however, GANs have demonstrated impressive performances. In the next section, we show that by suitably constraining the GAN optimization, the convergence rate can be enhanced significantly.

4 GANS WITH CONSTRAINED DISCRIMINATORS

In this section, first we review the min-max (dual) formulation of WGAN (Arjovsky et al., 2017). Then, we characterize the min-max formulation of the quadratic GAN. Finally, we show that a properly constrained quadratic GAN achieves the empirical PCA solution, which converges to the population optimal with an exponentially faster rate of convergence compared to the case when the discriminator is unconstrained.
Using the Kantorovich duality (Villani, 2008), the first-order Wasserstein distance W1(PY , Pg(X)) can be written as the following optimization (Arjovsky et al., 2017):

W1(PY , Pg(X)) = sup E (Y ) - (Y^ ) ,
(.):1-Lip

(13)

where the function (.) is restricted to be 1-Lipschitz. This dual formulation of W1 is then used in optimization (1) to implement WGAN in a min-max architecture similar to the one of the original GAN (Figure 2). In this architecture, (.) is implemented by deep neural networks.
In a similar way, one can write the second-order Wasserstein distance W22(PY , Pg(X)) as the following optimization (Villani, 2008):
W22(PY , Pg(X)) = E[ Y 2] + E[ g(X) 2] + 2 sup - E [(Y )] - E [(g(X))] , (14)
(.):convex
where (y^) := supv (vty^ - (v)) is the convex-conjugate of the function (.). Similarly, this dual formulation of W22 can be used to implement the quadratic GAN optimization (7) in a min-max architecture which can be interpreted as a game between optimizing two functions g(.) and (.) (Figure 2).
The following lemma characterizes the optimal discriminator of optimization (14) (Chernozhukov et al., 2017):

Lemma 3 let PY be absolutely continuous whose support contained in a convex set in Rd. For a fixed g(.)  G, let opt be the optimal discriminator function of optimization (14). This solution is unique. Moreover, we have

opt(Y ) d=ist g(X),

(15)

where d=ist means matching distributions.

In the LQG setup, since g(X) is Gaussian, opt is a linear function. Thus, without loss of generality, (.) in the discriminator optimization can be constrained to (y) = ytAy where A is positive

7

Under review as a conference paper at ICLR 2018

semidefinite. Therefore, we have

W22 PY , Pg(X) = E[ Y 2] + E[ g(X) 2] + 2

sup

- E [(Y )] - E [(g(X))]

(y)=ytAy,A 0

(16)

= T r(KY ) + T r(Kg(X)) + 2 sup - T r(AKY ) - T r(AKg(X)),
A0

where A is the pseudo inverse of the matrix A.

If we compute the right W22(QnY , Pg(X)) QYn is

hand side not linear

of (16) on the and therefore

empirical distribution QnY the optimal discriminator

, it will not be equal to is not quadratic. Nev-

ertheless, by computing the quadratically constrained discriminator optimization on the empirical

distribution, we obtain

1n n
i=1

yi 2 + E[ g(X) 2] + 2

sup

(y)=ytAy,A 0

-1 n

n

(yi) - E [(g(X))]

i=1

= T r(K^ Y ) + T r(Kg(X)) + 2 sup - T r(AK^ Y ) - T r(AKg(X))
A0

= W22(PZ , Pg(X)),

(17)

where K^ Y is the sample covariance matrix and Z  N (0, K^ Y ) 1. Therefore, the empirical constrained quadratic GAN solves the following optimization:

inf
g(.)G

W22(PZ , Pg(X)).

(18)

Using Theorem 1, the optimal g(X) to this problem is the empirical PCA solution, i.e. keeping the top k principal components of the empirical covarance matrix.

Theorem 3 Under the LQG setup, the empirical constrained quadratic GAN optimization is equivalent to the empirical PCA.

Consider the case where d = k (the case k < d is similar). The second term in the generalization distance dG(QYn , PY ) (10) is zero. Therefore, we have

dG(PY , QnY ) = W22(PZ , PY ) = W22(N (0, K^ Y ), N (0, KY ))

(19)

The W22 distance between two Gaussians depends only on the covariance matrices. More specifically:

W22(N (0, K^ Y ), N (0, KY )) = Tr(KY ) + Tr(K^ Y ) - 2Tr KY1/2K^ Y KY1/2 1/2 .

(20)

Hence, the convergence of this quantity only depends on the convergence of the sample covariance
to the population covariance, together with smoothness property of this function of the covariance matrices. The convergence been established to be at a quick rate of O~( d/n) (Rippl et al., 2016).

5 EXPERIMENTAL RESULTS
For experiments, we generate n i.i.d. samples from PY  N (0, Id), represented as QYn . We then fit a d dimensional Gaussian distribution N (µ, K) to QYn using two methods: Maximum Likelihood (ML) estimation, which computes the sample mean and the sample covariance; and WGAN (Arjovsky et al., 2017) with an affine generator function. Note that according to Theorem 3, ML is equivalent to the constrained quadratic GAN (18). Moreover, note that the WGAN implementation uses W1 and not W2 in its optimization. Although analyzing GANs with W2 is more tractable than
1Note that by considering (y) = ytAy + aty where A 0, we would have obtained Z  N µ^Y , K^ Y where µ^Y is the sample mean. For simplicity, we ignore the affine terms.

8

Under review as a conference paper at ICLR 2018
(a) (b)
Constrained Quadratic GAN (ML) WGAN

Constrained Quadratic GAN (ML) WGAN

Figure 3: Generalization errors of constrained quadratic GAN (ML) and WGAN under the LQG setup.

that of W1, in practice we do not expect a significant difference between their performance. Considering this and owing to the lack of an implementation of GANs with W2, we perform numerical experiments using the WGAN implementation. Details of the experiments can be found in Section B.
Let µ^ and K^ be the estimated mean and the covariance. For evaluation, we compute

µ^ 2 +

I - K^ 1/2

2 F

,

(21)

which is the W22 distance between N (0, Id) and N µ^, K^ (see Lemma 4 in Section C).

Figure 3 demonstrates the estimation error of the ML (constrained quadratic GAN)and WGAN methods for d = 5 and d = 10 and in different sample sizes. These figures are consistent with Theorem 2 and results of Section A which suggest that GAN's convergence can be slow owing to a bias in its optimal empirical solution with respect to the population one. Moreover, this figure shows that the convergence of the constrained quadratic GAN (ML) is fast. Finally, in our experimental results of Figure 3, one should take into the consideration practical issues of the WGAN implementation such as the use of the stochastic gradient descent, convergence to bad locals, etc.

REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573, 2017.
David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717, 2017.
Guillermo Canas and Lorenzo Rosasco. Learning probability measures with respect to optimal transport metrics. In Advances in Neural Information Processing Systems, pp. 2492­2500, 2012.
James W Cannon and William P Thurston. Group invariant peano curves. 1987.
Victor Chernozhukov, Alfred Galichon, Marc Hallin, Marc Henry, et al. Monge­kantorovich depth, quantiles, ranks and signs. The Annals of Statistics, 45(1):223­256, 2017.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Gintare Karolina Dziugaite, Daniel M Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906, 2015.
Clark R Givens, Rae Michael Shortt, et al. A class of wasserstein metrics for probability distributions. The Michigan Mathematical Journal, 31(2):231­240, 1984.

9

Under review as a conference paper at ICLR 2018
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Xin Guo, Johnny Hong, Tianyi Lin, and Nan Yang. Relaxed wasserstein with applications to gans. arXiv preprint arXiv:1705.07164, 2017.
Ian Jolliffe. Principal Component Analysis. Wiley Online Library, 2002. Christian Ledig, Lucas Theis, Ferenc Husza´r, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint arXiv:1609.04802, 2016. Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1718­1727, 2015. Shuang Liu, Olivier Bousquet, and Kamalika Chaudhuri. Approximation and convergence properties of generative adversarial learning. arXiv preprint arXiv:1705.08991, 2017. Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, and Zhen Wang. Multi-class generative adversarial networks with the l2 loss function. arXiv preprint arXiv:1611.04076, 2016. Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271­279, 2016. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016. Thomas Rippl, Axel Munk, and Anja Sturm. Limit laws of the empirical wasserstein distance: Gaussian distributions. Journal of Multivariate Analysis, 151:90­109, 2016. Ce´dric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of empirical measures in wasserstein distance. arXiv preprint arXiv:1707.00087, 2017.
10

Under review as a conference paper at ICLR 2018

Appendix

A A RATE-DISTORTION VIEW OF GANS

As (Arjovsky et al., 2017) suggested, there is a strong connection between GANs and the optimal transport problem (e.g., this connection is reflected in the use of the Wasserstein distance in GAN's optimization). Moreover, the optimal transport theory and the optimal quantization theory are strongly tied to one other (e.g., (Villani, 2008; Canas & Rosasco, 2012)). On the other hand, in the regime where the sample size n grows exponentially in d, Shannon has noted that the optimal quantization is strongly related to the rate-distortion optimization (Cover & Thomas, 2012). In this section, we exploit these connections to better understand GANs through the rate-distortion analysis. Note that in the light of Theorem 2 and without properly constraining the discriminator function, the regime where n is exponential in d provides an appropriate resolution to study GAN's problem.

Consider the following optimization:

min E Y - Z 2 ,
PY,Z
PY : fixed, I(Y ; Z)  Rd,

(22)

where I(Y ; Z) is the mutual information between Y and Z. Let PZ be an optimal solution of this optimization. Shannon has shown that in the regime where n = 2Rd, generating n i.i.d. samples from PZ is an optimal quantization strategy of PY (Cover & Thomas, 2012). We refer to this optimization as the rate-distortion optimization.

Using the rate-distortion theory, our goal is to understand W2 and therefore GANs when n grows exponentially in d. Consider the following optimization:

min E Y - Z 2 ,
PY,Z
PY : fixed, PZ = PY , I(Y ; Z)  Rd.

(23)

Note that unlike optimization (22) where PZ can be an arbitrary distribution, in optimization (23) PZ is constrained to be equal to PY . This is to have n i.i.d. samples from PZ = PY (represented as QnY ) be an optimal quantizer of PY .

Theorem 4 Let D(R) be the optimal value of optimization (23) where PY is absolutely continuous and L Lipschitz. Then, for a given > 0, we have

D(R)

.


W22(PY

,

QnY

)

.


D(R

-

),

(24)

.
where  means inequality in the asymptotic regime of n = 2dR.

Proof 4 See Section C.5.

Connecting W2 to the rate-distortion optimization allows us to exploit standard rate-distortion results to better understand Wasserstein distances and in turn GANs. For example,

Theorem 5 Let PY  N (0, Id). Then, the minimum value of optimization (23) is 2d 1 - 1 - 2-2R .

(25)

Proof 5 See Section C.6. For a large value of R, the first-order tailor expansion of (25) leads to an expression of d2-2R which provides a 2-2R =. n-2/d distortion per dimension.

11

Under review as a conference paper at ICLR 2018

Next, we consider the empirical GAN optimization (8) when d = k. This optimization can be

written

min W22(QYn , PY^ ),
PY^

(26)

where Y^ = g(X). Although Theorem 4 indicates a point-wise convergence of the objective of

this optimization to a rate-distortion optimization, for an unconstrained PY^ and an arbitrary PY , a uniform convergence may not occur. However, we conjecture that such a uniform convergence will

occur under the LQG setup. In such a case, the GAN optimization of (26) can be written as the

following optimization:

min min E Y^ - Z 2 ,
PY^ PZ,Y^

(27)

PY^ : fixed, PZ = PY ,
I(Y^ ; Z)  Rd.

Conjecture 1 Under the LQG setup, the empirical GAN optimization (26) and the rate-distortion optimization (27) are equivalent.

Note that unlike optimization (23) where both marginal distributions are fixed (and thus, it is a constrained rate-distortion optimization), in optimization (27), one of the marginal distributions (PY^ ) is not fixed due to the generator optimization. Therefore, optimization (27) is the same as the rate-distortion optimization (22).

Next, we analyze optimization (27) when PY  N (0, Id) which is the rate distortion problem for an i.i.d. Gaussian source (Cover & Thomas, 2012). In this case, the optimal PY^ of optimization (26) is

PY^ = N (0, (1 - D)Id) ,

(28)

where D = 2-2R.

Given conjecture 1, this result has two main consequences:

- In the context of GANs, with X  N (0, Id) and generator functions g  G, this result suggests that linear generators are optimal when PY is Gaussian (in the asymptotic regime). This can be viewed as an unsupervised analogue of the fact that the Minimum Mean Squared Error (MMSE) estimator is linear for the Gaussian supervised learning problem.
- The optimal generated distribution PY^ which is the minimum W2 distance fitting distribution to n i.i.d. samples of Y is always a biased estimate of the true distribution PY  N (0, Id). The bias is in fact simple: it is a shrinkage to the origin by the same amount on every dimension. The amount of shrinkage is precisely D = 2-2R.

So far in this section, we have considered the regime where k = d. The analysis of k < d can be

done similarly. For example, consider the following GAN optimization where Y^ is restricted to be

in a subspace S whose rank is k  d:

min W22(PY^ , QnY ).
PY^

(29)

One can decompose Y = YS + YS where YS is the projection of Y onto the subspace S. Thus, we can re-write optimization (29) as

EQYS [ YS

2] + min
PY^

W22(PY^ , QYnS ).

(30)

Similar to optimization (27) and given conjecture 1, we can re-write this optimization as

EQYS [ YS

2] + min
PYS ,Y^

E Y^ - YS 2 ,
PYS : fixed, I(Y^ ; YS)  Rk.

(31)

12

Under review as a conference paper at ICLR 2018

Let PY  US S USt .

N (0, Id) and PYS Moreover, suppose

 N (0, KS) where diagonal elements of

the S

eigen decomposition of KS is KS = are i2(S). Then, optimal PY^ for opti-

mization (31) is

 12 - D1

0

PY^



N

0,

US

 



...

0

... 22 - D2
...
0

0 

0

...

 

USt

 

,



k2 - Dk

(32)

where

Di

=

mini(, i2)

and



is

chosen

so

that

R

=

1 k

k i=1

1 2

log

i2/Di

(the reverse water-

filling solution (Cover & Thomas, 2012)). Similar to the d = k case, even through the generator

functions G were unconstrained, for Gaussian PY , the optimal generator is linear. Therefore, the

objective function of optimization (31) can be re-written as

kk
d - i2(S) + Di.
i=1 i=1

(33)

The GAN optimization aims to select a subspace to minimize this objective, or alternatively to solve

kk

max
S

i2(S) - Di.

i=1 i=1

(34)

The first term is the energy of the projected signal onto the subspace while the second term is the quantization cost (or, the optimal transport cost) in the subspace. The larger the energy of the signal is, the larger the quantization cost is, and vice versa. This tarde-off can lead to having an optimal solution of optimization (34) which is not equal to the PCA subspace. However, for a low-rank distribution PY , we show that optimization (34) always selects the PCA subspace:

Theorem 6 Let PY  N (0, diag(1, ..., 1, 0, ..., 0)) whose covariance has k non-zero eigenvalues. Then, the PCA-subspace is the optimal solution of optimization (34).

Proof 6 See Section C.7.

B DETAILS OF EXPERIMENTS
The WGAN is implemented in pytorch. Denote fully connected layer with the input dimension din and the output dimension dout as F C(din, dout). The generator can be represented as F C(d, d); and the discriminator can be represented as F C(d, nf )-ReLU -F C(nf , nf )-ReLU -F C(nf , nf )- ReLU - F C(nf , 1). The model is trained 100k iterations with batch size 128 with Adam optimizer. The learning rate is set to 2 × 10-4. As for hyper parameters, nf is set to 128, the ratio of iterations between discriminator and generator is set to 5, and the weight clipping threshold is set to -0.02 and 0.02. Both ML and WGAN are repeated 10 times for each setting, and the mean and standard deviation is calculated and plotted (68.3% confidence interval).

C PROOFS

C.1 NOTATION AND PRELIMINARY LEMMAS

For matrices we use bold-faced upper case letters, for vectors we use bold-faced lower case letters,

and for scalars we use regular lower case letters. For example, X represents a matrix, x represents a

vector, and x represents a scalar number. In is the identity matrix of size n × n. 1n1,n2 is the all one matrix of size n1 × n2. When no confusion arises, we drop the subscripts. 1{x = y} is the indicator function which is equal to one if x = y, otherwise it is zero. Tr(X) and Xt represent the trace and

the transpose of the matrix X, respectively. x 2 = xtx is the second norm of the vector x. When

no confusion arises, we drop the subscript. X is the operator (spectral) norm of the matrix X.

< x, y > is the inner product between vectors x and y. A is the pseudo inverse of the matrix A.

The eigen decomposition of the matrix A  Rn×n is denoted by A =

n i=1

i(A)ui

(A)ui

(A)t

,

13

Under review as a conference paper at ICLR 2018

where i(A) is the i-th largest eigenvalue of the matrix A corresponding to the eigenvector ui(A). We have 1(A)  2(A)  · · · . N (µ, K) is the Gaussian distribution with mean µ and the
covariance K. KL(PX , PY ) is the Kullback Leibler divergence between two distributions PX and PY . O~(d) is the same as O(d log(d)).

Lemma 4 Let Y  N (0, KY ) and Y^  N (0, KY^ ). Then, W22(PY , PY^ ) = Tr(KY ) + Tr(KY^ ) - 2Tr KY1^/2KY K1Y^/2 1/2 = Tr(KY ) + Tr(KY^ ) - 2Tr K1Y/2KY^ K1Y/2 1/2 .

(35)

Proof 7 See reference (Givens et al., 1984).

C.2 PROOF OF LEMMA 1 Let Y = YS + YS where YS represents the projection of Y onto the subspace S. Since
E Y - Y^ 2 = E YS 2 + E YS - Y^ 2 choosing Y^ = YS achieves the minimum of optimization (9).

(36)

C.3 PROOF OF THEOREM 1
Let S be a fixed subspace of rank k where Y^ lies on. According to Lemma 1, if Y^ is unconstrained, the optimal Y^  is the projection of Y onto S (i.e., Y^  = YS ). Moreover, since Y is Gaussian, Y^ is also Gaussian. Therefore, there exists a linear g(.) such that PY^  = Pg(X) where X  N (0, I). Thus, the problem simplifies to choosing a subspace where E YS 2 is maximized, which is the same as the PCA optimization.

C.4 PROOF OF THEOREM 2

Let y1,...,yn be n i.i.d. samples of PY . Let µ^ be the sample mean. Since PY is absolutely continuous, the optimal W2 coupling between QYn and PY is deterministic (Villani, 2008). Thus, every point yi is coupled with an optimal transport vornoi region with the centroid cy(µi ,K). Therefore, we
have

W22(N (µ, K) , QYn )

= µ 2 + T r() + 1 N N

yi

2- 2 N

N

yitcy(µi ,K)

i=1 i=1

= µ 2 + T r() + 1 N N

yi

2- 2 N

N
yit

U1/2Utcy(0i ,I) + µ

i=1 i=1

= µ 2 - 2µµ^ + T r() + 1 N N

yi 2 - 2T r(U1/2UtA)

i=1

(37)

where

1 A :=
N

N

cy(0i ,I)yit.

i=1

(38)

The first step in (37) follows from the definition of W2, the second step follows from the optimal coupling between N (µ, K) and N (0, I), and the third step follows from the matrix trace equalities.

Therefore,

µW22(N (µ, ) , QnY ) = 2µ - 2µ^,

(39)

14

Under review as a conference paper at ICLR 2018

which leads to µn = µ^. Moreover, each component of the sample mean is distributed according to N (0, 1/n). Thus, µN 2  2d/n which goes to zero with the rate of O~(d/n). Let i2 be the i-th diagonal element of . Moreover, define B = UtAU. Therefore, we have

i W22(N (µ, ) , QYn ) = 2i - 2bi,i,

(40)

where bi,i is the i-th diagonal element of the matrix B. Thus, i = bi,i and T r() = T r(B) = T r(A).

Furthermore, we have

W22(PY

,

QnY

)

=

d

+

1 N

N

yi 2 - 2T r(A),

i=1

(41)

which

goes

to

zero

with

the

rate

of

O(n-2/d)

(Canas

&

Rosasco,

2012).

Since

1 N

N i=1

yi 2 goes

to d with the rate of O( d/n) (because it has a -squared distribution), T r(A) goes to d with a

rate of O(n-2/d). Combining this result with T r() = T r(A) completes the proof.

C.5 PROOF OF THEOREM 4

The lower-bound follows directly from the rate-distortion theorem (Cover & Thomas, 2012). Thus, we focus on proving the upper bound. To show this, we use the triangle inequality of the W2 distance: W2(PY , QnY )  W2(PY , PZ ) + W2(PZ , QnY ), for some PZ that we will construct.
We first introduce some notation: Let A = {y1, ..., yn} be n i.i.d. samples from PY , with the empirical distribution QYn . Moreover, let B = {b1, ..., bn } be another n = 2d(R- ) i.i.d. samples from PY . Let Vi be the voronoi region assigned to point bi. Let

pVi = P[Y  Vi].

(42)

Now, define Ai as the subset of samples of A that fall in the voronoi region Vi and let Ni = |Ai|. Ni can be written as the sum of indicator functions:

n
Ni = 1{yj  Vi}.
j=1

(43)

Thus, Ni is Binomial with the parameter pVi .

Define the probability measure PZ on the set A such that for every yi  Vj, we have

PZ

:

yi



pi

=

pVj Nj

.

(44)

First, we show that W2(PZ , QnY ) goes to zero as n increases. Let Zi := n/pVi . Note that with
high probability |Zi| < n2-d for 0 < < . Thus, with high probability, |pi - 1/n| < 2-d /n. Therefore, with high probability, the total variation distance between PZ and QYn can be bounded as PZ - QYn T V  O(2-d ). Moreover, from Theorem 6.15 of (Villani, 2008), we have

W2(PZ , QnY )  r PZ - QYn T V ,

(45)

where r is the diameter of the space. In our case, r = O(d). Thus,

W2(PZ , QYn )  O(d2-d /2),

(46)

which goes to zero as n increases.

Next, we show that W22(PY , PZ ) is less than R(D- ). To show this, we will construct an admissible transport plan whose cost is less than R(D - ): in each region Vi, we solve an optimal transport
plan between PY |Y Vi and Ai points. Combining these local transport plan provides an admissible transport plan between PY and PZ . Thus, W22(PY , PZ ) is smaller than or equal to the aggregate cost of local transport plans.

15

Under review as a conference paper at ICLR 2018

Recall that the region Vi contains Ni points of A. As n  , Ni increases as well. Since PY is absolutely continuous and Lipschitz, as n  , PY in each region resembles a uniform distribution.
Since the number of points in the region increases, the optimal transport cost between PY |Y Vi and points Ai is smaller than the transport cost between PY |Y Vi and its centroid. Therefore, the overall transport cost between PY and PZ (by combining local transports) is smaller than D(R - ). This
completes the proof.

C.6 PROOF OF THEOREM 5

Let us first show that the claimed minimum value is achieved with a certain coupling dictated by the following virtual channel: Y = cZ +W where W follows N (0, (1-c2)Id) and is independent of Z.
Note that this coupling together with a choice PZ  N (0, Id) still respects PY = PZ  N (0, Id). Another constraint I(Y ; Z)  Rd yields a condition on c: c  1 - 2-2R. Notice that

Rd  I(Y ; Z) = h(Y ) - h(Y |Z) = h(Y ) - h(W )

=

1 2

log2(2e)d

-

1 2

log2(2e(1

-

c2))d

d1 = 2 log2 1 - c2 .
 This together with the minimum value of c = 1 - 2-2R then gives:

(47)

E Y - Z 2 = (c - 1)2 + 1 - c2 = 2(1 - c) = 2(1 - 1 - 2-2R).

(48)

Now let us prove the converse: the optimal solution cannot go below the claimed bound. Consider a constraint: dR  I(Y ; Z) = h(Y ) - h(Y |Z). Note that h(Y |Z) is maximized when (Y, Z) are jointly Gaussian. So in an effort to find a lower bound on I(Y ; Z), we consider such a jointly Gaussian setting, i.e., Y = cZ + W where W N (0, (1 - c2)Id), independent of Z. This together with a simple calculation then yields: c  1 - 2-2R. Putting this to the objective function, we get:
E Y - Z 2  2(1 - 1 - 2-2R).
This completes the proof.

C.7 PROOF OF THEOREM 6

The objective function of optimization (34) for the PCA subspace is k(1 - 2-2R). Now consider

a subspace S such that

k i=1

i2(S

)

=

k

where

0

<



<

1.

Below,

we

show

that

this

subspace

cannot be the optimal solution of optimization (34).

We have

k
2Rk = log i2(S)/Di

(49)

i=1

k
 i2(S) log i2(S)/Di

i=1

 k log(

k i Di )

where the first step follows from the fact that i2(S)  1 and the second step follows from the log-sum inequality. This inequality leads to

k
Di  k2-2R/.

(50)

i=1

Thus, for this subspace, the objective function of optimization (34) is upper bounded by k - k2-2R/, which for 0 <  < 1 is always less than k(1 - 2-2R), the PCA-subspace objective

function. This completes the proof.

16

