Under review as a conference paper at ICLR 2018
THE IMPLICIT BIAS OF GRADIENT DESCENT ON SEPARABLE DATA
Anonymous authors Paper under double-blind review
ABSTRACT
We show that gradient descent on an unregularized logistic regression problem with separable data converges to the max-margin solution. The result generalizes also to other monotone decreasing loss functions with an infimum at infinity, and we also discuss a multi-class generalizations to the cross entropy loss. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.
1 INTRODUCTION
It is becoming increasingly clear that implicit biases introduced by the optimization algorithm play a crucial role in deep learning and in the generalization ability of the learned models (Neyshabur et al., 2014; 2015; Zhang et al., 2017; Keskar et al., 2017; Neyshabur et al., 2017; Wilson et al., 2017). In particular, minimizing the training error, without any explicit regularization, over models with more parameters and more capacity then the number of training examples, often yields good generalization, despite the empirical optimization problem being highly underdetermined. That is, there are many global minima of the training objective, most of which will not generalize well, but the optimization algorithm (e.g. gradient descent) biases us toward a particular minimum that does generalize well. Unfortunately, we still do not have a good understanding of the biases introduced by different optimization algorithms in different situations.
We do have a decent understanding of the implicit regularization introduced by early stopping of stochastic methods or, at an extreme, of one-pass (no repetition) stochastic optimization. However, as discussed above, in deep learning we often benefit from implicit bias even when optimizing the (unregularized) training error to convergence, using stochastic or batch methods. For loss functions with attainable, finite, minimizers, such as the squared loss, we have some understanding of this: In particular, when minimizing an underdetermined least squares problem using gradient descent starting from the origin, we know we will converge to the minimum Euclidean norm solution. But the logistic loss, and its generalization the cross-entropy loss which is often used in deep learning, do not admit a finite minimizer on separable problems. Instead, to drive the loss toward zero and thus minimize it, the predictor must diverge toward infinity.
Do we still benefit from implicit regularization when minimizing the logistic loss on separable data? Clearly the norm of the predictor itself is not minimized, since it grows to infinity. However, for prediction, only the direction of the predictor, i.e. the normalized w(t)/ w(t) , is important. How does w(t)/ w(t) behave as t   when we minimize the logistic (or similar) loss using gradient descent on separable data, i.e., when it is possible to get zero misclassification error and thus drive the loss to zero?
In this paper, we show that even without any explicit regularization, when minimizing linearly separable logistic regression problems using gradient descent, we have that w(t)/ w(t) converges to the L2 maximum margin separator, i.e. to the solution of the hard margin SVM. This happens even though the norm w , nor the margin constraint, are in no way part of the objective nor explicitly introduced into optimization. More generally, we show the same behavior for generalized linear problems with any smooth, monotone strictly decreasing, lower bounded loss with an exponential
1

Under review as a conference paper at ICLR 2018

tail. Furthermore, we characterize the rate of this convergence, and show that it is rather slow, with the distance to the max-margin predictor decreasing only as O(1/ log(t)). This explains why the predictor continues to improve even when the training loss is already extremely small. We emphasize and demonstrate that this bias is specific to gradient descent, and changing the optimization algorithm, e.g. using adaptive learning rate methods such as ADAM (Kingma & Ba, 2014), changes this implicit bias.

2 MAIN RESULTS

Consider a dataset {xn, yn}nN=1 with binary labels yn  {-1, 1}. We analyze learning by minimizing an empirical loss of the form

N
L (w) =
n=1

ynw xn .

(2.1)

where w  Rd is the weight vector. A bias term could be added in the usual way, extending xn by an additional '1' component. To simplify notation, we assume that n : yn = 1 -- this is true without
loss of generality, since we can always re-define ynxn as xn.

We are particularly interested in problems that are linearly separable, and the loss is smooth monotone strictly decreasing and non-negative:
Assumption 1. The dataset is strictly linearly separable: w such that n : w xn > 0 . Assumption 2. (u) is a positive, differentiable, monotonically decreasing to zero1, (so u : (u) > 0, (u) < 0 and limu (u) = limu (u) = 0) and a -smooth function, i.e. its derivative is -Lipshitz.

Many common loss functions, including the logistic, exp-loss, probit and sigmoidal losses, follow Assumption 2. Under these conditions, the infimum of the optimization problem is zero, but it is not attained at any finite w. Furthermore, no finite critical point w exist. We consider minimizing eq. 2.1 using Gradient Descent (GD) with a fixed learning rate , i.e., with steps of the form:

N
w (t + 1) = w (t) - L (w) = w (t) - 
n=1

w (t) xn xn .

(2.2)

We do not require convexity. Under Assumptions 1 and 2, gradient descent converges to the global minimum (i.e. to zero loss) even without it:
Lemma 1. Let w (t) be the iterates of gradient descent (eq. 2.2) with  < 2-1 and any starting point w(0). Under Assumptions 1 and 2, we have: (1) limt L (w (t)) = 0, (2) limt w (t) =
, (3) n : limt w (t) xn > 0, and (4) t0: t > t0: w (t) xn > 0, that is, within a finite time we reach a predictor that perfectly separates the data.

Proof. Since the data is strictly linearly separable, w which linearly separates the data, and

therefore

N

w L (w) =

w xn w xn.

n=1

For any finite w, this sum cannot be equal to zero, as a sum of negative terms, since n : w xn > 0 and u : (u) < 0. Therefore, there are no finite critical points w, for which L (w) = 0. But gradient descent on a smooth loss with an appropriate stepsize is always guaranteed to converge to a critical point: L (w (t))  0 (see, e.g. Lemma 5 in Appendix A.4, slightly adapted from Ganti
(2015), Theorem 2). This necessarily implies that w (t)   while n : w (t) xn > 0 -- since
only then w (t) xn  0. Therefore, L (w)  0, so GD converges to the global minimum.

1The requirement of nonnegativity and that the loss asymptotes to zero is purely for convenience. It is enough to require the loss is monotone decreasing and bounded from below. Any such loss asymptotes to some constant, and is thus equivalent to one that satisfies this assumption, up to a shift by that constant.

2

Under review as a conference paper at ICLR 2018

The main question we ask is: can we characterize the direction in which w(t) diverges? That is, does the limit limt w (t) / w (t) always exists, and if so, what is it?

In order to analyze this limit, we will need to make a further assumption on the tail of the loss function:

Definition 2. A function f (u) has a "tight exponential tail", if there exist positive constants c, a, +, -, u+ and u- such that
u > u+ :f (u)  c (1 + exp (- +u)) e-au u > u- :f (u)  c (1 - exp (- -u)) e-au .
Assumption 3. The negative loss derivative - (u) has a tight exponential tail (Definition 2).

For example, the exponential loss (u) = e-u and the commonly used logistic loss (u) = log (1 + e-u) both follow this assumption with a = c = 1. We will assume a = c = 1 -- without loss of generality, since these constants can be always absorbed by re-scaling xn and .

We are now ready to state our main result:

Theorem 3. Given strictly linearly separable data (Assumption 1) and a -smooth decreasing loss function (Assumption 2) with an exponential tail (Assumption 3), then gradient descent (as in eq. 2.2) with stepsize  < 2-1 and any starting point w(0) will behave as:

w (t) = w^ log t +  (t) ,

(2.3)

where the residual  (t) is bounded and so

w (t) w^

lim =

t w (t)

w^

where w^ is the L2 max margin vector (the solution to the hard margin SVM):
w^ = argmin w 2 s.t. w xn  1.
wRd

(2.4)

Proof Sketch We first understand intuitively why an exponential tail of the loss entail asymptotic convergence to the max margin vector: Assume for simplicity that (u) = e-u exactly, and examine

the asymptotic regime of gradient descent in which n : w (t) xn  , as is guaranteed by
Lemma 1. If w (t) / w (t) converges to some limit, then we can write w (t) = g (t) w +  (t) such that g (t)  , n :xn w > 0, and limt  (t) /g (t) = 0. The gradient can then be writen as:

NN

- L (w) = exp -w (t) xn xn = exp -g (t) wxn exp - (t) xn xn .

n=1

n=1

(2.5)

As g(t)   and the exponents become more negative, only those samples with the largest (i.e. least

negative) exponents will contribute to the gradient. These are precisely the samples with smallest

margin argminnwxn, aka the "support vectors". The negative gradient (eq. 2.5) would then asymptotically become a non-negative linear combination of support vectors. Since the limit will be

dominated by these gradients, w will also be non-negative linear combination of support vectors,

and so will its scaling w^ = w/ minn wxn . We therefore have:

w^ = nxn n n  0 and w^ xn = 1 OR n = 0 and w^ xn > 1

(2.6)

n

These are precisely the KKT condition for the SVM problem (eq. 2.4) and we can conclude that w^ is indeed its solution and w is thus proportional to it.

To prove Theorem 3 rigorously, we need to show that w (t) / w (t) has a limit, that g (t) = log (t) and to bound the effect of various residual errors, such as gradients of non-support vectors and the fact that the loss is only approximately exponential. To do so, we substitute eq. 2.3 into the gradient descent dynamics (eq. 2.2), with w = w^ being the max margin vector and g(t) = log t. We then show that the increment in the norm of  (t) is bounded by C1t- for some C1 > 0 and  > 1, which is a converging series. This happens because the increment in the max margin term, w^ [log (t + 1) - log (t)]  w^ t-1, cancels out the dominant t-1 term in the gradient -L (w (t)) (eq. 2.5 with g (t) = log (t) and wxn = 1). A complete proof can be found in Appendix A.

3

Under review as a conference paper at ICLR 2018

More refined analysis: characterizing the residual We can furthermore characterize the asymp-
totic behavior of  (t). To do so, we need to refer to the KKT conditions (eq. 2.6) of the SVM problem (eq. 2.4) and the associated support vectors S = argminnw^ xn. The following refinement of Theorem 3 is also proved in Appendix A:

Theorem 4. Under the conditions of Theorem 3, if, in addition the support vectors span the data
(i.e. rank (XS ) = rank (X) where the columns of X are all samples and of XS are the support vectors), then limt  (t) = w~ , where w~ a is the unique solution to

n  S :  exp -xn w~ = n .

(2.7)

3 IMPLICATIONS: RATES OF CONVERGENCE

The solution in eq. 2.3 implies that w (t) / w (t) converges to the normalized max margin vector w^ / w^ . Moreover, this convergence is very slow-- logarithmic in the number of iterations. Specifically, in Appendix B we show that Theorem 3 implies the following tight rates of convergence:

The normalized weight vector converges to normalized max margin vector in L2 norm

w (t) - w^ = O 1 ,

w (t) w^

log t

(3.1)

and in angle and the margin converges as

1 - w (t) w^ = O w (t) w^

1 log2 t

,

(3.2)

1 - minn xn w (t) = O 1 .

w^ w (t)

log t

(3.3)

This slow convergence is in sharp contrast to the convergence of the (training) loss:

L (w (t)) = O 1 . t

(3.4)

A simple construction (also in Appendix B) shows that the rates in the above equations are tight. Thus, the convergence of w(t) to the max-margin w^ can be logarithmic in the loss itself, and we might need to wait until the loss is exponentially small in order to be close to the max-margin solution. This can help explain why continuing to optimize the training loss, even after the training error is zero and the training loss is extremely small, still improves generalization performance--our results suggests that the margin could still be improving significantly in this regime.

A numerical illustration of the convergence is depicted in Figure 3.1. As predicted by the theory, the norm w(t) grows logarithmically (note the semi-log scaling), and w(t) converges to the max-margin separator, but only logarithmically, while the loss itself decreases very rapidly (note the log-log scaling).

An important practical consequence of our theory, is that although the margin of w(t) keeps improving,

and so we can expect the population (or test) misclassification error of w(t) to improve, the same

cannot be said about the expected population loss (or test loss)! At the limit, the direction of w(t) will

converge toward the max margin predictor w^ . Although w^ has zero training error, it will not generally

have zero misclassification error on the population, or on a test or a validation set. Since the norm

of w(t) will increase, if we use the logistic loss or any other convex loss, the loss incurred on those misclassified points will also increase. More formally, consider the logistic loss (u) = log(1 + e-u)

and define also the hinge-at-zero loss h(u) = max(0, -u). Since w^ classifies all training points

correctly,

we

have

that

on

the

training

set

1 N

n h(w^ xn) = 0. However, on the population we

would expect some errors and so E[h(w^ x)] > 0. Since w(t)  w^ log t and (u)  h(u) as

  , we have:

E[ (w(t) x)]  E[ ((log t)w^ x)]  (log t)E[h(w^ x)] = (log t).

(3.5)

That is, the population loss increases logarithmically while the margin and the population misclassification error improve. Roughly speaking, the improvement in misclassification does not out-weight the increase in the loss of those points still misclassified.

4

Under review as a conference paper at ICLR 2018

x
2

(A) 3
2
1
0
-1
-2
-3 -3 -2 -1 0 1 2 3
x
1

(B) 1

Normalized ||w(t)||

0.5

0 100 101 102 103 104 105 106

(D) x 10-3

t

8

Angle gap

6

4

2

0 100 101 102 103 104 105 106
t

L(w(t))

(C) 100

10-5

GD

10-10

GDMO

100 101 102 103 104 105 106

(E) t

0.1

Margin gap

0.05

0 100 101 102 103 104 105 106
t

Figure 3.1: Visualization of or main results on a synthetic dataset in which the L2 max margin vector w^ is precisely known. (A) The dataset (positive and negatives samples (y = ±1) are respectively denoted by + and  ), max margin seperating hyperplane (black line), and the asymptotic solution
of GD (dashed blue). For both GD and GD with momentum (GDMO), we show: (B) The norm of
w (t), normalized so it would equal to 1 at the last iteration, to facilitate comparison. As expected (eq. 2.3), the norm increases logarithmically; (C) the training loss. As expected, it decreases as t-1 (eq. 3.4); and (D&E) the angle and margin gap of w (t) from w^ (eqs. 3.2 and 3.3). As expected,
these are logarithmically decreasing to zero. Implementation details: The dataset includes four support vectors: x1 = (0.5, 1.5) , x2 = (1.5, 0.5) with y1 = y2 = 1, and x3 =-x1, x4 = -x2 with y3 = y4 = -1 (the L2 normalized max margin vector is then w^ = (1, 1) / 2 with margin equal to 2 ), and 12 other random datapoints (6 from each class), that are not on the margin. We used
a learning rate  = 1/max (X), where max (X) is the maximal singular value of X, momentum  = 0.9 for GDMO, and initialized at the origin.

The increase in the test loss is practically important because the loss on a validation set is frequently used to monitor progress and decide on stopping. Similar to the population loss, the validation loss
Lval (w (t)) = xV w (t) x calculated on an independent validation set V, will increase logarithmically with t (since we would not expect zero validation error), which might cause us to think we are over-fitting or otherwise encourage us to stop the optimization. But this increase does not actually represent the model getting worse, merely w(t) getting larger, and in fact the model might be getting better (with larger margin and possibly smaller error rate).

4 EXTENSIONS
We discuss several possible extensions of our results.

4.1 MULTI-CLASS CLASSIFICATION WITH CROSS-ENTROPY LOSS

So far, we have discussed the problem of binary classification. For multi-class problems commonly encountered, we frequently learn a predictor wk for each class, and use the cross-entropy loss with a softmax output, which is a generalization of the logistic loss. What do the linear predictors wk(t) converge to if we minimize the cross-entropy loss by gradient descent on the predictors? In Appendix C we analyze this problem for separable data, and show that again, the predictors diverge to infinity and the loss converges to zero. Furthermore, we show that, generically, the loss converges to a logistic loss for transformed data, for which our theorems hold. This strongly suggests that gradient descent converges to a scaling of the K-class SVM solution:

K

arg min
w1 ,...,wK

wk 2 s.t. n, k = yn : wyn xn  wk xn + 1

k=1

(4.1)

We believe this can also be established rigorously and for generic exponential tailed multi-class loss.

5

Under review as a conference paper at ICLR 2018

x
2

(A)
50
0
-50 -50 0 50 x
1

Angle gap

Normalized ||w(t)||

(B) 1
0.5
0 100 101 102 103 104 105 106 (D) t
0.15 0.1 0.05
0 100 101 102 103 104 105 106
t

L(w(t))

(C) 100

10-10

GD

GDMO

10-20

ADAM

100 101 102 103 104 105 106

(E) t

2

Margin gap

1

0 100 101 102 103 104 105 106
t

Figure 4.1: Same as Fig. 3.1, except we multiplied all x2 values in the dastaset by 20, and also train using ADAM. The final weight vector produced after 2 · 106 epochs of optimization using ADAM (red dashed line) does not converge to L2 max margin solution (black line), in contrast to GD (blue dashed line), or GDMO.

4.2 OTHER OPTIMIZATION METHODS

In this paper we examined the implicit bias of gradient descent. Different optimization algorithms exhibit different biases, and understanding these biases and how they differ is crucial to understanding and constructing learning methods attuned to the inductive biases we expect. Can we characterize the implicit bias and convergence rate in other optimization methods?

In Figure 3.1 we see that adding momentum does not qualitative effect the bias induced by gradient descent. In Figure E.1 in Appendix E we also repeat the experiment using stochastic gradient descent, and observe a similar bias. This is consistent with the fact that momentum, acceleration and stochasticity do not change the bias when using gradient descent to optimize an under determined least squares problems. It would be beneficial, though, to rigorously understand how much we can generalize our result to gradient descent variants, and how the convergence rates might change in these cases.

Employing adaptive methods, such as AdaGrad (Duchi et al., 2011) and ADAM (Kingma & Ba, 2015), does significantly affect the bias. In Figure 4.1 we show the predictors obtained by ADAM and by gradient descent on a simple data set. Both methods converge to zero training error solutions. But although gradient descent converges to the L2 max margin predictor, as predicted by our theory, ADAM does not. The implicit bias of adaptive method has been a recent topic of interest, with Hoffer et al. (2017) and Wilson et al. (2017) suggesting they lead to worse generalization. Wilson et al. discuss the limit of AdaGrad on lest square problems, but fall short of providing an actual characterization of the limit. This is not surprising, as the limit of AdaGrad on least square problems is fragile and depends on the choice of stepsize and other parameters, and thus complicated to characterize. We expect our methodology could be used to precisely characterize the implicit bias of such methods on logistic regression problems. The asymptotic nature of the analysis is appealing here, as it is insensitive to the initial point, initial conditioning matrix, and large initial steps.

More broadly, it would be interesting to study the behavior of mirror descent and natural gradient

descent, and relate the bias they induce to the potential function or divergence underlying them. A

reasonable conjecture, which we have not yet investigated, is that for any potential function (w),

these methods converge to the maximum -margin solution arg minw (w)s.t.n : w xn  1.

Since mirror descent can be viewed as regularizing progress using (w), it is worth noting the results

of Rosset et al. (2004b): they considered the regularization path w = arg min L(w) + 

w

p p

for

similar loss function as we do and showed that lim0 w/ w p is proportional to the maximum Lp margin solution. Rosset et al. do not consider the effect of the optimization algorithm, and instead

add explicit regularization--here we are specifically interested in the bias implied by the algorithm

not by adding (even infinitesimal) explicit regularization.

Our analysis also covers the exp-loss used in boosting, as its tail is similar to that of the logistic loss. However, boosting is a coordinate descent procedure, and not a gradient descent procedure. Indeed, the coordinate descent interpretation of AdaBoost shows that coordinate descent on the exp-loss for a

6

Under review as a conference paper at ICLR 2018

Figure 4.2: Training of a convolutional neural network on CIFAR10 using stochastic gradient descent with constant learning rate and momentum, softmax output and a cross entropy loss, where we achieve 8.3% final validation error. We observe that, approximately: (1) The training loss decays as a t-1, (2) the L2 norm of last weight layer increases logarithmically, (3) after a while, the validation loss starts to increase, and (4) in contrast, the validation (classification) error slowly improves.

Epoch
L2 norm Train loss Train error Validation loss Validation error

50
13.6 0.1 4% 0.52 12.4%

100
16.5 0.03 1.2% 0.55 10.4%

200
19.6 0.02 0.6% 0.77 11.1%

400
20.3 0.002 0.07% 0.77 9.1%

2000
25.9 10-4 0% 1.01 8.92%

4000
27.54 3 · 10-5
0% 1.18 8.9%

Table 1: Sample values from various epochs in the experiment depicted in Fig. 4.2.

linearly separable problem is related to finding the maximum L1 margin solution (Schapire et al., 1998; Rosset et al., 2004a; Shalev-Shwartz & Singer, 2010).
4.3 DEEP NETWORKS
In this paper, we only consider linear prediction. Naturally, it is desirable to generalize our results also to non-linear models and especially multi-layer neural networks.
Even without a formal extension and description of the precise bias, our results already shed light on how minimizing the cross-entropy loss with gradient descent can have a margin maximizing effect, how the margin might improve only logarithmically slow, and why it might continue improving even as the validation loss increases. These effects are demonstrated in Figure 4.2 and Table 1 which portray typical training of a convolutional neural network using unregularized gradient descent2. As can be seen, the norm of the weight increases, but the validation error continues decreasing, albeit very slowly (as predicted by the theory), even after the training error is zero and the training loss is extremely small. We can now understand how even though the loss is already extremely small, some sort of margin might be gradually improving as we continue optimizing. We can also observe how the validation loss increases despite the validation error decreasing, as discussed in Section 3.
As an initial advance toward tackling deep network, we can point out that for two special cases, our results may be directly applied to multi-layered networks. First, our results may be applied exactly, as we show in Appendix D, if only a single weight layer is being optimized, and furthermore, after a sufficient number of iterations, the activation units stop switching and the training error goes to zero. Second, our results may also be applied directly to the last weight layer if the last hidden layer becomes fixed and linearly separable after a certain number of iterations. This can become true, either approximately, if the input to the last hidden layer is normalized (e.g., using batch norm), or exactly, if the last hidden layer is quantized (Hubara et al., 2016).
4.4 MATRIX FACTORIZATION
With multi-layered neural networks in mind, Gunasekar et al. (2017) recently embarked on a study of the implicit bias of under-determined matrix factorization problems, where we minimize the
2Code available here: https://github.com/paper-submissions/MaxMargin
7

Under review as a conference paper at ICLR 2018
squared loss of linear observation of a matrix by gradient descent on its factorization. Since a matrix factorization can be viewed as a two-layer network with linear activations, this is perhaps the simplest deep model one can study in full, and can thus provide insight and direction to studying more complex neural networks. Gunasekar et al. conjectured, and provided theoretical and empirical evidence, that gradient descent on the factorization for an under-determined problem converges to the minimum nuclear norm solution, but only if the initialization is infinitesimally close to zero and the step-sizes are infinitesimally small. With finite step-sizes or finite initialization, Gunasekar et al. could not characterize the bias. It would be interesting to study the same problem with a logistic loss instead of squared loss. Beyond the practical relevance of the logistic loss, taking our approach has the advantage that because of its asymptotic nature, it does not depend on the initialization and step-size. It thus might prove easier to analyze logistic regression on a matrix factorization instead of the least square problem, providing significant insight into the implicit biases of gradient descent on non-convex multi-layered optimization.
5 SUMMARY
We characterized the implicit bias induced by gradient descent when minimizing smooth monotone loss functions with an exponential tail. This is the type of loss commonly being minimized in deep learning. We can now rigorously understand:
1. How gradient descent, without early stopping, induces implicit L2 regularization and converges to the maximum L2 margin solution, when minimizing the logistic loss, or exploss, or any other monotone decreasing loss with appropriate tail. In particular, the non-tail part does not affect the bias and so the logistic loss and the exp-loss, although very different on non-separable problems, behave the same for separable problems. The bias is also independent of the step-size used (as long as it is small enough to ensure convergence) and (unlike for least square problem) is also independent on the initialization.
2. This convergence is very slow. This explains why it is worthwhile continuing to optimize long after we have zero training error, and even when the loss itself is already extremely small.
3. We should not rely on slow decrease of the training loss, or on no decrease of the validation loss, to decide when to stop. We might improve the validation, and test, errors even when the validation loss increases and even when the decrease in the training loss is tiny.
Perhaps that gradient descent leads to a max L2 margin solution is not a big surprise to those for whom the connection between L2 regularization and gradient descent is natural. Nevertheless, we are not familiar with any prior study or mention of this fact, let alone a rigorous analysis and study of how this bias is exact and independent of the initial point and the step-size. Furthermore, we also analyze the rate at which this happens, leading to the novel observations discussed above. Perhaps even more importantly, we hope that our analysis can open the door to further analysis of different optimization methods or in different models, including deep networks, where implicit regularization is not well understood even for least square problems, or where we do not have such a natural guess as for gradient descent on linear problems. Analyzing gradient descent on logistic/cross-entropy loss is not only arguably more relevant than the least square loss, but might also be technically easier.
REFERENCES
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
RadhaKrishna Ganti. EE6151, Convex optimization algorithms. Unconstrained minimization: Gradient descent algorithm, 2015.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro. Implicit Regularization in Matrix Factorization. arXiv, pp. 1­10, 2017.
Elad Hoffer, Itay Hubara, and D. Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In NIPS (oral presentation), pp. 1­13, may 2017.
I Hubara, M Courbariaux, D. Soudry, R El-yaniv, and Y Bengio. Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations. Accepted to JMLR, 2016.
8

Under review as a conference paper at ICLR 2018
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. ICLR, pp. 1­16, 2017. doi: 10.1227/01.NEU.0000255452.20602.C9.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In The International Conference on Learning Representations (ICLR), 2014.
Diederik P Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. In ICLR, pp. 1­13, 2015.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Ruslan R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. In Advances in Neural Information Processing Systems, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring Generalization in Deep Learning. arXiv, jun 2017.
Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin classifier. Journal of Machine Learning Research, 5(Aug):941­973, 2004a.
Saharon Rosset, Ji Zhu, and Trevor J Hastie. Margin Maximizing Loss Functions. In NIPS, pp. 1237­1244, 2004b. ISBN 0-262-20152-6.
Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A new explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651­1686, 1998.
Shai Shalev-Shwartz and Yoram Singer. On the equivalence of weak learnability and linear separability: new relaxations and efficient boosting algorithms. Machine Learning, 80(2):141­163, Sep 2010. ISSN 1573-0565. doi: 10.1007/s10994-010-5173-z.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The Marginal Value of Adaptive Gradient Methods in Machine Learning. arXiv, pp. 1­14, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In ICLR, 2017.
9

Under review as a conference paper at ICLR 2018

Appendix

A PROOF OF MAIN RESULTS

In the following proofs, for any solution w (t), we define r (t) = w (t) - w^ log t - w~ , where w^ and w~ follow the conditions of the Theorems 3 and 4, i.e. w^ is the L2 max margin vector and (eq. 2.4), so

n  S :

xn w^ = 1 ;



=

max
n/S

xn

w^

>

1

.

(A.1)

and eq. 2.7 holds

n  S :  exp -xn w~ = n ,

(A.2)

where we recall that we denoted XS  R|S|×d as the matrix of support vectors, which columns are the support vectors, and a subset S  {1, . . . , N } of the columns of X = [x1, . . . , xN ]  Rd×N . Note that, in general, if the support vectors do not span the data, the solution w~ to to eq. A.2 might

bot be unique, and we can use any solution in the proof.

We furthermore denote by Ci, i,ti (i  N) various positive constants which are independent of t. Lastly, we define P1  Rd×d as the orthogonal projection matrix3 to the subspace spanned by the support vectors (the columns of XS ), and P2 = I - P1 as the complementary projection (to the left
nullspace of XS ).

A.1 AUXILIARY LEMMATA

In our proofs, we use the following Lemmata:

Lemma 5. Let L (w) be a general -smooth non-negative objective. If  < 2-1, then for any

solution of GD

w (t + 1) = w (t) - L (w)

(A.3)

we have that

t u=0

L (w (u)) 2 <  and limt t

L (w (t)) 2 = 0.

Lemma 6. We have C1, t1 : t > t1 : (r (t + 1) - r (t)) r (t)  C1t- min(,-1-1.5 +,-1-0.5 -) .

(A.4)

Additionally,  1 > 0 , C2, t2, such that t > t2, such that if

P1r (t)  1,

(A.5)

then we can improve this bound to

(r (t + 1) - r (t)) r (t)  -C2t-1 < 0 .

(A.6)

We prove both Lemmata below, in appendix sections A.4 and A.5, after we prove of both theorems.

A.2 PROOF OF THEOREM 3

Our goal is to show that r (t) is bounded, and therefore  (t) = r (t) + w~ is bounded. To show this, we will upper bound the following equation

r (t + 1) 2 = r (t + 1) - r (t) 2 + 2 (r (t + 1) - r (t)) r (t) + r (t) 2

(A.7)

First, we note that first term in this equation can be upper-bounded by r (t + 1) - r (t) 2

(=1) w (t + 1) - w^ log (t + 1) - w~ - w (t) + w^ log (t) + w~ 2

(=2) -L (w (t)) - w^ [log (t + 1) - log (t)] 2 = 2 L (w (t)) 2 + w^ 2 log2 1 + t-1 + 2w^ L (w (t)) log 1 + t-1

(3)
 2

L (w (t)) 2 +

w^ 2 t-2

(A.8)

3This matrix can be written as P1 = XS X+S , where M+ is penrose-moorse pseudo inverse of M.

10

Under review as a conference paper at ICLR 2018

where in (1) we used eq. 2.3, in (2) we used eq. 2.2, and in (3) we used x > 0 : x  log (1 + x) >

0, and also that

N

w^ L (w (t)) =

w (t) xn w^ xn  0 ,

(A.9)

n=1

since w^ xn  1 (eq. A.1).

Also, from Lemma 5 we know that



L (w (t)) 2 = o t-1 and

L (w (t)) 2 <  .

t=0

(A.10)

Substituting eq. A.10 into eq. A.8, and recalling that a t- power series converges for some  > 1, we can find C0 such that



r (t + 1) - r (t) 2 = o t-1 and

r (t + 1) - r (t) 2 = C0 <  .

t=0

(A.11)

Note that this equation also implies that

t0 : t > t0 : | r (t + 1) - r (t) | < 0 .

(A.12)

Next, we would like to bound the second term in eq. A.7. From eq. A.4 in Lemma 6, we can find t1, C1 such that t > t1:

(r (t + 1) - r (t)) r (t)  C1t- min(,-1-1.5 +,-1-0.5 -) .

(A.13)

Thus, by combining eqs. A.13 and A.11 into eq. A.7, we find

r (t) 2 - r (t1) 2

t-1
= r (u + 1) 2 - r (u) 2

u=t1

t-1

 C0 + 2

C1u- min(,-1-1.5 +,-1-0.5 -)

u=t1

which is a bounded, since  > 1 (eq. A.1). Therefore, r (t) is bounded.

A.3 PROOF OF THEOREM 4

All that remains now is to show that r (t)  0 if rank (XS ) = rank (X). To do so, this proof will continue where the proof of Theorem 4 stopped, using notations and equations from that proof.
Since r (t) has bounded norm, its two orthogonal components r (t) = P1r (t) + P2r (t) also have bounded norms (recall where P1, P2 defined in the beginning of appendix section A). From eq. 2.2, L (w) is spanned by the columns of X. If rank (XS ) = rank (X), then it is also spanned by the columns of XS , and so P2L (w) = 0. Therefore, P2r (t) is not updated during GD, and remains constant. Since w~ in eq. 2.3 is also bounded, we can absorb this constant P2r (t) into w~ without affecting eq. 2.7 (since n  S : xn P2r (t) = 0). Thus, without loss of generality, we can assume that r (t) = P1r (t).
Now, recall eq. A.6 in Lemma 6

C2, t2 : t > t2 : (r (t + 1) - r (t)) r (t)  -C2t-1 < 0 .

Combining this with eqs. A.7 and A.11, implies that t3 > max [t2, t0] such that t > t3 such that r (t) > 1, we have that r (t + 1) 2 is a decreasing function since then

r (t + 1) 2 - r (t) 2  -C3t-1 < 0.

(A.14)

11

Under review as a conference paper at ICLR 2018

Additionally, this result also implies that we cannot have r (t) > 1 t > t3 , since then we arrive to the contradiction.

t-1 t-1

r (t) 2 - r (t3) 2 =

r (u + 1) 2 - r (u) 2  - C3u-1  -,

u=t3

u=t3

Therefore, t4 > t3 such that r (t4)  1. Recall also that r (t) is a decreasing function whenever r (t)  1 (eq. A.14). Also, recall that t4 > t0, so from eq. A.12, we have that t > t4,| r (t + 1) - r (t) | < 0. Combining these three facts we conclude that t > t4 :
r (t)  1 + 0. Since this reasoning holds  1, 0, this implies that r (t)  0.

A.4 PROOF OF LEMMA 5

Lemma 5. Let L (w) be a general -smooth non-negative objective. If  < 2-1, then for any

solution of GD

w (t + 1) = w (t) - L (w)

(A.3)

we have that

t u=0

L (w (u)) 2 <  and limt t

L (w (t)) 2 = 0.

This proof is a slightly modified version of the proof of Theorem 2 in (Ganti, 2015). Recall a well-known property of -smooth functions:

f (x) - f (y) - f (y) (x - y)   x - y 2 . 2
From the -smoothness of L (w)

(A.15)

L (w (t + 1))  L (w (t)) + L (w (t)) (w (t + 1) - w (t)) +  w (t + 1) - w (t) 2 2
= L (w (t)) -  L (w (t)) 2 + 2 L (w (t)) 2 2
= L (w (t)) -  1 -  L (w (t)) 2 2

Thus, we have

L (w (t)) - L (w (t + 1))  L (w (t)) 2



1

-

 2

which implies

t L (w (u)) 2  t L (w (u)) - L (w (u + 1)) = L (w (0)) - L (w (t + 1)) <  ,

u=0

u=0



1

-

 2



1

-

 2

since L (w (0)) <  and 0  L (w (t + 1)). Furthermore, since

t
L (w (u)) 2 <  ,

u=0
we also have t L (w (t)) 2  0, from Cauchy Condensation test, so L (w (t)) = o t-1/2 .

A.5 PROOF OF LEMMA 6

Recall that we defined r (t) = w (t) - w^ log t - w~ , with w^ and w~ follow the conditions of the Theorems 3 and 4, i.e. w^ is the L2 max margin vector and (eq. 2.4), and eq. 2.7 holds

Lemma 6. We have

n  S :  exp -xn w~ = n .

C1, t1 : t > t1 : (r (t + 1) - r (t)) r (t)  C1t- min(,-1-1.5 +,-1-0.5 -) .

(A.4)

Additionally,  1 > 0 , C2, t2, such that t > t2, such that if

P1r (t)  1,

(A.5)

then we can improve this bound to

(r (t + 1) - r (t)) r (t)  -C2t-1 < 0 .

(A.6)

12

Under review as a conference paper at ICLR 2018

Note that, from Lemma 1, n : limt w (t) xn = . In addition, from assumption 3 the negative loss derivative - (u) has an exponential tail e-u (recall we can assume a = c = 1 without
loss of generality). Combining both facts, we find that positive constants -, +, t- and t+ such that n

t > t+ : - w (t) xn  1 + exp - +w (t) xn exp -w (t) xn t > t- : - w (t) xn  1 - exp - -w (t) xn exp -w (t) xn

(A.16) (A.17)

Next, we examine the expression we wish to bound, recalling that r (t) = w (t) - w^ log t - w~ :

(r (t + 1) - r (t)) r (t)

= (-L (w (t)) - w^ [log (t + 1) - log (t)]) r (t)

N
= -
n=1

w (t) xn xn r (t) - w^ r (t) log 1 + t-1

= w^ r (t) t-1 - log 1 + t-1 - 
n/S

w (t) xn xn r (t)

-  t-1 exp -w~ xn +
nS

w (t) xn xn r (t)

where in last line we used eqs. 2.6 and 2.7 to obtain

(A.18)

w^ = nxn =  exp -w~ xn xn .

nS

nS

We examine the three terms in eq. A.18. The first term can be upper bounded by

w^ r (t) t-1 - log 1 + t-1

 max w^ r (t) , 0 t-1 - log 1 + t-1

(1)
 max w^ P1r (t) , 0 t-2

(2)


w^ 1t-2 o t-1

, if , if

P1r (t)  1 P1r (t) > 1

(A.19)

where in (1) we used that P2w^ = P2XS  = 0 from eq. 2.6, and in (2) we used that w^ r (t) = o (t), since

t
t-1w^ r (t) = t-1w^ w (0) -  L (w (u)) - w^ log (t) - w~

 o (1) - t-1w^

u=0

t


t

 L (w (u)) +

L (w (u))



u=0

 u= t +1

 o (1) - t-1



t max w^ L (w (u)) + t max w^ L (w (u))

u t

t ut

 o (1)

where in the last line we used that L (w (t)) = o (1), from Lemma 5.

13

Under review as a conference paper at ICLR 2018

Next, we upper bound the second term in eq. A.18, t > t+:

-
n/S

w (t) xn xn r (t)

-

w (t) xn xn r (t)

n/S: xn r(t)0

(1)
  1 + exp - +w (t) xn
n/S: xn r(t)0

exp -w (t) xn xn r (t)

(2)
  1 + t- +xn w^ exp - +w~ xn - xn r (t)
n/S: xn r(t)0

t-xn w^ exp -w~ xn - xn r (t) xn r (t)

(3)
  1 + t- +xn w^ exp - +w~ xn t-xn w^ exp -w~ xn

n/S: xn r(t)0

(4)
 N 1 + t- exp - min w~ xn

+ exp - min w~ xn t-

nn

2N exp

- min w~
n

xn

t- , t > t+

(A.20)

where in (1) we used eq. A.16, in (2) we used w (t) = w^ log t + w~ + r (t), in (3) we used xe-x  1 and xn r (t)  0, in (4) we used  > 1, from eq. A.1 and in (5) we defined t+ = max t+, exp minn w~ xn .

Lastly, the third term in eq. A.18 can be upper bounded by

 |S| max -
nS~

w (t) xn - t-1 exp -w~ xn xn r (t) .

(A.21)

where S~ n  S|t : xn r (t) > 0 ,
and k is the index that maximizes the expression in eq. A.21. Additionally, if P1r (t)  1 (as in Eq. A.5), we have that

min
nS

2 (1)

xn r (t)

= min
nS

xn P1r (t)

2

(2)


m2 in

(XS )

2 1

2>0

(A.22)

where in (1) we used P1 xn = xn n  S, in (2) we denoted by min (XS ), the minimal non-zero singular value of XS and used eq. A.5. Thus, the condition P1r (t)  1 implies that the set S~ is not empty. Using these notations, we upper bound eq. A.21 in various cases:

First, if xk r (t)  0, then eq. A.21 can be upper bounded t > t+, using eq. A.16, by t-1 |S| exp -w~ xk 1 + t- + exp - +w~ xn exp -xk r (t) - 1 xk r (t)
We divide into cases:

(A.23)

1. If xk r  C0t-0.5 + , then we can upper bound eq. A.23 with

 |S| exp

-w~

xk

exp

- + max w~
n

xn

C0t-1-1.5 + .

(A.24)

2. If xk r > C0t-0.5 + , then we can find t+ > t+ to upper bound eq. A.23 t > t+:

t-1 |S| e-w~ xk 1 + t- + e- +w~ xk exp -C0t-0.5 + - 1 xk r (t)

t-1 |S| e-w~ xk 1 + t- + e- +w~ xk 1 - C0t-0.5 + + C02t-1.5 + - 1 xk r (t)

(1)
 t-1 |S| e-w~ xk 1 - C0t-0.5 + + C02t-1.5 + e- + minn w~ xn t- + - C0t-0.5 + xk r (t)

(2)
 0, t > t+

(A.25)

where in (1) we use the fact that e-x  1 - x + x2 and in (2) we define t+ so that the previous expression is negative -- this is possible since t-0.5 + decreases slower then t- + .

14

Under review as a conference paper at ICLR 2018

3. If P1r (t)  1 (as in Eq. A.5), then eq. A.22 can be used to upper bound eq. A.23 by

-  |S| exp - max w~ xn 1 - 1 + t- + exp - + min w~ xn e- 2 2t-1 .
nn

 -  |S| exp

- max w~
n

xn

1 - e-0.5 2

2t-1, t > t+

(A.26)

Where we defined t+ > t+ such that t+ > exp minn w~ xn e0.5 2 - 1 -1/ + , and therefore t > t+ , we have 1 + t- + exp - +w~ xn e- 2 < e-0.5 2 .

Second, if xk r < 0, we again divide into cases:

1. If xk r  C0t-0.5 - , then, since - w (t) xn > 0, we can upper bound eq. A.21 with

t-1 |S| exp -w~ xk

xk r (t)

  |S| exp

- min w~
n

xn

C0t-1-0.5 -

(A.27)

2. If xk r > C0t-0.5 - , then, using eq. A.17, t > t-, we can find t- > t- such that upper bound eq. A.21 with zero t > t-, since

 |S| -t-1e-w~ xk - w (t) xk xk r (t)

 |S| -t-1e-w~ xk + 1 - exp - -w (t) xn exp -w (t) xn xk r (t)

= |S| e-w~ xk t-1 1 - 1 - e- -w~ xk t- - exp C0t-0.5 - C0t-0.5 -

(1)
 C0 |S| e-w~ xk t-1-0.5 - 1 - 1 - e- -w~ xk 1 + C0t-0.5 -

=C0 |S| e-w~ xk t-1-0.5 - t- - e- -w~ xk t- - - C0t-0.5 - + C0e-w~ xk t-1-0.5 -

<0 , t > t-

(A.28)

where in (1) we used x > 0 : ex  1 + x, and in the last line we were able to find t- such that this expression is negative since t-0.5 - decreases slower then t- - , such that this

expression is negative given sufficiently large t.

3. If P1r (t)  1 then, using eqs. A.17 and A.22, t > t-, we can find t- > t- and C0 such that we can upper bound eq. A.21 t > t-, with  |S| -t-1 exp -w~ xk + 1 - exp - -w (t) xn exp -w (t) xn xk r (t)  - t-1 exp -w~ xk  |S| 1 - t- - exp - -w~ xk exp ( 2) - 1 2

 - t-1 exp - max w~ xn  |S| exp ( 2) - 1 - t- - exp
n
 - C0t-1 , t > t-

2-

-

min
n

w~

x

2.

(A.29)

To conclude, we choose t0 = max t+ , t- :

1. If P1r (t)  1, we denote C0 as the minimum between C0 (eq. A.29) and  |S| exp - maxn w~ xn 1 - e-0.5 2 2 (eq. A.26). Then we find that eq. A.21 can be upper bounded by -C0 t-1, t > t0, given eq. A.5. Substituting this result, together with eqs. A.19 and A.20 into eq. A.18, we obtain t > t0
(r (t + 1) - r (t)) r (t)  -C0 t-1 + o t-1 . This implies that C2 < C0 and t2 > t0 such that eq. A.5 holds. This implies also that eq. A.4 holds for P1r (t)  1.
2. If P1r (t) < 1, we find that t > t0 , eq. A.21 can be upper bounded by either zero (eqs. A.25 and A.28), or terms proportional to t-1-1.5 + (eq. A.24) or t-1-0.5 - , (eq. A.27). Combining this together with eqs. A.19, A.20 into eq. A.18 we obtain (for some positive constants (C3, C4, C5,C6)
(r (t + 1) - r (t)) r (t)  C3t-1-1.5 + + C4t-1-0.5 - + C5t-2 + C6t- . Therefore, t1 > t0 and C1 such that eq. A.4 holds.

15

Under review as a conference paper at ICLR 2018

B CALCULATION OF CONVERGENCE RATES

From Theorem 3, we can write w (t) = w^ log t +  (t), where  (t) has bounded norm. Calculation of normalized weight vector (eq. 3.1):

w (t) w (t)
=  (t)

 (t) + w^ log t  (t) + w^ w^ log2 t + 2 (t)

w^ log t

= w^

 (t) / log t + w^ 1 + 2 (t) w^ / w^ 2 log t +  (t) 2 /

w^ 2 log2 t

1 =
w^



1  (t) + w^
log t

1 -

 (t) w^ 3 w^ 2 log t +  4

 (t) w^ w^ 2

2
-

  (t) 2 1
2 w^ 2  log2 t

+O


1 log3 t 

(B.1)

w^ =+
w^

 (t) - w^

w^ w^

 (t) w^ w^ 2

1 +O
log t

1 log2 t

=

w^ w^

+

I

-

w^ w^ w^

2

 (t) 1

1

+O w^ log t

log2 t

where

to

obtain

eq.

B.1

we

used

1 1+x

=

1

-

1 2

x

+

3 4

x2

+

O

x3

, and in the last line we used the

fact that  (t) has a bounded norm.

We use eq. 3.1 to calculate of angle (eq. 3.2):

w (t) w^ w (t) w^

w^ 1

= w^ 2

 (t) + w^ log t

 1  (t) w^ 3
1 - log t w^ 2 +  4

2
 (t) w^

  (t) 2 1

w^ 2 - 2 w^ 2  log2 t + O


1 log3 t 



=1 +

 2

(t) w^

2 2

1

-

3 2

 (t) w^ w^  (t)

2 11
 log2 t + O log3 t

Calculation of margin (eq. 3.3):

min
n

xn

w^

(t)

w^

=

min
n

xn

+ w^

 (t) - w^

w^ w^

 (t) w^ w^ 2

1 +O
log t

1 log2 t

11 =+
w^ w^

minn xn  (t) w^

-

 (t) w^

w^
2

1 +O
log t

1 log2 t

where in eq. B.2 we used eq. A.1.

16

(B.2)

Under review as a conference paper at ICLR 2018

Calculation of the training loss (eq. 3.4):

N

L (w (t)) 

1 + exp - +w (t) xn exp -w (t) xn

n=1

N

= 1 + exp - + ( (t) + w^ log t) xn exp - ( (t) + w^ log t) xn

n=1

N
= 1 + t- +w^ xn exp - + (t) xn exp - (t) xn t-w^ xn

n=1

1 =

e-(t) xn + O t- max(,1+ +) .

t

nS

Next, we give an example demonstrating the bounds above are strict. Consider optimization with and exponential loss (u) = e-u, and a single data point x = (1, 0). In this case w^ = (1, 0) and w^ = 1. We take the limit   0, and obtain the continuous time version of GD:
w 1 (t) = exp (-w (t)) ; w 2 (t) = 0.
We can analytically integrate these equations to obtain
w1 (t) = log (t + exp (w1 (0))) ; w2 (t) = w2 (0) .

Using this example with w2 (0) > 0, it is easy to see that the above upper bounds are strict. Lastly, recall that V is a set of indices for validation set samples. We calculate of the validation loss for logistic loss, if the error of the L2 max margin vector has some classification errors on the validation, i.e., k  V : w^ xk < 0:
Lval (w (t)) = log 1 + exp -w (t) xn
nV
 log 1 + exp -w (t) xk
= log 1 + exp - ( (t) + w^ log t) xk
= log exp - ( (t) + w^ log t) xk 1 + exp ( (t) + w^ log t) xk
 - ( (t) + w^ log t) xk + log 1 + exp ( (t) + w^ log t) xk
 - log tw^ xk +  (t) xk

C SOFTMAX OUTPUT WITH CROSSENTROPY LOSS

We examine multiclass classification. In the case the labels are the class index yn  {1, . . . , K} and we have a weight matrix W  RK×d with wk being the k-th row of W.
Furthermore, we define w = vec W , a basis vector ek  RK so that(ek)i = ki, and the matrix Ak  RdK×d so that Ak = ek  Id, where  is the Kronecker product and Id is the d-dimension identity matrix. Note that Ak w = wk.
Consider the cross entropy loss with softmax output

N
L (W) = - log
n=1

exp wyn xn

K k=1

exp

wk xn

17

Under review as a conference paper at ICLR 2018

Using our notation, this loss can be re-written as

N
L (w) = - log
n=1

exp w Ayn xn

K k=1

exp

(w

Ak xn )

NK

Therefore

= log

exp w (Ak - Ayn ) xn

n=1

k=1

N
L (w) =
n=1

K k=1

exp

w

(Ak - Ayn ) xn (Ak - Ayn ) xn

K r=1

exp

(w

(Ar - Ayn ) xn)

NK
=
n=1 k=1

1

K r=1

exp

(w

(Ar - Ak) xn) (Ak - Ayn ) xn .

(C.1)

If, again, make the assumption that the data is strictly linearly separable, i.e., in our notation

Assumption 4. w such that w (Ak - Ayn ) xn < 0 k = yn.

then the expression

NK
w L (w) =
n=1 k=1

w (Ak

K r=1

exp

(w

- Ayn ) (Ar -

xn Ak )

xn)

.

is strictly negative for any finite w. However, from Lemma 5, in gradient descent with learning

rate  > 2-1, we have that L (w (t))  0. This implies that: w (t)  , and k =

yn, r : w (t) (Ar - Ak) xn  , which implies k = yn, maxk w (t) (Ak - Ayn ) xn  -. Examining the loss (eq. C.1) we find that L (w (t))  0 in this case. Thus, we arrive to an
equivalent Lemma to Lemma 1, for this case:
Lemma 7. Let w (t) be the iterates of gradient descent (eq. 2.2) with  < 2-1, for crossentropy loss
operating on a softmax output, under the assumption of strict linear separability (Assumption 4), then:

(1) limt L (w (t)) = 0, (2) limt w (t) = , (3) n : limt w (t) (Ayn - Ak) xn >
0, and (4) t0: t > t0: w (t) (Ayn - Ak) xn > 0, that is, within a finite time we reach a predictor that perfectly separates the data.

Therefore, since
N
L (w (t)) = log
n=1

K
exp w (t)
k=1

(Ak - Ayn ) xn

N

 log
n=1

1 + max exp
k=yn

w (t)

(Ak - Ayn ) xn

,

(C.2)

where in the last line we assumed that (Ak - Ayn ) xn has a unique minimum in k (which is true if xn are in general position), since then the other exponential terms inside the log become negligible. If

argmax exp w (t) (Ak - Ayn ) xn
k=yn
has a limit kn, then we define x~n = (Ayn - Akn ) xn, so eq. C.2 is transformed to the standard logistic regression loss
N
log 1 + exp -w (t) x~n ,
n=1
to which our Theorems directly apply.

Therefore, w (t) / w (t)  w^ where

w^ = argmin w 2 s.t. w x~n  1
w

Recalling that Ak w = wk, we can re-write this as

K

arg min
w1 ,...,wK

wk 2 s.t. n, k = yn : wyn xn  wk xn + 1

k=1

18

Under review as a conference paper at ICLR 2018

D DEEP NETWORKS, IF ONLY A SINGLE LAYER IS OPTIMIZED

We examine a deep neural network (DNN) with m = 1, . . . , L layers, piecewise linear activation
functions fl and loss function following assumption 3, parameterized by weights matrices Wl. Since fl are piecewise linear, we can write for almost every u: fl (u) = fl (u) u (an element-wise product). Given an input sample xn, for each layer l the input un,l and output vn,l are calculated sequentially in a "forward propagation"

un,l =vn,l-1Wl ; vn,l-1 = fl (un,l)

(D.1)

initialized by vn,0 = xn. Then, given the DNN output un,L and target yn  {-1, 1} the loss (ynun,L) can be calculated.

During training, the gradients of the loss are calculated using the chain rule in a "back-propagation"

n,l-1 = [fl (un,l)] Wl n,l

(D.2)

initialized by n,L = 1. Finally, the weights are updated with GD. The basic update (without weight sharing) is

N Wl (t + 1) - Wl (t) = - n=1 Wl (ynun,L)

(D.3)

N
= - yn (ynun,L) n,lvn,l-1 .
n=1

(D.4)

N
= - yn
n=1

ynn,lWlvn,l-1 . n,lvn,l-1 ,

(D.5)

where in the last line we used

L-1

l : un,L = WL

fl (un,m) Wm xn = n,lWlvn,l-1 .

m=1

Denoting x~n,l = ynn,l  vn,l-1 and wl = vec Wl we obtain

N

wl (t + 1) - wl (t) = -

wl x~n,l x~n,l .

n=1

We got the same update as in eq. 2.2. Thus, if x~n,l does not change between iterations and becomes linearly separable so the training error can go to zero, we can apply Theorem 3. This can happen if
optimize only Wl, and the activation units stop crossing their thresholds, after a sufficient number of iterations.

E AN EXPERIMENT WITH STOCHASTIC GRADIENT DESCENT

x
2

(A) 3
2
1
0
-1
-2
-3 -3 -2 -1 0 1 2 3
x
1

(B) 1

Normalized ||w(t)||

0.5

0 100 101 102 103 104 105

(D) x 10-3

t

3

Angle gap

2

1

0 100 101 102 103 104 105
t

L(w(t))

(C) 100

10-5

GD

10-10

GDMO

100 101 102 103 104 105

(E) t

0.06

Margin gap

0.04

0.02

0 100 101 102 103 104 105
t

Figure E.1: Same as Fig. 3.1, except stochastic gradient decent is used (with mini-batch of size 4), instead of GD.

19

