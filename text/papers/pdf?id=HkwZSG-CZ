Under review as a conference paper at ICLR 2018
BREAKING THE SOFTMAX BOTTLENECK: A HIGH-RANK RNN LANGUAGE MODEL
Anonymous authors Paper under double-blind review
ABSTRACT
We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively.
1 INTRODUCTION
As a fundamental task in natural language processing, statistical language modeling has gone through significant development from traditional Ngram language models to neural language models in the last decade (Bengio et al., 2003; Mnih & Hinton, 2007; Mikolov et al., 2010). Despite the huge variety of models, as a density estimation problem, language modeling mostly relies on a universal auto-regressive factorization of the joint probability and then models each conditional factor using different approaches. Specifically, given a corpus of tokens X = (X1, . . . , XT ), the joint probability P (X) factorizes as P (X) = t P (Xt | X<t) = t P (Xt | Ct), where Ct = X<t is referred to as the context of the conditional probability hereafter.
Based on the factorization, recurrent neural networks (RNN) based language models achieve stateof-the-art results on various benchmarks (Merity et al., 2017; Melis et al., 2017; Krause et al., 2017). A standard approach is to use a recurrent network to encode the context into a fixed size vector, which is then multiplied by the word embeddings (Inan et al., 2016) using dot product to obtain the logits. The logits are consumed by the Softmax function to give a categorical probability distribution over the next token. In spite of the expressiveness of RNNs as universal approximators (Schäfer & Zimmermann, 2006), an unclear question is whether the combination of dot product and Softmax is capable of modeling the conditional probability, which can vary dramatically with the change of the context.
In this work, we study the expressiveness of the aforementioned Softmax-based recurrent language models from a perspective of matrix factorization. We show that learning a Softmax-based recurrent language model with the standard formulation is essentially equivalent to solving a matrix factorization problem. More importantly, due to the fact that natural language is highly context-dependent, the matrix to be factorized can be high-rank. This further implies that standard Softmax-based language models with distributed (output) word embeddings do not have enough capacity to model natural language. We call this the Softmax bottleneck.
We propose a simple and effective method to address the Softmax bottleneck. Specifically, we introduce discrete latent variables into a recurrent language model, and formulate the next-token probability distribution as a Mixture of Softmaxes (MoS). Mixture of Softmaxes is more expressive than Softmax and other surrogates considered in prior work. Moreover, we show that MoS learns matrices that have much larger normalized singular values and thus much higher rank than Softmax and other baselines on real-world datasets.
We evaluate our proposed approach on standard language modeling benchmarks. MoS substantially improves over the current state-of-the-art results on benchmarks, by up to 3.6 points in terms of
1

Under review as a conference paper at ICLR 2018

perplexity, reaching perplexities 47.69 on Penn Treebank and 40.68 on WikiText-2. We further apply MoS to a dialog dataset and show improved performance over Softmax and other baselines.
Our contribution is two-fold. First, we identify the Softmax bottleneck by formulating language modeling as a matrix factorization problem. Second, we propose a simple and effective method that substantially improves over the current state-of-the-art results.

2 LANGUAGE MODELING AS MATRIX FACTORIZATION

As discussed in Section 1, with the autoregressive factorization, language modeling can be reduced to modeling the conditional distribution of the next token x given the context c. Though one might argue that a natural language allows an infinite number of contexts due to its compositionality (Pinker, 1994), we proceed with our analysis by considering a finite set of possible contexts. The unboundedness of natural language does not affect our conclusions, which will be discussed later.
We consider a natural language as a finite set of pairs of a context and its conditional next-token distribution1 L = {(c1, P (X|c1)), · · · , (cN , P (X|cN ))}, where N is the number of possible contexts. We assume P  > 0 everywhere to account for errors and flexibility in natural language. Let {x1, x2, · · · , xM } denote a set of M possible tokens in the language L. The objective of a language model is to learn a model distribution P(X|C) parameterized by  to match the true data distribution P (X|C).
In this work, we study the expressiveness of the parametric model class P(X|C). In other words, we are asking the following question: given a natural language L, does there exist a parameter  such that P(X|c) = P (X|c) for all c in L?
We start by looking at a Softmax-based model class since it is widely used.

2.1 SOFTMAX

The majority of parametric language models use a Softmax function operating on a context vector (or hidden state) hc and a word embedding wx to define the conditional distribution P(x|c). More specifically, the model distribution is usually written as

P(x|c) =

exp hc wx x exp hc wx

(1)

where hc is a function of c, and wx is a function of x. Both functions are parameterized by . Both the context vector hc and the word embedding wx have the same dimension d. The dot product hc wx is called a logit.

To help discuss the expressiveness of Softmax, we define three matrices:

 hc1 

H

=

  

hc2 ···

  

;

 wx1 

W

=

  

wx2 ···

 

;



 log P (x1|c1),

A

=

  

log

P

(x1|c2), ...

log P (x2|c1) log P (x2|c2)
...

··· ···
...

log P (xM |c1) 

log P (xM |c2) 

...

 

hcN

wxM

log P (x1|cN ), log P (x2|cN ) · · · log P (xM |cN )

where H  RN×d, W  RM×d, A  RN×M , and the rows of H, W, and A correspond to context vectors, word embeddings, and log probabilities of the true data distribution respectively.
We use the subscript  because (H, W) is effectively a function indexed by the parameter , from the joint function family U. Concretely, H is implemented as deep neural networks, such as a recurrent network, while W is instantiated as an embedding lookup.

We further specify a set of matrices formed by applying row-wise shift to A

F (A) = {A + JN,M | is diagonal and   RN×N },

where JN,M is an all-ones matrix with size N × M . Essentially, the row-wise shift operation adds an arbitrary real number to each row of A. Thus, F (A) is an infinite set. Notably, the set F (A) has
two important properties (see Appendix for the proof), which are key to our analysis.

1We use capital letters for variables and small letters for constants.

2

Under review as a conference paper at ICLR 2018
Property 1. For any matrix A , A  F (A) if and only if Softmax(A ) = P . In other words, F (A) defines the set of all possible logits that correspond to the true data distribution. Property 2. For any A1 = A2  F (A), |rank(A1) - rank(A2)|  1. In other words, all matrices in F (A) have similar ranks, with the maximum rank difference being 1.
Based on the Property 1 of F (A), we immediately have the following Lemma. Lemma 1. Given a model parameter , HW  F (A) if and only if P(X|c) = P (X|c) for all c in L.
Now the expressiveness question becomes: does there exist a parameter  and A  F (A) such that
HW = A . This is essentially a matrix factorization problem. We want the model to learn matrices H and W that are able to factorize some matrix A  F (A). First, note that for a valid factorization to exist, the rank of HW has to be at least as large as the rank of A . Further, since H  RN×d and W  RM×d, the rank of HW is strictly upper bounded by the embedding size d. As a result, if d  rank(A ), a universal approximator can theoretically recover A . However, if d < rank(A ), no matter how expressive the function family U is, no (H, W) can even theoretically recover A . We summarize the reasoning above as follows (see Appendix for the proof). Proposition 1. Given that the function family U is a universal approximator, there exists a parameter  such that P(X|c) = P (X|c) for all c in L if and only if d  minA F (A) rank(A ).
Combining Proposition 1 with the Property 2 of F (A), we are now able to state the Softmax Bottleneck problem formally. Corollary 1. (Softmax Bottleneck) If d < rank(A) - 1, for any function family U and any model parameter , there exists a context c in L such that P(X|c) = P (X|c).
The above corollary indicates that when the dimension d is too small, Softmax does not have the capacity to express the true data distribution. Clearly, this conclusion does not restrict to a finite language L. When L is infinite, one can always take a finite subset and the Softmax bottleneck still exists. Next, we discuss why the Softmax bottleneck is an issue by presenting our hypothesis that A is high-rank for natural language.
2.2 HYPOTHESIS: NATURAL LANGUAGE IS HIGH-RANK
We hypothesize that for a natural language L, the log probability matrix A is a high-rank matrix. It is difficult (if possible) to rigorously prove this hypothesis since we do not have access to the true data distribution of a natural language. However, it is suggested by the following observations:
· Natural language is highly context-dependent (Mikolov & Zweig, 2012). For example, the token "north" is likely to be followed by "korea" or "korean" in a news article on international politics, which however is unlikely in a textbook on U.S. domestic history. Such subtle context dependency should result in a high-rank matrix A, because it would be hard to find a set of bases such that the conditional log probabilities can always be expressed as a linear combination of the bases.
· If A is low-rank, it means humans only need a limited number (e.g. a few hundred) of distinct basic semantic meanings, and all other semantic meanings can be created by (potentially) negating and (weighted) averaging these basic meanings. However, a few hundred meanings may not be enough to cover everyday meanings, not to mention niche meanings in specialized domains. Also, there is no evidence showing that semantic meanings are fully linearly correlated.
· Empirically, our high-rank language model outperforms conventional low-rank language models on several benchmarks, as shown in Section 3.
Given the hypothesis that natural language is high-rank, it is clear that the Softmax bottleneck limits the expressiveness of the models. In practice, the embedding dimension d is usually set at the scale of 102, while the rank of A can possibly be as high as M (at the scale of 105), which is orders of magnitude larger than d. Softmax is effectively learning a low-rank approximation to A, and our experiments suggest that such approximation loses the ability to model context dependency, both qualitatively and quantitatively (Cf. Section 3).
3

Under review as a conference paper at ICLR 2018

2.3 EASY FIXES?
Identifying the Softmax bottleneck immediately suggests some possible "easy fixes". First, as considered by a lot of prior work, one can employ a non-parametric model, namely an Ngram model (Kneser & Ney, 1995). Ngram models are not constrained by any parametric forms so it can universally approximate any natural language, given enough parameters. Second, it is possible to increase the dimension d (e.g., to match M ) so that the model can express a high-rank matrix A.
However, these two methods increase the number of parameters dramatically, compared to using a low-dimensional Softmax. More specifically, an Ngram needs (N × M ) parameters in order to express A, where N is potentially unbounded. Similarly, a high-dimensional Softmax requires (M × M ) parameters for the word embeddings. Increasing the number of model parameters easily leads to overfitting. In past work, Kneser & Ney (1995) used back-off to alleviate overfitting. Moreover, as deep learning models were tuned by extensive hyper-parameter search, increasing the dimension d beyond several hundred is not helpful2 (Merity et al., 2017; Melis et al., 2017; Krause et al., 2017).
Clearly there is a tradeoff between expressiveness and generalization on language modeling. Naively increasing the expressiveness hurts generalization. Below, we introduce an alternative approach that increases the expressiveness without exploding the parametric space.

2.4 MIXTURE OF SOFTMAXES: A HIGH-RANK LANGUAGE MODEL

We propose a high-rank language model called Mixture of Softmaxes (MoS) to alleviate the Softmax bottleneck issue. MoS formulates the conditional distribution as

K
P(x|c) = c,k
k=1

exp hc,kwx ; x exp hc,kwx

K
s.t. c,k = 1
k=1

where c,k is the prior or mixture weight of the k-th component, and hc,k is the k-th context vec-

tor associated with context c. In other words, MoS computes K Softmax distributions and uses a

weighted average of them as the next-token probability distribution. Similar to prior work on re-

current language modeling (Merity et al., 2017; Melis et al., 2017; Krause et al., 2017), we first

apply a stack of recurrent layers on top of X to obtain a sequence of hidden states (g1, · · · , gT ).

The prior and the context vector for context ct are parameterized as ct,k =

andexp w,kgt

K k

=1

exp

w
,k

gt

hct,k = tanh(Wh,kgt) where w,k and Wh,k are model parameters.

Our method is simple and easy to implement, and has the following advantages:

· Improved expressiveness (compared to Softmax). MoS is theoretically more (or at least equally) expressive compared to Softmax given the same dimension d. This can be seen by the fact that MoS with K = 1 is reduced to Softmax. More importantly, MoS effectively approximates A by
K
A^ MoS = log k exp(H,kW )
k=1
where k is an (N × N ) diagonal matrix with elements being the prior c,k. Because A^ MoS is a nonlinear function (log_sum_exp) of the context vectors and the word embeddings, A^ MoS can be arbitrarily high-rank. As a result, MoS does not suffer from the rank limitation, compared to Softmax.
· Improved generalization (compared to Ngram). Ngram models and high-dimensional Softmax (Cf. Section 2.3) improve the expressiveness but do not generalize well. In contrast, MoS does not have a generalization issue due to the following reasons. First, MoS defines the following generative process: a discrete latent variable k is first sampled from {1, · · · , K}, and then the next token is sampled based on the k-th Softmax component. By doing so we introduce an inductive bias that the next token is generated based on a latent discrete decision (e.g., a topic), which is often safe in language modeling (Blei et al., 2003). Second, since A^ MoS is defined by a nonlinear function and not restricted by the rank bottleneck, in practice it is possible to reduce
2This is also confirmed by our preliminary experiments.

4

Under review as a conference paper at ICLR 2018

d to compensate for the increase of model parameters brought by the mixture structure. As a result, MoS has a similar model size compared to Softmax and thus is not prone to overfitting.

2.5 MIXTURE OF CONTEXTS: A LOW-RANK BASELINE

Another possible approach is to directly mix the context vectors (or logits) before taking the Softmax, rather than mixing the probabilities afterwards as in MoS. Specifically, the conditional distribution is parameterized as

P(x|c) =

exp x exp

K k=1

c,k hc,k

K k=1

c,k hc,k

wx wx

=

exp x exp

K k=1

c,k

hc,k

wx

K k=1

c,k hc,k

wx

,

(2)

where hc,k and c,k share the same parameterization as in MoS. Despite its superficial similarity to

MoS, this model, which we refer to as mixture of contexts (MoC), actually suffers from the same

rank limitation problem as Softmax. This can be easily seen by defining h c =

K k=1

c,k hc,k

,

which turns the MoC parameterization (2) into P(x|c) =

exp h c wx x exp h c wx

.

Note

that

this

is

equiv-

alent to the Softmax parameterization (1). Thus, performing mixture in the feature space can only

make the function family U more expressive, but does not change the fact that the rank of HW is upper bounded by the embedding dimension d. In our experiments, we implement MoC as a

baseline and compare it experimentally to MoS.

3 EXPERIMENTS
Following previous work (Krause et al., 2017; Merity et al., 2017; Melis et al., 2017), we evaluate the proposed MoS model on two widely used language modeling datasets, namely Peen Treebank (PTB) (Mikolov et al., 2010) and WikiText-2 (WT2) (Merity et al., 2016) based on perplexity. For fair comparison, we closely follow the regularization and optimization techniques introduced by Merity et al. (2017). We conduct hyper-parameter search for MoS based on the validation performance while limiting the model size (see Appendix for our hyper-parameters).
To show that the MoS is a generic structure that can be used to model other context-dependent distributions, we additionally conduct experiments in the dialog domain. We use the Switchboard dataset (Godfrey & Holliman, 1997) preprocessed by Zhao et al. (2017)3 to train a Seq2Seq (Sutskever et al., 2014) model with MoS added to the decoder RNN. Then, a Seq2Seq model using Softmax and another one augmented by MoC with comparable parameter sizes are used as baselines. For evaluation, we include both the perplexity and the precision/recall of Smoothed Sentence-level BLEU, as suggested by Zhao et al. (2017). When generating responses, we use beam search with beam size 10, restrict the maximum length to 30, and retain the top-5 responses.
Main Results
The language modeling results on PTB and WT2 are presented in Table 1 and Table 2 respectively. With a comparable number of parameters, MoS outperforms all baselines with or without dynamic evaluation, and substantially improves over the current state of the art, by up to 3.6 points in perplexity.
Further, the experiment result for Switchboard is summarized in Table 34. Clearly, on all metrics, MoS outperforms MoC and Softmax, showing its general effectiveness.
Ablation study
To further verify the improvement shown above does come from the MoS structure rather than adding another hidden layer or finding a particular set of hyper-parameters, we conduct an ablation study on both PTB and WT2. Firstly, we compare MoS with an MoC architecture with the same number of layers, hidden sizes, and embedding sizes, which thus has the same number of parameters. In addition, we adopt the hyper-parameters used to obtain the best MoS model (denoted as
3https://github.com/snakeztc/NeuralDialog-CVAE/tree/master/data 4The numbers are not directly comparable to Zhao et al. (2017) since their Seq2Seq implementation and evaluation scripts are not publicly available.

5

Under review as a conference paper at ICLR 2018

Model
Mikolov & Zweig (2012) ­ RNN-LDA + KN-5 + cache Zaremba et al. (2014) ­ LSTM Gal & Ghahramani (2016) ­ Variational LSTM (MC) Kim et al. (2016) ­ CharCNN Merity et al. (2016) ­ Pointer Sentinel-LSTM Grave et al. (2016) ­ LSTM + continuous cache pointer Inan et al. (2016) ­ Tied Variational LSTM + augmented loss Zilly et al. (2016) ­ Variational RHN Zoph & Le (2016) ­ NAS Cell Melis et al. (2017) ­ 2-layer skip connection LSTM
Merity et al. (2017) ­ AWD-LSTM w/o finetune Merity et al. (2017) ­ AWD-LSTM Ours ­ AWD-LSTM-MoS w/o finetune Ours ­ AWD-LSTM-MoS
Merity et al. (2017) ­ AWD-LSTM + continuous cache pointer Krause et al. (2017) ­ AWD-LSTM + dynamic evaluation Ours ­ AWD-LSTM-MoS + dynamic evaluation

#Param
9M 20M 20M 19M 21M
24M 23M 25M 24M
24M 24M 22M 22M
24M 24M 22M

Validation
86.2
72.4 75.7 67.9 60.9
60.7 60.0 58.08 56.54
53.9 51.6 48.33

Test
92.0 82.7 78.6 78.9 70.9 72.1 73.2 65.4 64.0 58.3
58.8 57.3 55.97 54.44
52.8 51.1 47.69

Table 1: Single model perplexity on validation and test sets on Penn Treebank. Baseline results are obtained from Merity et al. (2017) and Krause et al. (2017).  indicates using dynamic evaluation.

Model
Inan et al. (2016) ­ Variational LSTM + augmented loss Grave et al. (2016) ­ LSTM + continuous cache pointer Melis et al. (2017) ­ 2-layer skip connection LSTM
Merity et al. (2017) ­ AWD-LSTM w/o finetune Merity et al. (2017) ­ AWD-LSTM Ours ­ AWD-LSTM-MoS w/o finetune Ours ­ AWD-LSTM-MoS
Merity et al. (2017) ­ AWD-LSTM + continuous cache pointer  Krause et al. (2017) ­ AWD-LSTM + dynamic evaluation Ours ­ AWD-LSTM-MoS + dynamical evaluation

#Param
28M -
24M
33M 33M 35M 35M
33M 33M 35M

Validation
91.5 -
69.1
69.1 68.6 66.01 63.88
53.8 46.4 42.41

Test
87.0 68.9 65.9
66.0 65.8 63.33 61.45
52.0 44.3 40.68

Table 2: Single model perplexity over WikiText-2. Baseline results are obtained from Merity et al. (2017) and Krause et al. (2017).  indicates using dynamic evaluation.

MoS hyper-parameters), and train a baseline AWD-LSTM. To avoid distractive factors and save computational resources, all ablative experiments are based on models without finetuing or dynamic evaluation.
The results are shown in Table 4. Compared to the vanilla AWD-LSTM, though being more expressive, MoC performs only better on PTB, but worse on WT2. It suggests that simply adding another hidden layer or employing a mixture structure in the feature space does not guarantee a better performance. On the other hand, training AWD-LSTM using MoS hyper-parameters severely hurts the performance, which rules out hyper-parameters as the main source of improvement. Moreover, MoC is consistently outperformed by MoS, which well matches our theoretical analysis in Section 2 and shows the benefits of a high-rank language model.
Quantitative analysis
We take a step further to examine whether the empirical observation matches our analysis in Section 2 quantitatively.
Firstly, we directly study the empirical log-probability matrices induced by different models ­ MoS, MoC, and Softmax. On the validation or test set of PTB with tokens X = {X1, . . . , XT }, we compute all the log probabilities {log P (Xi | X<i)  RM }Tt=1 for each token using all three models. Then, for each model, we stack all T log-probability vectors into a T × M matrix, resulting
6

Under review as a conference paper at ICLR 2018

Model
Seq2Seq-Softmax Seq2Seq-MoC Seq2Seq-MoS

Perplexity
34.657 33.291 32.727

BLEU-1 prec recall
0.249 0.188 0.259 0.198 0.272 0.206

BLEU-2 prec recall
0.193 0.151 0.202 0.159 0.213 0.166

BLEU-3 prec recall
0.168 0.133 0.176 0.140 0.185 0.146

BLEU-4 prec recall
0.141 0.111 0.148 0.117 0.157 0.123

Table 3: Evaluation scores on Switchboard.

Model
AWD-LSTM-MoS AWD-LSTM-MoC AWD-LSTM (Merity et al. (2017) hyper-parameters) AWD-LSTM (MoS hyper-parameters)

PTB Validation Test

58.08 59.82 61.49 78.86

55.97 57.55 58.95 74.86

WT2 Validation Test

66.01 68.76 68.73 72.73

63.33 65.98 65.40 69.18

Table 4: Ablation study on Penn Treebank and WikiText-2 without finetuning or dynamical evaluation.

in A^ MoS, A^ MoC and A^ Softmax. Theoretically, the number of non-zero singular values of a matrix is equal to its rank. However, performing singular value decomposition of real valued matrices using numerical approaches often encounter roundoff errors. Hence, we adopt the expected roundoff error suggested by Press (2007) when estimating the ranks of A^ MoS, A^ MoC and A^ Softmax.
The estimated ranks are shown in the left half of Table 5. As predicted by our theoretical analysis, the matrix ranks induced by Softmax and MoC are both limited by the corresponding embedding sizes. By contrast, the matrix rank obtained from MoS does not suffer from this constraint, almost reaching full rank (M = 10000).
In addition, we visualize the distribution of the singular values. To account for the different magnitudes of singular values from different models, we first normalize all singular values to [0, 1]. Then, we plot the cumulative percentage of normalized singular values, i.e., percentage of normalized singular values below a threshold, in Figure 1. As we can see, most of the singular values of Softmax and MoC concentrate on an area with very low values. In comparison, the concentration area of the MoS singular values is not only several orders larger, but also spans a much wider region. Intuitively, MoS utilizes the corresponding singular vectors to capture a larger and more diverse set of contexts.
What's more, if a model can better capture the distinctions among contexts, we expect the next-step conditional distributions to be less similar to each on average. Based on this intuition, we use the expected pairwise Kullback­Leibler divergence (KLD), i.e., Ec,c C [KLD(P (X | c) P (X | c ))] where C denotes all possible contexts, as another metric to evaluate the three models. Practically, we sample c, c from validation or test data of PTB to get the empirical estimations for the three models, which are shown in the right half of Table 5. As we expected, MoS achieves higher expected pairwise KLD, indicating its superiority in covering more contexts of the next-token distribution.
Qualitative analysis
Finally, we conduct a qualitative study on PTB to see how MoS improves the next-token prediction in detail. Since MoC shows a stronger performance than Softmax on PTB, we focus on the comparison between MoC and MoS. Concretely, given the same context (previous tokens), we search for prediction steps where MoS achieves lower negative log loss than MoC by a margin. We show some representative cases in Table 6 with the following observations:
· Comparing the first two cases, given the same preceding word "N", MoS flexibly adjusts its top predictions based on the different topic quantities being discussed in the context. In comparison, MoC emits quite similar top choices regardless of the context, suggesting its inferiority in make context-dependent predictions.
· In the 3rd case, the context is about international politics, where country/region names are likely to appear. MoS captures this nuance well, and yields top choices that can be used to complete a country name given the immediate preceding word "south". Similarly, in the 4th case, MoS is able to include "ual", a core entity of discussion in the context, in its top predictions. In contrast, MoC gives rather generic predictions irrieselevant to the context in both cases.

7

Under review as a conference paper at ICLR 2018

Model
Softmax MoC MoS

Log-Prob Rank Validation Test

400 280 9981

400 280 9981

Pairwise KLD Validation Test

4.869 4.955 5.400

4.763 4.864 5.284

Table 5: Quantitative analysis on Penn Treebank. To ensure comparable model sizes, the embedding sizes of Softmax, MoC and MoS used here are 400, 280, 280 respectively. The vocabulary size, i.e., M , is 10,000 for all models.

Cumulative percentage

100% 80% 60% 40% 20% 0%
10 10

10 8 10 6 10 4 10 2 Normalized singular value in log scale

Softmax MoC MoS
100

Figure 1: Cumulative percentage of normalized singulars given a value in [0, 1].

· For the 5th and the 6th example, we see MoS is able to exploit less common words accurately according to the context, while MoC fails to yield such choices. This well matches our analysis that MoS has the capacity of modeling context-dependent language.
4 RELATED WORK
In a general sense, Mixture of Softmaxes proposed in this work can be seen as a particular instantiation of the long-existing idea called Mixture of Experts (MoE) (Jacobs et al., 1991). However, there are two core differences. Firstly, MoE has usually been instantiated as mixture of Gaussians to model data in continuous domains (Jacobs et al., 1991; Graves, 2013; Bazzani et al., 2016). More importantly, the motivation of using the mixture structure is distinct. For Gaussian mixture models, the mixture structure is employed to allow for a parameterized multi-modal distribution. By contrast, Softmax by itself can parameterize a multi-modal distribution, and MoS is introduced to break the Softmax bottleneck as discussed in Section 2.
There has been previous work (Eigen et al., 2013; Shazeer et al., 2017) proposing architectures that can be categorized as instantiations of MoC, since the mixture structure is employed in the feature space.5 The target of Eigen et al. (2013) is to create a more expressive feed-forward layer through the mixture structure. In comparison, Shazeer et al. (2017) focuses on a sparse gating mechanism also on the feature level, which enables efficient conditional computation and allows the training of a very large neural architecture. In addition to having different motivations from our work, all these MoC variants suffer from the same rank limitation problem as discussed in Section 2.
Finally, several previous works have tried to introduce latent variables into sequence modeling (Bayer & Osendorfer, 2014; Gregor et al., 2015; Chung et al., 2015; Gan et al., 2015; Fraccaro et al., 2016; Chung et al., 2016). Except for (Chung et al., 2016), these structures all define a continuous latent variable for each step of the RNN computation, and rely on the SGVB estimator (Kingma & Welling, 2013) to optimize a variational lower bound of the log-likelihood. Since exact integration is infeasible, these models cannot estimate the likelihood (perplexity) exactly at test time. Moreover, for discrete data, the variational lower bound is usually too loose to yield a competitive approximation compared to standard auto-regressive models. As an exception, Chung et al. (2016) utilizes Bernoulli latent variables to model the hierarchical structure in language, where the Bernoulli sampling is replaced by a thresholding operation at test time to give perplexity estimation.
5 CONCLUSIONS
Under the matrix factorization framework, the expressiveness of Softmax-based language models is limited by the dimension of the word embeddings, which is termed as the Softmax bottleneck. Our proposed MoS model improves the expressiveness over Softmax, and at the same time avoids overfitting compared to non-parametric models and naively increasing the word embedding dimensions. Our method improves the current state-of-the-art results on standard benchmarks by a large margin, which in turn justifies our theoretical reasoning: it is important to have a high-rank model for natural language.
5Although Shazeer et al. (2017) name their architecture as MoE, it is not a standard MoE (Jacobs et al., 1991) and should be classified as MoC under our terminology.

8

Under review as a conference paper at ICLR 2018

Context
MoS top-5 MoC top-5 Reference

managed properly and with a long-term outlook these can become investment-grade quality properties <eos> canadian <unk> production totaled N metric tons in the week ended oct. N up N N from the preceding week 's total of N __?__

million 0.38

tons 0.24

billion 0.09

barrels 0.06

ounces 0.04

billion 0.39

million 0.36

trillion 0.05

<eos> 0.04

N 0.03

canadian <unk> production totaled N metric tons in the week ended oct. N up N N from the preceding week 's total of N tons statistics canada a federal agency said <eos>

Context
MoS top-5 MoC top-5 Reference

the thriving <unk> street area offers <unk> of about $ N a square foot as do <unk> locations along lower fifth avenue <eos> by contrast <unk> in the best retail locations in boston san francisco and chicago rarely top $ N __?__

<eos> 0.36

a 0.13

to 0.07

for 0.07

and 0.06

million 0.39

billion 0.36

<eos> 0.05

to 0.04

of 0.03

by contrast <unk> in the best retail locations in boston san francisco and chicago rarely top $ N a square foot <eos>

Context
MoS top-5 MoC top-5 Reference

as other <unk> governments particularly poland and the soviet union have recently discovered initial steps to open up society can create a momentum for radical change that becomes difficult if not impossible to control <eos> as the days go by the south __?__

africa 0.15

african 0.15

<eos> 0.14

korea 0.08

korean 0.05

<eos> 0.38

and 0.08

of 0.06

or 0.05

<unk> 0.04

as the days go by the south african government will be ever more hard pressed to justify the continued <unk> of mr. <unk> as well as the continued banning of the anc and enforcement of the state of emergency <eos>

Context
MoS top-5 MoC top-5 Reference

shares of ual the parent of united airlines were extremely active all day friday reacting to news and rumors about the proposed $ N billion buy-out of the airline by an <unk> group <eos> wall street 's takeover-stock speculators or risk arbitragers had placed unusually large bets that a takeover would succeed and __?__

the 0.14

that 0.07

ual 0.07

<unk> 0.03

it 0.02

the 0.10

<unk> 0.06

that 0.05

in 0.02

it 0.02

wall street 's takeover-stock speculators or risk arbitragers had placed unusually large bets that a takeover would succeed and ual stock would rise <eos>

Context
MoS top-5 MoC top-5 Reference

the government is watching closely to see if their presence in the <unk> leads to increased <unk> protests and violence if it does pretoria will use this as a reason to keep mr. <unk> behind bars <eos> pretoria has n't forgotten why they were all sentenced to life <unk> in the first place for sabotage and __?__

<unk> 0.47

violence 0.11 conspiracy 0.03 incest 0.03

civil 0.03

<unk> 0.41

the 0.03

a 0.02

other 0.02

in 0.01

pretoria has n't forgotten why they were all sentenced to life <unk> in the first place for sabotage and conspiracy to <unk> the government <eos>

Context
MoS top-5 MoC top-5 Reference

china 's <unk> <unk> program has achieved some successes in <unk> runaway economic growth and stabilizing prices but has failed to eliminate serious defects in state planning and an <unk> drain on state budgets <eos> the official china daily said retail prices of <unk> foods have n't risen since last december but acknowledged that huge government __?__

subsidies 0.15 spending 0.08 officials 0.04

costs 0.04

<unk> 0.03

officials 0.04

figures 0.03

efforts 0.03

<unk> 0.03

costs 0.03

the official china daily said retail prices of <unk> foods have n't risen since last december but acknowledged that huge government subsidies were a main factor in keeping prices down <eos>

Table 6: Compaison of next-token prediction on Penn Treebank test data. N stands for a number as the result of preprocessing (Mikolov et al., 2010). The context shown only includes the previous sentence and the current sentence the prediction step resides in.

9

Under review as a conference paper at ICLR 2018
REFERENCES
Justin Bayer and Christian Osendorfer. Learning stochastic recurrent networks. arXiv preprint arXiv:1411.7610, 2014.
Loris Bazzani, Hugo Larochelle, and Lorenzo Torresani. Recurrent mixture density network for spatiotemporal visual attention. arXiv preprint arXiv:1603.08199, 2016.
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137­1155, 2003.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993­1022, 2003.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pp. 2980­2988, 2015.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. arXiv preprint arXiv:1609.01704, 2016.
David Eigen, Marc'Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.
Marco Fraccaro, Søren Kaae Sønderby, Ulrich Paquet, and Ole Winther. Sequential neural models with stochastic layers. In Advances in Neural Information Processing Systems, pp. 2199­2207, 2016.
Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Advances in neural information processing systems, pp. 1019­1027, 2016.
Zhe Gan, Chunyuan Li, Ricardo Henao, David E Carlson, and Lawrence Carin. Deep temporal sigmoid belief networks for sequence modeling. In Advances in Neural Information Processing Systems, pp. 2467­2475, 2015.
John J Godfrey and Edward Holliman. Switchboard-1 release 2. Linguistic Data Consortium, Philadelphia, 1997.
Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426, 2016.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A recurrent neural network for image generation. arXiv preprint arXiv:1502.04623, 2015.
Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79­87, 1991.
Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. In AAAI, pp. 2741­2749, 2016.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Reinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp. 181­184. IEEE, 1995.
Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural sequence models. arXiv preprint arXiv:1709.07432, 2017.
10

Under review as a conference paper at ICLR 2018
Gábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589, 2017.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm language models. arXiv preprint arXiv:1708.02182, 2017.
Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model. SLT, 12:234­239, 2012.
Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocky`, and Sanjeev Khudanpur. Recurrent neural network based language model. In Interspeech, volume 2, pp. 3, 2010.
Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pp. 641­648. ACM, 2007.
Steven Pinker. The language instinct, 1994. William H Press. Numerical recipes 3rd edition: The art of scientific computing. Cambridge uni-
versity press, 2007. Anton Maximilian Schäfer and Hans Georg Zimmermann. Recurrent neural networks are universal
approximators. In International Conference on Artificial Neural Networks, pp. 632­640. Springer, 2006. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104­3112, 2014. Li Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Proceedings of the 30th international conference on machine learning (ICML-13), pp. 1058­1066, 2013. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014. Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. arXiv preprint arXiv:1703.10960, 2017. Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, and Jürgen Schmidhuber. Recurrent highway networks. arXiv preprint arXiv:1607.03474, 2016. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016.
11

Under review as a conference paper at ICLR 2018

A PROOFS

Proof of Property 1

Proof. For any A  F (A), let PA (X|C) denote the distribution defined by applying Softmax on the logits given by A . Consider row i column j, by definition any entry in A can be expressed as
Aij = Aij + ii. It follows

PA (xj|ci) =

exp Aij = k exp Aik

exp(Aij + ii) = k exp(Aik + ii)

exp Aij k exp Aik

=

P (xj |ci)

For any A  {A | Softmax(A ) = P }, for any i and j, we have PA (xj|ci) = PA(xj|ci)
It follows that for any i, j, and k,

PA PA

(xj |ci ) (xk |ci )

=

exp Aij exp Aik

=

exp Aij exp Aik

=

PA(xj |ci) PA (xk |ci )

As a result,

Aij - Aij = Aik - Aik

This means each row in A can be obtained by adding a real number to the corresponding row in A. Therefore, there exists a diagonal matrix   RN×N such that

A = A + JN,M

It follows that A  F (A).

Proof of Property 2

Proof. For any A1 and A2 in F (A), by definition we have A1 = A + 1JN,M , and A2 = A + 2JN,M where 1 and 2 are two diagonal matrices. It can be rewritten as
A1 = A2 + (1 - 2)JN,M
Let S be a maximum set of linearly independent rows in A2. Let eN be an all-ones vector with dimension N . The i-th row vector a1,i in A1 can be written as
a1,i = a2,i + (1,ii - 2,ii)eN
Because a2,i is a linear combination of vectors in S, a1,i is a linear combination of vectors in S  {eN }. It follows that
rank(A1)  rank(A2) + 1

Similarly, we can derive

rank(A2)  rank(A1) + 1

Therefore,

|rank(A1) - rank(A2)|  1

Proof of Proposition 1
Proof. If there exists a parameter  such that P(X|c) = P (X|c) for all c in L, by Lemma 1, we have HW  F (A). As a result, there exists a matrix A  F (A) such that HW = A . Because H and W are of dimensions (N × d) and (M × d) respectively, we have
d  rank(A )  min rank(A )
A F (A)
If d  minA F (A) rank(A ), there exist matrices A  F (A), H  RN×d and W  RM×d, such that A can be factorized as A = H W . Because U is a universal approximator, there exists  such that H = H and W = W . By Lemma 1, P(X|c) = P (X|c) for all c in L.
12

Under review as a conference paper at ICLR 2018

B HYPER-PARAMETERS

The hyper-parameters used for MoS in language modeling experiment is summarized below.

Hyper-parameter
Learning rate Batch size Embedding size RNN hidden sizes Number of mixture components
Word-level V-dropout Embedding V-dropout Hidden state V-dropout Recurrent weight dropout (Wan et al., 2013) Context vector V-dropout

PTB
20 12 280 [960, 960, 620] 15
0.10 0.55 0.20 0.50 0.30

WT2
15 15 300 [1150,1150,650] 15
0.10 0.40 0.225 0.50 0.30

Table 7: Hyper-parameters used for MoS. V-dropout abbreviates variational dropout (Gal & Ghahramani, 2016). See (Merity et al., 2017) for more detailed descriptions.
The hyper-parameters used for dynamic evaluation of MoS is summarized below.

Hyper-parameter Batch size learning rate ()


PTB 100 0.002 0.001 0.075

WT2 100 0.002 0.002 0.02

Table 8: Hyper-parameters used for dynamic evaluation of MoS. See (Krause et al., 2017) for more detailed descriptions.

13

