Under review as a conference paper at ICLR 2018
RETHINKING THE SMALLER-NORM-LESSINFORMATIVE ASSUMPTION IN CHANNEL PRUNING OF CONVOLUTION LAYERS
Anonymous authors Paper under double-blind review
ABSTRACT
Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions on resourcelimited scenarios. A widely-used practice in relevant work assumes that a smallernorm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs), which does not critically rely on this assumption. Instead, it focuses on direct simplification of the channelto-channel computation graph of a CNN without the need of performing a computational difficult and not always useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: the first being to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels being constant, and the second being to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to reproduce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and the competitive performance.
1 INTRODUCTION
Not all computations in a deep neural network are of equal importance. In the conventional deep learning pipeline, an expert crafts a neural architecture and trains it against a prepared dataset. The success of training a deep model often requires trial and error, and such loop usually has little control on prioritizing the computations happening in the neural network. Recently researchers started to develop model-simplification methods for convolutional neural networks (CNNs), bearing in mind that some computations are indeed non-critical or redundant and hence can be safely removed from a trained model without substantially degrading the model's performance. Such methods not only accelerate computational efficiency but also sometimes alleviate the model's overfitting effects.
Discovering which subsets of the computations of a trained CNN are more reasonable to prune, however, is nontrivial. Existing methods can be categorized from either the learning perspective or the computational perspective. From the learning perspective, some methods use a data-independent approach where the training data does not assist in determining which part of a trained CNN should be pruned, e.g. He et al. (2017) and Zhang et al. (2016), while others use a data-dependent approach through typically a joint optimization in generating pruning decisions, e.g., Han et al. (2015) and Anwar et al. (2017). From the computational perspective, while most approaches focused on setting the dense weights of convolutions or linear maps to be structured sparse, we propose here a method adopting a new conception to achieve in effect the same goal.
Instead of regarding the computations of a CNN as a collection of separate computations sitting at different layers, we view it as a network flow that delivers information from the input to the output through different channels across different layers. We believe saving computations of a CNN is not only about reducing what are calculated in an individual layer, but perhaps more importantly also about understanding how each channel is contributing to the entire information flow in the underlying passing graph as well as removing channels that are less responsible to such process.
1

Under review as a conference paper at ICLR 2018
Inspired by this new conception, we propose to design a "gate" at each channel of a CNN, controlling whether its received information is actually sent out to other channels after processing. If a channel "gate" closes, its output will always be a constant. In fact, each designed "gate" will have a prior intention to close, unless it has a "strong" duty in sending some of its received information from the input to follow-up layers. We find that implementing this idea in pruning CNNs is unsophisticated, as will be detailed in Sec 4.
Our method neither introduces any extra parameters to the existing CNN, nor changes its computation graph. In fact, it only introduces marginal overheads to existing gradient training of CNN. It also possess an attractive feature that one can successively build multiple compact models with different inference performances in a single round of resource-intensive training (as were done in our experiments). This eases the process to choose a balanced model to deploy in production. Probably, the only applicability constraint of our method is that all convolutional layers and fully-connected layer (except the last layer) in the CNN should be batch normalized (Ioffe & Szegedy, 2015). Given batch normalization has becomes a widely adopted ingredient in designing state-of-the-art deep learning models, and many successful CNN models are using it, we believe our approach has a wide scope of potential impacts.1
In this paper, we start from rethinking a basic assumption widely explored in existing channel pruning work. We point out several issues and gaps in realizing this assumption successfully. Then, we propose our alternative approach, which work around several numerical difficulties. Finally, we experiment our method across different benchmarks and validate its usefulness and strengths.
2 RELATED WORK
Reducing the size of neural network for speeding up its computational performance at inference time has been a long studied topic in the community of neural network and deep learning. Pioneer works include Optimal Brain Damage (LeCun et al., 1990) and Optimal Brain Surgeon (Hassibi & Stork, 1993). More recent developments focused on either reducing the structural complexity of a provided network or training a compact or simplified network from scratch. Our work can be categorized into the former type. Thus, the literature review below revolves around that reducing the structural complexity.
To reduce the structural complexity of deep learning models, previous work have largely focused on sparsifying the weights of convolutional kernels or the feature maps across multiple layers in a network (Anwar et al., 2017; Han et al., 2015). More recently, some work proposes to impose structured sparsity on those vector components motivated from the implementation perspective on specialized hardware (Wen et al., 2016; Zhou et al., 2016; Alvarez & Salzmann, 2016; Lebedev & Lempitsky, 2016). Yet as argued by authors of (Molchanov et al., 2017), regularization-based pruning techniques require per layer sensitivity analysis which adds extra computations. Molchanov et al. (2017) relies on global rescaling of criteria for all layers and does not require sensitivity estimation, a beneficial feature that our approach also has. To our knowledge, it is also much unclear how widely useful are those work in deep learning. In Section 3, we discuss in details the potential issues in regularization-based pruning techniques potentially hurting them being widely applicable, especially for those which regularize high-dimensional tensor parameters or use magnitude-based pruning methods. Our approach works around the mentioned issues by constraining the anticipated pruning operations only on batch normalized convolutional layers. Instead of posing structured sparsity on kernels or feature maps, we enforce sparsity on the scaling parameter  in batch normalization operator. This blocks the sample-wise information passing through part of the channels in convolution layer, and in effect implies one can safely remove those channels. A recent work (Huang & Wang, 2017) used similar technique as ours to remove unimportant residual modules in ResNet by introducing extra scaling factors to the original network, but some optimization subtleties as pointed out in our paper were not well explained. Another independent, parallel work called NetworkSlimming (Liu et al., 2017) also aims to sparsify the scaling parameters of batch normalization. But
1For convolution layer which is not originally trained with batch normalization, one can still convert it into a "nearequivalent" convolution layer with batch normalization by removing the bias term b and properly setting  =  + ,  = b + µ, where  and µ are estimated from the outputs of the convolution across all training samples.
2

Under review as a conference paper at ICLR 2018

instead of using off-the-shelf gradient learning like theirs, we propose a new algorithmic approach based on ISTA and rescaling trick, improving robustness and speed of the undergoing optimization.
3 RETHINKING THE SMALLER-NORM-LESS-INFORMATIVE ASSUMPTION

In most regularized linear regressions, a large-norm coefficient is often a strong indicator of a highly informative feature. This has been widely perceived in statistics and machine learning community, and removing features which have a small coefficient does not substantially affect the regression errors. Therefore, it has been an established practice to use tractable norm to regularize the parameters in optimizing a model and pick the important ones by comparing their norms after training. However, this assumption is often untrue for nonconvex learning. One has to carefully consider two issues outlined below.

Model Reparameterization. Consider to find a deep linear (convolutional) network subject to a least square with Lasso: for  > 0,

n

min
{Wi }2i=n1

E(x,y)D

W2n  . . .  W2  W1  x - y

2+
i=1

W2i 1 .

The above formulation is not a well-defined problem because for any parameter set {Wi}i2=n1, one can always find another parameter set {Wi }i2=n1 such that it achieves a smaller total loss while keeping the corresponding l0 norm unchanged by actually setting

Wi = Wi, i = 1, 3, . . . , 2n - 1 and Wi = Wi/, i = 2, 4, . . . , 2n ,
where  > 1. In another word, for any > 0, one can always find a parameter set {Wi}2i=n1 (which is usually non-sparse) that minimizes the first least square loss while having its second Lasso term less than .

We note that gradient-based learning is highly inefficient in exploring such model reparameterization patterns. In fact, there are some recent discussions around this (Dinh et al., 2017). If one adopts a pre-trained model, and augments its original objective with a new norm-based parameter regularization, the new gradient updates may just increase rapidly or it may take a very long time for the variables traveling along the model's reparameterization trajectory. This highlights a theoretical gap questioning existing sparsity inducing formulation and actual computational algorithms whether they can achieve widely satisfactory parameter sparsification for deep learning models.

Transform Invariance. Another example is penalizing l1- or l2-norms of filters in convolution layer which is then followed by a batch normalization: at the l-th layer, we let

xl+1 = max{ · BNµ,, (W l  xl) + , 0},

where  and  are vectors whose length is the number of channels. Likewise, one can easily see that any uniform scaling of W l which changes its l1- and l2-norms would have no effects on the output xl+1. Alternatively speaking, if one is interested in minimizing the weight norms of multiple layers together, it becomes unclear how to choose proper penalty for each layer. Theoretically, there always exists an optimizer that can change the weight to one with infinitesimal magnitude without hurting any inference performance. It is worth noting that some existing work used a layer-by-layer greedy strategy to avoid this issue (He et al., 2017; Zhang et al., 2016).

Based on this discussion, many existing works which claim to use Lasso, group Lasso, or thresholding to enforce parameter sparsity have some theoretical gaps to bridge. In fact, many heuristic algorithms in neural net pruning actually do not naturally generate a sparse parameterized solution. More often, thresholding is used to directly set certain subset of the parameters in the network to zeros, which can be problematic. The reason is in essence around two questions. First, by setting parameters less than a threshold to zeros, will the functionality of neural net be preserved approximately with some guarantees? If yes, then under what conditions? Second, how should one set those thresholds for weights across different layers? Not every layer contributes equally in a neural net. It is expected that some layers act critically for the performance but only use a small computation

3

Under review as a conference paper at ICLR 2018

and memory budget, while some other layers help marginally for the performance but consume a lot resources. It is naturally more desirable to prune calculations in the latter kind of layers than the former.
In contrast with these existing approaches, we focus on enforcing sparsity of a tiny set of parameters in CNN -- scale parameter s in all batch normalization. Not only placing sparse constraints on  is simpler and easier to monitor, but more importantly, we have two strong reasons:
1. Every  always multiplies a normalized random variable, thus the channel importance becomes comparable across different layers by measuring the magnitude values of ;
2. The reparameterization effect across different layers is avoided if its follow-up convolution layer is also batch normalized. In other words, the impacts from the scale changes of  parameter are independent across different layers.

4 CHANNEL PRUNING OF BATCH-NORMALIZED CNN

We describe the basic principle and algorithm of our channel pruning technique.

4.1 PRELIMINARIES

Pruning constant channels. Consider convolution with batch normalization:
xl+1 = max l · BNµl,l, l (W l  xl) + l, 0 .
For the ease of notation, we let  = l. Note that if some element in  is set to zero, say, [k] = 0, its output image x:l,+:,1:,k becomes a constant k, and a convolution of a constant image channel is almost everywhere constant (except for padding regions, an issue to be discussed later). Therefore, we show those constant image channels can be pruned while the same functionality of network is approximately kept:

· If the follow-up convolution layer does not have batch normalization,

xl+2 = max W l+1  xl+1 + bl+1, 0 ,

its values (a.k.a. elements in ) is absorbed into the bias term by the following equation

bln+e1w := bl+1 + I( = 0) · ReLU()T sum reduced(W:l,+:,·1,·) ,

such that

xl+2  max W l+1  xl+1 + bln+e1w, 0 ,

where  denotes the convolution operator which is only calculated along channels indexed by non-zeros of .

· If the follow-up convolution layer has batch normalization,

xl+2 = max l+1 · BNµl+1,l+1, l+1 W l+1  xl+1 + l+1, 0 , instead its moving average is updated as

such that

µln+e1w := µl+1 - I( = 0) · ReLU()T sum reduced(W:l,+:,·1,·) ,

xl+2  max

 · BNl+1

µnl+e1w ,l+1, l+1

W l+1  xl+1

+ l+1, 0

.

Remark that the approximation () is strictly equivalence (=) if no padding is used in the convolution operator , a feature that the parallel work Liu et al. (2017) does not possess. When the original model uses padding in computing convolution layers, the network function is not strictly preserved after pruning. In our practice, we fine-tune the pruned network to fix such performance degradation at last. In short, we formulate the network pruning problem as simple as to set more elements in  to zero. It is also much easier to deploy the pruned model, because no extra parameters or layers are introduced into the original model.

4

Under review as a conference paper at ICLR 2018

To better understand how it works in an entire CNN, imagine a channel-to-channel computation graph formed by the connections between layers. In this graph, each channel is a node, their inference dependencies are represented by directed edges. The  parameter serves as a "dam" at each node, deciding whether let the received information "flood" through to other nodes following the graph. An end-to-end training of channel pruning is essentially like a flood control system. There suppose to be rich information of the input distribution, and in two ways, much of the original input information is lost along the way of CNN inference, and the useful part -- that is supposed to be preserved by the network inference -- should be label sensitive. Conventional CNN has one way to reduce information: transforming feature maps (non-invertible) via forward propagation. Our approach introduces the other way: block information at each channel by forcing its output being constant using ISTA.

ISTA. Despite the gap between Lasso and sparsity in the non-convex settings, we found that ISTA (Beck & Teboulle, 2009) is still a useful sparse promoting method. But we just need to use it more carefully. Specifically, we adopt ISTA in the updates of s. The basic idea is to project the parameter at every step of gradient descent to a potentially more sparse one subject to a proxy problem: let l denote the training loss of interest, at the (t + 1)-th step, we set

1

t+1

=

min


µt

 - t + µt lt

2+ 

1,

(1)

where lt is the derivative with respect to  computed at step t, µt is the learning rate,  is the penalty. In the stochastic learning, lt is estimated from a mini-batch at each step. Eq. (1) has closed form solution as
t+1 = proxµt(t - µt lt) ,
where prox(x) = max{|x| - , 0} · sgn(x). The ISTA method essentially serves as a "flood control system" in our end-to-end learning, where the functionality of each  is like that of a dam. When
 is zero, the information flood is totally blocked, while  = 0, the same amount of information is
passed through in form of geometric quantities whose magnitudes are proportional to .

Scaling effect. One can also see that if  is scaled by  meanwhile W l+1 is scaled by 1/, that is,

 := ,

W l+1 := 1 W l+1 

the output xl+2 is unchanged for the same input xl. Despite not changing the output, scaling of  and W l+1 also scales the gradients l and W l+1 l by 1/ and , respectively. As we observed, the parameter dynamics of gradient learning with ISTA depends on the scaling factor  if one decides to choose it other than 1.0. Intuitively, if  is large, the optimization of W l+1 is progressed much
slower than that of .

4.2 THE ALGORITHM

We describe our algorithm below. The following method applies to both training from scratch or re-training from a pre-trained model. Given a training loss l, a convolutional neural net N , and hyper-parameters , , µ0, our method proceeds as follows:

1. Computation of sparse penalty for each layer. Compute the memory cost per channel for each layer denoted by l and set the ISTA penalty for layer l to l. Here



l

=

1 Iwi · Ihi

kwl

· khl

·

cl-1

+
l

kwl
T (l)

·

khl

· cl

+ Iwl · Ihl 

,

(2)

where
· Iwi · Ihi is the size of input image of the neural network. · kwl · khl is the kernel size of the convolution at layer l. Likewise, kwl · khl is the kernel
size of follow-up convolution at layer l . · T (l) represents the set of the follow-up convolutional layers of layer l · cl-1 denotes the channel size of the previous layer, which the l-th convolution operates
over; and cl denotes the channel size of one follow-up layer l .

5

Under review as a conference paper at ICLR 2018
· Iwl · Ihl is the image size of the feature map at layer l.
2. -W rescaling trick. For layers whose channels are going to get reduced, scale all ls in batch normalizations by  meanwhile scale weights in their follow-up convolutions by 1/.
3. End-to-End training with ISTA on . Train N by the regular SGD, with the exception that ls are updated by ISTA, where the initial learning rate is µ0. Train N until the loss l plateaus, the total sparsity of ls converges, and Lasso  l l l 1 converges.
4. Post-process to remove constant channels. Prune channels in layer l whose elements in l are zero and output the pruned model N by absorbing all constant channels into followup layers (as described in the earlier section.).
5. -W rescaling trick. For ls and weights in N which were scaled in Step 2 before training, scale them by 1/ and  respectively (scaling back).
6. Fine-tune N using regular stochastic gradient learning.
Remark that choosing a proper  as used in Steps 2 and 5 is necessary for using a large µt ·  in ISTA, which makes the sparsification progress of ls faster.
5 EXPERIMENTS
5.1 CIFAR-10 EXPERIMENT
We experiment with the standard image classification benchmark CIFAR-10 with two different network architectures: ConvNet and ResNet-20 (He et al., 2016). We resize images to 32 × 32 and zero-pad them to 40 × 40. We pre-process the padded images by randomly cropping with size 32 × 32, randomly flipping, randomly adjusting brightness and contrast, and standardizing them such that their pixel values have zero mean and one variance.
ConvNet For reducing the channels in ConvNet, we are interested in studying whether one can easily convert a over-parameterized network into a compact one. We start with a standard 4-layer convolutional neural network whose network attributes are specified in Table 1. We use a fixed learning rate µt = 0.01, scaling parameter  = 1.0, and set batch size to 125.
Model A is trained from scratch using the base model with an initial warm-up  = 0.0002 for 30k steps, and then is trained by raising up  to 0.001. After the termination criterion are met, we prune the channels of the base model to generate a smaller network called model A. We evaluate the classification performance of model A with the running exponential average of its parameters. It is found that the test accuracy of model A is even better than the base model. Next, we start from the pre-trained model A to create model B by raising  up to 0.002. We end up with a smaller network called model B, which is about 1% worse than model A, but saves about one third parameters. Likewise, we start from the pre-trained model B to create model C. The detailed statistics and its pruned channel size are reported in Table 1.
We have two major observations from the experiment: (1) When the base network is overparameterized, our approach not only significantly reduce the number of channels of the base model but also improves its generalization performance on the test set. (2) Performance degradation seems unavoidable when the channels in a network are saturated, our approach gives satisfactory trade-off between test accuracy and model efficiency.
ResNet-20 We also want to verify our second observation with the state-of-art models. We choose the popular ResNet-20 as our base model for the CIFAR-10 benchmark, whose test accuracy is 92%. We focus on pruning the channels in the residual modules in ResNet-20, which has 9 convolutions in total. As detailed in Table 2, model A is trained from scratch using ResNet-20's network structure as its base model. We use a warm-up  = 0.001 for 30k steps and then train with  = 0.005. We are able to remove 37% parameters from ResNet-20 with only about 1 percent accuracy loss. Likewise, Model B is created from model A with a higher penalty  = 0.01.
6

Under review as a conference paper at ICLR 2018

layer conv1 pool1 conv2 pool2 conv3 pool4
fc  param. size test accuracy (%)

output 32 × 32 16 × 16 16 × 16
8×8 8×8 4×4 1×1

kernel 5×5 3×3 5×5 3×3 3×3 3×3 4×4

base channel
96
192
192
384
1,986,760 89.0

model A channel
53
86
67
128 0.001 309,655 89.5

model B channel
41
64
52
128 0.002 207,583 87.6

model C channel
31
52
40
127 0.008 144,935 86.0

Table 1: Comparisons between different pruned networks and the base network.

group - block

1-1 1-2 1-3 2-1 2-2 2-3 3-1 3-2 3-3

ResNet-20

channels

16 16 16 32 32 32 64 64 64

param size. = 281,304

test accuracy (%) = 92.0

model A

channels

12 6 11 32 28 28 47 34 25

param size. = 176,596

test accuracy (%) = 90.9

model B

channels

8 2 7 27 18 16 25 9 8

param size. = 90,504

test accuracy (%) = 88.8

Table 2: Comparisons between ResNet-20 and its two pruned versions. The last columns are the number of channels of each residual modules after pruning.

5.2 ILSVRC2012 EXPERIMENT
We experiment our approach with the pre-trained ResNet-101 on ILSVRC2012 image classification dataset (He et al., 2016). ResNet-101 is one of the state-of-the-art network architecture in ImageNet Challenge. We follow the standard pipeline to pre-process images to 224×224 for training ResNets. We adopt the pre-trained TensorFlow ResNet-101 model whose single crop error rate is 23.6% with about 4.47 × 107 parameters. 2 We set the scaling parameter  = 0.01, the initial learning rate µt = 0.001, the sparsity penalty  = 0.1 and the batch size = 128 (across 4 GPUs). The learning rate is decayed every four epochs with rate 0.86. We create two pruned models from the different iterations of training ResNet-101: one has 2.36 × 107 parameters and the other has 1.73 × 107 parameters. We then fine-tune these two models using the standard way for training ResNet-101, and report their error rates. The Top-5 error rate increases of both models are less than 0.5%. The Top-1 error rates are summarized in Table 3. To the best of our knowledge, only a few work has reported their performances on this very large-scale benchmark w.r.t. the Top-1 errors. We compare our approach with some recent work in terms of models' parameter size, flops, and error rates. As shown in Table 3, our model v2 has achieved a compression ratio more than 2.5 while still maintains more than 1% lower error rates than that of other state of the art models at comparable size of parameters.
5.3 IMAGE FOREGROUND-BACKGROUND SEGMENTATION EXPERIMENT
As we discussed about the two major observations in Section 5.1, a more appealing scenario is to apply our approach in pruning channels of over-parameterized model. It often happens when one adopts a pre-trained network on a large task (such as ImageNet classification) and fine-tunes the model to a different and smaller task (Molchanov et al., 2017). In this case, one might expect that
2https://github.com/tensorflow/models/tree/master/slim
7

Under review as a conference paper at ICLR 2018

network resnet-50 pruned (Huang & Wang, 2017)
resnet-101 pruned (v2, ours) resnet-34 pruned (Li et al., 2017)
resnet-34 resnet-101 pruned (v1, ours)
resnet-50 resnet-101

param size.  1.65 × 107
1.73 × 107 1.93 × 107 2.16 × 107
2.36 × 107 2.5 × 107
4.47 × 107

flops 3.03 × 109 3.69 × 109 2.76 × 109 3.64 × 109
4.47 × 109 4.08 × 109
7.8 × 109

error (%)  26.8 25.44
27.8 26.8 24.73 24.8 23.6

ratio 66% 39% 89%
53%
-

Table 3: Attributes of different versions of ResNet and their single crop errors on ILSVRC2012 benchmark. The last column means the parameter size of pruned model vs. the base model.

some channels that were useful in the first pre-training task are not quite contributing to the outputs of the second task.
We describe an image segmentation experiment whose neural network model is composed from an inception-like network branch and a densenet network branch. The entire network takes a 224 × 224 image and outputs binary mask at the same size. The inception branch is mainly used for locating the foreground objects while the densenet network branch is used to refine the boundaries around the segmented objects. This model is originally trained on multiple datasets.
In our experiment, we attempt to prune channels in both the inception branch and densenet branch. We set  = 0.01,  = 0.5, µt = 2 × 10-5, and batch size = 24. We train the pre-trained base model until all termination criterion are met, and build the pruned model for fine-tuning. The pruned model saves 86% parameters and 81% flops of the base model. We also compare the finetuned pruned model with the pre-trained base model across different test benchmark. It shows that pruned model actually improves over the base model on four of the five test datasets with about 2%  5%, while performs worse than the base model on the most challenged dataset DUT-Omron, whose foregrounds might contain multiple objects.

test dataset (#images) MSRA10K (2,500)
DUT-Omron (1,292) Flickr-portrait (150)
Flickr-hp (300) COCO-person (50)
param. size flops

base model
mIOU 83.4% 83.2% 88.6% 84.5% 84.1% 1.02 × 107 5.68 × 109

pruned model
mIOU 85.5% 79.1% 93.3% 89.5% 87.5% 1.41 × 106 1.08 × 109

Table 4: mIOU reported on different test datasets for the base model and the pruned model.

6 CONCLUSIONS
We proposed a model pruning technique that focuses on simplifying the computation graph of a deep convolutional neural networks. Our approach adopts ISTA to update the  parameter in batch normalization operator embedded in each convolution. To accelerate the progress of model pruning, we use a -W rescaling trick before and after stochastic training. Our method cleverly avoids some possible numerical difficulties such as mentioned in other regularization based related work, hence is easier to apply for practitioners. We empirically validate our method through several benchmarks and show its usefulness and competitiveness in building compact CNN models.
REFERENCES
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. In Advances in Neural Information Processing Systems, pp. 2270­2278, 2016.
8

Under review as a conference paper at ICLR 2018
Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured pruning of deep convolutional neural networks. ACM Journal on Emerging Technologies in Computing Systems (JETC), 13(3):32, 2017.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183­202, 2009.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In Proceedings of International Conference on Machine Learning, 2017.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems, pp. 1135­1143, 2015.
Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain surgeon. In Advances in Neural Information Processing Systems, pp. 164­171, 1993.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In Proceedings of International Conference on Computer Vision, 2017.
Zehao Huang and Naiyan Wang. Data-driven sparse structure selection for deep neural networks. arXiv preprint arXiv:1707.01213, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2554­2564, 2016.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in Neural Information Processing Systems, pp. 598­605, 1990.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In International Conference on Learning Representations, 2017.
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. arXiv preprint arXiv:1708.06519, 2017.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient transfer learning. In International Conference on Learning Representations, 2017.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in Neural Information Processing Systems, pp. 2074­2082, 2016.
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional networks for classification and detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(10):1943­1955, 2016.
Hao Zhou, Jose M Alvarez, and Fatih Porikli. Less is more: Towards compact cnns. In European Conference on Computer Vision, pp. 662­677. Springer, 2016.
9

Under review as a conference paper at ICLR 2018
Figure 1: Visualization of the number of pruned channels at each convolution in the inception branch. Colored regions represents the number of channels kept. The height of each bar represents the size of feature map, and the width of each bar represents the size of channels. It is observed that most of channels in the bottom layers are kept while most of channels in the top layers are pruned.
10

