Under review as a conference paper at ICLR 2018
LEARNING REPRESENTATIONS AND GENERATIVE MODELS FOR 3D POINT CLOUDS
Anonymous authors Paper under double-blind review
ABSTRACT
Three-dimensional geometric data offer an excellent domain for studying representation learning and generative modeling. In this paper, we look at geometric data represented as point clouds. We introduce a deep autoencoder (AE) network with excellent reconstruction quality and generalization ability. The learned representations outperform the state of the art in 3D recognition tasks and enable basic shape editing applications via simple algebraic manipulations, such as semantic part editing, shape analogies and shape interpolation. We also perform a thorough study of different generative models including: GANs operating on the raw point clouds, significantly improved GANs trained in the fixed latent space our AEs and Gaussian mixture models (GMM). Interestingly, GMMs trained in the latent space of our AEs produce samples of the best fidelity and diversity. To perform our quantitative evaluation of generative models, we propose simple measures of fidelity and diversity based on optimally matching between sets point clouds.
1 INTRODUCTION
Three-dimensional (3D) representations of real-life objects are a core tool for vision, robotics, medicine and augmented / virtual reality applications. Popular recent encodings include view-based projections , volumetric grids and graphs ; those complement more traditional shape representations such as 3D meshes, level set functions, curve-based CAD models and constructive solid geometry. These encodings, while effective in their respective domains (e.g. acquisition or rendering), are often poor in semantics. For example, naïvely interpolating between two different cars in a view-based representation does not yield a representation of an 'intermediate' car. Furthermore, these raw, high-dimensional representations are typically not well suited for the design of generative models via classic statistical methods. As such, editing and designing new objects with such representations frequently involves the construction and manipulation of complex, object-specific parametric models linking the semantics to the representation. This may require significant expertise and effort.
Recent advances in deep learning bring the promise of a data-driven approach. In domains where data is plentiful, deep learning tools have eliminated the need for hand-crafting features and models. Deep learning architectures like autoencoders (AEs) (Rumelhart et al., 1988; Kingma & Welling, 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Radford et al., 2015; Denton et al., 2015; Che et al., 2016) are successful at learning complex data representations and generating realistic samples from complex underlying distributions. Recently, deep learning architectures for view-based projections (Su et al., 2015; Wei et al., 2016; Kalogerakis et al., 2016), volumetric grids (Qi et al., 2016b; Wu et al., 2015; Hegde & Zadeh, 2016) and graphs (Bruna et al., 2013; Henaff et al., 2015; Defferrard et al., 2016; Yi et al., 2016b) have appeared in the 3D machine learning literature.
In this paper we focus on point clouds: a relatively unexplored 3D modality. Point clouds provide a homogeneous, expressive and compact representation of surface geometry, easily amenable to geometric operations. These properties make them attractive from a learning point of view. In addition, they come up as the output of common range-scanning acquisition pipelines used in devices like the Kinect and iPhone's recent face identification feature. Only a handful of deep architectures for 3D point clouds exist in the literature: PointNet (Qi et al., 2016a; 2017) successfully tackled classification and segmentation tasks; Kalogerakis et al. (2016) used point-clouds as an intermediate step in their pipeline. We provide the first results that use deep architectures to learn representations and generative models for point clouds.
1

Under review as a conference paper at ICLR 2018

Generative models have garnered increased attention recently in the deep learning community with the introduction of GANs (Goodfellow et al., 2014). An issue with GAN-based generative pipelines is that training them is notoriously hard and unstable (Salimans et al., 2016). More importantly, there is no universally accepted way to evaluate generative models. In evaluating generative models one is interested in both fidelity, i.e. how much generated points resemble the actual data, and coverage, i.e. what fraction of the data distribution a generated sample represents. The latter is especially important given the tendency of certain GANs to exhibit mode collapse. In particular, it assigns the same penalty to models that miss isolated examples interspersed all over, and models that miss whole contiguous pieces of the data manifold. We provide simple methods to deal with both issues (training and evaluation) in our target domain. Our specific contributions are:
· We design a new AE architecture ­inspired by recent architectures used for classification (Qi et al., 2016a)­ that is capable of learning compact representations of point clouds with excellent reconstruction quality even on unseen samples. The learned representations are (i) good for classification via simple methods (SVM), improving on the state of the art (Wu et al., 2016); (ii) suitable for meaningful interpolations and semantic operations.
· We create the first set of generative models which (i) can generate point clouds measurably similar to the training data and held-out test data; (ii) provide good coverage of the training and test dataset. We argue that jointly learning the representation and training the GAN is unnecessary for our modality. We propose a workflow that first learns a representation by training an AE with a compact bottleneck layer, then trains a plain GAN in that fixed latent representation. We point to theory (Arjovsky & Bottou, 2017) that supports this idea, and verify it empirically. Latent GANs are much easier to train than monolithic (raw) GANS and achieve superior reconstruction with much better coverage. Somewhat surprisingly, GMMs trained in the latent space of fixed AEs achieve the best performance across the board.
· To support our qualitative evaluation, we perform a careful study of various old and new metrics, in terms of their applicability (i) as objectives for learning good representations; (ii) for the evaluation of generated samples. We propose a simple metric based on an optimal matching that can be used as a measure of coverage. We find that Chamfer distances fail to discriminate certain pathological cases from good examples.
· We show that training multi-class GANs works almost on par with dedicated GANs trained per-object-category, as long as the GANs are trained the latent space.
The rest of this paper is organized as follows: Section 2 outlines the necessary background and building blocks for our work and introduces our evaluation metrics. Section 3 introduces our models for latent representations and generation of point clouds. In Section 4, we evaluate all of our models both quantitatively and qualitatively, and analyze their behaviour. Further results and evaluation can be found in the appendix. The code for all our models will be made publicly available on github.

2 BACKGROUND

Autoencoders. Autoencoders (AE - inset) are deep architectures that aim to reproduce their input. They are especially useful, when

x

E

z

Dx

they contain a narrow bottleneck layer between input and output.

Upon successful training, the bottleneck layer corresponds to a low-

dimensional representation, a code for the dataset. The Encoder (E) learns to compress a data point x

into its latent representation, z. The Decoder (D) can then reproduce x from its encoded version z.

Generative Adversarial Networks. GANs are state-of-the-art
generative models. The basic architecture (inset) is based on a
adversarial game between a generator (G) and a discriminator (D).
The generator aims to synthesize samples that look indistinguishable from real data (drawn from x  pdata) by passing a randomly drawn sample z  pz through the generator function G. The discriminator tries to tell synthesized from real samples. The most commonly used
losses for the discriminator and generator networks are:

z Gx x
DATA

J (D)((D), (G)) = -Expdata log D(x) - Ezpz log (1 - D (G(z))) ,

D (1)

2

Under review as a conference paper at ICLR 2018

J (G)((D), (G)) = -Ezpz log D(G(z)) ,

(2)

where (D), (G) are the parameters for the discriminator and the generator network respectively. In addition to the classical GAN formulation, we also use the Wasserstein GAN (Gulrajani et al., 2017), which has shown improved stability during training.

Challenges specific to point cloud geometry. Point clouds as an input modality present a unique set of challenges when building a network architecture. As an example, the convolution operator ­ now ubiquitous in image-processing pipelines ­ requires the signal (in our case, geometry) to be defined on top of an underlying grid-like structure. Such a structure is not available in raw point clouds, which renders them significantly more difficult to encode than e.g. images or voxel grids. Recent classification work on point clouds (PointNet ­ Qi et al. (2016a)) bypasses this issue by circumventing 2D convolutions. Another issue with point clouds as a representation is that they are unordered - any permutation of a point set still describes the same shape. This complicates comparisons between two point sets, typically needed to define a loss function. This unorderedness of point clouds also creates the need for making the encoded feature permutation invariant.

Point-set distances. Two permutation-invariant metrics for comparing unordered point sets have

been proposed in the literature (Fan et al., 2016). On the one hand, the Earth Mover's distance

(EMD) (Rubner et al., 2000) is the solution of a transportation problem which attempts to transform

one set to the other. For two equally sized subsets S1  R3, S2  R3, their EMD is defined by

dEMD(S1, S2) = min

x - (x) 2. where  is a bijection. Interpreted as a loss, EMD is

:S1S2 xS1

differentiable almost everywhere. On the other hand, the (still differentiable but more computationally

efficient) Chamfer (pseudo)-distance (CD) measures the distance between each point in one set to its

nearest neighbor in the other set, dCH (S1, S2) =

minyS2

x-y

2 2

+

minxS1 x - y 22.

xS1

yS2

Evaluation Metrics for representations and generative models. In the remainder of the paper, we will frequently be needing to compare a given set (distribution) of points clouds, whether reconstructed or synthesized, to its ground truth counterpart. For example, one might want to assess the quality of a representation model, in terms of how well it matches the training set or a held-out test set. Such a comparison might be done to evaluate the faithfulness and/or diversity of a generative model, and measure potential mode-collapse. To measure how well a point-cloud distribution A matches a ground truth distribution G, we use the following metrics:
Coverage. For each point-cloud in A we find its closest neighbor in G; closeness can be computed using either CD or EMD, thus yielding two different metrics, COV-CD and COV-EMD. Coverage is measured as the fraction of the point-clouds in G that were matched to point-clouds in A. A high coverage score typically indicates that most of G is roughly represented within A.
Minimum Matching Distance (MMD). Coverage is not representative of the fidelity of A with respect to G as matched elements need not be close. To capture fidelity, we find the matching between G and A with the lowest sum of distances (MMD). Either of the structural distances can be used, yielding MMD-CD and MMD-EMD. We report the average of distances in the matching. MMD measures the distances in the pairwise matchings, so it correlates with how realistic the elements of A are.
Jensen-Shannon Divergence (JSD). The Jensen-Shannon divergence of the empirical distribution of individual points in the pointclouds. We measure this via estimated random variables defined in the ambient 3D space; this is possible because our training data are pre-aligned in 3D space, hence also the decoded and generated data. Specifically, we consider a 283 regularly-spaced voxel grid centered in the sphere of diameter 1. We construct a 283 random variable vector for A by counting the number of 3D points lying within each voxel, across all point clouds of A; similarly for G. We report the JSD of these two random vectors.

3 REPRESENTATION AND GENERATIVE MODELS
In this section we describe the architectures of our representation and generative models for point clouds, starting from our autoencoder design. Later, we introduce a GAN architecture tailored to

3

Under review as a conference paper at ICLR 2018

point-cloud data, followed by a more efficient pipeline that first trains an AE and trains a much smaller GAN in the learned latent space, and a simpler generative model based on Gaussian Mixtures.

3.1 LEARNING REPRESENTATIONS OF 3D POINT CLOUDS
The input to our AE network is a point cloud with 2048 points (2048 × 3 matrix), representing a 3D shape. The encoder architecture follows the principle of Qi et al. (2016a): MLP-layers with increasing feature size, ending with a "symmetric" function. This approach encodes every point independently and uses a permutation-invariant (symmetric) function to make a joint representation. In our implementation we use 5 MLP layers, each followed by a ReLU and a batch-norm layer. The output of the last MLP layer is passed to a feature-wise maximum to produce a k-dimensional vector which is the basis for our latent space. The decoder transforms the latent vector with 3 fully connected layers, the first two having ReLUs, to produce a 2048 × 3 output. For a permutation invariant objective, we explore both the efficient EMD-distance approximation (Fan et al., 2016) and the Chamfer-Distance as our structural losses; this yields two distinct AE models, referred to as AE-EMD and AE-CD (detailed architecture parameters can be found in Appendix A). To determine an appropriate size for the latent-space, we constructed AEs differing only by k  4, 8 . . . , 512 and trained them with point-clouds of a single object class, under the two losses. We repeated this procedure with pseudo-random weight initializations three times (see appendix, Fig. 12) and found that k = 128 had the best generalization error on the test data, while achieving minimal reconstruction error on the the train split.

3.2 GENERATIVE MODELS FOR POINT CLOUDS
Raw point cloud GAN (r-GAN). The first version of our generative model operates directly on the raw 2048 × 3 point set input ­ to the best of our knowledge this work is the first to present a GAN for point clouds. The architecture of the discriminator is identical to the AE (modulo the filter-sizes and the number of neurons), without any batch-norm and with leaky ReLUs Maas et al. (2013) instead or ReLUs. The output of the last fully connected layer is fed into a sigmoid neuron. The generator takes as input a 128-dimensional noise vector and maps it to a 2048 × 3 output by 5 FC-ReLU layers.

Latent-space GAN (l-GAN). In our l-GAN, in-

stead of operating on the raw point cloud input, we

pass the data through our pre-trained autoencoder, x trained separately for each object class with the EMD (or Chamfer) loss function. Both the generator and the discriminator of the GAN then operate on the 128-

E

r Gz
z GAN

D

Dx

dimensional bottleneck variable of the AE. Finally,

once the GAN training is over, the output of the generator is decoded to a point cloud via the AE

decoder. The architecture for the l-GAN is significantly simpler than the one of the r-GAN. We found

that very shallow designs for both the generator and discriminator (in our case, one hidden FC layer

for the generator and two FC for the discriminator) are sufficient to produce realistic results.

Gaussian Mixture Model. In addition to the l-GANs, we also train a family of Gaussian Mixture Models (GMMs) on the latent spaces learned by our AEs. We fitted GMMs with varying numbers of Gaussian components, and experimented with both diagonal and full covariance matrices for the Gaussians. The GMMs can be turned into point-cloud generators by first sampling the latent-space from the GMM distribution and accordingly using the AE's decoder, similarly to the l-GANs.

4 EVALUATION AND RESULTS
Our source for shapes is the ShapeNet repository (Chang et al., 2015); we pre-center all shapes into a sphere of diameter 1. Unless otherwise stated, we train specific per-class models, and split the models in each class into training/testing/validation set using a 85%-5%-10% split.
4

Under review as a conference paper at ICLR 2018

Figure 1: Reconstructions of unseen shapes from the test split of the input data. The leftmost image of each pair shows the ground truth shape, the rightmost the shape produced after encoding and decoding using our class-specific AEs.

Figure 2: Interpolating between different point clouds, using our latent space representation.

4.1 EVALUATING THE LATENT REPRESENTATION
Classification. A common technique for evaluating the quality of unsupervised representation learning algorithms is to apply them as a feature extractor on supervised datasets and evaluate the performance of linear models fitted on top of these features. We use this technique to evaluate the performance of the latent "features" computed by our AE. For this experiment to be meaningful, the AE was trained across all different shape categories: we used 57,000 models from ShapeNet from 55 categories of man-made objects. Exclusively for this experiment, we used a bigger bottleneck of 512, increased the number of neurons and applied batch-norm to the decoder as well. To obtain features for an input 3D shape, we feed forward to the network its point-cloud and extract the 512-dimensional bottleneck layer vector. This feature is then processed by a linear classification SVM trained on the de-facto 3D classification benchmark of ModelNet Wu et al. (2015). Table 1 shows comparative results. Note that previous state of the art Wu et al. (2016) uses several layers of a GAN to derive a 7168-long feature; our 512-dimensional feature is more intuitive and parsimonious.

Dataset
MN10 MN40

SPH[1]
79.8% 68.2%

LFD[2]
79.9% 75.5%

T-L-Net[3]
74.4%

VConv-DAE[4]
80.5% 75.5%

3D-GAN[5]
91.0% 83.3%

ours - EMD
95.3% 84.0%

ours - CD
95.0% 84.5%

Table 1: Classification performance on ModelNet40 and ModelNet10. All methods train a linear SVM with features derived in an unsupervised manner. Comparing to [1] Kazhdan et al. (2003), [2] Chen et al. (2003), [3] Girdhar et al. (2016a), [4] Sharma et al. (2016), [5] Wu et al. (2016).

The decoupling of latent representation from generation allows flexibly choosing the AE loss, which can improve the learned feature. On ModelNet10, which includes primarily larger objects than ModelNet40, an AE trained with the EMD loss performs slightly better than one trained with the CD. On the other hand, when the variation within the collection increases, CD produces better results. This is perhaps due to its more local and less smooth nature, which allows it to understand rough edges and some high frequency geometric details. Finally, note that since our AEs were not trained on ModelNet, this experiment also demonstrates the domain-robustness of our learned features.

Qualitative Evaluation. To visually assess the quality of the learned representation, we show some reconstruction results in Fig. 1. Here, we use our AEs to encode samples from the test split of the ground truth dataset (the leftmost of each pair of images) and then decode them and compare them visually to the input (the rightmost image). These results shows the ability of our learned representation to generalize to unseen shapes. In addition to reconstruction, our learned latent representation enables a number of interesting shape editing applications, including shape interpolations (Fig. 2), part editing and shape analogies. More results are showcases in Appendix E.

5

Under review as a conference paper at ICLR 2018

Generalization Ability. Our AEs are able to reconstruct unseen shapes; this is highlighted not only in the results of Figure 1, but also in quantitative measurements of the fidelity and coverage of the reconstructed ground truth datasets (see appendix, Table 5) and by the comparable reconstruction quality on the training vs. test splits (appendix, Figure 12).

4.2 EVALUATING THE GENERATIVE MODELS
We train and compare a total of five generative models on the data distribution of point-clouds of the chair category. We begin by establishing the two AEs with the 128-dimensional bottleneck, trained with the CD or EMD loss respectively ­ referred to as AE-CD and AE-EMD. Both AEs were stopped at the epoch at which the average reconstruction error with respect to our validation dataset was minimized (see Appendix, Figure 12). We train an l-GAN in each of the AE-CD and AE-EMD latent spaces. In the space associated only with the AE-EMD we train a further two models: an identical (architecture-wise) l-GAN that utilizes the Wasserstein objective with gradient-penalty (Gulrajani et al., 2017), and a family of GMMs. Lastly, we also train an r-GAN directly on the point cloud data.

Model Selection. All GANs are trained for maximally 2000 epochs; for each GAN, we select one of its training epochs to obtain the "final" model, based on how well the synthetic results match the ground-truth distribution. Specifically, at a given epoch, we use the GAN to generate a set of synthetic point clouds, and measure the distance between this set and the validation set (Section 2). We avoid measuring this distance using MMD-EMD, given the high computational cost of EMD. Instead, we use either the JSD or MMD-CD metrics to compare the synthetic dataset to the validation dataset. To further reduce the computational cost of model selection, we only check every 100 epochs (50 for r-GAN). The epochs at which the various models were selected using the JSD criterion are shown in Table 3. Using the same criterion, we also select the number and covariance type of Gaussian components for the GMM, and obtain the optimal value of 32 components. GMMs performed much better with full (not diagonal) covariance matrices, suggesting strong correlations between the latent dimensions; see appendix (Fig. 14). When using MMD-CD as the selection criterion, we obtain models of similar quality and at similar stopping epochs (appendix, Table 6); the optimal number of Gaussians in this case was 40.

Metric
JSD Classification
MMD-CD

r-GAN
0.1660 84.10 0.0017

Wu et al. (2016)
0.1705 87.00 0.0042

l-GAN (AE-CD)
0.0372 96.10 0.0015

l-GAN (AE-EMD)
0.0188 94.53 0.0018

l-WGAN (AE-EMD)
0.0077 89.35 0.0015

GMM (AE-EMD)
0.0048 87.40 0.0014

Table 2: Evaluating 5 generators on train-split of chair dataset on epochs/models selected via minimal JSD on the validation-split. Note that the average classification score attained by the ground-truth point clouds was 84.7%.

Method
r-GAN l-GAN (AE-CD) l-GAN (AE-EMD) l-WGAN (AE-EMD) GMM-32-F (AE-EMD)

Epoch
1700 300 100 1800
-

JSD
0.1764 0.0486 0.0308 0.0227 0.0202

MMD-CD
0.0020 0.0020 0.0023 0.0019 0.0018

MMD-EMD
0.1230 0.0796 0.0697 0.0660 0.0651

COV-EMD
19.0 32.2 57.1 66.9 67.4

COV-CD
52.3 59.4 59.3 67.6 68.9

Table 3: Evaluating 5 generators on test-split of chair dataset on epochs/models selected via minimal JSD on the validation-split. The reported scores are averages of 3 pseudo-random repetitions. GMM32-F stands for a GMM with 32 Gaussian components with full covariances.

Quantitative Evaluation. Upon selection of the models, we compare them with respect to their capacity to generate synthetic samples. In two different sets of experiments, we measure how well the distribution of the generated samples resembles both the train and test splits of the ground truth distribution, by using our models to generate a set of synthetic point clouds and using the metrics
6

Under review as a conference paper at ICLR 2018
Figure 3: The CD distance is less faithful than EMD to visual quality of synthetic results; in this case it favors r-GAN results, due to the presence of high-density areas in the synthesized point sets.
from Section 2 to compare against the train or test set distributions respectively. The train split results are reported in Table 2. We also show the average classification probability for those samples being recognized as a chair using the PointNet classifier (Qi et al., 2016a), which is state of the art for classifying point clouds. A similar experiment is ran to measure how well the synthetic samples match the test split dataset; here, we repeat the experiment with three pseudo-random seeds and report the average measurements in Table 3, for various comparison metrics. Perhaps surprisingly, training a simple Gaussian mixture model in the latent space of the EMD-based AE yields the best results in terms of both fidelity and coverage. Furthermore, GMMs are particularly easy to train. Additionally, the achieved fidelity and coverage are very close to the reconstruction baseline, namely, the lower bounds for the JSD and MMD achieved by the AE on which the generative models operate (appendix, Table 5). For example, the AE-EMD achieved an MMD-EMD of 0.05 with respect to the ground truth training data , which is comparable with the MMD-EMD value of 0.06 achieved by the GMMs with respect to the test data. Finally, by comparing Table 2 and Table 3 we can again establish the generalization ability of our models, since their performance for the training vs. testing splits is comparable. This is highlighted in more detail in Fig. 13 in the appendix.
Note: The number of synthetic point clouds we generate for the train split experiment is equal to the size of the train dataset. For the test split experiment, as well as for the validation split comparisons done for model selection, we generate synthetic datasets that are three times bigger than the ground truth dataset (the test resp. validation set); this is possible due to the relatively small size of the test resp. validation sets, and helps reduce sampling bias. This is only necessary when measuring MMD or Coverage statistics.
Fidelity of metrics. In Table 3 we note that the MMD-CD distance to the test set appears to be relatively small for the r-GANs. This seeming advantage of the r-GANs is counter to what a qualitative inspection of the results yields. We attribute this effect to the inadequacy of the chamfer distance to distinguish pathological cases such as the ones demonstrated in Fig. 3. In the figure, we show two triplets of images. In each triplet, an r-GAN and an l-GAN is used to generate a synthetic set of point clouds; the left triplet shows an l-GAN on the AE-CD and the right an l-GAN on the AE-EMD. For a given ground truth point cloud from the test set (leftmost image of each triplet), we find its nearest neighbor in each synthetic set under the chamfer distance - the middle image in each triplet shows the nearest neighbor in the synthetic results of the r-GAN and the right most image the nearest neighbor in the l-GAN set. We report the distances between these nearest neighbors and the ground truth using both CD and EMD (in-image numbers). Note that the CD values miss the fact that the r-GAN results are visibly of lesser quality. The underlying reason appears to be that r-GANs tend to generate clouds with many points concentrated in the areas that are most likely to be occupied in the underlying shape class (e.g. the seat of chairs in the figure). This implies that one of the two terms in the CD ­namely, the one going from the synthetic point cloud to the ground truth­ is likely to be very small for r-GAN results. The "blindness" of the CD metric to only partial matches between shapes has the additional interesting side-effect that the CD-based coverage metric is consistently bigger than that reported by EMD, as noted in Table 3. Instead, the EMD distance promotes a one-to-one mapping and thus correlates more strongly to visual quality; this means that it heavily penalizes the r-GAN result both in terms of MMD and coverage.
Training trends. We performed extensive measurements during training of our models, to understand their behavior during training, as shown in Fig. 4. On the left, we plot the JSD distance between the ground truth test set and synthetic datasets generated by the various models at various epochs of training. On the right, we also plot the EMD-based MMD and Coverage between the same two sets, where larger marker symbols denote a higher epoch. In general, r-GAN struggles to provide
7

Under review as a conference paper at ICLR 2018
good coverage of the test set no matter the metric used; which alludes to the well-established fact that end-to-end GANs are generally difficult to train. The l-GAN (AE-CD) performs better in terms of fidelity with much fewer epochs as measured by JSD/MMD-EMD, but its coverage remains low. We attribute this to the CD promoting unnatural topologies ­ cf. Fig. 3 that visually shows this phenomenon. Switching to an EMD-based AE for the representation and otherwise using the same latent GAN architecture (l-GAN, AE-EMD), yields a dramatic improvement in coverage and fidelity. Both l-GANs though suffer from the known issue of mode collapse: Half-way through training, first coverage starts dropping with fidelity still at good levels, which implies that they are overfitting a small subset of the data. Later on, this is followed by a more comprehensive collapse, with coverage dropping as low as 0.5%. Switching to a latent WGAN largely eliminates this collapse, as expected.
Figure 4: Training trends for the various generative models, in terms of coverage / fidelity to the ground truth test dataset. On the right, the curve markers indicate epochs 1, 10, 100, 200, 400, 1000, 1500, 2000, with larger symbols denoting higher epochs. See text for more details.
Comparisons to voxel-based methods. To the best of our knowledge we are the first to propose GANs on point-cloud data. To find out how our models fare against other 3D generative methods, in Table 2 and Fig. 4 we compare to a recent voxel-grid based approach (Wu et al., 2016) in terms of the JSD on the training set of the chair category - other shape categories can be found in the appendix (Table 7). We convert their voxel grid output into a point-set with 2048 points by performing farthest-point-sampling on the isosurface of the grid values. Per the authors' suggestion, we used an isovalue parameter of 0.1 and isolated the largest connected component from the isosurface. Since Wu et al. (2016) do not use any train/test split, we perform 5 rounds of sampling 1k synthetic results from their models, and report the best values of the computed evaluation metrics in Table 2. The r-GAN mildly outperforms Wu et al. (2016) in terms of its diversity (as measured by JSD/MMD), while also creating realistic-looking results, as shown by the classification score. The l-GANs perform even better, both in terms of classification and diversity, with less training epochs. Note also that the training time for one epoch of the l-GAN is more than an order of magnitude smaller than for the r-GAN, due to its much smaller architecture and dimensionality. For fairness, we acknowledge that since Wu et al. (2016) operates on voxel grids, it is not necessarily on equal standing when it comes to generating point clouds.
Qualitative evaluation. In Fig. 5, we show some synthetic results produced by our l-GANs (top row) and the 32-component GMM, both trained on the AE-EMD latent space. We notice high quality results from either model - this highlights the strength of our learned representation, which makes it possible for the simple GMM model to perform well. The shapes (after decoding) corresponding to the 32 means of the Gaussian components can be found in the appendix (Fig. 15), as well as results using the r-GAN (Fig. 11). The l-GAN produces crisper and less noisy results than the r-GAN, demonstrating an advantage of using a good structural loss on the decoupled, pre-trained AE.
Extensions to multiple classes We have performed preliminary experiments with an AE-EMD trained on a mixed set containing point clouds from 5 categories (chair, airplane, car, table, sofa). The training and testing datasets for this AE were constructed by randomly picking and adding models from each class; 2K models per class for the training set, 200 models for testing and 100 for validation. The multi-class AE has the same bottleneck size of 128 and was trained for 1000 epochs. We compare against the class-specific AEs with the 85-5-10 train-val-test-split, which we trained for 500 epochs. The precise AE model in all cases was selected based on the minimal reconstruction loss on the the
8

Under review as a conference paper at ICLR 2018

Figure 5: Synthetic point clouds generated by samples produced with l-GAN (top) and 32-component GMM (bottom), both trained on the latent space of an AE using the EMD loss.
respective validation set. On top of all six AEs, we train six l-WGANs for 2K epochs, and evaluate their fidelity/coverage using the MMD-CD between the respective testing sets and a synthesized dataset of 3x the size, as above. It turns out that the l-WGANs based on the multi-class AE perform similarly to the dedicated class-specifically trained ones (Table 4). A qualitative comparison (Fig. 6) also reveals that by using a multi-class AE-EMD we do not sacrifice much in terms of visual quality compared to the dedicated AEs.

Figure 6: Synthetic point clouds generated by samples produced with l-WGANs trained in the latent space of an AE-EMD trained on a multi-class dataset.

airplane car chair sofa table average multi-class

train 0.0004 0.0006 0.0015 0.0011 0.0013 0.0010 test 0.0006 0.0007 0.0019 0.0014 0.0017 0.0013

0.0011 0.0014

Table 4: MMD-CD measurements for l-WGANs stopped at the two-thousand epoch and trained on the latent spaces of dedicated (left 5 columns) and multi-class EMD-AEs (right column) .

Limitations. Fig. 7 shows some failure cases of our models. Chairs with rare geometries (left two images) are sometimes not faithfully decoded. Additionally, the AEs may miss high-frequency geometric details, e.g. a hole in the back of a chair (middle), thus altering the style of the input shape. Finally, the r-GAN often struggles to create realistic-looking shapes (right) for some shape classes ­ while the r-GAN chairs that are easily visually recognizable, it has a harder time on cars. Designing more robust raw-GANs for point clouds remain an interesting avenue for future work.
5 RELATED WORK
A number of recent works (Wu et al. (2016), Wang et al. (2016), Girdhar et al. (2016b), Brock et al. (2016), Maimaitimin et al. (2017), Zhu et al. (2016)) have explored generative and discriminative representations for geometry. They operate on different modalities, typically voxel grids or view-based image projections. To the best of our knowledge, our work is the first to study such representations for point clouds.
Training Gaussian mixture models (GMM) in the latent space of an autoencoder is closely related to VAEs (Kingma & Welling, 2013). One documented issue with VAEs is over-regularization: the regularization term associated with the prior, is often so strong that reconstruction quality suffers (Bowman et al., 2015; Sønderby et al., 2016; Kingma et al., 2016; Dilokthanakul et al., 2016) The literature contains methods that start only with a reconstruction penalty and slowly increase the weight of the regularizer. In our case, we find that fixing the AE before we train our generative models yields good results.
9

Under review as a conference paper at ICLR 2018
Figure 7: Limitations: The AEs might fail to reconstruct shapes of uncommon/overly detailed geometry (left four images). The r-GAN may synthesize noisy/unrealistic results, cf. a car (right).
6 CONCLUSION
We presented a novel set of architectures for 3D point-cloud representation learning and generation. Our results show good generalization to unseen data and our representations encode meaningful semantics. In particular our generative models are able to produce faithful samples and cover most of the ground truth distribution without memorizing a few examples. Interestingly, we see that the best-performing generative model in our experiments is a GMM trained in the fixed latent space of an AE. While, this might not be a universal result, it suggests that simple classic tools should not be dismissed. A thorough investigation on the conditions under which simple latent GMMs are as powerful as adversarially trained models would be of significant interest.
REFERENCES
Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adversarial networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR, volume 2016, 2017.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
André Brock, Theodore Lim, James M. Ritchie, and Nick Weston. Generative and discriminative voxel modeling with convolutional neural networks. CoRR, 2016.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. CoRR, abs/1312.6203, 2013.
Angel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. Shapenet: An information-rich 3d model repository. CoRR, abs/1512.03012, 2015.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung. On Visual Similarity Based 3D Model Retrieval. Computer Graphics Forum, 2003. ISSN 1467-8659. doi: 10.1111/1467-8659. 00669.
Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 3837­3845, 2016.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in neural information processing systems, pp. 1486­1494, 2015.
Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.
10

Under review as a conference paper at ICLR 2018
Haoqiang Fan, Hao Su, and Leonidas J. Guibas. A point set generation network for 3d object reconstruction from a single image. CoRR, abs/1612.00603, 2016.
Rohit Girdhar, David F. Fouhey, Mikel Rodriguez, and Abhinav Gupta. Learning a Predictable and Generative Vector Representation for Objects, pp. 484­499. Springer International Publishing, Cham, 2016a. ISBN 978-3-319-46466-4. doi: 10.1007/978-3-319-46466-4_29.
Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Abhinav Gupta. Learning a predictable and generative vector representation for objects. In European Conference on Computer Vision, pp. 484­499. Springer, 2016b.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 2672­2680. Curran Associates, Inc., 2014.
Ishaan Gulrajani, Faruk Ahmed, Martín Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. CoRR, abs/1704.00028, 2017.
Vishakh Hegde and Reza Zadeh. Fusionnet: 3d object classification using multiple data representations. arXiv preprint arXiv:1607.05695, 2016.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. CoRR, 2015.
Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, and Siddhartha Chaudhuri. 3d shape segmentation with projective convolutional networks. CoRR, abs/1612.02808, 2016.
Michael Kazhdan, Thomas Funkhouser, and Szymon Rusinkiewicz. Rotation invariant spherical harmonic representation of 3d shape descriptors. In Proceedings of the 2003 Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, SGP '03, pp. 156­164, Aire-la-Ville, Switzerland, Switzerland, 2003. Eurographics Association. ISBN 1-58113-687-0.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse autoregressive flow. arXiv preprint arXiv:1606.04934, 2016.
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In Proceedings of the 30 th International Conference on Ma- chine Learning (ICML-13), 2013.
Maierdan Maimaitimin, Keigo Watanabe, and Shoichi Maeyama. Stacked convolutional autoencoders for surface recognition based on 3d point cloud data. Artificial Life and Robotics, pp. 1­6, 2017. ISSN 1614-7456. doi: 10.1007/s10015-017-0350-9.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. arXiv preprint arXiv:1612.00593, 2016a.
Charles Ruizhongtai Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan Yan, and Leonidas J. Guibas. Volumetric and multi-view cnns for object classification on 3d data. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 5648­5656, 2016b. doi: 10.1109/CVPR.2016.609.
Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. CoRR, abs/1706.02413, 2017. URL http: //arxiv.org/abs/1706.02413.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. The earth mover's distance as a metric for image retrieval. International Journal of Computer Vision, (2):99­121, 2000. ISSN 1573-1405. doi: 10.1023/A:1026543900054.
11

Under review as a conference paper at ICLR 2018
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988.
Raif M. Rustamov, Maks Ovsjanikov, Omri Azencot, Mirela Ben-Chen, Frédéric Chazal, and Leonidas Guibas. Map-based exploration of intrinsic shape differences and variability. ACM Trans. Graph., 32(4):72:1­72:12, July 2013. ISSN 0730-0301. doi: 10.1145/2461912.2461959. URL http://doi.acm.org/10.1145/2461912.2461959.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2226­2234, 2016.
Abhishek Sharma, Oliver Grau, and Mario Fritz. Vconv-dae: Deep volumetric shape learning without object labels. In Geometry Meets Deep Learning Workshop at European Conference on Computer Vision (ECCV-W), 2016.
Casper Kaae Sønderby, Tapani Raiko, Lars Maaløe, Søren Kaae Sønderby, and Ole Winther. How to train deep variational autoencoders and probabilistic ladder networks. arXiv preprint arXiv:1602.02282, 2016.
Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik G. Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 945­953, 2015. doi: 10.1109/ICCV.2015.114.
Yueqing Wang, Zhige Xie, Kai Xu, Yong Dou, and Yuanwu Lei. An efficient and effective convolutional auto-encoder extreme learning machine network for 3d feature learning. Neurocomputing, 174:988­998, 2016.
Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne Vouga, and Hao Li. Dense human body correspondences using convolutional networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 1544­1553, 2016. doi: 10.1109/CVPR.2016.171.
Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 82­90. Curran Associates, Inc., 2016.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pp. 1912­1920, 2015. doi: 10.1109/CVPR.2015.7298801.
Li Yi, Vladimir G. Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas J. Guibas. A scalable active framework for region annotation in 3d shape collections. ACM Trans. Graph., (6):210:1­210:12, 2016a.
Li Yi, Hao Su, Xingwen Guo, and Leonidas J. Guibas. Syncspeccnn: Synchronized spectral CNN for 3d shape segmentation. CoRR, 2016b.
Zhuotun Zhu, Xinggang Wang, Song Bai, Cong Yao, and Xiang Bai. Deep learning representation using autoencoder for 3d shape retrieval. Neurocomputing, 2016. ISSN 0925-2312. doi: https: //doi.org/10.1016/j.neucom.2015.08.127.
A AE DETAILS
With the exception of the larger AE used for the cross-class classification experiment of Section 4.1, the first 4 MLP layers of our AE were implemented as 1D-convolutions with ReLUs and stride of 1, i.e. treating each point independently. Their filter sizes were 64, 128, 128, 256 and k respectively, with k being the bottle-neck size. The decoder had 3 FC-ReLU layers with 256, 256, 2048 × 3
12

Under review as a conference paper at ICLR 2018

neurons each. For training we used Adam with initial learning rate of 0.0005, beta-1 of 0.9 and a batch size of 50 point-clouds.
Different AE setups (denoising/regularised) brought no noticeable advantage over our "vanilla" architecture. Adding drop-out layers resulted in worse reconstructions and using batch-norm on the encoder only, sped up training and gave us slightly better generalization error when the AE was trained with single-class data.

B R-GAN DETAILS
The discriminator's first 5 layers are MLPs with filter sizes of {64, 128, 256, 256, 512} each and leaky-ReLU between them. They are followed by a feature-wise max-pool. The last 2 FC-leakyReLU layers have {128, 64}, neurons each leading to single sigmoid neuron. We use a leak of 0.2 units.
The generator consists of 5 FC-ReLU layers with {64, 128, 512, 1024, 2048 × 3} neurons each. We trained r-GAN with ADAM with initial learning rate of 0.0001, and beta-1 of 0.5 in batches of size 50. The noise vector was drawn by a spherical Gaussian of 128 dimensions with zero mean and 0.2 units of standard deviation.

C L-GAN DETAILS
The discriminator consists of 2 FC-ReLU layers with {256, 512} neurons each and a final FC layer with a single sigmoid neuron. The generator consists of 2 FC-ReLUs with {128, 128} neurons each. When used the l-Wasserstein-GAN, we used a gradient penalty regularizer  = 10. The training parameters (learning rate, batch size) and the generator's noise distribution were the same used for the r-GAN.

D AE RECONSTRUCTION QUALITY

Table 5 shows the reconstruction quality of the two AEs (CD- and EMD-based), in terms of the JSD of the reconstructed datasets (train or test, as noted in the table) with respect to their ground truth counterparts. Note that, in general, the quality of reconstruction is comparable between the training and test datasets, indicating that the AEs are indeed able to generalize.

Method JSD (Tr) JSD (Te) MMD-CD (Tr) MMD-EMD (Tr)

AE-CD 0.0216 0.0243 AE-EMD 0.0028 0.0067

0.0004 0.0005

0.0753 0.0527

Table 5: Effect of loss-type for AE reconstructions. The EMD loss gives rise to reconstructions with significantly better JSD compared to Chamfer. MMD-measurements favor the AE that was trained with the same loss under which the MMD measurement is computed. (Tr: Train split, Te: Test split)

E APPLICATIONS OF THE LATENT SPACE REPRESENTATION

For shape editing applications, we use the embedding we learned with the AE-EMD trained across all 55 object classes, not separately per-category. This showcases its ability to encode features for different shapes, and enables interesting applications involving different kinds of shapes.

Editing shape parts. We use the shape annotations of Yi et al.Yi et al. (2016a) as guidance to

modify shapes. As an example, assume that a given object category (e.g. chairs) can be further

subdivided into two sub-categories A and B: every object A  A possesses a certain structural

property (e.g. has armrests, is four-legged, etc.) and objects B  B do not. Using our latent

representation we can model this structural difference between the two sub-categories by the difference

between their average latent representations xB - xA, where xA = xA, xB = xB. Then,

AA

BB

13

Under review as a conference paper at ICLR 2018
Figure 8: Editing parts in point clouds using vector arithmetic on the AE latent space. Left to right: tuning the appearance of cars towards the shape of convertibles, adding armrests to chairs, removing handle from mug.
Figure 9: Interpolating between different point clouds, using our latent space representation. Note the interpolation between structurally and topologically different shapes. given an object A  A, we can change its property by transforming its latent representation: xA = xA + xB - xA, and decode xA to obtain A  B. This process is shown in Figure 8. Note that the volume of chairs with armrests is on average 10% larger than the chairs without, which is reflected in the output of this process. Interpolating shapes. By linearly interpolating between the latent representations of two shapes and decoding the result we obtain intermediate variants between the two shapes. This produces a "morph-like" sequence with the two shapes at its end points (Fig. 2, 9). Our latent representation is powerful enough to support removing and merging shape parts, which enables morphing between shapes of significantly different appearance. Our cross-category latent representation enables morphing between shapes of different types, cfg. the second row for an interpolation between a bench and a sofa. More results are available in the additional material. Shape Analogies. Another demonstration of the euclidean nature of the latent space is demonstrated by finding "analogous" shapes by a combination of linear manipulations and euclidean nearestneighbor searching. Concretely, we find the difference vector between A and A , we add it to shape B and search in the latent space for the nearest-neighbor of that result, which yields shape B . We demonstrate the finding in Fig. 10 with images taken from the meshes used to derive the underlying point-clouds to help the visualization. Finding shape analogies has been of interest recently in the geometry processing community Rustamov et al. (2013).
F FURTHER EVALUATION AND RESULTS
14

Under review as a conference paper at ICLR 2018

A A'

B B'

Figure 10: Shape Analogies using our learned representation. Shape B relates to B in the same way that shape A relates to A.

Figure 11: Synthetic results produced by the r-GAN.

Reconstruction Loss

Reconstruction Loss

#10-3 1.5
1

chamfer

train test val

0.08 0.07 0.06

emd

train test val

0.05

0.5

0.04

1 10 100 1000

1 10 100 1000

Bottleneck Size

Bottleneck Size

Figure 12: The optimal bottleneck size was fixed at 128 by observing the reconstruction loss of the AEs, shown here for various bottleneck sizes.

JSD

0.8 JSD in Train and Test Latent GAN (EMD AE) Train
0.7 Latent GAN (EMD AE) Test Latent WGAN (EMD AE) Train
0.6 Latent WGAN (EMD AE) Test Raw GAN Train
0.5 Raw GAN Test

0.009 MMD-CD in Train and Test Latent GAN (EMD AE) Train
0.008 Latent GAN (EMD AE) Test Latent WGAN (EMD AE) Train
0.007 Latent WGAN (EMD AE) Test Raw GAN Train
0.006 Raw GAN Test

JSD

0.4 0.005

0.3 0.004

0.2 0.003

0.1 0.002

0.0 1

500

Ep10o0c0hs

1500

2000

0.001 0

200 400 Epochs 600 800 1000

Figure 13: Generalization error of the various GAN models, at various training epochs. Generalization is estimated using the JSD (left) and MMD-CD (right) metrics, which measure closeness of the synthetic results to the training resp. test ground truth distributions. The plots show the measurements of various GANs.

15

Under review as a conference paper at ICLR 2018
Figure 14: GMM model selection. GMMs with a varying number of Gaussians and covariance type are trained on the latent space learned by and AE trained with EMD and a bottleneck of 128. Models with a full covariance matrix achieve significantly smaller JSD than models trained with diagonal covariance. For those with full covariance, 30 or more clusters seem sufficient to achieve minimal JSD. On the right, the values in a typical covariance matrix of a Gaussian component are shown in pseudocolor - note the strong off-diagonal components.
Figure 15: The 32 centers of the GMM fitted to the latent codes, and decoded using the decoder of the AE-EMD.
16

Under review as a conference paper at ICLR 2018

Method
r-GAN l-GAN (AE-CD) l-GAN (AE-EMD) l-WGAN (AE-EMD) GMM-40-F (AE-EMD)

Epoch
1350 300 200 1700
-

JSD
0.1893 0.0463 0.0319 0.0240 0.0182

MMD-CD
0.0020 0.0020 0.0022 0.0020 0.0018

MMD-EMD
0.1265 0.0800 0.0684 0.0664 0.0646

COV-EMD
19.4 32.6 57.6 64.2 68.6

COV-CD
54.7 58.2 58.7 64.7 69.3

Table 6: Evaluation of five generators on test-split of chair data on epochs/models that were selected via minimal MMD-CD on the validation-split. The reported scores are averages of three pseudorandom repetitions. Compare this with Table 3. Note that the overall quality of the selected models remains the same, irrespective of the metric used for the selection. GMM-40-F stands for a GMM with 40 Gaussian components with full covariances.

MMD-CD

0.014 0.012 0.010 0.008 0.006 0.004 0.002 0.000 0

Raw GAN Latent GAN (Chamfer AE) Latent WGAN (EMD AE) Latent GAN (EMD AE)

500 Ep1o00c0hs 1500

2000

Figure 16: Training trends in terms of the MMD-CD metric for the various GANs. Here, we sample a set of synthetic point-clouds for each model, of size 3x the size of the ground truth test dataset, and measure how well this synthetic dataset matches the ground truth in terms of MMD-CD. This plot complements Fig. 4 (left), where a different evaluation measure was used - note the similar behaviour.

Class Wu et al. (2016)

airplane car rifle sofa table

0.1890 0.2012 0.1812 0.2472

L-GAN (AE-EMD)

train
0.0149 0.0081 0.0212 0.0102 0.0058

test
0.0268 0.0109 0.0364 0.0102 0.0177

Full GMM/32 (AE-EMD)

train
0.0065 0.0063 0.0092 0.0102 0.0035

test
0.0191 0.0108 0.0214 0.0101 0.0143

Table 7: JSD-based comparison between Wu et al. (2016) and our generative models. Full GMM/32 stands for a GM model trained on the latent space of our AE with the EMD structural loss. Note that the l-GAN here uses the same "vanilla" adversarial objective as Wu et al. (2016).

17

