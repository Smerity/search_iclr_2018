Under review as a conference paper at ICLR 2018
GENERALIZED GRAPH EMBEDDING MODELS
Anonymous authors Paper under double-blind review
ABSTRACT
Many types of relations in physical, biological, social and information systems can be modeled as homogeneous or heterogeneous concept graphs. Hence, learning from and with graph embeddings has drawn a great deal of research interest recently, but only ad hoc solutions have been obtained this far. In this paper, we conjecture that the one-shot supervised learning mechanism is a bottleneck in improving the performance of the graph embedding learning algorithms, and propose to extend this by introducing a multi-shot unsupervised learning framework. Empirical results on several real-world data set show that the proposed model consistently and significantly outperforms existing state-of-the-art approaches on knowledge base completion and graph based multi-label classification tasks.
1 INTRODUCTION
Recent studies have highlighted the importance of learning distributed representations for symbolic data in a wide variety of artificial intelligence tasks (Bengio et al., 2013). Research on word embeddings (Mikolov et al., 2013) has led to breakthroughs in many related areas, such as machine translation (Bahdanau et al., 2015), question answering (Xiong et al., 2016), and visual-semantic alignments (Karpathy & Fei-Fei, 2017). However, learning to predict for large-scale knowledge graphs (KGs) is still a challenging problem left, this is largely due to the diversity of the ontologies, and the semantic richness of the concepts, which makes it really hard to generate proper and universally applicable graph embeddings, simply based on word-level embeddings (Cai et al., 2017).
Being able to generate reasonable and accurate distributed representations for large-scale knowledge graphs would be particularly valuable, in that it may help predict unobserved facts from limited concepts, uncover gaps in our knowledge, suggest new downstream applications, which clearly reflects the central concerns of the artificial intelligence (Nickel et al., 2016a; Henaff et al., 2017). Therefore, massive attention has been devoted to the potential of embedding entities and relationships of multi-relational data in low-dimensional vector spaces in recent years (Wang et al., 2017).
In this paper, we consider the problem of developing simple and efficient model for learning neural representation of generalized knowledge graphs, including the multi-relational heterogeneous graphs, and more specifically defined homogeneous graphs (such as social and biological networks).
Following the pioneer work of Nickel et al. (2011) and Bordes et al. (2013), almost all of the stateof-the-art approaches try to model the graph embedding learning problem as supervised binary classification problems, their objective functions are usually one-shot (single purpose) . We argue that prior research in this area might have been affected and biased by " established priors", which prevents the formulation of a methodology that is objective enough to cope with the highly sparse knowledge graphs. We propose to handle the embedded learning problem of knowledge graphs with an unsupervised neural network model, called the Graph Embedding Network (GEN). The proposed model consists of three simple multi-layer perceptron (MLP) cells, each cell operates in response to a different "query" with regard to the input fact, which will be trained sequentially. The formulation of the model is inspired by the neural sequence-to-sequence (seq2seq) model (Sutskever et al., 2014), except that we attempt to use the MLP cells to mimic the sequence learning capability of the recurrent neural network (RNN), to model the semantic structure of the knowledge graphs.
The major contribution of this paper is that: (1) we propose GEN, a novel and efficient multishot framework for embedding learning in generalized knowledge graphs. (2) We show how GEN is in accordance with established principles in cognitive science, providing flexibility in learning representations that works on graphs conforming to different domains.
1

Under review as a conference paper at ICLR 2018

2 RELATED WORKS

During the last few years, an increasing amount of research attention has been devoted to the challenge of representation learning on knowledge graphs, especially focused on the potential benefits for the knowledge base completion (KBC) tasks, including the link prediction problem and the relation prediction problem. Among which, the relation translating model TransE (Bordes et al., 2013), the tensor factorization based semantic matching model RESCAL (Nickel et al., 2011), and the neural network based semantic matching model ER-MLP (Dong et al., 2014; Nickel et al., 2016b), are probably the most heavily studied from the methodology perspective. For good surveys on such embedding learning algorithms, see Nickel et al. (2016a), Wang et al. (2017), and Cai et al. (2017).

Broadly speaking, related works can be divided into two categories: linear and non-linear, according to whether the output embedding has a reasonable linear interpretation. State-of-the-art linear models include the TransE, RESCAL, TranH (Wang et al., 2014), DistMult (Yang et al.), and ANALOGY (Liu et al., 2017), while the popular non-linear models include the ER-MLP, ComplEX1 (Trouillon et al., 2016), HoIE (Nickel et al., 2016b), ProjE (Shi & Weninger, 2017) and ConvE (Dettmers et al., 2017). The proposed GEN model is also a non-linear model.

The graph embedding learning models that is most closely related to this work is probably the ProjE model, which makes use of an embedding projection function defined as:
h(r, t) = g(w0 · f (w1rr + w1t t + b1) + b0)
where h, r, t denote the embedding vectors, f (·) and g(·) are non-linear activation functions, w0, w1r and w1t are learnable weight matrices, b0 and b1 are bias vectors. The output ranking scores of entity h with regard to the given query (?, r, t) can be obtained through a softmax function:

Score(hi, r, t) = sof tmax {h(r, t)}i
However, as one could see from above functions, the ProjE model is built upon the query (?, r, t), hence is a one-shot solution, which is distinctly different from our GEN model. Still another difference lies in the definition of the objective loss function, the ProjE model choose to use the (selective) cross-entropy loss based on the open world assumption, while our model uses a simplified cross-entropy loss based on the close world assumption. In order to save the computation cost, the ProjE model introduced a negative sampling process, this could cause potential risks for introducing additional bias. Besides, its candidate sampling process is time consuming and hard to be paralleled.

Another model that is closely related to the GEN model is the ER-MLP model, which can be inter-

preted as creating representation for each element of triples and deriving their existence from this

representation (Nickel et al., 2016a). The ER-MLP model can be defined as:

Score(h, r, t) = wT g {CT (h



r



} t)

where symbol  denotes the vector concatenation operator, vector w and matrix C are global weight vectors shared by all the entities and relations, g(·) is an element-wise non-linear activation function. This model is built upon the fourth query as we defined in Section 3, it is a supervised solution, which is quite different from ours. One well-known disadvantage of the ER-MLP is that, even properly regularized, it is still easily prone to over-fitting on knowledge graph datasets (Nickel et al., 2016b), therefore we do not compare with it in this work, but instead with the ProjE model.

As mentioned before, the primary motivation of this study is to develop a graph embedding model that is universally applicable to a wide variety of situations. In order to verify the validity of our solution on heterogeneous networks, we further test it on multi-label network classification tasks for social networks (BlogCatalog) and biological networks (Protein-Protein Interaction), and compare our results with two state-of-the-art techniques, namely, DeepWalk (Perozzi et al., 2014) and node2vec (Grover & Leskovec, 2016). Both of them are derived directly from the word2vec model (Mikolov et al., 2013), which creating node embeddings of the graphs based on the skip-gram framework, and train the model with corpus generated through random walking on that graph. However, it is shown that the random walk sampling can be insufficient for supervised learning tasks in the sparse network environment (Liu et al., 2016). Our results support this conjecture, the experimental results on benchmark tests provide strong evidence that our model performs much better.

1The ComplEX model can be seen as an extension of the DistMult model in the complex space, albeit there is no nonlinear transformations applied, we treat it as a non-linear model here.

2

Under review as a conference paper at ICLR 2018

Embedding Dictionary

Ne

Nr

Entity Softmax Vector

Ne u k k u 2d

W1e , b1e W0e , be0

E_CELL Sharing parameters

E_CELL

Nr u k W1r , b1r k u 2d W0r , br0

R_CELL

(h,r,t) (h,r,?) (?,r,t) (h,?,t)

concatenation
Relation Softmax Vector
concatenation

Figure 1: GEN: A Graph Embedding Model

W1e

,

b

e 1

W0e

,

b

e 0

W1r

,

b

r 1

W0r

,

b

r 0

3 APPROACH AND MODEL ARCHITECTURE
Most of the prevalent semantic knowledge databases are built upon the Resource Description Framework (RDF) , in which the facts are represented and stored in the form of SPO (Subject, Predicate, Object) triples. Following the convention, we will use the symbol (h, r, t) to represent a unit of facts, in which h, r and t denote the head entity, the relation, and the tail entity, respectively.
The primary motivation of this paper is to develop a representation learning method that is suitable and flexible enough for modeling different types of knowledge graphs from a universal perspective. To achieve this objective, the most important problems to be faced are associated with: how to define the optimization problem and how to solve it. As mentioned above, previous works only consider a one-shot mapping from the embedding space to the criterion space, which we conjecture, would be vulnerable to loss considerable amount of the structured semantic information. For instance, if given a fact (Elvis Presley, profession, singer), one could immediately learn the following queries:
· Q1: What is the profession of Elvis Presley? A1: singer.
· Q2: Can you name a person whose profession is singer? A2: Elvis Presley.
· Q3: What is the possible relationship in between Elvis Presley and singer? A3: profession.
· Q4: Is it true that Elvis Presley's profession is singer? A4: Yes.
In fact, this is the actual way we humans learn the meaning of concepts expressed by a statement. These self-labeled queries reflect the following modeling philosophy: (1) (h, r)  t; (2) (t, r)  h; (3) (h, t)  r; (4) (h, r, t)  T/F; respectively. This has been exclusively adopted by the previous research. However, none of them have systematically investigated the effect of combination all of such information. In this section, we propose a novel multi-shot model to solve this problem. For a more detailed discussion of the motivation and intuition behind this model, see Appendix A.
3.1 OVERVIEW OF THE MULTI-SHOT LEARNING FRAMEWORK
The proposed model (GEN) is designed to process data in sequential form. As shown in Fig.1, GEN consists of three components (cells), each corresponding to an individual query with regard to the given input triple. In this study, we propose to use a 2-layer MLP network to deal with the parameter estimation problem for each query individually, although it can be substituted by any other one-shot models, we only report the test results on MLP cells for simplicity. In training mode, the training set is fed into the system sequentially, each of the triple is decomposed into three self-labeled queries: (h, r, ?)  t, (?, r, t)  h, and (h, ?, t)  r. Each of the queries is fetched into the corresponding cell in order to update the parameters. Since for any given triple, our model would read it from three different perspective, we call it "multi-shot model" to distinguish it from other related works.
3

Under review as a conference paper at ICLR 2018

W1r , b1r W0r , br0

ST-1

W1e , b1e

W0e

,

b

e 0

GEN
ST
t
E_CELL

r
R_CELL
ht h
E_CELL

W1r , b1r W0r , br0

ST+1

W1e , b1e

W0e

,

b

e 0

hr

tr

Input Sequence: hr t

Figure 2: The seq2seq representation of the GEN model.

Parameters of the model can be logically divided into two parts. Firstly, the distribution representation of the entities and the relations are defined in the same d-dimensional space, which, as shown in Fig.1, are organized together as a learnable dictionary of embeddings. Secondly, there exist two types of MLP cells in the model, one deals with the entity prediction tasks, the other is responsible for the relation prediction tasks, which are marked as "E CELL" and "R CELL" respectively. Each individual cell has its own parameter set {W0, b0; W1, b1} representing certain network structures. Please note that two E CELLs are introduced to learn from the labeled entities, based on query (h, r, ?) and (?, r, t). According to our modeling hypothesis, which claims that all of the relations should be treated conceptually instead of syntactically, we propose to share parameters between the E CELLs, the intuition behind is to let them share their memory of each known facts from both side of the relation, so that after training with enough knowledge, the E CELLs will eventually able to learn how to correctly distinguish between valid and invalid entities for the given queries.
Another theoretical explanation of the GEN model is given below. We consider the proposed model as a variant of the RNN model, or more precisely, a neural seq2seq model, as illustrated in Fig.2. When training with the graph (a "document" of triples), the GEN model is organized as a stacked RNN, which consists of two chains: the E CELL chains and the R CELL chains. For any given input (h, r, t), each of the cells works as an individual seq2seq model according to its responsive query. For instance, the R CELL is responsible to query (h, ?, t)  r, it will take the embedding of h and r as input, and take r as its target label, and the parameters (memory) of the R CELL will be updated through back-propagation according to the discrepancy between the prediction results (in this case the softmax vector) and the desired label r. Therefore, the proposed model is completely unsupervised, which is distinctly different from previous works. Also please note that due to the lack of semantic connections between the adjacent triples in the input sequence, we did not consider the "long term memory" in this model, as usually did in real RNN models. Therefore, there only exists one "global memory" in this model -- the parameter of the two types of cells, which is responsible for "learning to remember" the rules of how the knowledge graph is constructed.
3.2 DEFINITION OF THE GEN CELLS
The network structure of the E CELLs and the R CELLs are quite similar, the only difference is that they have different number of neurons in the hidden layer and the output layer, which are defined as hyper-parameters as shown in Fig.1. For simplicity, we only present the implementation details of the E CELLs here. In order to answer query (h, ?, t)  r, the hidden layer of the E CELL takes input from the embedding dictionary according to label h and r, the hidden layer is defined as:

x1 = f (Woe · x0 + b0)

(1)

where x0 = [h  r], denotes the concatenation of the embedding vectors, hence the x0 is a 2d × 1 real-value vector. Woe is a k × 2d weights matrix, b0 is a k × 1 bias vector, k denotes the number of neurons in the hidden layer, and f (·) is a non-linear activation function, in this work, we use the

4

Under review as a conference paper at ICLR 2018

rectified linear unit (ReLU) function for all the experiments (Nair & Hinton, 2010). The output layer takes the hidden state vector x1 as input, mapping it to the target label space:

y^ = g(W1e · x1 + b1)

(2)

where W1e is a Ne × k weights matrix, b1 is a Ne × 1 bias vector, Ne denotes the number of entities in the dictionary, g(·) denotes the softmax function. Hence, y^ is a Ne × 1 probability vector, which means that, when training the model with a given fact (h, r, t) to answer the query (h, r, ?), the predictive results output by the model is a probabilistic distribution over all of the possible candidate entities. The cross-entropy loss with regard to prediction results is then defined as:

Ne L(y^) = - y[i]log(y^[i]) + (1 - y[i])log(1 - y^[i])
i=1

(3)

where y denotes the ground truth, which is a one-hot vector exclusively activated by t. To speed-up the stochastic convex optimization process, we use a mini-batch setting, and rewrite the averaged cross-entropy loss over a batch of multiple samples of size N as following simplified form:

L(y)

=

-1 N

N log(y^i[

ti

])

i=1

(4)

where the subscript i denotes the i-th sample of the batch, ti represent the index of label t in the ground truth vector of that sample. Eq.4 is computationally efficient, however, it tend to ignores the existing knowledge for query (h, r, ?) other than the current fact (h, r, t), which has been proven to be useful for improving performance (Shi & Weninger, 2017). But, our experimental results show that the impact of such a problem can be controlled by means of collaborative correction with related facts under our model framework, which further demonstrate the validity of our modelling assumptions. Hopefully, the lessons learned for designing reasonable and computationally efficient cost functions in this study can serve as exemplars for future work.

4 EXPERIMENTAL RESULTS
We evaluate the proposed model on two distinctly different types of graph embedding learning tasks. Firstly, we evaluate our model on knowledge base completion tasks with the conventional datasets FB15K and WN182, and their upgrade version FB15k-237 and WN18RR3. Secondly, we evaluate our model on graph based multi-label classification tasks with two benchmark datasets from the complex network research area: BlogCatalog and Protein-Protein Interaction (PPI)4. Background information of the datasets and the implementation details of our model are given in Appendix B.
4.1 EVALUATION ON KNOWLEDGE BASE COMPLETION TASKS
The aim of the first evaluation was to assess the performance of the proposed model in link prediction tasks, by comparing it with other state-of-the-art approaches. We report the filtered P @N scores following the protocols proposed by Bordes et al. (2013), which means that all of the known facts would be screened out from the ranking list before calculating the statistics of the hits. The numerical results are presented in Table 1, where the highest scores in each column are presented in bold.
We reproduced all of the results of the existing studies (mostly with the released code), whereas some of which are below the reported record. For a fair comparison of the models, we cite those numbers from the original publications (marked with  symbols). Also, it seems that results reported by Dettmers et al. (2017) only consider the tail entity prediction scenario (without averaging with the head entity prediction results), hence we report two version of the test results of our model, the averaged version is named as GEN(avg.), while the tail entity prediction results are reported with
2Available online at: https://everest.hds.utc.fr/doku.php?id=en:transe 3Available online at: https://github.com/TimDettmers/ConvE 4Available online at: https://snap.stanford.edu/node2vec/

5

Under review as a conference paper at ICLR 2018

Table 1: Link prediction results on WN18, FB15K and WN18RR, FB15K-237 (symbols:  denotes the value is cited from the original source,  denotes the result comes from (Dettmers et al., 2017))

Datasets Measures TransE TransH HoIE Analogy DistMult ComplEX ConvE ProjE GEN(avg.) GEN(opt) GEN(tail)

P@1
44.5 33.7 93.0 93.9 72.8 93.6 93.5
75.7
64.2 90.6 65.0

WN18
P@3
85.9 79.3 94.5 94.4 91.4 94.5 94.7 87.8
91.8 94.1 91.8

P@10
93.8 87.4 94.9 94.7 93.6 94.7 95.5
95.1
94.1 94.5 94.2

WN18RR

P@1 P@3 P@10

2.7 33.1 42.7

1.9 33.7 40.4

35.6 37.8 39.3

37.9 38.9 41.1 30.6

39.2 43.9 45.8 36.0

41.0 49.1 50.7 41.1

31.8 41.7 46.0

37.8 40.2 43.0

38.3 40.5 43.1

39.0 41.7 44.5

FB15K

P@1 P@3 P@10

36.1 59.0 76.2

33.0 40.2 64.6 54.6 59.9 67.0
57.5

59.1 61.3 78.5 73.3 75.9 80.1
66.32

70.7 73.9 85.4 82.4 84.0 87.3 88.4

76.4 84.1 88.8

77.7 84.7 89.0

78.9 86.9 91.6

FB15K-237

P@1 P@3 P@10

17.6 29.6 44.6

19.3 34.0 44.7

8.2 15.2 26.1

13.2 15.5 15.2 22.0

22.8 26.3 26.3 33.0

37.2 41.9 41.9 45.8

17.3 28.0 43.0

20.4 31.3 45.8

20.8 32.1 46.2

29.5 42.3 57.7

model named GEN(tail). Besides, we found that our model tends to remember the reverse facts with regard to the triples that has been processed during the training phase. We argue that this is an inherent characteristic of our modeling methodology, since it would treat such reverse facts as conceptually correct. Therefore, we also report P@N scores after screening out such reverse facts, this model is named as GEN(opt). We consider that under certain practical circumstances, it is reasonable to care about such results, because the reverse facts are direct reflections of the known facts, and in many scenarios, they themselves are useful and effective facts.
From Table 1 one could see that the performance of ComplEX seems much more competitive than other models on both of the WordNet subset, however, according to our tests, TransE and HoIE perform (generalized) more stable than others for all of the subtasks. Also please note that, after filtering out the reverse facts from the ranking list, we recorded a significant increase in P@1 score on WN18, which was not observed in other models. Since most of the semantic relations defined in WordNet are reflexive (Miller, 1995), we believe that these results help verify the efficacy of our model framework. Further evidence can be found by looking at evaluation results on FB15K and FB15K-237, in which our model consistently and significantly outperforms others for all settings.
The goal of the second evaluation was three-folded. (1) To assess the relation prediction performance of our model. (2) To verify the validity of the multi-shot learning framework. (3) To evaluate the quality (representability) of different embedding schemes. To achieve this goal, we carried out a group of experiments depicted in Table 2, where the model name shown in the parentheses indicate that the test is based on the embeddings generated by that model, but being re-trained with our model for fair comparison. For example, before testing the GEN(TransE) model, we need to train a GEN model with TransE embeddings, the only difference is that the pre-trained embeddings will not be updated during the training process, such that the quality of the different embedding schemes can be assessed more objectively. The results of GEN(HoIE) were obtained similarly from the pre-trained HoIE embeddings. The pre-trained word2vec embedding5 and GloVe embedding6 are obtained from the publicly available dictionaries released respectively by Google and Stanford NLP Group for research purpose, which are also heavily studied by recent researches. For entities and relations consisting of many words, we use the weighted sum of the word embeddings as their distributed representation for the test. The three models listed in the bottom of Table 2 demonstrate the oneshot learning capability of GEN, for instance, the results of GEN(h, r  t) were obtained by only considering the query (h, r, ?) during the training stage.
From the studies, the following conclusions could be obtained. (1) The performance of GEN on relation prediction tasks has been demonstrated. However, it seems that such strong performance mainly comes from our GEN framework, under which the predictive capability of a variety of em-
5Available at: https://code.google.com/archive/p/word2vec; version: GoogleNews-vectors-negative300. 6Available at: https://nlp.stanford.edu/projects/glove/; file version: glove.42B.300d.
6

Under review as a conference paper at ICLR 2018

Table 2: Empirical comparison of the embedding schemes on FB15K dataset

Tasks Models \ Measures
GEN(GloVe) GEN(word2vec)
GEN(HoIE) GEN(TransE)
GEN GEN(h, r  t) GEN(t, r  h) GEN(h, t  r)

Head Entity Prediction

P@1 P@10

39.79

68.80

48.05

75.81

30.55

58.86

47.91

77.58

73.85

86.01

36.18

62.88

32.47

58.11

26.34

49.42

Tail Entity Prediction

P@1

P@10

44.64

74.72

52.09

81.34

35.66

64.84

52.25

82.75

78.86

91.64

36.85

63.38

40.40

67.72

30.11

54.41

Relation Prediction

P@1 P@10

85.24

98.57

86.50

98.77

92.28

99.68

93.15

99.71

93.99

99.75

86.61

98.49

86.44

98.41

94.11

99.75

beddings can be enhanced. In considering the ratio of the number of facts to relations involved, this problem seems much easier than the link prediction problem. (2) The validity of the multi-shot framework has been verified, since each of the one-shot GEN model performs significantly worse than the multi-shot model for almost all the tests, except that in relation prediction tasks, GEN(h, t  r) performs comparable to GEN, this is probably because that it was exclusively trained for that task, which is prone to overfit the data. (3) Comparing with their performance on link prediction tasks, we argue that the embeddings generated by GEN are probably more representative and informative than other embedding schemes, which we will provide more empirical (visual) evidence in Appendix C.
4.2 EVALUATION ON GRAPH BASED MULTI-LABEL CLASSIFICATION TASKS
In previous section, the term "knowledge graph" was used to refer to a multi-relational database, in which the entities were engaged in one or more heterogeneous relations, which means the relations related with a entity may range over different domains. In this section, we consider the problem of embedding learning on another type of graph -- the homogeneous graphs (networks), in which the entities were engaged in a specific relationship, which is a natural structure people use to model the physical world, such as the various social network and the biological information systems. In this study, we consider it as a generalized form of the knowledge graphs, and attempt to come up with a general-purpose framework that could be used for embedding learning on different graphs.
To verify the validity of the proposed model, we evaluate GEN by comparing its performance on some benchmark multi-label classification tasks with the state-of-the-art DeepWalk and Node2vec models. Besides, we also report results on TransE and HoIE embeddings for comparison purpose, the supervised model used for multi-label classification are identical to each other (but differ from the embeddings). For fair comparison, all of the results with regard to the DeepWalk (Perozzi et al., 2014) and Node2vec (Grover & Leskovec, 2016) are cited from their original sources.
Following the convention of previous authors, we randomly sample a portion of the labeled nodes as training set (and the rest are used for test), we repeat this process 9 times (with the training ratio increased from 10% to 90%), and report two of the averaged measures (w.r.t. recall, precision, and F1-measure) on each of the test, namely, macro-average and micro-average. The Macro-F1 weights equally all the categories regardless of how many labels belong to it, while the Micro-F1 weights equally all the labels, thus favouring the performance on common categories.
Numerical results are presented in Table 3 and 4 respectively, the highest scores in each column are presented in bold face. From Table 3 one could see that the performance of DeepWalk proves much more competitive than other models when labeled data is sparse, but GEN still consistently outperforms when given 50% of the data, which demonstrates the validity of the proposed embedding learning framework for modeling author connections on social networks. Next, we investigate the performance of our model on even more sparse graphs, i.e. the Protein-Protein Interactions network. Table 4 shows that GEN performs consistently and significantly better than other baselines. In fact, when trained with only 20% of the labeled proteins, GEN performs significantly better than other approaches when they are given 90% of the data. We argue that this strong performance not only indicates that our model is flexible enough to the biological networks, but also provides new insights into their underlying biological mechanisms. Also please note that Macro-F1 scores in Table 3
7

Under review as a conference paper at ICLR 2018

Table 3: Multi-label classification results on BlogCatalog dataset

Measures Micro-F1
Macro-F1

Models DeepWalk Node2vec
TransE HoIE GEN DeepWalk Node2vec TransE HoIE GEN

10% 36.00 34.64 16.71 30.88 27.61 21.30 16.52 2.69 13.86 19.32

20% 38.20 36.15 17.10 33.31 31.38 23.80 18.81 3.09 17.10 23.26

30% 39.60 36.63 17.44 34.63 35.02 25.30 19.81 3.33 18.98 26.74

40% 40.30 37.01 17.64 35.70 38.55 26.30 20.09 3.52 20.84 31.06

50% 41.00 37.20 17.77 36.17 41.19 27.30 20.97 3.41 20.77 33.53

60% 41.30 37.38 18.50 37.31 44.40 27.60 21.50 3.85 22.65 36.57

70% 41.50 38.05 19.13 40.21 45.78 27.90 22.37
4.14 25.64 38.83

80% 41.50 38.27 19.62 38.79 48.87 28.20 23.16 4.63 23.06 40.27

90% 42.00 40.91 20.50 40.69 51.84 28.90 24.60
5.33 27.79 44.60

Measures Micro-F1
Macro-F1

Table 4: Multi-label classification results on PPI dataset

Models DeepWalk Node2vec
TransE HoIE GEN DeepWalk Node2vec TransE HoIE GEN

10% 15.36 16.32 12.80 14.85 16.36 12.93 13.00 8.71 9.36 14.74

20% 17.40 17.94 17.69 18.95 27.31 14.46 15.56 11.45 16.10 25.83

30% 18.26 19.14 20.94 21.52 27.97 15.94 16.82 16.43 17.55 27.04

40% 19.41 19.68 23.57 24.58 32.73 17.05 17.28 19.00 20.76 31.27

50% 19.75 20.32 24.58 27.55 38.10 17.74 17.92 20.37 23.96 35.98

60% 20.23 21.80 27.32 29.34 42.85 18.05 18.37 22.69 24.92 40.82

70% 20.46 21.76 30.42 31.03 46.43 18.41 19.60 25.42 26.82 45.02

80% 21.52 22.50 31.84 33.56 51.09 18.52 20.72 27.35 30.26 50.35

90% 21.79 22.88 35.20 35.71 55.16 20.03 21.28 30.53 32.45 52.92

and 4 demonstrate that, comparing with other embedding schemes, GEN performs more stable (and better) in both common and rare categories, which indicates that the embeddings generated by GEN are probably more representative and informative than other solutions, thus the supervised model built on top of it is less vulnerable to global under-fitting and local over-fitting.
5 CONCLUSION AND FUTURE WORK
Representation learning of knowledge graphs is a key concern for artificial intelligence and cognitive science. Many types of relations in physical, biological, social and information systems can be modeled with concept (knowledge) graphs. In this paper, we present an efficient scalable framework for learning conceptual embeddings of entities and relations in generalized knowledge graphs, including the homogeneous and heterogeneous graphs. We give evidence that the proposed model learns good representations of all these graphs for knowledge inference and supervised learning. For future work, we plan to investigate more thoroughly the efficacy of the proposed modeling framework, with respect to the decomposition of the semantic information conveyed by the linked concepts into elementary information, i.e. the four Q&A pairs. Also, we seek to enhance the quality of scientific investigations and theoretical conceptualizations on graph embedding learning in the context of semantic interoperability, for there is usually no possibility to interpret the embedded information meaningfully and accurately in order to produce useful results as defined by existing algorithms.
ACKNOWLEDGMENTS
We are grateful to the anonymous reviewers for taking time read and provide helpful comments.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Representations, San Diego, CA, USA, 2015. CoRR.
Yoshua Bengio, Nicolas Le Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex neural networks. In Proceedings of the 18th International Conference on Neural Information Processing Systems, pp. 123­130, Vancouver, British Columbia, Canada, 2005. MIT Press.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Trans. on Pattern Analysis and Machine Intelligence, 35(8):1798­1828, 2013.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating Embeddings for Modeling Multi-relational Data. In Proceedings of the 26th International Conference on Neural Information Processing Systems, pp. 2787­2795, Nevada, USA, 2013. Curran Associates, Inc.
Hongyun Cai, Vincent W. Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph embedding: Problems, techniques and applications. CoRR, arXiv:1709.07604, 2017.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2d knowledge graph embeddings. CoRR, arXiv:1409.0473:1­14, 2017.
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 601­610, New York, USA, 2014. ACM.
Lucas Drumond, Steffen Rendle, and Lars Schmidt-Thieme. Predicting rdf triples in incomplete knowledge bases with tensor factorization. In Proceedings of the 27th Annual ACM Symposium on Applied Computing, pp. 326­331, Trento, Italy, 2012. ACM.
Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 855­864, San Francisco, CA, USA, 2016. ACM.
Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. In Proceedings of the 5th International Conference on Learning Representations, Toulon, France, 2017. CoRR.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):664­676, 2017.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the 29th AAAI Conference on Artificial Intelligence, pp. 2181­2187, Austin, Texas, USA, 2015. AAAI Press.
Hanxiao Liu, Yuexin Wu, and Yiming Yang. Analogical inference for multi-relational embeddings. In Proc. of the 34th International Conf. on Machine Learning, pp. 2168­2178. PMLR, 2017.
Qiao Liu, Liuyi Jiang, Minghao Han, Yao Liu, and Zhiguang Qin. Hierarchical random walk inference in knowledge graphs. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, pp. 445­454. ACM, 2016.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems, pp. 3111­3119, Nevada, USA, 2013.
George A. Miller. Wordnet: A lexical database for english. Comm. of the ACM, 38(11):39­41, 1995.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on International Conference on Machine Learning, pp. 807­814, Haifa, Israel, 2010. Omnipress.
9

Under review as a conference paper at ICLR 2018
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on Machine Learning, pp. 809­816, Washington, USA, 2011. Omnipress.
Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):11­33, 2016a.
Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of knowledge graphs. In Proceedings of the 30th AAAI Conference on Artificial Intelligence, pp. 1955­1961, Phoenix, AZ, USA, 2016b. AAAI Press.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 701­710, New York, NY, USA, 2014. ACM.
Baoxu Shi and Tim Weninger. Proje: Embedding projection for knowledge graph completion. In Proceedings of the 31st AAAI Conference on Artificial Intelligence, pp. 1236­1242, San Francisco, CA, USA, 2017. AAAI Press.
Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. Reasoning with neural tensor networks for knowledge base completion. In Proceedings of the 27th Annual Conference on Neural Information Processing Systems, pp. 926­934, Lake Tahoe, Nevada, USA, 2013.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems, pp. 3104­3112, Montreal, Canada, 2014. MIT Press.
Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference. In Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality, pp. 57­66, Beijing, China, 2015. Association for Computational Linguistics.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Complex embeddings for simple link prediction. In Proceedings of the 33rd International Conference on Machine Learning, pp. 2071­2080, New York, NY, USA, 2016. JMLR.
Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey of approaches and applications. IEEE Trans. on Knowledge and Data Engineering, PP(99), 2017.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the 28th AAAI Conference on Artificial Intelligence, pp. 1112­1119. AAAI Press, 2014.
Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual question answering, volume 48 of Proceedings of the 33rd International Conference on Machine Learning. JMLR.org, New York, NY, USA, 2016.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations for learning and inference in knowledge bases. In Proceedings of the 3rd International Conference on Learning Representations, volume arXiv:1412.6575, pp. 12 pages. CoRR.
10

Under review as a conference paper at ICLR 2018
APPENDIX A: MOTIVATION AND INTUITION
To get an intuitive understanding of the problem, consider the following examples taken from three typical KGs that have been heavily studied by the academic and industrial communities:
· (Elvis Presley, instance of, rock star) : taken from the WordNet7, one of the largest online lexical database of English, in which each distinct concept (called synset) are interlinked by means of rigidly defined (hence limited) conceptual-semantic or lexical relations.
· (Elvis Presley, /people/person/profession, singer) : taken from the Freebase8, which was once to be the largest collaboratively edited knowledge base (deprecated at this time and absorbed by the Wikidata project). In which each named entities are interlinked by means of fine-grained relation types defined in the meta-schema. Due to the loosely-defined nature of the relation types, redundancy or alternate facts are allowed to exist simultaneously, such as, (Elvis Presley, profession, musician) and (Elvis Presley, profession, actor).
· (Elvis Presley, rdf:type, American rock singers) : taken from the YAGO9, one of the largest and most active semantic knowledge base developed at the Max Planck Institute for Computer Science in Saarbru¨cken , which combines the clean taxonomy (relation types) of WordNet with the richness of the Wikipedia category system (classes of entities ).
As can be perceived from above examples, the use of different ontologies can lead to different (and incoherent) relations between the same pair of concepts, similarly, applying different ontologies can lead to diverse kinds of conceptualizations. Therefore, it is (arguably) impractical to rely on using the word-level embeddings to precisely represent the knowledge graphs under the diverse conditions, and it is necessary to develop a universal solution that is applicable to all of the ontology infrastructures, for phrase-level embedding learning of the different concept representations.
As mentioned in Section 3, in order to develop a representation learning method that is flexible enough for modeling different types of knowledge graphs, the most important problems to be faced are associated with how to define the optimization problem and how to solve it. According to our survey, most state-of-the-art models, including the translating models derived from the TransE (Bordes et al., 2013; Lin et al., 2015), the latent semantic models derived from the RESCAL (Nickel et al., 2011; 2016b), and the neural network models derived from the NTN (Socher et al., 2013), were all trying to define the graph embedding learning problem as a supervised binary classification problem, in which the optimization objectives are defined in the form of a relation-specific cost function of the entity and/or relation embeddings, and then to solve it with a stochastic gradient decent (SGD) process. Typical criteria used to evaluate the cost functions include the logistic loss and the pairwise margin-based criterion, and the negative samples used for training the model are usually sampled from the complement of the knowledge graph based on the open world assumption (Drumond et al., 2012). However, we doubt that there are many situations where such modeling strategies would have theoretical and practical disadvantages.
Firstly, we speculate that the reason why most previous studies did not consider the first and second queries simultaneously (see Section 3), is probably due to the difficult in modeling the inverse semantic relatedness of the entities from the given fact. In other words, shall we use the embedding of r to represent its reverse r? If we do so, it seems that it will inevitably lead to semantic paradox like: Presley's profession is Presley, since from the model's perspective, there is no difference between the entity Presley and other entities that may appear on both side of the relation profession. Considering the sparsity of the knowledge graph, models trained with limited facts would very likely tend to give higher scores to the entities that have been "seen in the right place".
In order to solve this problem, we propose to model the facts conceptually instead of concretely (or literally, syntactically), which means that we will focus on the semantic meanings of the embeddings (of the entities and relations), rather than their syntactic features. Such a conceptual embedding scheme allow us to unify the representation of a relation (r) and its reverse counterpart (r'), and to accommodate the lexical variety in use by various knowledge bases.
7http://wordnet.princeton.edu 8https://developers.google.com/freebase 9https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago
11

Under review as a conference paper at ICLR 2018

Dataset WN18 WN18RR FB15K FB15K-327 Dataset BlogCatalog
PPI

# entities 40,943 40,943 14,951 14,541 # nodes 10,312 3,890

Table 5: Statistics of the data sets

# relations 18 11
1,345 237 # edges

# training set 141,442 86,835 483,142 272,115
# categories

# validation set 5,000 3,034 50,000 17,535 # labels

333,983 38,292

39 50

14,476 6,640

# test set 5,000 3,134 59,071 20,466
Avg. labels 1.40 1.70

The intuition behind is, for any given fact (h, r, t), one would instantly recognize the bidirectional semantic connection between h and t, without need of translating it into (t, r, h) explicitly in his/her mind. We believe this is crucial for efficient utilization of the structure information of the KGs for representation learning, empirical evidence is provided in Section 4 and Appendix C, respectively.
Secondly, we propose to use unsupervised learning techniques for graph embedding learning tasks, because: (1) Presently, almost all of the large-scale knowledge graphs are extremely sparse, which would unavoidably degrade the quality and reliability of the supervised learning algorithms. Further, considering the relation-specific solution that dominates the current research, the situation might get even worse. (2) Selecting negative examples for pair-wise training would be tricky and expensive, since in practice, it is very hard to generate a "proper and informative" negative sample responsive to each of the positive examples. For example, when learning from the fact (Einstein, employer, IAS), the false fact (Einstein, employer, Stanford) would seem to be more reasonable and informative than (Einstein, employer, singer) -- if the objective is to further improve the predictive capability of the model to discriminate between similar objects.
To solve the data sparsity problem, we propose to model each of the facts as a short sentence, the entire KG can be regarded as a huge document, so that it can be processed by unsupervised encoderdecoder neural models, which has demonstrate to be efficient and useful in concept learning from the large-scale and feature sparse data (Sutskever et al., 2014). In order to avoid the sampling bias due to the selection of uninformative entities, we propose to use the softmax cross-entropy loss as a measure of the predictive discrepancy for model training, because its probability interpretation is more objective than those squared or logistic errors conventionally used in this area, and, it has been proven to be convex for the MLP we used in this paper (Bengio et al., 2005).
APPENDIX B: BACKGROUND INFORMATION AND IMPLEMENTATION DETAILS
B.1 DATASETS
WN18 is a subset of WordNet, which contains mostly the conceptual-semantic and lexical relations, and the entities are organized in a strictly hierarchical manner. FB15k is a subset of Freebase, which contains facts gathered from Wikipedia, mostly focused on the topic of movies and sports.
These datasets have been used as a de facto benchmarks for comparative evaluation, however, recent research (Toutanova & Chen, 2015; Dettmers et al., 2017) show that the test sets of WN18 and FB15k contain a lot of reversed triples that have been presented in the training set, i.e., (h, r, t) versus (t, r, h). Which, we consider would favor our model over those one-shot alternatives.
Therefore, we provide results on FB15k-237, which is introduced by Toutanova & Chen (2015), it is a subset of FB15K where reversing relations are removed. And, we also test on WN18RR provided by Dettmers et al. (2017), which is a reverse duplication free new sample of WordNet.
The multi-relational data sampled from WordNet and Freebase can be seen as typical of the heterogeneous graphs, in order to verify the generality of the developed model, we also perform evaluation in the multi-label classification setting on some typical homogeneous graphs.
12

Under review as a conference paper at ICLR 2018
Figure 3: Venn diagrams of the top-N cosine similarity ranking results of the relation embeddings generated by four typical embedding schemes on FB15K dataset.
BlogCatalog is a social network sampled from the BlogCatalog website, which contains only one relationship: the social connection between the blog authors, while the labels represent the interested topic categories provided by the bloggers. Protein-Protein Interactions is a biological network sampled from the PPI network for Homo Sapiens, which also contains only one relationship: the existence of interactions between the proteins, while the labels represent the biological states of the proteins. In the training set of these graph corpus, every entity (node) is assigned one or more labels from a finite set, the task is to predict the labels for the nodes in the test set. The statistics of these data sets are summarized in Table 5.
B.2 EXPERIMENTAL SETUP We optimized the hyper-parameters of all the datasets via extensive grid search and selected the model with the best filtered P@10 score on the validation set. Hyper-parameter ranges for the grid search were the following: embedding dimension d in {50, 100, 200, 300}, hidden layer dimension k in {256, 512, 1024, 2048} , MLP dropout rate p in {0.0, 0.1, 0.2, 0.3}, learning rate  in {0.001, 0.01, 0.1, 1, 5, 10}, learning rate decay  in {0.7, 0.75, 0.8, 0.85, 0.9, 0.95}. In this study, we use the following combination of parameters for all of the graph embedding learning tasks :
· E CELLS: {d : 200, k : 2048, p : 0.2,  : 5,  : 0.9. · R CELLS: {d : 200, k : 512, p : 0.2,  : 5,  : 0.9}. · Mini-batch Settings: {batch size : 512, epoch : 50} For multi-label classification tasks, we implement a single layer perceptron model for multi-task learning with: {k : 128,  : 0.1,  : 0.9}, which is selected through grid search with the best averaged Macro-F1 score on randomly sampled validation set from the labeled nodes.
APPENDIX C: INVESTIGATING AND VISUALIZING THE EMBEDDING SCHEMES
In this section, we provide qualitative analysis on four typical embedding schemes (GEN, HoIE, TransE and word2vec), with the intention of better understanding the connection between the existing graph embedding schemes, and highlighting areas that remain poorly understood for further investigation. The reason we choose these models (except the word2vec) is because, according to our tests, they have demonstrated to be efficient and scalable to large-scale problems, and are also exhibiting good generalization ability on real data sets. We also consider the word2vec embeddings because we found that with the help of our multi-shot learning model, it achieves state-of-the-art performance on most of the knowledge base completion tasks (see Section 4), which is interesting, and worth some consideration (probably indicates a promising potential for transfer learning). The argument in the first case is to claim that the graph embeddings generated by GEN are different from other solutions. We calculate the cosine similarities for each pair of the 1,345 relations in
13

Under review as a conference paper at ICLR 2018
Figure 4: Visualization analysis of the GEN embedding space by using of the principal component analysis on embedding of the entities for relation prediction tasks. The case is taken from the FB15K test set, with all of the triples related to relation #425: /location/location/time zones.
Figure 5: Visualization analysis of the TransE, HoIE and word2vec embedding schems by using of the principal component analysis on embedding of the entities for relation prediction tasks. The case is taken from the FB15K test set, relation #425: /location/location/time zones.
FB15K w.r.t. four embedding schemes respectively, and compare their top-N ranking list (which is a set of relation pairs for each embedding scheme) through Venn diagrams, as illustrated in Fig.3. Figure 3 reveals very different clustering pattern between GEN and other alternatives in their corresponding embedding space. What's particularly interesting is that, although the TranE model is inspired by and heavily reliant on the spatial distribution of the wrod2vec embeddings (Bordes et al., 2013), but they are, in fact, not similar at all. On the contrary, the results of TransE and HoIE share a lot of similarities, it appears that in their top-300 and top-500 lists, almost half of the relation pairs were contained in their intersection. Which probably indicates that the translating embedding hypothesis (Bordes et al., 2013) is theoretically similar in nature to the holographic embedding hypothesis (Nickel et al., 2016b), when used for graph modeling. This is not an easily testable hypothesis, we consider it to be an open question, which we hope to further explore in the future.
14

Under review as a conference paper at ICLR 2018
The goal of the second experiment is to verify the claim that the embeddings generated by GEN are more representative and informative than other embedding schemes. Here we provide a case study on a randomly selected relation from FB15K, namely "/location/location/time zones". There are 137 triples related to this relation (#425) in the test set, all of the head entities are names of the countries or regions, and the tail entities are the corresponding time zones. The heads are uniquely different from each other, while there are only 10 different time zones existed in the tails.
We plot all of the 137 triples in Fig.4, in which (Fig.4a and Fig4b) the input multi-dimensional vectors are projected to a 2-dimensional subspace spanned by x and y, by using of the principal component analysis (PCA), then we choose the first two principal components as the principal axes. In Fig.4a, the input is the concatenation of the head and tail entity of each triple, i.e. (h  t), with the intention of investigating the patterns of such feature vectors for relation prediction tasks. Hence, we choose the name of the tails as legend labels. As can be seen from Fig.4a, the feature vectors of the 137 triples shows clear clustering tendencies with regard to the categories in their tail entities. Based on this observation, we further plot the hidden layer of the R CELL (which is a 512-dimensional vector in this case) located before the output layer in our GEN model, as depicted in Fig.4b. From Fig.4b one could see that the distance between the data points is amplified, and the distinction becomes more prominent. We plot the cumulative softmax in Fig.4c, in which the X-axis represents the 1,345 type of relations in FB15K, Y-axis denotes the cumulative softmax values. The curve is obtained by adding all of the softmax vectors output by GEN with regard to the 137 triples. Obviously, the only peak observed in Fig.4c clearly exhibit that GEN can make good use of these (concatenated) features to identify the corresponding relations correctly.
For comparison purpose, we also visualize the other three embedding schemes with the same protocol, as illustrated in Fig.5. Since the corresponding models do not use MLP for relation prediction, we can not plot their "hidden state" and "accumulate softmax" for the second and the third subplots, hence we choose to visualize their predictive criterion vectors and output ranking list instead. The processing practice is consistent with the protocol of the original literature. Specifically, for TransE, we plot (t - h) as the hidden state for relation prediction, and calculate the 1-norm distance |ri - (t - h)|1 w.r.t each of the relation ri in FB15K, then we process the distance vector with the softmax function for calculation of the accumulate softmax. While for HoIE, we plot the circular correlation vector (ht) as the hidden state, and calculate the cosine similarity of (ht)·ri w.r.t each of the relation ri in FB15K, then we use the obtained (cosine) vector to calculate the accumulate softmax. For word2vec embeddings, we use the same protocol as dealing with TransE.
From Fig.5 one could see that, the concatenated embedding vectors of TranE and HoIE shows similar clustering pattern as the GEN case, which help explaining the reason that under our multi-shot learning framework, the embeddings generated by these models perform similar in relation prediction tasks (see Table 2). It also provides evidence for our conjecture that these two embedding schemes could be inherently similar to each other. Form their criterion vectors (the second subplot for each models), one could see that their clustering pattern is not as clear as the case of GEN, which help explain their performance on relation prediction tasks (as shown in the third subplot)10. We consider this as solid support for the validity of the proposed multi-shot learning framework.
More evidence could be found with our source code release, which will be made publicly available on GitHub to encourage reproducible research, after anonymous review.
10 The alternative peaks appeared in subplot Fig.5c and Fig.5f are: · #891: "/base/schemastaging/phone open times/time zone", and · #583: "/time/time zone/locations in this time zone".
15

