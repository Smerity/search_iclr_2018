Under review as a conference paper at ICLR 2018
SPONTANEOUS SYMMETRY BREAKING IN DEEP NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We propose a framework to understand the unprecedented performance and robustness of deep neural networks using field theory. Correlations between the weights within the same layer can be described by symmetries in that layer, and networks generalize better if such symmetries are broken to reduce the redundancies of the weights. Using a two parameter field theory, we find that the network can break such symmetries itself towards the end of training in a process commonly known in physics as spontaneous symmetry breaking. This corresponds to a network generalizing itself without any user input layers to break the symmetry, but by communication with adjacent layers. In the layer decoupling limit applicable to residual networks (He et al., 2015a), we show that the remnant symmetries that survive the non-linear layers are spontaneously broken based on empirical results. The Lagrangian for the non-linear and weight layers together has striking similarities with the one in quantum field theory of a scalar. Using results from quantum field theory we show that our framework is able to explain many experimentally observed phenomena, such as training on random labels with zero error (Zhang et al., 2017), the information bottleneck and the phase transition out of it and the following increase in gradient variances (Shwartz-Ziv & Tishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more.
1 INTRODUCTION
Deep neural networks have been used in image recognition tasks with great success. The first of its kind, AlexNet (Krizhevsky et al., 2012), led to many other neural architectures have been proposed to achieve start-of-the-art results in image processing at the time. Some of the notable architectures include, VGG (Simonyan & Zisserman, 2015), Inception (Szegedy et al., 2015) and Residual networks (ResNet)(He et al., 2015a).
Understanding the inner workings of deep neural networks remains a difficult task. It has been discovered that the training process ceases when it goes through an information bottleneck (ShwartzZiv & Tishby, 2017) until learning rate is decreased to a suitable amount then the network under goes a phase transition. Deep networks appear to be able to regularize themselves and able to train on randomly labeled data Zhang et al. (2017) with zero training error. The gradients in deep neural networks behaves as white noise over the layers Balduzzi et al. (2017). And many other unexplained phenomena.
A recent work (Anonymous, 2018) showed that the ensemble behavior and binomial path lengths (Veit et al., 2016) of ResNets can be explained by just a Taylor series expansion to first order in the decoupling limit. They found that the series approximation generates a symmetry breaking layer that reduces the redundancy of weights, leading to a better generalization. Because the ResNet does not contain such symmetry breaking layers in the architecture. They suggest that ResNets are able to break the symmetry by the communication between the layers. Another recent work also employed the Taylor expansion to investigate ResNets (Jastrzebski et al., 2017).
In statistical terms, a quantum theory describes errors from the mean of random variables. We wish to study how error propagate through each layer in the network, layer by layer. In the limit of a continuous sample space, the quantum theory becomes a quantum field theory. The effects of sampling error and labelling error can then be investigated. It is well known in physics that a scalar field can drive a phase transition. Using a scalar field theory we show that a phase transition must
1

Under review as a conference paper at ICLR 2018

exist towards the end of training based on empirical results. It is also responsible for the remarkable performance of deep networks compared to other classical models. In Appendix D, We explain that quantum field theory is likely one of the simplest model that can describe a deep network layer by layer in the decoupling limit.
Much of the literature on neural network design focuses on different neural architecture that breaks symmetry explicitly, rather than spontaneously. For instance, non-linear layers explicitly breaks the symmetry of affine transformations. There is little discussion on spontaneous symmetry breaking. In neural networks, the Goldstone theorem in field theory states that for every continuous symmetry that is spontaneously broken, there exists a weight with zero Hessian eigenvalue at the loss minimum. No such weights would appear if the symmetries are explicitly broken. It turns out that many seemingly different experimental results can be explained by the presence of these zero eigenvalue weights. In this work, we exploit the layer decoupling limit applicable to ResNets to approximate the loss functions with a power series in symmetry invariant quantities and illustrate that spontaneous symmetry breaking of affine symmetries is the sufficient and necessary condition for a deep network to attain its unprecedented power.
The organization of this paper is as follows. The background on deep neural networks and field theory is given in Section 2. Section 3 shows that remnant symmetries can exist in a neural network and that the weights can be approximated by a scalar field. Experimental results that confirm our theory is given in Section 4. We summarize more evidence from other experiments in Appendix A. A review of field theory is given in Appendix B. An explicit example of spontaneous symmetry breaking is shown in Appendix C.

2 BACKGROUND AND FRAMEWORK

In this section we introduce our frame work using a field theory based on Lagrangian mechanics.

2.1 DEEP NEURAL NETWORKS

A deep neural network consists of layers of neurons. Suppose that the first layer of neurons with weight matrix W1 and bias b1 takes input x1 and outputs y1
y1 = W1x1 + b1 = W1x1,
where x = (x , 1) and W1 = (W1, b1), where W1 and b1 are real valued. Now suppose that R1 denotes a nonlinear operator corresponding to a sigmoid or ReLU layer located after the weight layer, so that
y1 = R1W1x1.
For a neural network with T repeating units, the output for layer t is

yt = RtWtyt-1 = RtWtRt-1Wt-1 . . . R1W1x1
t-1
= Rt-nWt-n x1.
n=0

(1)

2.2 SYMMETRIES IN NEURAL NETWORKS
We now show the necessary and sufficient conditions of preserving symmetry. We explicitly include symmetry transformations in Equation (1) and investigate the effects caused by a symmetry transformation of the input in subsequent layers. Suppose Qt  G is a transformation matrix in some Lie group G for all t. Note that the Qt are not parameters to be estimated. We write yt = yt(Qt), where the dependence on Qt is obtained from a transformation on the input, xt(Qt) = Qtxt, and the weights, Wt(Qt)
yt(Qt) = RtWt(Qt)xt(Qt).
If G is a symmetry group, then yt is covariant with xt, such that yt(Qt) = Qtyt. This requires two conditions to be satisfied. First, Wt(Qt) = QtWtQt-1, where Q-t 1Qt = I and the existence of the inverse is trivial because G is a group and Qt  G. The second is the commutativity between Rt and

2

Under review as a conference paper at ICLR 2018

Qt, such that RtQt = QtRt. For example, if gt  Aff(D), the group of affine transformations, Rt may not commute with gt. However, commutativity is satisfied when the transformation corresponds to the 2D rotation of feature maps.

Including transformation matrices, the output at layer t is

yt(Qt) =

t-1
Rt-nQt-nWt-nQt--1n Q1x1.
n=0

2.3 THE LOSS FUNCTIONAL

Statistical learning requires the loss function to be minimized. It can be written in the form of a mu-

tual information, training error, or the Kullback-Leibler divergence. In this section we approximate

the loss function in the continuum limit of samples and layers. Then we define the loss functional to

transition into Lagrangian mechanics and field theory. Let zi = (Xi, Yi)  X be the i-th input sam-

ple in data set X , (Xi, Yi) are the features and the desired outputs, respectively, and i  {1, . . . , N }.

The loss function is

1N

L= N

Li(Xi, Yi, W, Q),

i=1

where W = (W1, . . . , WT ), and Q = (Q1, . . . , QT ), Qt  G where G is a Lie group, and T is the depth of the network. Taking the continuum limit,

L p(X, Y)Lx(X, Y, W, Q)dXdY,
X

where p(X, Y) is the joint distribution of X and Y. Using the first fundamental theorem of calculus and taking the continuous layers (t) limit, we write

Lx

Lx(t = 0) +

T dLx(X, Y, W(t), Q(t)) dt, 0 dt

where Lx(t = 0) is the value of the loss before training. We let Lx,t = dLx/dt be the loss rate per layer. The loss rate Lx,t is bounded from below. Therefore

T
min Lx(X, Y, W, Q) = Lx(t = 0) + min Lx,t(X, Y, W(t), Q(t))dt.
W 0W
Minimizing the loss rate guarantees the minimization of the total loss. We require Lx,t to be invariant under symmetry transformations. That is, if Q1(t), Q2(t)  G. Then

Lx,t(X, Y, W(t), Q1(t)) = Lx,t(X, Y, W(t), Q2(t)).

However if Q1(t) and Q2(t) do not belong in the same symmetry group, the above equality does not necessarily hold. Now we define the loss functional for a deep neural network

P[W, Q] =

p(X, Y)Lx,t(X, Y, W(t), Q(t))dXdYdt.

X ×{0,T }

2.4 LAGRANGIAN MECHANICS AND FIELD THEORY

Having defined the loss functional, we can transition into Lagrangian dynamics to give a description of the feature map flow at each layer. Let the minimizer of the loss rate be

W(X, Y, Q(t), t) = arg min p(X, Y)Lx,t(X, Y, W(t), Q(t)).
W

From now on, we combine z = (X, Y) as Y only appears in W in this formalism, each Y determines a trajactory for the representation flow determined by Lagrangian mechanics. Now we define, for each i-th element of W(t), and a non-linear operator R(t) acting on W(t) such that the loss minimum is centered at the origin,

wi(z, Q(t), t) = R(t)W i(z, Q(t), t) - R(t)W i(z, Q(t), t).

(2)

3

Under review as a conference paper at ICLR 2018

We now define the Lagrangian density,
L = L(z, w, tw, zw, Q(t))
and L = T - V, where T is the kinetic energy and V is the potential energy. We define the potential energy to be
V = p(z)Lx,t(z, W(t), Q(t)),
The probability density p(z) and the loss rate Lx,t are invariant under symmetry transformations. Therefore V is an invariant as well.

Definition: Orthogonal Group The orthogonal group O(D) is the group of all D × D matrices such that OT O = I.
We now set up the conditions to obtain a series expansion of V around the minimum wi(Qt) = 0. First, since V is an invariant. Each term in the series expansion must be an invariant such that f (wi(Qt), wi(Qt)) = f (wi, wi) for all Qt  G. Suppose G = O(D), the orthogonal group and that wT (Qt) = wT QT and w(Qt) = Qtw. So wiwi is an invariant. Then f = wiwi is invariant for all Qt where the Einstein summation convention was used

D
wiwi = wiwi = wT w,
i=1
Now we perform a Taylor series expansion about the minimum wi = 0 of the potential,
V = C + wiHjiwj + wiwj mij nwmwn + O((wiwi)6),
where Hji = wi wj V is the Hessian matrix, and similarly for mij n. The overall constant C can be ignored without loss of generality. Because V is an even function in wi around the minimum, we must have
D
wiHjiwj = wiHiiwi.
i=1
The O(D) symmetry enforces that all weight Hessian eigenvalues to be Hii = m2/2 for some constant m2. This can be seen in the O(2) case, with constants a, b, a = b, Q  O(2) such that w1(Q) = w2 and w2(Q) = w1,
aw1(Q)2 + bw2(Q)2 = aw22 + bw12,
this does not equal aw12 + bw22, so the O(2) symmetry implies a = b. This can be generalized to the O(D) case. For the quartic term, the requirement that V be even around the minimum gives

D

wiwj imj nwmwn =

iijj wj wj wiwi.

i,j=1

Similarly the O(D) symmetry implies iiii = /4 for some constant  and zero for any other elements, the potential is

V

=

m2 2

wiwi

+

 4

(wiwi)2

,

where the numerical factors were added for convention. The power series is a good approximation in the decoupling limit which may be applicable for Residual Networks.1 For the kinetic term T ,
we expand in power series of the derivatives,

T

=

1 wi wi 2 t t

-

c2 wi wi 2 z z

+ O((tw)4, (zw)4),

1The output of a residual unit is yt = xt + Ft(xt, W1t, W2t), with Ft xt and Ft(xt, W1t, W2t) = Rt2Wt2Rt1Wt1xt. After centering the loss minimum at the origin with Equation 2 and suitably normalizing, we can rewrite this as Ft = awT w, with a 1 in the decoupling limit. See Peskin & Schroeder (1995) for the normalization of a scalar field.

4

Under review as a conference paper at ICLR 2018

where

the

coefficient

for

(tw)2

is

fixed

by

the

Hamiltonian

kinetic

energy

1 2

(tw)2.

Higher

order

terms in (tw)2 are negligible in the decoupling limit. If the model is robust, then higher order terms

in (zw)2 can be neglected as well. 2 The Lagrangian density is 3

L

=

1 2

(tw)2

-

1 2

(z

w)2

-

m2 w2 2

-

 (w2)2, 4

where we have set w2 = wiwi and absorbed c into z without loss of generality. This is precisely the

Lagrangian for a scalar field in field theory. Standard results for a scalar field theory can be found

in Appendix B. To account for the effect of the learning rate, we employ results from thermal field

theory (Kapusta & Gale, 2006) and we identify the temperature with the learning rate . So that now

m2

=

-µ2

+

1 4

2,

with

µ2

>

0.

2.5 SPONTANEOUS SYMMETRY BREAKING

Spontaneous symmetry breaking describes a phase transition of a deep neural network. Consider the following scalar field potential invariant under O(D ) transformations,

V(wi, )

=

m2() 2

wiwi

+

 4

(wiwi)2,

where

m2()

=

-µ2

+

1 4

2,

µ2

>

0

and

learning

rate

.

There

exists

a

value

of



=

c

such

that

m2 = 0. In the first phase,  > c, the loss minimum is at w0i = 0, where

w0i()

=

arg

min
wi

V (wi ,

).

When the learning rate  drops sufficiently low, the symmetry is spontaneously broken and the phase transition begins. The loss minimum bifurcates at  = c into

wi( < c) = ±

-m2() .


This occurs when the Hessian eigenvalue becomes negative, m2() < 0, when  < c.

This phenomenon has profound implications. It is responsible for phase transition in neural networks and generates long range correlation between representations and the desired output. Details from field theory can be found in Appendix C. Figure 1 depicts the shape of the loss rate during spontaneous symmetry breaking with a single weight w, and the orthogonal group O(D ) is reduced to a reflection symmetry O(1) = {1, -1} such that w(Q) = ±w. At  > c, the loss rate has a loss minima at point A. When the learning rate decreases, such that  < c, the critical point at A becomes unstable and new minima with equal loss rate are generated. The weight must go through B to get to the new minimum C. If the learning rate is too small, the weight will be stuck near A. This explains why a cyclical learning rate can outperform a monotonic decreasing learning rate (Smith & Topin, 2017).

Because the loss rate is invariant still to the sponteneously broken symmetry, any new minima generated from spontaneous symmetry breaking must have the same loss rate. If there is a unbroken continuous symmetry remaining, there would be a connected loss rate surface corresponding to the new minima generated by the unbroken symmetry. Spontaneous symmetry breaking splits the weights into two sets, w  (, ). The direction along this degenerate minima in weight space corresponds to . And the direction in weight space orthogonal to  is . This has been shown experimentally by Goodfellow et al. (2014) in Figure 19. We show the case for the breaking of O(3) to O(2) in Figure 2.

2The kinetic term T is not invariant under transformation Q(t). To obtain invariance twi is to be replaced by the covariant derivative Dtwi so that (Dtwi)2 is invariant under Q(t)(Peskin & Schroeder, 1995). The covariant derivative is Dtwi(z, t, Q(t)) = twi(z, t, Q(t)) + B(z, t, Q(t))ijwj(z, t, Q(t)), with B(z, t, Qt) = Q(t)B(z, t)Q(t)-1. The new fields B introduced for invariance is not responsible for
spontaneous symmetry breaking, the focus of this paper. So we will not consider them further. 3Formally, the zw term should be part of the potential V, as T contains only tw terms. However we
adhere to the field theory literature and put the zw term in T with a minus sign.

5

Under review as a conference paper at ICLR 2018

Figure 1: The characteristics of the loss rate with spontaneous symmetry breaking. The dashed line
corresponds to the symmetric phase, while the solid line corresponds to the broken phase. Only at w = 0 the reflection symmetry w(Q) = ±w is respected. Here, w = .

3 SYMMETRIES IN NEURAL NETWORKS
In this section we show that spontaneous symmetry breaking occurs in neural networks. First, we show that learning by deep neural networks can be considered solely as breaking the symmetries in the weights. Then we show that some non-linear layers can preserve symmetries across the nonlinear layers. Then we show that weight pairs in adjacent layers, but not within the same layer, is approximately an invariant under the remnant symmetry leftover by the non-linearities. We assume that the weights are scalar fields invariant under the affine Aff(D ) group for some D and find that experimental results show that deep neural networks undergo spontaneous symmetry breaking.

Theorem 1: Deep feedforward networks learn by breaking symmetries Proof: Let Ai be an

operator representing any sequence of layers, and let a network formed by applying Ai repeatedly

such that xout = (

M i=1

Ai)xin

.

Suppose that Ai

 Aff(D), the symmetry group of all affine

transformations. We have L =

D i=1

Ai,

where

L

 Aff(D).

Then xout

=

Lxin

for some L



Aff(D) and xout can be computed by a single affine transformation L. When Ai contains a non-

linearity for some i, this symmetry is explicitly broken by the nonlinearity and the layers learn a

more generalized representation of the input.

Now we show that ReLU preserves some continuous symmetries.

Theorem 2: ReLU reduces the symmetry of an Aff(D) invariant to some subgroup Aff(D ), where D < D. Proof: Suppose R denotes the ReLU operator with output yt and Qt  Aff(D) acts on the input xt, where R(x) = max(0, x). Let xT x be an invariant under Aff(D) and let xT = (, ),  < 0 and  > 0. Let a = Rx = (, 0). Then aT a = xT RRx = T . Then aT a is an invariant under Aff(D ) where D = dim . Note that i can be transformed into a negative
value as it has passed the ReLU already.

Corollary If there exists a group G that commutes with a nonlinear operator R, such that QR = RQ, for all Q  G, then R preserves the symmetry G.

6

Under review as a conference paper at ICLR 2018

Definition: Remnant Symmetry If Qt  G commutes with a non-linear operator Rt for all Qt, then G is a remnant symmetry at layer t.
For the loss function Li(Xi, Yi, W, Q) to be invariant, we need the predicted output yT to be covariant with xi. Similarly for an invariant loss rate Lx,t we require yt to be covariant with xt. The following theorem shows that a pair of weights in adjacent layers can be considered an invariant for power series expansion.

Theorem 3: Neural network weights in adjacent layers form an approximate invariant Suppose a neural network consists of affine layers followed by a continuous non-linearity, Rt, and that the weights at layer t, Wt(Qt) = QtWtQ-t 1, and that Qt  H is a remnant symmetry such that QtRt = RtQt. Then wtwt-1 can be considered as an invariant for the loss rate.
Proof: Consider x(Qt) = Qtxt, then
yt(Qt) = RtWt(Qt)xt(Qt) = RtQtWtQ-t 1Qtxt = RtQtWtxt = QtRtWtxt,
where in the last line QtRt = RtQt was used, so yt(Qt) = Qtyt is covariant with xt. Now, xt = Rt-1Wt-1xt-1, so that
yt(Qt) = QtRtWtRt-1Wt-1xt-1.
The pair (RtWt)(Rt-1Wt-1) can be considered an invariant under the ramnant symmetry at layer t. Let wt = RtWt - RtWt. Therefore wtwt-1 is an invariant. In the continuous layer limit, wtwt-1 tends to w(t)T w(t) such that w(t) is the first layer and w(t)T corresponds to the one after. Therefore w(t) can be considered as D scalar fields under the remnant symmetry. The remnant symmetry is not exact in general. For sigmoid functions it is only an approximation. The crucial feature for the remnant symmetry is that it is continuous so that strong correlation between inputs and outputs can be generated from spontaneous symmetry breaking. In the following we will only consider exact remnant symmetries. We will state the Goldstone Theorem from field theory without proof.

Theorem (Goldstone) For every spontaneously broken continuous symmetry, there exist a weight  with zero eigenvalue in the Hessian m2 = 0.
In any case, we will adhere to the case where the remnant symmetry is an orthogonal group O(D ). Note that W is a D × D matrix and D < D. We choose a subset   RD of W such that T  is invariant under Aff(D ). Now that we have an invariant, we can write down the Lagrangian for a deep feedforward network for the weights responsible for spontaneous symmetry breaking.

The Lagrangian for deep feedforward networks in the decoupling limit

R(i),

L

=

1 2

(t

i

)2

-

1 2

(z

i

)2

-

m2() 2

i

i

-

 4

(

ii

)2

.

Let i = R(i) - (3)

Now we can use standard field theory results and apply it to deep neural networks. A review for field theory is given in Appendix B. The formalism for spontaneous symmetry breaking is given in Appendix C.

4 MAIN RESULTS
In this section we assume that the non-linear operator is a piecewise linear function such as ReLU and set R = I to be the identity and restrict our attention to the symmetry preserving part of R (see theorem 2). Our discussion also applies to other piecewise-linear activation functions. According to the Goldstone theorem, spontaneous symmetry breaking splits the set of weight deviations  into two sets (, ) with different behaviors. Weights  with zero eigenvalues and a spectrum dominated

7

Under review as a conference paper at ICLR 2018

by small frequencies k in its correlation function.4 The other weights , have Hessian eigenvalues µ2 as the weights before the symmetry is broken. In Appendix C, a standard calculation in field
theory shows that the correlation functions of the weights have the form

i P,(t, k) = 20 exp - i0t ,

(4)

where 0 =

|k|2 + m2,, m2

=

-µ2

+

1 4

2

and m2

=

1 4



2.

The correlation function of

 approaches infinity as k  0, as m2  0. As the loss is minimized, this corresponds to large

correlation between representations and the desired output over all sample space and layers t.

Spontaneous symmetry breaking and the information bottleneck The neural network under-
goes a phase transition out of the information bottleneck via spontaneous symmetry breaking described in Section 2.5. Before the phase transition, the weights  have positive Hessian eigenvalues m2. After the phase transition, weights  with zero Hessian eigenvalues are generated by spontaneous symmetry breaking. The correlation function for the  weights is concentrated around small values of |k|, see Equation (4), with 0 = |k| for any t. This corresponds to a highly correlated representations across the sample (input) space and layers. Because the loss is minimized, the fea-
ture maps across the network is highly correlated with the desired output. And a large correlation
across the sample space means that the representations are independent of the input. This is shown in Figure 2 of Shwartz-Ziv & Tishby (2017). After phase transition, I(Y ; T ) 1 bit for all layers T , and I(X; T ) is small even for representations in early layers.

Gradient variance explosion It has been shown that the variance in weight gradients in the same layer grow by an order of magnitude during the end of training (Shwartz-Ziv & Tishby, 2017). We also connect this to spontaneous symmetry breaking. As two sets of weights, (, ) are generated with different distributions. Considering them as the same object would result in a larger variance.

Robustness of deep neural networks We find that neural networks are resilient to overfitting. Recall that the fluctuation in the weights can arise from sampling noise. Then (zwi)2 can be a measure of model robustness. A small value denotes the weights' resistance to sampling noise. If the network were to overfit, the weights would be very sensitive to sampling error. After spontaneous symmetry breaking, weights at the loss minimum with zero eigenvalues obey the Klein-Gordon equation with m2 = 0,
(z)2 = (z)(z) = |k|2,  = exp(it - ik · z).
The singularity in the correlation function suggests |k|2 0. The zero eigenvalue weights provide robustness to the model. Zhang et al. (2017) referred to this phenomenon as implicit regularization.

5 CONCLUDING REMARKS
In this work we solved one of the most puzzling mysteries of deep learning by showing that deep neural networks undergo spontaneous symmetry breaking. This is a first attempt to describe a neural network with a scalar quantum field theory. We have shed light on many unexplained phenomenon observed in experiments, summarized in Appendix A.
One may wonder why our theoretical model works so well explaining the experimental results with just two parameters. It is due to the decoupling limit such that a power series in the loss function is a good approximation to the network. In our case, the two expansion coefficients are the lowest number of possible parameters that is able to describe the phase transition observed near the end of training, where the performance of the deep network improves drastically. It is no coincidence that our model can explain the empirical observations after the phase transition. In fact, our model can describe, at least qualitatively, the behaviors of phase transition in networks that the decoupling limit may not apply to. This suggests that the interactions with nearby layers are responsible for the phase transition.
4The correlation functions i(z, t)i(z , t ) for i  {1, . . . , D }, is a measure of similarity of the weight across at z and z . A singularity in frequency domain at |k| = 0 corresponds to a high correlation between the weights across all sample space z = (X, Y) and layers t. This can be interpreted as the correlation length.

8

Under review as a conference paper at ICLR 2018

A VALIDATION OF PROPOSED FRAMEWORK FOR NEURAL NETWORKS IN THE LITERATURE

In this section we summarize other experimental findings that can be explained by the proposed field theory and the perspective of symmetry breaking. Here Q  G acts on the the input and hidden variables x, h, as Qx, Qh.

· The shape of the loss function after spontaneous symmetry breaking has the same shape observed by Goodfellow et al. (2014) towards the end of training, see Figure 1.

· The training error typically drops drastically when learning rate is decreased. This occurs when the learning rate drops below c, forcing a phase transition so that new minima develop. See Figure 1.

· A cyclical learning rate (Smith & Topin, 2017) helps to get to the new minimum faster, see Section 2.5.

· Stochasticity in gradient descent juggles the loss function such that the weights are no longer at the local maximum of Figure 1. A gradient descent step is taken to further take the weights towards the local minimum. Stochasticity helps the network to generalize better.

· When the learning rate is too small to move away from A in Figure 1. PReLU's (He et al., 2015b) could move the weight away from A through the training of the non-linearity. This corresponds to breaking the symmetry explicitly in Theorem 1.

· Results from Shwartz-Ziv & Tishby (2017) are due to spontaneous symmetry breaking, see Section 4.

· Deep neural networks can train on random labels with low training loss as feature maps are highly correlated with their respective desired output. Zhang et al. (2017) observed that a deep neural network can achieve zero training error on random labels. This shows that small Hessian eigenvalues is not the only condition that determines robustness.

· Identity mapping outperforms other skip connections (He et al., 2016) is a result of the

residual unit's output being small. Then the residual units can be decoupled leading to

a small  and so it is easier for spontaneous symmetry breaking to occur, from m2 =

-µ2

+

1 4

2.

· Skip connection across residual units breaks additional symmetry. Suppose now an identity

skip connection connects x1 and the output of F2. Now perform a symmetry transformation on x1 and x2, Q1 and Q2  G , respectively. Then the output after two residual untis is

Qx3 = Q1x1 + Q2x2 + Q2F2. Neither Q = Q1 nor Q = Q2 can satisfy the covariance

under G. This is observed by Orhan & Pitkow (2017).

· The shattered gradient problem (Balduzzi et al., 2017). It is observed that the gradient in

deep (non-residual) networks is very close to white noise. This is reflected in the expo-

nential in Equation (7). This effect on ResNet is reduced because of the decoupling limit

  0. This leads to the weight eigenvalues m2 being larger in non-residual networks

owing to m2

=

-µ2

+

1 4

2.

And so a higher oscillation frequency in the correlation

function.

· In recurrent neural networks, multiplicative gating (Yuhuai et al., 2016) combines the input x and the hidden state h by an element-wise product. Their method outperforms the method with an addition x+h because the multiplication gating breaks the covariance of the output. A transformation Qx  Qh = Q(x  h), whereas for addition the output remains covariant Qx + Qh = Q(x + h).

B REVIEW OF FIELD THEORY
In this section we state the relevant results in field theory without proof. We use Lagrangian mechanics for fields w(x, t). Equations of motion for fields are the solution to the Euler-Lagrange equation, which is a result from the principle of least action. The action, S, is
S[w] = L(t, w, tw)dt,

9

Under review as a conference paper at ICLR 2018

where L is the Lagrangian. Define the Lagrangian density

L(t) = L(z, t, w, z,tw)dz.

The action in term of the Lagrangian density is

S[w] = L(z, t, w, z,tw)dzdt.

The Lagrangian can be written as a kinetic term T , and a potential term V (loss function),

L=T -V

For a real scalar field w(x, t),

T =1 2

w t

2- 1 2

w z

2

=

1 2

(t

w)2

-

1 2

(zw)2

where we have set the constant c2 = 1 without loss of generality. The potential for a scalar field that allows spontaneous symmetry breaking has the form

V = m2 w2 +  w4. 24
In the decoupling limit,   0, the equation of motion for w is the Klein-Gordon Equation [(t)2 - (z)2 - m2]w = 0.

In the limit of m2  0, the Klein-Gordon Equation reduces to the wave equation with solution

 where i = -1.

w(z, t) = ei(t-k·z),

One can treat w as a random variable such that the probability distribution (a functional) of the scalar field w(z, t) is p[w] = exp(-S[w])/Z, where Z is some normalizing factor. The distribution peaks at the solution of the Klein-Gordon equation since it minimizes the action S. Now we can define the correlation function between w(z1, t1) and w(z2, t2),

1 w(z1, t1)w(z2, t2) = Z

w(z1, t1)w(z2, t2)e-S[w]Dw,

where Dw denotes the integral over all paths from (z1, t1) to (z2, t2). In the decoupling limit   0, it can be shown that

exp(-S[w]) = exp - w(t2 - z2 + m2)w dzdt ,

where Stokes theorem was used and the term on the boundary of (sample) space is set to zero. The above integral in the exponent is quadratic in w and the integral over Dw can be done in a similar manner to Gaussian integrals. The correlation function of the fields across two points in space and time is
w(z1, t1)w(z2, t2) = G(z1, t1, z2, t2),

where G(z1, t1, z2, t2) is the Green's function to the Klein-Gordon equation, satisfying

(t2 - z2 + m2)G(z1, t1, z2, t2) = (z1 - z2)(t1 - t2).

The Fourier transformation of the correlation function is

1 G(, k) = 2 - |k|2 - m2 ,

m2 > 0.

An inverse transform over  gives

with 02 = |k|2 + m2.

G(t, k) = i e-i0t, 20

10

Under review as a conference paper at ICLR 2018

Figure 2: The loss after spontaneous symmetry breaking from othogonal group O(3) to O(2). The symmetry transformations of O(2), which are 1-D rotations, form the degenerate minima.

C SPONTANEOUS SYMMETRY BREAKING IN THE ORTHOGONAL GROUP O(D )

In

this

section

we

show

that

weights



with

small,

near

zero,

eigenvalues

m2

=

1 4

2

are

generated

by spontaneous symmetry breaking. Note that we can write the Lagrangian in Equation (3) as

L = T - V. Consider weights  that transforms under O(D ), from Equation (3)

T

=

1 2

(ti)2

-

1 2

(z

i)2,

V

=

m2 2



ii

+

 4

(ii)2.

(5)

When

m2

=

-µ2

+

1 4

2

<

0,

it

can

be

shown

that

in

this

case

the

loss

minimum

is

no

longer

at

i = 0, but it has a degenerate minima on the surface such that i(i)2 = v, where v = -m2/.

Now we pick a point on this loss minima and expand around it. Write i = (k, v + ), where

k  {1, . . . , D - 1}. Intuitively, the k fields are in the subspace of degenerate minima and  is

the field orthogonal to . Then it can be shown that the Lagrangian can be written as

L = T + T - V - V - V,

where, in the weak coupling limit   0,

T

=

1 2

(t

k

)2

-

1 2

(zk

)2

,

T

=

1 2

(t

)2

-

1 2

(z)2

,

V = O(),

V

=

- 1 m22, 2

V = O(),

(6)

11

Under review as a conference paper at ICLR 2018

the fields  and  decouple from each other and can be treated separately. The  fields satisfy the Klein-Gordon Equation ( - m2) = 0, with = t2 - z2. The  fields satisfy the waveequation,  = 0. The correlation functions of the weights across sample space and layers, P =
(z , t )(z, t) and P = (z , t )(z, t) are the Green's functions of the respective equations
of motion. Fourier transforming the correlation functions give

i P,(t, k) = 20 exp - i0t ,

(7)

where 0 =

|k|2

+ |m2,|,

and

m2

=

1 4

2

0. The correlation function P is dominated by

values of |k| 0. Therefore    as 2  0. On the other hand, it can be shown that 

is damped by the weight eigenvalues |m2|. The singularity in the correlation function means that

the value of the weights at the start of the layer is highly correlated with the ones in later layers.

In the language of group theory. The O(D) symmetry is broken down to O(D-1). Elements of O(D) are the D ×D orthogonal matrices, which have D(D -1)/2 independent continous symmetries (e.g. the Euler angles in D = 3). The number of continuous broken from O(D) to O(D - 1) is D - 1. In the above example we showed that this corresponds to the D - 1 k fields. Each of which have
zero Hessian eigenvalue.

Even though we formulated our field theory based on the decoupling limit of ResNets, the result of infinite correlation is very general and can be applied even if the decoupling limit is not valid. It is a direct result of spontaneous symmetry breaking. We state the Goldstone Theorem without proof.

Theorem (Goldstone): For every continuous symmetry that is spontaneously broken, a weight  with zero Hessian eigenvalue is generated at zero temperature (learning rate ).

D WHY QUANTUM FIELD THEORY?
In brief, the formalism for spontaneous symmetry breaking is mostly done in quantum field theory. In terms of statistics, quantum mechanics is the study of errors. We also believe that it is a good approximation to deep neural networks in the presence of the non-linear operators. The non-linear operators quantizes the input. Let R denotes the opertor corresponding to a sigmoid, say, then the output is R(W) {0, +1} for the most part. And the negative end of ReLU is zero.
Let us take a step back and go through the logical steps to understand that a scalar quantum field theory is perhaps one of the simplest model one can consider to describe a neural network layer by layer, in the decoupling limit. We wish to formulate a dynamical model to describe the weights layer by layer,
1. We know that the outputs of non-linearities are quantized. And they need to be quantized to break the affine symmetry (see Theorem 1). This leads to quantum mechanics.
2. Quantum mechanics does not admit spontaneous symmetry breaking (Zee, 2010). 3. The decoupling limit allows spontaneous symmetry breaking in quantum mechanics. 4. The decoupling limit and non-linearity together is quantum field theory.
Therefore, if one wishes to model the outputs of non-linearities in the decoupling limit. There is no choice but to employ quantum field theory.

REFERENCES
Anonymous. Decoupling the layers in residual networks. International Conference on Learning Representations, 2018.
D. Balduzzi et al. The shattered gradients problem: If resnets are the answer, then what is the question? 2017. https://arxiv.org/abs/1702.08591.
I. J. Goodfellow et al. Qualitatively characterizing neural network optimization problems. 2014. https://arxiv.org/abs/1412.6544.

12

Under review as a conference paper at ICLR 2018
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. CoRR, 2015a. https://arxiv.org/abs/1512.03385.
K. He et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. 2015b. https://arxiv.org/abs/1502.01852.
K. He et al. Identity mappings in deep residual networks. European Conference on Computer Vision. Springer International Publishing, 2016.
S. Jastrzebski et al. Residual connections encourage iterative inference. 2017. https://arxiv. org/abs/1710.04773.
J. I. Kapusta and C. Gale. Finite-temperature field theory: principles and applications. Cambridge University Press, 2nd edition, 2006.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. NIPS, 2012.
E. Orhan and X. Pitkow. Skip connections eliminate singularities. 2017. https://arxiv.org/ abs/1701.09175.
M Peskin and D. V. Schroeder. Introduction to quantum field theory. Westview Press, 1995. R. Shwartz-Ziv and N. Tishby. Opening the black box of deep neural networks via information.
2017. https://arxiv.org/abs/1703.00810. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. ICLR, 2015. L. N. Smith and N. Topin. Super-convergence: Very fast training of residual networks using large
learning rates. 2017. https://arxiv.org/abs/1708.07120. C. Szegedy et al. Going deeper with convolutions. CVPR, 2015. A. Veit, Wilber M., and S. Belongie. Residual networks behave like ensembles of relatively shallow
networks. 2016. https://arXiv.org/abs/1605.06431. W. Yuhuai et al. On multiplicative integration with recurrent neural networks. NIPS, 2016. https:
//arxiv.org/abs/1703.00810. A. Zee. Quantum field theory in a nutshell. Princeton University Press, 2010. C. Zhang et al. Understanding deep learning requires rethinking generalization. 2017.
13

