Under review as a conference paper at ICLR 2018

SEMANTIC INTERPOLATION IN IMPLICIT MODELS
Anonymous authors Paper under double-blind review

ABSTRACT
In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths. Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths, but they are also guaranteed to pass through high-density regions, irrespective of the dimensionality of the latent space. Experiments on standard benchmark image datasets demonstrate clear visual improvements in the quality of the generated samples and exhibit more meaningful interpolation paths.

1 INTRODUCTION

Continuous latent variable models have been developed and studied in statistics for almost a cen-
tury, with factor analysis (Young (1941); Bartholomew (1987)) being the most paradigmatic and
widespread model family. In the neural network community, autoencoders have been used to find
low-dimensional codes with low reconstruction error (Baldi & Hornik (1989); DeMers & Cottrell
(1993)). Recently there has been an increased interest in implict models, where a complex gener-
ative mechanism is driven by a source of randomness (cf. MacKay (1995)). This includes popular
architectures known as Variational Autoencoders (VAEs, Kingma & Welling (2013); Rezende et al.
(2014)) as well as Generative Adversarial Networks (GANs, Goodfellow et al. (2014)). Typically, one defines a deterministic mechanism or generator G : Rd  Rm, z  G(z) = x, parametrized by  and often implemented as a deep neural network (DNN). This map is then hooked up to a code distribution z  Pz, to induce a distribution x  Px. It is known that under mild regularity conditions, by a suitable choice of generator, any Px can be obtained from an arbitrary fixed Pz (cf. Kallenberg (2006)). Relying on the representational power and flexibility of DNNs, this has led to the view that code distributions should be simple, e.g. most commonly Pz = N (0, I). Implicit models are essentially sampling devices that can be trained and used without explicit access to the
densities they define. They have shown great promise in producing samples that are perceptually in-
distinguishable from samples generated by nature (Radford et al. (2015)) and are currently a subject
of extensive investigation.

Earlier work on embedding models such as the word embeddings of Mikolov et al. (2013a), has shown how semantic relations and analogies are naturally captured by the affine structure of embeddings (Mikolov et al. (2013c); Levy & Goldberg (2014)). This has inspired the use of affine vector arithmetic and linear interpolation in implicit models such as GANs, where it has shown to lead to semantic interpolations in image space (cf. Radford et al. (2015); White (2016)). These traversal experiments have also been used to justify that deep generative models do not only memorize the training data, but do learn models that generalize to unseen data. However, as pointed out by White (2016), the commonly used linear interpolation has one major flaw in that it ignores the manifold structure of the latent space. Indeed, traversing the latent space along straight lines may lead through low-density regions, where ­ by definition ­ the generator has not been trained (well). This is easy to understand as for z  N (0, I) we get that

z 2  2(d),

and hence E z 2 = d,

Var

z2 2 =.

dd

(1)

Relative to the average, the variance of the squared code vector norm vanishes like O(1/d) with the latent space dimensionality. Thus as d increases, the probability mass concentrates in a thin shell

1

Under review as a conference paper at ICLR 2018

1.0 0.5 0.0 0.5 1.0

1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.0 0.5 0.0 0.5 1.0

Figure 1: Illustration showing the distribution of the mass for a Normal prior distribution. In highdimensions, most the mass concentrates in the outer shell shown in blue while the middle shown in red is almost massless. Linearly interpolating between 2 points produces the green dotted trajectory thus traversing a region of the space with low probability mass. In contrast, following the great circle arc in blue produces samples that are more likely but these paths can also be unstable as they exhibit higher variance (see main text).
 around the (d - 1)-sphere with radius d. As shown in Figure 1, the interpolation paths between any two points on this sphere will always pass through the interior, and the larger their distance, the closer the path's midpoint will be to the origin.
Although we are not the first to point out this weakness, there has not been any rigorous attempts to analyze this phenomenon and to come up with a substantially improved interpolation scheme. The proposal by White (2016) is to use spherical linear interpolation. However, note that this scheme produces interpolation curves that are very similar to great circle arcs. These paths can be unstable (think of slightly perturbing points at opposing poles of the hyper-sphere) as well as unnecessarily long, passing through codes of images, say, that have very little in common with the ones at either endpoint.
Towards this goal, we make the following contributions:
i) We properly analyze the phenomenon by characterizing the KL-divergence between the latent code prior and the effective distribution induced on interpolated in-between points.
ii) We propose a modified prior distribution that effectively puts more probability mass closer to the origin and that provably controls the KL-divergence in a dimension-independent manner.
iii) We argue that linear interpolation by straight lines in this new model does not suffer from the problem identified in the original model.
iv) We provide extensive experiments that demonstrate the different nature of the interpolation paths and that show clear evidence of improved visual quality of in-between samples for images.
2 METHOD
2.1 NAIVE SAMPLING FROM A NORMAL DISTRIBUTION
Distributional mismatch We consider the common GAN framework with generator G and latent code vectors sampled from an isotropic normal distribution, i.e. z  N (0, 2I). In the typical
2

Under review as a conference paper at ICLR 2018

traversal experiment, one considers two code vectors z0, z1  Rd sampled independently and interpolates between them with a straight line h : [0; 1]  Rd, t  h(t) = (1 - t)z0 + tz1. In doing so, one expects, for instance, that the mid-point z = h(1/2) should correspond to a sample
that semantically relates to both G(z0) and G(z1). However, based on the arguments made before, it is often found in practice that the code h(1/2) falls in a latent space region of low probability

mass. Consequently, the generated samples are often not representative of the data distribution. To

elucidate this further, note that the distribution of the squared norm of midpoints can be worked out as follows

z

2= 1 4

z0

2+

1 4

z1

2

1 4

2(d) + 2(d)

= 1 2(2d) . 4

(2)



In particular this implies that in expectation the norm of the midpoint is a factor of 1/ 2 smaller

than the norm at the endpoints. What conclusion can we draw from this observation? Mainly that the process used to train the generator network and the evaluation strategy used when traversing

the latent space are not consistent. Indeed, at training time, the generator network is trained with

vectors whose norm follows a 2(d) distribution. At test time, however, the traversal procedure

passes through midpoints whose squared norms follow a 2(2d) distribution scaled back by a factor

of

1 4

.

Clearly

this

leads

to

a

problematic

train-test

mismatch.

Formal Analysis Before we formalize the observations we made so far, let us collect some prop-

erties of the 2-distribution that are needed for the analysis below. In statistics, the 2 distribution

is usually introduced via Eq. (1). Its density has support on the non-negative reals and can shown to

be given by

1

f2 (u; d)

=

2

d 2



d 2

u

d 2

-1

e-

u 2

,

for u  0

(3)

The 2(d) distribution is a special case of a Gamma distribution 

d 2

,

2

with shape parameter d/2

and scale 2. This generalization is helpful as we can now also identify the midpoint distribution

as

a

Gamma

distribution,

namely

(d,

1 2

).

The following lemma gives a characterization of the

KL-divergence between these two latent space distributions.

Lemma 1.

Let z  (k, 2) and z



(2k,

1 2

)

for

any



>

0,

then

KL(z z ) = -k(k) + ln (2k) + k(3 - 2 ln 4), (k)

(4)

where  and  are the Gamma- and Digamma-functions, respectively.

Proof. Using straightforward calculus.

One

can

also

work

out

what

happens

in

the

relevant

limit

of

k

=

d 2



.

Corollary 2.

lim KL(z z ) = ,

d KL(z z ) > 0 ,

k

dk

lim d KL(z z ) = 2 - ln 4  0.6137, and k dk

d2

lim
k

dk2

KL(z

z ) = 0.

(5)

In summary, KL(z z ) strictly increases with k, asymptotically growing linearly in k with the given
rate. However, as pointed out by Arjovsky & Bottou (2017), we need a sufficiently high latent space
dimension that at least matches the intrinsic dimensionality of the data manifold. Otherwise it is impossible for Pz to be continuous and then stability issues commonly observed with GANs may occur. Thus it seems hard to avoid the blow-up of the KL-divergence that comes with large d.

Spherical interpolation One remedy to counteract the above problem is to use spherical interpolation, essentially generalizing the notion of geodesics on a hyper-sphere. We have already eluded to the proposal of White (2016), which uses the interpolation with  := (z0, z1),

sin((1 - t)) sin(t) h(t) = sin() z0 + sin() z1,

z

=

h(

1 2

)

=

sin(

 2

)

sin()

(z0

+

z1)

=

1

cos(

 2

)

z0

+ 2

z1

.

(6)

3

Under review as a conference paper at ICLR 2018

It is easy to check that if z0 = z1 = r, then this curve follows the great circle with radius r that connects z0 and z1. In the more general case as long as ||  /2, we get the bounds min{ z0 , z1 }  h(t)  max{ z0 , z1 }. While this interpolation formula may be appropriate in the original context of animating rotations as in Shoemake (1985), it is known that for larger
angles it does not lead to semantically meaningful interpolation paths for GANs as these paths get
too long, often passing through images that are seemingly unrelated. In addition, spherical inter-
polation destroys the simple affine vector arithmetic that has proven so useful in other contexts and
that has shown to disentangle the nonlinear factors of variation into simple linear statistics (Mikolov
et al. (2013a)). Therefore, it has been our goal to fix the divergence problem in a way that allows us
to stick to linear interpolation in code space.

2.2 GAMMA DISTANCE MODEL

There is nothing sacred about the isotropic normal distribution as a code vector distribution other

than a certain non-informativeness in the absence of other requirements. Here we suggest to keep

the isotropic nature of the latent space distribution, but to modify the distribution of the norm or

distance from the origin. Thus we factor the distribution as follows:

v  Uniform(Sd-1),

r  Pr,

 z= r v.

(7)

The choice Pr = 2(d) brings us back to the normal case and the problems that come with it. Instead, we eliminate the dimension dependency in the choice of Pr. One simple way to accomplish
that is to stay within the family of -distributions with fixed shape and set

Pr

=

(

1 2

,

),

 > 0.

(8)

In particular, if  = 2, this results in the same marginal distribution over norms than in the 1dimensional Gaussian case, thereby counteracting the concentration effect on the hyper-sphere of radius d.

Proposition 1. Using the model described in Eqs. (7)-(8), we have

z0

2,

z1

2



(

1 2

,

),

z0 + z1 2

2



(1,

1 4

).

(9)

Furthermore the KL divergence between z and mid points z is given by

KL(z

z

)=

1 (3 +  - ln 4)



1 ,

22

where  is the Euler-Mascheroni constant.

(10)

What have we gained? We would like to make two observations: (i) Eq (10) shows that the KL divergence is constant and does not grow with the latent space dimension. In Figure 3 we compare the KL divergence for the two sampling procedures, which clearly demonstrates that the gamma sampling procedure is beneficial over almost the entire range of possible dimensions d. (ii) While retaining a constant divergence, the gamma sampling procedure still offers the ability to tune the noise level through the free scale parameter  (similar to  for the original sampling procedure).

4

Under review as a conference paper at ICLR 2018

KL p(x)

PDF of (k, )
1.0
0.8
0.6

k = 0.5, = 2.0 k = 1.0, = 0.5 k = 10.0, = 2.0 k = 20.0, = 0.5

0.4

0.2

0.0 0 5 10 1x5 20 25 30
Figure 2: PDF of different Gamma distributions. The lines k = 10 and k = 20 correspond to z and z in the case of a normal prior of dimension 20. Note the difference in overlap compared to our suggested prior that corresponds to k = 0.5 and k = 1.

KL divergence between z and z
normal prior suggested prior at k = 21 101
100
10 3 10 2 10 1 100 101 102 k (= 21 of dimension of latent space in normal prior)
Figure 3: KL divergence between length of samples z and mid-points z for different priors. For a normal prior, the KL increases linearly with the dimension of the latent space. Using our suggested prior, the KL remains constant, independent of the latent space dimension.
5

Under review as a conference paper at ICLR 2018
3 EXPERIMENTS
Experimental results. The setup used for the experiments presented below closely follows popular setups in GAN research and is detailed in the Appendix. 3.1 SAMPLES FROM GAN WITH -PRIOR Figure 3.1 compares the samples generated from a Normal prior to the gamma prior for different benchmark image datasets. In addition to straightforwardly replacing the noise sampling, we used no additional tricks to obtain these results. This shows that GANs with gamma priors can be trained just as easily as traditional GANs.
(a) Samples from CelebA with normal (left) and gamma (right) prior
(b) Samples from LSUN kitchen with normal (left) and gamma (right) prior
(c) Samples from LSUN bedroom with normal (left) and gamma (right) prior.
(d) Samples from MNIST with normal (left) and gamma (right) prior.
(e) Samples from SVHN with normal (left) and gamma (right) prior.
(f) Samples from CIFAR10 with normal (left) and gamma (right) prior. 6

Under review as a conference paper at ICLR 2018
3.2 TRAVERSAL EXPERIMENTS Here we perform two different types of traversals from the same pair of points of same length lying on opposite sides of the center of the sphere. While one traversal goes straight (in a Euclidean sense) through the middle, the other traversal goes along a geodesic on the sphere. We compare the two traversal paths for a model trained using a multivariate normal prior and for a model using our suggested gamma prior. Along with sampled traversal trajectories, we also show the discriminator activation along these trajectories, averaged over 1000 trajectory samples. Plotted are the mean discriminator activation and one standard deviation. More traversal samples can be found in the Appendix. 3.2.1 SPHERE GEODESIC TRAVERSAL Figures 4 and 5 show traversals and discriminator activation along a great circle on the sphere in latent space. Note that often, the path taken is visiting realistic and interesting samples, but is not semantically interpolating between the given pair of endpoints. Also note that the discriminator activation stays the same along the paths, meaning it judges all samples as equally likely.
Figure 4: Sphere geodesic traversal on CelebA with a multivariate normal prior.
7

Under review as a conference paper at ICLR 2018
Figure 5: Sphere geodesic traversal on CelebA with a gamma prior. 3.2.2 STRAIGHT EUCLIDEAN TRAVERSAL Figures 6 and 7 show traversals and discriminator activation along a straight line between the two endpoints. For the normal prior, this results in garbage, since we pass through latent space that the GAN has never seen during training. Another indication of this deficiency is the fact that the discriminator activation goes down drastically around the mid-point of the traversal. For the gamma prior, however, the straight traversal results in a smooth interpolation between the endpoints. Note that these traversals are much more semantic in nature, with the samples along the path really lying in between the endpoints. Also note the emergence of a mean sample when looking at the mid-points. In the case of faces, these mid-points, which are points closest to the origin, tend to be very common looking faces, looking straight ahead and having little uncommon features. This emergence is even more pronounced in the traversals on the LSUN datasets in the Appendix.
8

Under review as a conference paper at ICLR 2018
Figure 6: Straight Euclidean traversal on CelebA with a multivariate normal prior.
Figure 7: Straight Euclidean traversal on CelebA with a gamma prior. 9

Under review as a conference paper at ICLR 2018
3.3 LATENT MEAN SAMPLES Since we noticed in our traversal experiments with gamma priors the interesting phenomenon that the mid-points of the sampled pairs of endpoints tend to converge to what one might call mean samples, we took our trained models and specifically sampled points close to the coordinate origin in order to directly obtain such mean samples. Figure 8 shows these mean samples for our different datasets.

(a) CelebA

(b) LSUN kitchen

(c) LSUN bedroom

(d) MNIST

(e) SVHN

(f) CIFAR10

Figure 8: Samples around the coordinate origin in latent space on GANs trained with a gamma prior.

3.4 ALGEBRA EXPERIMENTS

Since we've established that using our gamma priors results in the latent space becoming more Euclidean in nature, we can ask whether this also helps for another task people are often using to evaluate generative models.

We perform various analogy experiments such as the one described in Mikolov et al. (2013b) who demonstrated words vectors exhibit relationships such as "Paris - France + Italy = Rome". In order to perform such experiments, we use the CelebA dataset that provides multiple binary attribute labels for each sample. Consider two attributes, A and B. We denote by [A, B] a pair of samples that have both attributes, by [a, b] samples that have none of the two, and [A, b], [a, B] samples that have only one of the attributes.

For each pair of attributes A, B, we want that

z([A, B]) - z([a, B]) + z([a, b])=~ z([A, b])

(11)

where z([·, ·]) denotes the mean latent representation of a set of samples with (or without) the given attributes.

Using a pre-trained model, we sample a batch of samples from the generator from each of the four categories using a classifier to decide which category a sample belongs to. We then quantify to what degree the analogy described in Equation 11 holds using the following Latent Algebra Score (LAS):

2 LAS = N (N - 1)
A=B

z([A, B]) - z([a, B]) + z([a, b]) - z([A, b])

2 2

m

z

2 2

(12)

where

N

is

the

number

of

binary

attributes

and

m

z

2 2

is

the

mean

squared

norm

of

all

used

latent

vectors. The results shown in Table 1 reveal that the gamma sampling procedure produces better

analogies compared to the standard sampling with a normal prior.

10

Under review as a conference paper at ICLR 2018

Prior

LAS

normal 0.007496 gamma 0.005638

Table 1: Latent Algebra Score for CelebA (lower is better)

4 RELATED WORK
Learned latent representations often allow for vector space arithmetic to translate to semantic operations in the data space Radford et al. (2015); Larsen et al. (2015). Early observations showing that the latent space of a GAN finds semantic directions in the data space (e.g. corresponding to eyeglasses and smiles) were made in Radford et al. (2015). Recent work has also focused on learning better similarity metrics Larsen et al. (2015) or providing a finer semantic decomposition of the latent space Donahue et al. (2017). As a consequence, the evaluation of current GAN models is often done by sampling pair of points and linear interpolating between them in the latent space, or performing other types of noise vector arithmetic Bojanowski et al. (2017). This results in sampling the latent space from locations with very low probability mass. This observation was also made in White (2016) who suggested replacing linear interpolation with spherical linear interpolation which prevents diverging from the model's prior distribution.
5 CONCLUSION
While the standard way of sampling latent vectors for GANs is based on using a Normal distribution over the latent space, we showed that it might produce samples that are not likely under the model distribution. We discussed how this procedure suffers from the curse of dimensionality and demonstrated how a simple alternative procedure solves this problem. Finally, we provided an extensive set of experiments that clearly demonstrate visual improvements in the samples generated using our gamma sampling procedure.
REFERENCES
Martin Arjovsky and Le´on Bottou. Towards principled methods for training generative adversarial networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR, volume 2016, 2017.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural networks, 2(1):53­58, 1989.
D. J. Bartholomew. Latent Variable Models and Factor Analysis. London: Griffin, 1987.
Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space of generative networks. arXiv preprint arXiv:1707.05776, 2017.
David DeMers and GW Cottrell. n­linear dimensionality reduction. Adv. Neural Inform. Process. Sys, 5:580­587, 1993.
Chris Donahue, Akshay Balsubramani, Julian McAuley, and Zachary C Lipton. Semantically decomposing the latent spaces of generative adversarial networks. arXiv preprint arXiv:1705.07904, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. pp. 2672­2680, 2014.
Olav Kallenberg. Foundations of modern probability. Springer Science & Business Media, 2006.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. arXiv.org, December 2013.

11

Under review as a conference paper at ICLR 2018
Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.
Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicit word representations. In Proceedings of the eighteenth conference on computational natural language learning, pp. 171­180, 2014.
David JC MacKay. Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 354(1):73­80, 1995.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. arXiv.org, January 2013a.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013b.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In hlt-Naacl, volume 13, pp. 746­751, 2013c.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. November 2015.
D J Rezende, S Mohamed, and D Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv.org, 2014.
Ken Shoemake. Animating rotation with quaternion curves. In ACM SIGGRAPH computer graphics, volume 19, pp. 245­254. ACM, 1985.
Tom White. Sampling generative networks: Notes on a few effective techniques. arXiv preprint arXiv:1609.04468, 2016.
Gale Young. Maximum likelihood estimation and factor analysis. Psychometrika, 6(1):49­53, 1941.
12

Under review as a conference paper at ICLR 2018
A MORE TRAVERSAL EXPERIMENTS
A.1 STRAIGHT EUCLIDEAN TRAVERSAL
Figure 9: Straight Euclidean traversal on LSUN kitchen with a normal (top) and gamma (bottom) prior.
13

Under review as a conference paper at ICLR 2018
Figure 10: Straight Euclidean traversal on LSUN bedroom with a normal (top) and gamma (bottom) prior.
14

Under review as a conference paper at ICLR 2018
Figure 11: Straight Euclidean traversal on MNIST with a normal (top) and gamma (bottom) prior. 15

Under review as a conference paper at ICLR 2018
Figure 12: Straight Euclidean traversal on SVHN with a normal (top) and gamma (bottom) prior. 16

Under review as a conference paper at ICLR 2018
Figure 13: Straight Euclidean traversal on CIFAR10 with a normal (top) and gamma (bottom) prior. 17

Under review as a conference paper at ICLR 2018 A.2 SPHERE GEODESIC TRAVERSAL
Figure 14: Sphere geodesic traversal on LSUN kitchen with a normal (top) and gamma (bottom) prior.
18

Under review as a conference paper at ICLR 2018
Figure 15: Sphere geodesic traversal on LSUN bedroom with a normal (top) and gamma (bottom) prior.
19

Under review as a conference paper at ICLR 2018
Figure 16: Sphere geodesic traversal on MNIST with a normal (top) and gamma (bottom) prior. 20

Under review as a conference paper at ICLR 2018
Figure 17: Sphere geodesic traversal on SVHN with a normal (top) and gamma (bottom) prior. 21

Under review as a conference paper at ICLR 2018
Figure 18: Sphere geodesic traversal on CIFAR10 with a normal (top) and gamma (bottom) prior. 22

Under review as a conference paper at ICLR 2018
B EXPERIMENT SETUP
For our experiments, we use a standard DCGAN architecture featuring 5 deconvolutional and convolutional layers with 4×4 filters applied in strides of 2 in the generator and discriminator, respectively. We use ReLU nonlinearities and batch normalization in the generator, while the discriminator features leaky ReLU nonlinearities and batch normalization from the 2nd layer on. The latent space for all models is of dimension 100 and the scale parameters for both the normal and gamma distributions are set to 1.0. The networks are trained using RMSProp with a learning rate of 0.0003 and mini-batches of size 100. The samples for the CelebA dataset have been cropped to 118 × 118 and then resized to 64 × 64.
23

