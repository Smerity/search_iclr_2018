Under review as a conference paper at ICLR 2018
A DIRT-T APPROACH TO UNSUPERVISED DOMAIN ADAPTATION
Anonymous authors Paper under double-blind review
ABSTRACT
Domain adaptation refers to the problem of how to leverage labels in one source domain to boost up learning performance in a new target domain where labels are scarcely available or completely unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, feature distribution matching is under-constrained, 2) in the non-conservative domain adaptation setting, where no single classifier can perform well jointly in both the source and target domains, domain adversarial training is over-constrained. In this paper, we address these issues through the lense of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: (1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; (2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T)1 model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on several visual domain adaptation benchmarks.
1 INTRODUCTION
The development of deep neural networks has enabled impressive performance in a wide variety of machine learning tasks. However, these advancements often rely on the existence of a large amount of labeled training data. In many cases, direct access to vast quantities of labeled data to the task of interest (the target domain) is either costly or otherwise absent, but labels are readily available for related training sets (the source domain). A notable example of this scenario occurs when the source domain consists of richly-annotated synthetic or semi-synthetic data, but the target domain consists of unannotated real-world data (Sun & Saenko, 2014; Vazquez et al., 2014). However, the source data distribution is often dissimilar to the target data distribution, and the resulting significant covariate shift is often detrimental to the performance of the source-trained model when applied to the target domain (Shimodaira, 2000).
Solving the covariate shift problem of this nature is commonly referred to as domain adaptation. In this paper, we consider a challenging setting of domain adaptation where 1) we are provided with fully-labeled source samples and completely-unlabeled target samples, and 2) the existence of a classifier in the hypothesis class with low error on both source and target distributions is not guaranteed. Borrowing approximately the terminology from Ben-David et al. (2010b), we refer to this setting as unsupervised, non-conservative domain adaptation.
To tackle unsupervised domain adaptation, Ganin & Lempitsky (2015) proposed to constrain the classifier to only rely on domain-invariant features. This is achieved by training the classifier to perform well on the source domain while minimizing the divergence between features extracted from the source versus target domains. To achieve divergence minimization, Ganin & Lempitsky
1Pronounce as "dirty."
1

Under review as a conference paper at ICLR 2018

(2015) employ domain adversarial training. We highlight two issues with this approach: 1) when the feature function is high-capacity and the source-target supports are disjoint, the domain-invariance constraint is potentially very weak (see Section 3), and 2) good generalization on the source domain hurts target performance in the non-conservative setting.
Saito et al. (2017) addressed these issues by replacing domain adversarial training with asymmetric tri-training (ATT), which relies on the assumption that target samples that are labeled by a sourcetrained classifier with high confidence are correctly labeled by the source classifier. In this paper, we consider an orthogonal assumption: the cluster assumption (Chapelle & Zien, 2005), that covariate distribution contains separated data clusters and that data samples in the same cluster share the same class label. This assumption introduces an additional bias where we seek decision boundaries that do not go through high-density regions. Based on this intuition, we propose two novel models: (1) the Virtual Adversarial Domain Adaptation (VADA) model which incorporates an additional virtual adversarial training (Miyato et al., 2017) and conditional entropy loss to push the decision boundaries away from the empirical data, and (2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model which uses natural gradient to further refine the output of the VADA model while focusing purely on the target domain. We demonstrate that
1. In conservative domain adaptation, where the classifier is trained to perform well on the source domain, VADA can be used to further constrain the hypothesis space by penalizing violations of the clustering assumption, thereby improving domain adversarial training.
2. In non-conservative domain adaptation, where we account for the mismatch between the source and target optimal classifiers, DIRT-T allows us to transition from a good joint (source and target) classifier (VADA) to a better target domain classifier. Interestingly, we demonstrate the advantage of natural gradients in DIRT-T refinement steps.
We report results for domain adaptation in digits classification (MNIST-M, MNIST, SYN DIGITS, SVHN), traffic sign classification (SYN SIGNS, GTSRB), and general object classification (STL-10, CIFAR-10). We show that, in nearly all experiments, VADA improves upon previous methods and that DIRT-T improves upon VADA, setting new state-of-the-art performance across a wide range of visual domain adaptation benchmarks. In adapting MNIST  SVHN, a very challenging task, we out-perform ATT by over 20%.

2 RELATED WORK

Given the extensive literature on domain adaptation, we highlight several works most relevant to our paper. Shimodaira (2000); Mansour et al. (2009) proposed to correct for covariate shift by re-weighting the source samples such that the discrepancy between the target distribution and reweighted source distribution is minimized. Such a procedure is problematic, however, if the source and target distributions do not contain sufficient overlap. Huang et al. (2007); Long et al. (2015); Ganin & Lempitsky (2015) proposed to instead project both distributions into some feature space and encourage distribution matching in the feature space. Ganin & Lempitsky (2015) in particular encouraged feature matching via domain adversarial training, which corresponds approximately to Jensen-Shannon divergence minimization (Goodfellow et al., 2014). To better perform nonconservative domain adaptation, Saito et al. (2017) proposed to modify tri-training (Zhou & Li, 2005) for domain adaptation, leveraging the assumption that highly-confident predictions are correct predictions (Zhu, 2005). Several of aforementioned methods are based on Ben-David et al. (2010a)'s theoretical analysis of domain adaptation, which states the following,

Theorem 1 (Ben-David et al., 2010a) Let H be the hypothesis space and let (Xs, s) and (Xt, t) be the two domains and their corresponding generalization error functions. Then for any h  H,

t(h)



1 2

dHH

(Ds,

Dt)

+

s

(h)

+

min
h H

t(h ) +

s(h ),

(1)

where dHH denotes the HH-distance between the domains Xs and Xt,

dHH = 2 sup |ExDs [h(x) = h (x)] - ExDt [h(x) = h (x)]| .
h,h H

(2)

2

Under review as a conference paper at ICLR 2018

Intuitively, dHH measures the extent to which small changes to the hypothesis in the source domain can lead to large changes in the target domain. It is evident that dHH relates intimately to the complexity of the hypothesis space and the divergence between the source and target domains. For disjoint domains and infinite-capacity models, dHH is maximal.
Another critical component to our paper is the cluster assumption, which states that the decision boundary should not cross high-density regions, but instead lie in low-density regions (Chapelle & Zien, 2005). This assumption has been extensively studied and leveraged for semi-supervised learning, leading to proposals such as conditional entropy minimization (Grandvalet & Bengio, 2005) and pseudo-labeling (Lee, 2013). More recently, the cluster assumption has led to many successful semi-supervised learning algorithms such as semi-supervised generative adversarial networks (Dai et al., 2017), virtual adversarial training (Miyato et al., 2017), and self/temporal-ensembling (Laine & Aila, 2016; Tarvainen & Valpola, 2017). Given the success of the cluster assumption in semi-supervised learning, we propose to apply it to domain adaptation. Independently of our work, (French et al., 2017) demonstrated the application of self-ensembling to domain adaptation. However, our work additionally considers the application of the cluster assumption to non-conservative domain adaptation.

3 LIMITATION OF DOMAIN ADVERSARIAL TRAINING

Before describing our model, we first highlight that domain adversarial training may not be sufficient
for domain adaptation if the feature extraction function has high-capacity. Consider a classifier h, parameterized by , that maps inputs to the K-simplex, where K is the number of classes. Suppose
the classifier h = g  f can be decomposed as the composite of an embedding function f : X  Z
and embedding classifier g : Z  Y. For the source domain, let Ds be the joint distribution over input x and one-hot label y and let Xs be the marginal input distribution. (Dt, Xt) are analogously defined for the target domain. Let (Ls, Ld) be the loss functions

Ly(; Ds) = Ex,yDs y ln h(x)

Ld(;

Ds,

Dt)

=

max
D

ExDs

[ln

D(f (x))]

+

ExDt

[ln(1

-

D(f (x)))]

,

(3) (4)

where we maximize over discriminators D : Z  (0, 1). Then Ly is the cross-entropy objective and D is a domain discriminator. Domain adversarial training minimizes the objective

min.


Ly

(;

Ds)

+

d

Ld

(;

Ds

,

Dt

),

(5)

where d is a weighting factor. Minimization of Ld encourages the learning of a feature extractor f for which the Jensen-Shannon divergence between f (Xs) and f (Xt) is small. Ganin & Lempitsky (2015) suggest that successful adaptation tends to occur when the source generalization error and
feature divergence are both small.

It is easy, however, to construct situations where the suggestion does not hold. In particular, if f is infinite-capacity and the source-target supports are disjoint, then f can employ arbitrary transformations to the target subspace so as to match the source feature distribution. A formal statement and proof is provided in Appendix E. We verify empirically that, for sufficiently deep layers, jointly achieving small source generalization error and feature divergence does not imply high accuracy on the target task (Table 3). Given the inherent limitations of domain adversarial training, we wish to identify additional constraints that one can place on the model to achieve better, more reliable domain adaptation.

4 CONSTRAINING VIA CONDITIONAL ENTROPY MINIMIZATION
In this paper, we apply the cluster assumption to domain adaptation. The cluster assumption assumes that the input distribution X contains density clusters and that points in the same cluster come from the same class. This assumption has been extensively studied and applied successfully to a wide range of classification tasks (see Section 2). If the cluster assumption holds, the optimal decision boundaries should occur far away from data-dense regions in the space of X (Chapelle & Zien, 2005). Following Grandvalet & Bengio (2005), we achieve this behavior via minimization of the

3

Under review as a conference paper at ICLR 2018

{xs, ys} {xt}

Cross-Entropy+VAT Divergence
Conditional Entropy+VAT

Figure 1: VADA improves upon domain adversarial training by additionally penalizing violations of the cluster assumption.

conditional entropy with respect to the target distribution,

Lc(; Dt) = -ExDt h(x) ln h(x) .

(6)

Intuitively, minimizing the conditional entropy forces the classifier to be confident on the unlabeled target data, which occurs if the classifier places its decision boundaries far from the target data-dense regions. In practice, the conditional entropy must be empirically estimated using the available data. However, Grandvalet & Bengio (2005) notes that this approximation breaks down if the classifier h is not locally-Lipschitz. To prevent this, we propose to explicitly incorporate the locally-Lipschitz constraint via virtual adversarial training (Miyato et al., 2017) and add to the objective function the additional term

Lv(; D) = ExD

max
r

DKL(h^(x)

h(x + r))

,

(7)

which enforces classifier consistency around the norm-ball neighborhood of each sample x, where ^ is a copy of . Note that virtual adversarial training can be applied with respect to either the target or source distributions. We can combine the conditional entropy minimization objective and domain
adversarial training to yield

min.


Ly

(;

Ds)

+

dLd(;

Ds,

Dt)

+

sLv

(;

Ds

)

+

t

[Lv

(;

Dt)

+

Lc

(;

Dt)]

,

(8)

a basic combination of domain adversarial training and semi-supervised training objectives. We
refer to this as the Virtual Adversarial Domain Adaptation (VADA) model. Empirically, we observed that the hyperparameters (d, s, t) are easy to choose and work well across multiple tasks (Appendix B).

HH-Distance Minimization. VADA aligns well with the theory of domain adaptation provided in Theorem 1. Let the loss,

Lt() = Lv(; Dt) + Lc(; Dt),

(9)

be a proxy measure for the degree of violation of the target-side cluster assumption. For a reasonably small choice of t, VADA can penalize models with high Lt while still enabling decently small source generalization error. This penalization effectively rejects hypotheses which egregiously vi-
olate the target-side cluster assumption. By rejecting such hypotheses from the hypothesis space H, VADA reduces dHH and yields a tighter bound on the target generalization error. We verify empirically that VADA achieves significant improvements over existing models on multiple domain
adaptation benchmarks (Table 1).

5 DECISION-BOUNDARY ITERATIVE REFINEMENT TRAINING

In non-conservative domain adaptation, we account for the following inequality,

min t(h) < t(ha) where ha = arg min s(h) + t(h),

hH

hH

(10)

4

Under review as a conference paper at ICLR 2018

VADA

DIRT-T

Figure 2: DIRT-T uses VADA as initialization. After removing the source training signal, DIRTT minimizes cluster assumption violation in the target domain through a series of natural gradient steps.

where ( s, t) are generalization error functions for the source and target domains. This means that, for a given hypothesis class H, the optimal classifier in the source domain does not coincide with the optimal classifier in the target domain.
We assume that the optimality gap in Eq. (10) results from violation of the cluster assumption. In other words, we suppose that any source-optimal classifier drawn from our hypothesis space necessarily violates the cluster assumption in the target domain. Since VADA is still constrained to do well on the source domain (ensured by choosing a small t), it will still violate the target-side cluster assumption to some extent.
Under this assumption, the natural solution is to initialize with the VADA model and then further minimize the cluster assumption violation in the target domain. In particular, we first use VADA to learn an initial classifier h0 . Next, we incrementally push the classifier's decision boundary away from data-dense regions by minimizing the proxy target-side cluster assumption violation loss Lt in Eq. (9). We denote this procedure Decision-boundary Iterative Refinement Training (DIRT).

5.1 DECISION-BOUNDARY ITERATIVE REFINEMENT TRAINING WITH A TEACHER

Stochastic gradient descent minimizes the proxy loss Lt by selecting gradient steps  according to the following objective,

min.


Lt

(

+

)

s.t.   ,

(11) (12)

which defines the neighborhood in the parameter space. This notion of neighborhood is sensitive to the parameterization of the model; depending on the parameterization, a seemingly small step 
may result in a vastly different classifier. This contradicts our intention of incrementally and locally
pushing the decision boundary to a local conditional entropy minimum, which requires that the decision boundary of h+ stay close to that of h. It is therefore important to define a neighborhood that is parameterization-invariant. Following Pascanu & Bengio (2013), we instead select  using
the following objective,

min.


Lt(

+

)

s.t. ExDt [DKL(h(x) h+(x))]  .

(13)

Each optimization step now solves for a gradient step  that minimizes the conditional entropy,
subject to the constraint that the Kullback-Leibler divergence between h(x) and h+(x) is small for x  Xt. The corresponding Lagrangian suggests that one can instead minimize a sequence of

5

Under review as a conference paper at ICLR 2018

optimization problems

min. tLt( + ) + tE
t

DKL(ht-1 (x)

ht (x))

,

(14)

that approximates the application of a series of natural gradient steps.

In practice, each of optimization problems in Eq. (14) can be solved approximately via a finite num-
ber of stochastic gradient descent steps. We denote the number of steps taken to be the refinement interval B. Similar to Tarvainen & Valpola (2017), we use the Adam Optimizer with Polyak averaging (Polyak & Juditsky, 1992). We interpret ht-1 as a (sub-optimal) teacher for the student model ht , which is trained to stay close to the teacher model while seeking to reduce the cluster assumption violation. As a result, we denote this model as Decision-boundary Iterative Refinement
Training with a Teacher (DIRT-T).

Weakly-Supervised Learning. This sequence of optimization problems has a natural interpretation
that exposes a connection to weakly-supervised learning. In each optimization problem, the teacher model ht-1 pseudo-labels the target samples with noisy labels. Rather than naively training the student model ht on the noisy labels, the additional training signal Lt allows the student model to place its decision boundaries further from the data. If the clustering assumption holds and the initial
noisy labels are sufficiently similar to the true labels, conditional entropy minimization can improve
the placement of the decision boundaries (Reed et al., 2014).

Domain Adaptation. An alternative interpretation is that DIRT-T is the recursive extension of
VADA, where the act of pseudo-labeling of the target distribution constructs a new "source" domain
(i.e. target distribution Xt with pseudo-labels). The sequence of optimization problems can then be seen as a sequence of non-conservative domain adaptation problems in which Xs = Xt but ps(y | x) = pt(y | x), where ps(y | x) = ht-1 (x) and pt(y | x) is the true conditional label distribution in the target domain. Since dHH is strictly zero in this sequence of optimization problems, domain adversarial training is no longer necessary. Furthermore, if Lt minimization does improve the student classifier, then the gap in Eq. (10) should get smaller each time the source
domain is updated.

6 EXPERIMENTS
In principle, our method can be applied to any domain adaptation tasks so long as one can define a reasonable notion of neighborhood for virtual adversarial training (Miyato et al., 2016). For comparison against Saito et al. (2017) and French et al. (2017), we limit the scope of our paper to extensive evaluation in visual domain adaptation and evaluate on MNIST, MNIST-M, Street View House Numbers (SVHN), Synthetic Digits (SYN DIGITS), Synthetic Traffic Signs (SYN SIGNS), the German Traffic Signs Recognition Benchmark (GTSRB), CIFAR-10, and STL-10.
6.1 IMPLEMENTATION DETAIL
Architecture We use a small CNN for the digits and traffic sign domain adaptation experiments, and a larger CNN for domain adaptation between CIFAR-10 and STL-10. Both architectures are available in Appendix A. For fair comparison, we additionally report the performance of source-only baseline models and demonstrate that the significant improvements are attributable to our method.
Replacing gradient reversal. In contrast to Ganin & Lempitsky (2015), which proposed to implement domain adversarial training via gradient reversal, we follow the suggestion in Goodfellow et al. (2014) and instead optimize via alternating updates to the discriminator and encoder.
Instance normalization. We explored the application of instance normalization as a pre-processing step to the input image. This procedure makes the classifier invariant to channel-wide shifts and rescaling of pixel intensities. A discussion of instance normalization for domain adaptation is provided in Appendix D. We show in Figure 3 the effect of applying instance normalization to the input image.
Hyperparameters. For each task, we tuned the four hyperparameters (d, s, t, ) by randomly selecting 1000 labeled target samples from the training set and that as our validation set. We observed that extensive hyperparameter-tuning is not necessary to achieve state-of-the-art performance.

6

Under review as a conference paper at ICLR 2018

Figure 3: Effect of applying instance normalization to the input image. In clockwise direction: MNIST-M, GTSRB, SVHN, and CIFAR-10. In each quadrant, the top row is the original image, and the bottom row is the instance-normalized image.

In all experiments with instance-normalized inputs, we restrict our hyperparameter search for each task to d = {0, 10-2}, s = {0, 1}, t = {10-2, 10-1}. We fixed  = 10-2. Note that the decision to turn (d, s) on or off that can often be determined a priori. A complete list of the hyperparameters are provided in Appendix B.
6.2 MODEL EVALUATION

Source Target
MMD (Long et al., 2015) DANN (Ganin & Lempitsky, 2015) DRCN (Ghifary et al., 2016) DSN (Bousmalis et al., 2016) kNN-Ad (Sener et al., 2016) ATT (Saito et al., 2017) -model (aug) (French et al., 2017)
Source-Only VADA DIRT-T
Source-Only VADA DIRT-T

MNIST SVHN MNIST DIGITS MNIST-M MNIST SVHN SVHN

76.9 81.5
83.2 86.7 94.2
-

71.1 71.1 82.0 82.7 78.8 86.2 92.0

35.7 40.1
40.3 52.8 71.4

88.0 90.3
91.2
92.9 94.2

Without Instance-Normalized Input:

58.5 97.7 98.9

77.0 97.9 99.4

27.9 47.5 54.5

86.9 94.8 96.1

With Instance-Normalized Input:

59.9 95.7 98.7

82.4 94.5 99.4

40.9 73.3 76.5

88.6 94.9 96.2

SIGNS GTSRB
91.1 88.7
93.1
96.2 98.4
79.6 98.8 99.5
86.2 99.2 99.6

CIFAR STL
66.4 76.3
76.3 80.0
-
77.0 78.3
-

STL CIFAR
58.7 64.2
63.6 73.5 75.3
62.6 71.4 73.3

Table 1: Results of the domain adaptation experiments. In all settings, both VADA and DIRT-T achieve state-of-the-art performance in all settings.

Source Target
ATT -model (aug) DIRT-T DIRT-T (W.I.N.I.)

MNIST MNIST-M
37.1 -
40.4 38.8

SVHN MNIST
16.1 3.7 22.4 17.0

MNIST SVHN
17.9 18.1 26.6 35.6

DIGITS SVHN
9.0 10.6 9.2 7.6

SIGNS GTSRB
20.5 1.0 19.9 13.4

CIFAR STL
4.5 3.7 1.3

STL CIFAR
7.4 11.7 10.7

Table 2: Additional comparison of the margin of improvement computed by taking the reported performance of each model and subtracting the reported source-only performance in the respective papers. W.I.N.I. indicates "with instance-normalized input."

MNIST  MNIST-M. We first evaluation the adaptation from MNIST to MNIST-M. MNIST-M is constructed by blending MNIST digits with random color patches from the BSDS500 dataset. Our method outperforms previous methods by 4%.
MNIST  SVHN. The distribution shift is exacerbated when adapting between MNIST and SVHN. Whereas MNIST consists of black-and-white handwritten digits, SVHN consists of crops of colored, street house numbers. Because MNIST has a significantly lower intrinsic dimensionality that SVHN, the adaptation from MNIST  SVHN is especially challenging when the input is not pre-processed via instance normalization. When instance normalization is applied, we achieve a strong state-ofthe-art performance 76.5% and an equally impressive margin-of-improvement over source-only of 35.6%. Interestingly, by reducing the refinement interval B and taking noisier natural gradient steps,

7

Under review as a conference paper at ICLR 2018

we were occasionally able to achieve accuracies as high as 87%. However, due to the high-variance associated with this, we omit reporting this configuration in Table 1.
SYN DIGITS  SVHN. The adaptation from SYN DIGITS  SVHN reflect a common adaptation problem of transferring from synthetic images to real images. The SYN DIGITS dataset consist of 500000 images generated from Windows fonts by varying the text, positioning, orientation, background, stroke color, and the amount of blur.
SYN SIGNS  GTSRB. This setting provide an additional demonstration of adapting from synthetic images to real images. Unlike SYN DIGITS  SVHN, SYN SIGNS  GTSRB contains 43 classes instead of 10.
STL  CIFAR. Both STL-10 and CIFAR-10 are 10-class image datasets. These two datasets contain nine overlapping classes. Following the procedure in French et al. (2017), we removed the non-overlapping classes ("frog" and "monkey") and reduce to a 9-class classification problem. We achieve state-of-the-art performance in both adaptation directions. In STL  CIFAR, we achieve a 11.7% margin-of-improvement and a performance accuracy of 73.3%. Note that because STL-10 contains a very small training set, it is difficult to estimate the conditional entropy, thus making DIRT-T unreliable for CIFAR  STL.
Overall. We achieve state-of-the-art results across all tasks. In three of the tasks (SYN DIGITS  SVHN, SYN SIGNS  GTSRB, and CIFAR  STL), our improvement margin over the sourceonly model is competitive against previous models. In the remaining four tasks, we achieve substantial margin of improvement compared to previous models. Our closest competitor is the -model. However, unlike the -model, we do not perform data augmentation.
6.3 ANALYSIS OF DIRT-T

KL

2.5 2.0

beta=0 beta=1e-2

1.5

1.0

0.5

0.0

0 10000 20000 30000 40000 50000 60000 70000 80000 Iteration

Test Accuracy

1.00

0.95

0.90

0.85

0.80

beta=0 beta=1e-2

0 10000 20000 30000 40000 50000 60000 70000 80000 Iteration

(a) SVHN  MNIST

17.5

15.0

12.5

10.0

7.5

5.0

2.5

0.0

0

beta=0 beta=1e-2

5000

10000

15000

20000

Iteration

25000

30000

Test Accuracy

0.7 0.6 0.5 0.4 0.3 0.2 0.1
0

beta=0 beta=1e-2
5000 10000 15000 20000 25000 30000 Iteration

(b) STL  CIFAR

KL

Figure 4: Comparing model behavior with and without the application of the KL-term. At iteration 0, we begin with the VADA initialization and apply the DIRT-T algorithm.

When considering Eq. (14), it is natural to ask whether defining the neighborhood with respect to the classifier is truly necessary. In Figure 4, we demonstrate in SVHN  MNIST and STL  CIFAR that removal of the KL-term negatively impacts the model. Since the MNIST data manifold is low-dimensional and contains easily identifiable clusters, applying naive gradient descent (Eq. (12)) can also boost the test accuracy during initial training. However, without the KL constraint, the classifier can sometimes deviate significantly from the neighborhood of the previous classifier, and the resulting spikes in the KL-term correspond to sharp drops in target test accuracy. In STL  CIFAR, where the data manifold is much more complex and contains less obvious clusters, naive gradient descent causes immediate decline in the target test accuracy.
8

Under review as a conference paper at ICLR 2018

(a) Source-Only

(b) VADA

(c) DIRT-T

Figure 5: T-SNE plot of the last hidden layer for MNIST (blue)  SVHN (red). We used the model without instance normalization to highlight the further improvement that DIRT-T provides.

6.4 VISUALIZATION OF REPRESENTATION
We further analyze the behavior of VADA and DIRT-T by showing T-SNE embeddings of the last hidden layer of the model trained to adapt from MNIST  SVHN. In Figure 5, source-only training shows strong clustering of the MNIST samples (blue) and performs poorly on SVHN (red). VADA offers significant improvement and exhibits signs of clustering on SVHN. DIRT-T begins with the VADA initialization and further enhances the clustering, resulting in the best performance on MNIST  SVHN.
6.5 DOMAIN ADVERSARIAL TRAINING: LAYER ABLATION

Layer
L-0 L-1 L-2 L-3 L-4 L-5 L-6 L-7

JSD 
0.001 0.002 0.353 0.036 0.012 0.235 0.486 0.644

DANN Source Accuracy
78.0 98.6 16.4 94.8 97.0 99.3 99.2 99.0

Target Accuracy
24.7 35.0 10.3 33.8 40.0 57.9 60.3 52.5

JSD 
0.001 0.007 0.383 0.034 0.020 0.244 0.509 0.608

VADA Source Accuracy
24.9 12.0 11.5 67.8 96.8 99.4 99.3 99.1

Target Accuracy
18.4 11.6 9.9 37.1 61.5 73.3 70.4 70.5

Table 3: Comparison of model behavior when domain adversarial training is applied to various layers. We denote the very last (simplex) layer of the neural network as L and ablatively domain adversarial training to the last eight layers. A lower bound on the Jensen-Shannon Divergence is computed by training a logistic regression model to predict domain origin when given the layer embeddings.

In Table 3, we applied domain adversarial training to various layers of a Domain Adversarial Neural Network (Ganin & Lempitsky, 2015) trained to adapt MNIST  SVHN. With the exception of layers L - 2 and L - 0, which experienced training instability, the general observation is that as the layer gets deeper, the additional capacity of the corresponding embedding function allows better matching of the source and target distributions without hurting source generalization accuracy. This demonstrates that the combination of low divergence and high source accuracy does not imply better adaptation to the target domain. Interestingly, when the classifier is regularized to be locally-Lipschitz via VADA, the combination of low divergence and high source accuracy appears to correlate more strongly with better adaptation.

7 CONCLUSION
In this paper, we presented two novel models for domain adaptation inspired by the cluster assumption. Our first model, VADA, performs domain adversarial training with an added term that penalizes violations of the cluster assumption. Our second model, DIRT-T, is an extension of VADA that recursively refines the VADA classifier by untethering the model from the source training signal and applying approximate natural gradients to further minimize the cluster assumption violation. Our experiments demonstrate that VADA achieves strong performance across several visual domain adaptation benchmarks, and DIRT-T further improves VADA performance. Our proposed models

9

Under review as a conference paper at ICLR 2018
open up several possibilities for future work. One possibility is to apply DIRT-T to weakly supervised learning; another is to improve the natural gradient approximation via K-FAC (Martens & Grosse, 2015) and PPO (Schulman et al., 2017). Given the strong performance of our models, we also recommend them for other downstream domain adaptation applications.
REFERENCES
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine learning, 79(1):151­175, 2010a.
Shai Ben-David, Tyler Lu, Teresa Luu, and Da´vid Pa´l. Impossibility theorems for domain adaptation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 129­136, 2010b.
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain separation networks. In Advances in Neural Information Processing Systems, pp. 343­ 351, 2016.
Olivier Chapelle and Alexander Zien. Semi-supervised classification by low density separation. In AISTATS, pp. 57­64, 2005.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semisupervised learning that requires a bad gan. arXiv preprint arXiv:1705.09783, 2017.
Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for domain adaptation. arXiv preprint arXiv:1706.05208, 2017.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, pp. 1180­1189, 2015.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep reconstruction-classification networks for unsupervised domain adaptation. In European Conference on Computer Vision, pp. 597­613. Springer, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in neural information processing systems, pp. 529­536, 2005.
Jiayuan Huang, Arthur Gretton, Karsten M Borgwardt, Bernhard Scho¨lkopf, and Alex J Smola. Correcting sample selection bias by unlabeled data. In Advances in neural information processing systems, pp. 601­608, 2007.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016.
Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML, volume 3, pp. 2, 2013.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pp. 97­105, 2015.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In International Conference on Machine Learning, pp. 2408­2417, 2015.
Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Virtual adversarial training for semi-supervised text classification. stat, 1050:25, 2016.
10

Under review as a conference paper at ICLR 2018
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint arXiv:1301.3584, 2013.
Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838­855, 1992.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.
Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised domain adaptation. arXiv preprint arXiv:1702.08400, 2017.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Ozan Sener, Hyun Oh Song, Ashutosh Saxena, and Silvio Savarese. Learning transferrable representations for unsupervised domain adaptation. In Advances in Neural Information Processing Systems, pp. 2110­2118, 2016.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of statistical planning and inference, 90(2):227­244, 2000.
Baochen Sun and Kate Saenko. From virtual to reality: Fast adaptation of virtual object detectors to real domains. In BMVC, volume 1, pp. 3, 2014.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. 2017.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
David Vazquez, Antonio M Lopez, Javier Marin, Daniel Ponsa, and David Geronimo. Virtual and real world adaptation for pedestrian detection. IEEE transactions on pattern analysis and machine intelligence, 36(4):797­809, 2014.
Zhi-Hua Zhou and Ming Li. Tri-training: Exploiting unlabeled data using three classifiers. IEEE Transactions on knowledge and Data Engineering, 17(11):1529­1541, 2005.
Xiaojin Zhu. Semi-supervised learning literature survey. 2005.
11

Under review as a conference paper at ICLR 2018

A ARCHITECTURES

Layer Index
L - 18
L - 17
L - 16 L - 15 L - 14
L - 13 L - 12 L - 11
L - 10 L-9 L-8
L-7 L-6 L-5
L-4 L-3 L-2
L-1
L-0

Small CNN

Large CNN

32 x 32 x 3 Image

Instance Normalization (optional)

3 x 3 conv. 64 lReLU 3 x 3 conv. 64 lReLU 3 x 3 conv. 64 lReLU

3 x 3 conv. 96 lReLU 3 x 3 conv. 96 lReLU 3 x 3 conv. 96 lReLU

2 x 2 max-pool, stride 2 dropout, p = 0.5
gaussian dropout,  = 1

3 x 3 conv. 64 lReLU 3 x 3 conv. 192 lReLU 3 x 3 conv. 64 lReLU 3 x 3 conv. 192 lReLU 3 x 3 conv. 64 lReLU 3 x 3 conv. 192 lReLU

2 x 2 max-pool, stride 2 dropout, p = 0.5
gaussian dropout,  = 1

3 x 3 conv. 64 lReLU 3 x 3 conv. 192 lReLU 3 x 3 conv. 64 lReLU 3 x 3 conv. 192 lReLU 3 x 3 conv. 64 lReLU 3 x 3 conv. 192 lReLU

global average pool

10 dense, softmax

Table 4: Small and Large CNN architectures. Leaky ReLU parameter a = 0.1. All convolutional and dense layers in the classifier are pre-activation batch-normalized. All images are resized to 32x32x3. Note the use of Gaussian dropout: this addition was motivated by initial experiments in which we observed that domain adversarial training appears to contract the feature space.
Domain Discriminator
Layer L - 5 Output
100 dense, ReLU
1 dense, sigmoid

Table 5: Domain discriminator architecture.

12

Under review as a conference paper at ICLR 2018

B HYPERPARAMETERS

We observed that extensive hyperparameter-tuning is not necessary to achieve state-of-the-art performance. To demonstrate this, we restrict our hyperparameter search for each task to d = {0, 10-2}, s = {0, 1}, t = {10-2, 10-1}, in all experiments with instance-normalized inputs. We fixed  = 10-2. Note that the decision to turn (d, s) on or off that can often be determined a priori based on prior belief regarding the extent to covariate shift. In the absence of such prior belief, a reliable choice is (d = 10-2, s = 1, t = 10-2,  = 10-2).

Task

Instance-Normalized d s t



MNIST  MNIST-M SVHN  MNIST MNIST  SVHN MNIST  SVHN DIGITS  SVHN SIGNS  GTSRB CIFAR  STL STL  CIFAR

Yes, No Yes, No
Yes No Yes, No Yes, No Yes, No Yes, No

10-2 1 10-2 10-2 10-2 0 10-2 10-2 10-2 1 10-2 10-2 10-2 1 10-2 10-3 10-2 1 10-1 10-2 10-2 1 10-2 10-2
0 1 10-1 10-2 0 0 10-1 10-2

Table 6: Hyperparameters for each task, both with and without instance-normalized input. The only exception is MNIST  SVHN without instance-normalized input. In this specific case, dHH is sufficiently large that conditional entropy minimization quickly finds a degenerate solution in the target domain. To counter this, we remove conditional entropy minimization (but keep the target-side virtual adversarial training) only during VADA. We apply target-side conditional entropy minimization and virtual adversarial training during DIRT-T. To compensate, we use a lower  during the DIRT-T phase to allow for larger natural gradient steps.
When the target domain is MNIST/MNIST-M, the task is sufficiently simple that we only allocate B = 500 iterations to each optimization problem in Eq. (14). In all other cases, we set the refinement interval B = 5000. We apply Adam Optimizer (learning rate = 0.001, 1 = 0.5, 2 = 0.999) with Polyak averaging (more accurately, we apply an exponential moving average with momentum = 0.998 to the parameter trajectory). VADA was trained for 80000 iterations and DIRT-T takes VADA as initialization and was trained for up to 80000 iterations.

C REPLACING GRADIENT REVERSAL

We note from Goodfellow et al. (2014) that the gradient of f ln(1 - D(f (x))) is weaker than -f ln D(f (x)) during initial training since the latter rescales the gradient by 1/D(f (x)). Following this observation, we replace the gradient reversal procedure with alternating minimization
of

min
D

-ExDs

[ln

D(f (x))]

-

ExDt

[ln

1

-

D(f (x))]

min


-ExDt

[ln

D(f (x))]

-

ExDs

[ln

1

-

D(f (x))]

.

In some of our initial experiments, we observed the replacement of gradient reversal with alternating updates stabilizes domain adversarial training.

D INSTANCE NORMALIZATION FOR DOMAIN ADAPTATION

Theorem 1 suggests that we should identify ways of constraining the hypothesis space without hurting the global optimal classifier for the joint task. We propose to further constrain our model by introducing instance normalization as a pre-processing step for the input data. Instance normalization was proposed for style transfer Ulyanov et al. (2016) and applies the operation

(x(i))

=

x(i) - µ(x(i) (x(i))

)

,

(15)

13

Under review as a conference paper at ICLR 2018

where x(i)  RH×W ×C denotes the ith sample and µ,  : RH×W ×C  RC are functions that compute the mean and standard deviation across the spatial dimensions. A notable property of in-
stance normalization is that it is invariant to channel-wide scaling and shifting of the input elements. Formally, consider scaling and shift variables ,   RC . If  0 and (x(i)) is defined, then

(x(i)) = (x(i) + ).

(16)

For visual data the application of instance normalization to the input layer makes the classifier invariant to channel-wide shifts and scaling of the pixel intensities. For most visual tasks, sensitivity to channel-wide pixel intensity changes is not critical to the success of the classifier. As such, instance normalization of the input may help reduce dHH without hurting the globally optimal classifier. Interestingly, Figure 3 shows that input instance normalization is not equivalent to gray-scaling, since color is partially preserved. To test the effect of instance normalization, we report results both with and without the use of instance-normalized inputs.

E LIMITATION OF DOMAIN ADVERSARIAL TRAINING

Simplicity, we restrict ourselves to the case where the marginal label distributions Ys and Yt are the same, and which we shall refer to as simply Y , and that Y is a random variable uniformly distributed
over the support Y = {1, . . . , K}. Next, we construct the following probabilistic graphical model

y  Cat(1/K) z  Uniform([y, y + 1)) xs  ps(x | z) xt  pt(x | z).

(17)

We consider any source and target domains whose underlying data-generating function satisfies this generative model. Next, we relax the feature extraction function f to be a stochastic function f : X × E  Z where E (in an abuse of notation) is a multivariate Gaussian random variable--a
condition that holds in practice since noise is often injected into the classifier. And let the embedding probabilistic classifier g simply be

g(z) = p(y | z),

(18)

and let the corresponding hard classifier be

g^(z) = arg max p(y | z).
y

(19)

Proposition 1 Let (Xs, Xt) be random variables corresponding to the source and target domains from Eq. (17). If f has infinite-capacity and (Xs, Xt) have disjoint supports, then
1. there exists f  such that f (Xs, E) = f (Xt, E) and h(x) = EE [g  f (x, )] is the Bayes-optimal classifier in both source and target domains.

2. there also exists f~ such that f~(Xs, E) = f~(Xt, E) and h~(x) = EE g  f~(x, ) is the Bayes-optimal classifier in the source domain but does not generalize in the target domain.

Proof sketch. The optimal-Bayes classifier in each domain is

p(y | xs) = Ep(z|xs) [g(z)]

(20)

p(y | xt) = Ep(z|xt) [g(z)] .

(21)

Since f is infinite-capacity, let fs simulate sampling from p(z | xs) and ft sample from p(z | xt). Since Xs and Xt are disjoint, let D : X  {0, 1} classify the domain origin, where D(x) = 1  x  Xt. Then, construct the jointly optimal classifier as

f (x) = D(x) · ft(x) + (1 - D(x)) · fs(x).

(22)

Now consider the set of all volume-preserving bijective function v : [1, K + 1)  [1, K + 1). Let v~ maximize

max
v

Ep(y ,z |xt )

[|y

=

g^(v(z))|]

.

(23)

14

Under review as a conference paper at ICLR 2018

Define the classifier f~ as

f~(x) = D(x) · v~  ft(x) + (1 - D(x)) · fs(x).

(24)

f~(x) will thus generalize well in the source domain and produce domain-invariant features, but perform poorly in the target domain.

15

