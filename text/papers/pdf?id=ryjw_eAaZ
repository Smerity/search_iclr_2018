Under review as a conference paper at ICLR 2018
UNSUPERVISED DEEP STRUCTURE LEARNING BY RECURSIVE DEPENDENCY ANALYSIS
Anonymous authors Paper under double-blind review
ABSTRACT
We introduce an unsupervised structure learning algorithm for deep, feed-forward, neural networks. We propose a new interpretation for depth and inter-layer connectivity where a hierarchy of independencies in the input distribution is encoded in the network structure. This results in structures allowing neurons to connect to neurons in any deeper layer skipping intermediate layers. Moreover, neurons in deeper layers encode low-order (small condition sets) independencies and have a wide scope of the input, whereas neurons in the first layers encode higher-order (larger condition sets) independencies and have a narrower scope. Thus, the depth of the network is automatically determined--equal to the maximal order of independence in the input distribution, which is the recursion-depth of the algorithm. The proposed algorithm constructs two main graphical models: 1) a generative latent graph (a deep belief network) learned from data and 2) a deep discriminative graph constructed from the generative latent graph. We prove that conditional dependencies between the nodes in the learned generative latent graph are preserved in the class-conditional discriminative graph. Finally, a deep neural network structure is constructed based on the discriminative graph. We demonstrate on image classification benchmarks that the algorithm replaces the deepest layers (convolutional and dense layers) of common convolutional networks, achieving high classification accuracy, while constructing significantly smaller structures. The proposed structure learning algorithm requires a small computational cost and runs efficiently on a standard desktop CPU.
1 INTRODUCTION
Over the last decade, deep neural networks have proven their effectiveness in solving many challenging problems in various domains such as speech recognition (Graves & Schmidhuber, 2005), computer vision (Krizhevsky et al., 2012; Girshick et al., 2014; Simonyan & Zisserman, 2014; Szegedy et al., 2015) and machine translation (Collobert et al., 2011b). As compute resources became more available, large scale models having millions of parameters could be trained on massive volumes of data, to achieve state-of-the-art solutions for these high dimensionality problems. Building these models requires various design choices such as network topology, cost function, optimization technique, and the configuration of related hyper-parameters.
In this paper, we focus on the design of network topology--structure learning. Generally, exploration of this design space is a time consuming iterative process that requires close supervision by a human expert. Many studies provide guidelines for design choices such as network depth (Simonyan & Zisserman, 2014), layer width (Zagoruyko & Komodakis, 2016), building blocks (Szegedy et al., 2015), and connectivity (He et al., 2016; Huang et al., 2016). Based on these guidelines, these studies propose several meta-architectures, trained on huge volumes of data. These were applied to other tasks by leveraging the representational power of their convolutional layers and fine-tuning their deepest layers for the task at hand (Donahue et al., 2014; Hinton et al., 2015; Long et al., 2015; Chen et al., 2015; Liu et al., 2015). However, these meta-architecture may be unnecessarily large and require large computational power and memory for training and inference.
The problem of model structure learning has been widely researched for many years in the probabilistic graphical models domain. Specifically, Bayesian networks for density estimation and causal discovery (Pearl, 2009; Spirtes et al., 2000). Two main approaches were studied: score-based
1

Under review as a conference paper at ICLR 2018
(search-and-score) and constraint-based. Score-based approaches combine a scoring function, such as BDe (Cooper & Herskovits, 1992) and BIC (Ripley, 2007), with a strategy for searching through the space of structures, such as greedy equivalence search (Chickering, 2002). Adams et al. (2010) introduced an algorithm for sampling deep belief networks (generative model) and demonstrated its applicability to high-dimensional image datasets.
Constraint-based approaches (Pearl, 2009; Spirtes et al., 2000) find the optimal structures in the large sample limit by testing conditional independence (CI) between pairs of variables. They are generally faster than score-based approaches (Yehezkel & Lerner, 2009) and have a well-defined stopping criterion (e.g., maximal order of conditional independence). However, these methods are sensitive to errors in the independence tests, especially in the case of high-order conditional-independence tests and small training sets.
Motivated by these methods, we propose a new interpretation for depth and inter-layer connectivity in deep neural networks. We derive a structure learning algorithm such that a hierarchy of independencies in the input distribution is encoded in the network structure, where the first layers encode higher-order independencies than deeper layers. Thus, the number of layers is automatically determined. Moreover, a neuron in a layer is allowed to connect to neurons in deeper layers skipping intermediate layers. An example structure, learned for MNIST dataset, is given in Figure 1.
We describe our recursive algorithm in two steps. In Section 2 we describe a base case--a singlelayer structure learning. In Section 3 we describe multi-layer structure learning by applying the key concepts of the base case, recursively (proofs are provided in Appendix A). In Section 4 we discuss related work. We provide experimental results in Section 5, and conclude in Section 6.
Preliminaries. Consider X = {Xi}Ni=1 a set of observed (input) random variables, H = {Hj}jK=1 a set of latent variables, and Y a class variable. Our algorithm constructs three graphical models and an auxiliary graph. Each variable is represented by a single node and a single edge may connect two distinct nodes. Graph G is a generative DAG defined over the observed and latent variables X  H. Graph GInv is called a stochastic inverse of G. Graph GD is a discriminative model defined over the observed, latent, and class variables X  H  Y . An auxiliary graph GX is defined over X (a CPDAG; an equivalence class of a Bayesian network) and is generated and maintained as an internal state of the algorithm. The parents set of a node X in G is denoted P a(X; G). The order of an independence relation is defined to be the condition set size. For example, if X1 and X2 are independent given X3 and X4, denoted X1  X2|{X3, X4}, then the independence order is two.
input layer

copy conc atenate gather layer dense layer

output layer

Figure 1: An example of a structure learned by our algorithm (classifying MNIST digits). Neurons in a layer may connect to neurons in any deeper layer. Depth is determined automatically. Each gather layer selects a subset of the input, where each input variable is gathered only once. A neural route, starting with a gather layer, passes through densely connected layers where it may split (copy) and merge (concatenate) with other routes in correspondence with the hierarchy of independencies identified by the algorithm. All routes merge into the final output layer (e.g., a softmax layer).

2

Under review as a conference paper at ICLR 2018

2 SINGLE LAYER STRUCTURE LEARNING

We start by describing the key concepts of our approach using a simple scenario: learning the connectivity of a single-layer neural network.

2.1 CONSTRUCTING A GENERATIVE GRAPH

Assume the input joint distribution p(X) complies with the following property.
Assumption 1. The joint distribution p(X) is faithful to a DAG G over observed X and latent nodes H, where for all X  X and H  H, P a(X; G)  H and P a(H; G)  H\H.

p X;G =

p X, H; G dH =

N
p H p Xi P a(Xi; G) dH.
i=1

(1)

Note that the generative graphical model G can be described as a layered deep belief network where parents of a node in layer m can be in any deeper layer, indexes greater than m, and not restricted to the next layer m + 1. This differs from the common definition of deep belief networks (Hinton et al., 2006; Adams et al., 2010) where the parents are restricted to layer m + 1.
It is desired to learn an efficient graph G having small sets of parents and a simple factorization of p(H) while maintaining high expressive power. We first construct an auxiliary graph, a CPDAG (Spirtes et al., 2000), GX over X (an equivalence class of a fully visible Bayesian network) encoding only marginal independencies1 (empty condition sets) and then construct G such that it can mimic GX over X, denoted GX G (Pearl, 2009). That is, preserving all conditional dependencies of X in GX .
The simplest connected DAG that encodes statistical independence is the v-structure, a structure with three nodes X1  X3  X2 in which X1 and X2 are marginally independent X1  X2 and conditionally dependent X1X2|X3. In graphs encoding only marginal independencies, dependent nodes form a clique. We follow the procedure described by Yehezkel & Lerner (2009) and decompose X into autonomous sets (complying with the Markov property) where one set, denoted XD (descendants), is the common child of all other sets, denoted XA1, . . . , XAK (ancestor sets). We select XD to be the set of nodes that have the lowest topological order in GX . Then, by removing XD from GX (temporarily for this step), the resulting K disjoint sets of nodes (corresponding to K disjoint substructures) form the K ancestor sets {XAi}iK=1. See an example in Figure 2.
Next, G is initialized to an empty graph over X. Then, for each ancestor set XAi a latent variable Hi is introduced and assigned to be a common parent of the pair (XAi, XD). Thus,

p X;G =


K



p Hi

p X Hi 

p X H dH.

i=1

X X A i

X XD

(2)

Note that the parents of two ancestor sets are distinct, whereas the parents set of the descendant set is composed of all the latent variables.
In the auxiliary graph GX , for each of the resulting v-structures (XAi  XD  XAj), a link between a parent and a child can be replaced by a common latent parent without introducing new independencies. For example, in Figure 2-[b], XA1 = {A}, XA2 = {B}, and XD = {C, D, E}. Adding a common latent parent (Figure 3-[a]) HA (or HB) and removing all the edges from XA1 (or XA2) to XD preserves the conditional dependence A  B|{C, D, E}.
Algorithm 1 summarizes the procedure of constructing G having a single latent layer. Note that we do not claim to identify the presence of confounders and their inter-relations as in Elidan et al. (2001); Silva et al. (2006); Asbeh & Lerner (2016). Instead, we augment a fully observed Bayesian network with latent variables, while preserving conditional dependence.

1In Section 3, conditional independencies are considered, the construction of GX and G is interleaved, and the ability of G to mimic GX over X is described recursively.

3

Under review as a conference paper at ICLR 2018

AB

AB

C DC D

E
[a]

E
[b]

Figure 2: [a] An example of a Bayesian network encoding ground-truth conditional independencies
(a DAG underlying observed data) and [b] a corresponding CPDAG (GX ) constructed by testing only marginal independencies. Only A and B are marginally independent (d-separated in [a], A  B),
where C and D are marginally dependent (C  D) and therefore connected in [b]. Thus, nodes
{C, D, E}, having the lowest topological order, form a descendant set XD = {C, D, E} and nodes A and B form two distinct ancestor sets, XA1 = {A}, XA2 = {B}--disjoint if {C, D, E} is removed from the graph.

HA HB

HA HB

HA HB

Y

HA HB

A
[a]

C ED

BA
[b]

C ED

BA
[c]

C ED

BA
[d]

C ED

B

Figure 3: [a] An example of a graph G (corresponding to GX in Figure 2-[b]). [b] A stochastic inverse generated by the algorithm presented by Stuhlmu¨ller et al. (2013). [c] A stochastic inverse
generated by our method where the graph is a projection of a latent structure. A dependency induced by a latent Q is described using a bi-directional edge HA  HB. [d] A discriminative structure GD having a class node Y that provides an explaining away relation for HA  HB. That is, the latent Q is replaced by an observed common child Y .

2.2 CONSTRUCTING A STOCHASTIC INVERSE
It is important to note that G represents a generative distribution of X and is constructed in an unsupervised manner (class variable Y is ignored). Hence, we construct GInv, a graphical model that preserves all conditional dependencies in G but has a different node ordering in which the observed variables, X, have the highest topological order (parentless)--a stochastic inverse of G.
Stuhlmu¨ller et al. (2013); Paige & Wood (2016) presented a heuristic algorithm for constructing such stochastic inverses where the structure is a DAG (an example is given in Figure 3-[b]). However, these DAGs, though preserving all conditional dependencies, may omit many independencies and add new edges between layers. We avoid limiting GInv to a DAG and instead limit it to be a projection of another latent structure (Pearl, 2009). That is, we assume the presence of additional hidden variables Q that are not in GInv but induce dependency2 among H. For clarity, we omit these variables from the graph and use bi-directional edges to represent the dependency induced by them. An example is given in Figure 3[c] where a bi-directional edge represents the effect of some variable Q  Q on HA and HB. We construct GInv in two steps:
1. Invert all G edges (invert inter-layer connectivity).
2. Connect each pair of latent variables, sharing a common child in G, with a bi-directional edge.
This simple procedure ensures G GInv over X  H while maintaining the exact same number of edges between the layers (Proposition 1, Appendix A).
2For example, "interactive forks" (Pearl, 2009).
4

Under review as a conference paper at ICLR 2018

Algorithm 1: Marginal Connectivity Learning

Input: X: observed nodes, and Indep: an oracle for testing statistical independence. Output: G, a latent structure over X and H

1 initialize GX - a complete graph over X

2 begin 3 foreach pair of connected nodes Xi, Xj in GX if Indep(Xi, Xj) 4 do
5 disconnect Xi and Xj 6 direct edges Xi  Xc  Xj for every common neighbor Xc

find independencies

7 XD - nodes having the lowest topological order 8 {XAi}iK=1 - disjoint sets, after removing XD from GX
9 G - an empty graph over X 10 add K latent variables H = {Hi}iK=1 to G 11 set each Hi to be a parent of {XA1  XD}
12 return G

identify autonomous sets
create a latent layer connect

2.3 CONSTRUCTING A DISCRIMINATIVE GRAPH
Recall that G encodes the generative distribution of X and GInv is the stochastic inverse. We further construct a discriminative graph GD by replacing bi-directional dependency relations in GInv, induced by Q, with explaining-away relations by adding the observed class variable Y . Node Y is set in GD to be the common child of the leaves in GInv (latents introduced after testing marginal independencies) (see an example in Figure 3-[d]). This preserves the conditional dependency relations of GInv. That is, GD can mimic GInv over X and H given Y (Proposition 2, Appendix A). It is interesting to note that the generative and discriminative graphs share the exact same inter-layer connectivity (inverted edge-directions). Moreover, introducing node Y provides an "explaining away" relation between latents, uniquely for the classification task at hand.

2.4 CONSTRUCTING A FEED-FORWARD NEURAL NETWORK

We construct a neural network based on the connectivity in GD. Sigmoid belief networks (Neal, 1992) have been shown to be powerful neural network density estimators (Larochelle & Murray,
2011; Germain et al., 2015). In these networks, conditional probabilities are defined as logistic regressors. Similarly, for GD we may define for each latent variable H  H,

p(H = 1|X ) = sigm W X + b

(3)

where sigm(x) = 1/(1 + exp(-x)), X = P a(H ; GD), and (W , b ) are the parameters of the neural network. Nair & Hinton (2010) proposed replacing each binary stochastic node H by an infinite number of copies having the same weights but with decreasing bias offsets by one. They showed that this infinite set can be approximated by

N
sigm(v - i + 0.5)  log(1 + ev),
i=1

(4)

where v = W X + b . They further approximate this function by max(0, v + ) where is a zerocentered Gaussian noise. Following these approximations, they provide an approximate probabilistic interpretation for the ReLU function, max(0, v). As demonstrated by Jarrett et al. (2009) and Nair & Hinton (2010), these units are able to learn better features for object classification in images.
In order to further increase the representational power, we represent each H by a set of neurons having ReLU activation functions. That is, each latent variable H in GD is represented in the neural network by a dense (fully-connected) layer. Finally, the class node Y is represented by a softmax layer.

5

Under review as a conference paper at ICLR 2018

3 RECURSIVE MULTI-LAYER STRUCTURE LEARNING
We now extend the method of learning the connectivity of a single layer into a method of learning multi-layered structures. The key idea is to recursively introduce a new and deeper latent layer by testing n-th order conditional independence (n is the condition set size) and connect it to latent layers created by previous recursive calls that tested conditional independence of order n + 1. The method is described in Algorithm 2. It is important to note that conditional independence is tested only between input variables X and condition sets do not include latent variables. Conditioning on latent variables or testing independence between them is not required as the algorithm adds these latent variables in a specific manner, preserving conditional dependencies between the input variables.

Algorithm 2: Recursive Latent Structure Learning (multi-layer)
1 RecurLatStruct (GX , X, Xex, n) Input: an initial DAG GX over observed X & exogenous nodes Xex and a desired resolution n. Output: G, a latent structure over X and H

2 if the maximal indegree of GX (X) is below n + 1 then 3 G -an observed layer X 4 return G

exit condition

5 GX -IncreaseResolution(GX , n)

n-th order independencies

6 {XD, XA1, . . . , XAK } -SplitAutonomous(X, GX )

identify autonomies

7 for i  {1 . . . K} do 8 GAi - RecurLatStruct(GX , XAi, Xex, n + 1)
9 GD - RecurLatStruct(GX , XD, Xex  {XAi}Ki=1, n + 1)

a recursive call a recursive call

10 G - Group(GD, GA1, . . . , GAK )

merge results

11 create latent variables H(n) = {H1(n), . . . , HK(n)} in G

create a latent layer

12

set

each

Hi(n)

to

be

a

parent

of

{H

(n+1) Ai



H

(n+1) D

}

connect

13

where

H

A

(n+1) i

and

H

(n+1) D

are

the

sets

of

parentless

latents

in

GA i

and

GD,

respectively.

14 return G

The algorithm maintains and recursively updates an auxiliary graph GX (a CPDAG) over X and utilizes it to construct G. Yehezkel & Lerner (2009) introduced an efficient algorithm (RAI) for constructing a CPDAG over X by a recursive application of conditional independence tests with increasing condition set sizes (n). Our algorithm is based on this framework for updating the auxiliary graph GX (Algorithm 2, lines 5 and 6).
The algorithm starts with n = 0, GX a complete graph, and a set of exogenous nodes Xex = . The set Xex is exogenous to GX and consists of parents of X.
The function IncreaseResolution (Algorithm 2-line 5) disconnects (in GX ) conditionally independent variables in two steps. First, it tests dependency between Xex and X, i.e., X  X |S for every connected pair X  X and X  Xex given a condition set S  {Xex  X} of size n. Next, it tests dependency within X, i.e., Xi  Xj|S for every connected pair Xi, Xj  X given a condition set S  {Xex  X} of size n. After removing the corresponding edges, the remaining edges are directed by applying two rules (Pearl, 2009; Spirtes et al., 2000). First, v-structures are identified and directed. Then, edges are continually directed, by avoiding the creation of new v-structures and directed cycles, until no more edges can be directed. Following the terminology of Yehezkel & Lerner (2009), we say that this function increases the graph d-separation resolution from n - 1 to n.
The function SplitAutonomous (Algorithm 2-line 6) identifies autonomous sets in a graph in two steps, as described in Algorithm 1 lines 7 and 8. An autonomous set in GX includes all its nodes' parents (complying with the Markov property) and therefore a corresponding latent structure can be constructed independently using a recursive call. Thus, the algorithm is recursively and independently called for the ancestor sets (Algorithm 2 lines 7­8), and then called for the descendant set while treating the ancestor sets as exogenous (Algorithm 2 line 9).
6

Under review as a conference paper at ICLR 2018

AB

AB

HA HB

HC HD

HC HD

C DC D

E
[a]

E
[b]

A CED B

A CED B

[c] [d]

Figure 4: An example trace of Algorithm 2 where p(X) is faithful to the DAG in Figure 3-[a].
[a] a CPDAG encoding only marginal independencies (n = 0) and the identified autonomous substructures (line 6 in the algorithm). [b] A CPDAG GX over {C, D, E} encoding conditional independencies up to second order n = 2 (nodes A and B are exogenous). [c] Graph G is created for the autonomous set {C, D, E} by introducing latents {HC, HD}. At n = 2 nodes C and D are identified as autonomous ancestors and E as an autonomous descendant. [d] Graph G is created at n = 0 for all nodes {A, B, HC , HD} by introducing {HA, HB}, where {HC , HD} represent the autonomous subset {C, D, E}.

HA HB

HA HB

Y

HC HD

HC HD

HA HC

HB HD

A
[a]

CED B

A

[b]

CED B

A

[c]

CED B

Figure 5: An example of [a] a structure G (the corresponding auxiliary graph GX is depicted in Figure 2-[a]), [b] its stochastic inverse GInv described as a projection of another latent structure, and [c] a corresponding discriminative model obtained by expressing dependency relations among
latents (bi-directional edges) with an "explaining away" relation induced by a class node.

Each recursive call returns a latent structure for each autonomous set. Recall that each latent structure encodes a generative distribution over the observed variables where layer H(n+1), the last added layer (parentless nodes), is a representation of the input X  X. By considering only layer H(n+1) of each latent structure, we have the same simple scenario discussed in Section 2--learning the connectivity between H(n), a new latent layer, and H(n+1), treated as an "input" layer. Thus, latent variables are introduced as parents of the H(n+1) layers, as described in Algorithm 2 lines 11­13. A simplified example is given in Figure 4.
Next, a stochastic inverse GInv is constructed as described in Section 2--all the edge directions are inverted and bi-directional edges are added between every pair of latents sharing a common child in G. An example graph G and a corresponding stochastic inverse GInv are given in Figure 5. A discriminative structure GD is then constructed by removing all the bi-directional edges and adding the class node Y as a common child of layer H(0), the last latent layer that is added (Figure 5-[c]). Finally, a neural network is constructed based on the connectivity of GD. That is, each latent node, H  H(n), is replaced by a set of neurons, and each edge between two latents, H  H(n) and H  H(n+1), is replaced by a bipartite graph connecting the neurons corresponding to H and H .
4 RELATED WORK
Recent studies have focused on automating the exploration of the design space, posing it as a hyperparameter optimization problem and proposing various approaches to solve it. (Miconi, 2016) learns the topology of an RNN network introducing structural parameters into the model and optimize them along with the model weights by the common gradient descent methods. Smith et al. (2016) takes a similar approach incorporating the structure learning into the parameter learning scheme, gradually growing the network up to a maximum size.
A common approach is to define the design space in a way that enables a feasible exploration process and design an effective method for exploring it. Zoph & Le (2016) (NAS) first define a set of hyper-parameters characterizing a layer (number of filters, kernel size, stride). Then they use a
7

Under review as a conference paper at ICLR 2018
controller-RNN for finding the optimal sequence of layer configurations for a "trainee network". This is done using policy gradients (REINFORCE) for optimizing the objective function that is based on the accuracy achieved by the "trainee" on a validation set. Although this work demonstrates capabilities to solve large-scale problems (Imagenet), it comes with huge computational cost. In a following work, Zoph et al. (2017) address the same problem but apply a hierarchical approach. They use NAS to design network modules on a small-scale dataset (CIFAR-10) and transfer this knowledge to a large-scale problem by learning the optimal topology composed of these modules. Baker et al. (2016) use reinforcement learning as well and apply Q-learning with epsilon-greedy exploration strategy and experience replay. Negrinho & Gordon (2017) propose a language that allows a human expert to compactly represent a complex search-space over architectures and hyperparameters as a tree and then use methods such as MCTS or SMBO to traverse this tree. Smithson et al. (2016) present a multi objective design space exploration, taking into account not only the classification accuracy but also the computational cost. In order to reduce the cost involved in evaluating the network's accuracy, they train a Response Surface Model that predicts the accuracy at much lower cost, reducing the number of candidates that go through actual validation accuracy evaluation. Another common approach for architecture search is based on evolutionary strategies to define and search the design space. (Real et al., 2017; Miikkulainen et al., 2017) use evolutionary algorithm to evolve an initial model or blueprint based on its validation performance.
Common to all these recent studies is the fact that structure learning is done in a supervised manner, eventually learning a discriminative model. Moreoever, these approaches require huge compute resources, rendering the solution unfeasible for most applications given limited compute and time resources.
5 EXPERIMENTS
We evaluate the quality of the learned structure in two experiments:
· Classification accuracy as a function of network depth and size for a structure learned directly from MNIST pixels.
· Classification accuracy as a function of network size on a range of benchmarks and compared to common topologies.
All the experiments were repeated five times where average and standard deviation of the classification accuracy were recorded. In all of our experiments, we used a ReLU function for activation, ADAM (Kingma & Ba, 2015) for optimization, and applied batch normalization (Ioffe & Szegedy, 2015) followed by dropout (Srivastava et al., 2014) to all the dense layers. All optimization hyperparameters that were tuned for the vanilla topologies were also used, without additional tuning, for the learned structures. For the learned structures, all layers were allocated an equal number of neurons. Threshold for independence tests, and the number of neurons-per-layer were selected by using a validation set. Only test-set accuracy is reported.
Our structure learning algorithm was implemented using the Bayesian network toolbox (Murphy, 2001) and Matlab. We used Torch7 (Collobert et al., 2011a) and Keras (Chollet, 2015) with the TensorFlow (Abadi et al., 2015) back-end for optimizing the parameters of both the vanilla and learned structures.
5.1 NETWORK DEPTH, NUMBER OF PARAMETERS, AND ACCURACY
We analyze the accuracy of structures learned by our algorithm as a function of the number of layers and parameters. Although network depth is automatically determined by the algorithm, it is implicitly controlled by the threshold used to test conditional independence (partial-correlation test in our experiments). For example, a high threshold may cause detection of many independencies leading to early termination of the algorithm and a shallow network (a low threshold has the opposite effect). Thus, four different networks having 2, 3, 4, and 5 layers, using four different thresholds, are learned for MNIST. We also select three configurations of neurons-per-layer: a baseline (normalized to 100%), and two configurations in which the number of neurons-per-layer is 50%, and 37.5% of the baseline network (equal number of neurons are allocated for each layer).
8

Under review as a conference paper at ICLR 2018

Classification accuracies are summarized in Table 1. When the number of neurons-per-layers is large enough (100%) a 3-layer network achieves the highest classification accuracy of 99.07% (standard deviation is 0.01) where a 2-layer dense network has only a slight degradation in accuracy, 99.04%. For comparison, networks with 2 and 3 fully connected layers (structure is not learned) with similar number of parameters achieve 98.4% and 98.75%, respectively. This demonstrates the efficiency of our algorithm when learning a structure having a small number of layers. In addition, for a smaller neuron allocation (50%), deeper structures learned by our algorithm have higher accuracy than shallower ones. However, a decrease in the neurons-per-layer allocation has a greater impact on accuracy for deeper structures.

neurons per layer 2 layers 3 layers 4 layers 5 layers

100% 50% 37.5%

99.04 98.96 98.96

99.07 98.98 98.94

99.07 99.02 98.93

99.07 99.02 98.93

Table 1: Classification accuracy [%] of structures learned from MNIST images as a function of network depth and number of neurons-per-layer (normalized). For comparison, when a structure is not learned, networks with 2 and 3 dense layers, achieve 98.4% and 98.75% accuracy, respectively (having the same size as learned structures at 100% neurons-per-layer).

5.2 LEARNING THE STRUCTURE OF THE DEEPEST LAYERS IN COMMON TOPOLOGIES
We evaluate the quality of learned structures using five image classification benchmarks. We compare the learned structures to common topologies (and simpler hand-crafted structures), which we call "vanilla topologies", with respect to network size and classification accuracy. The benchmarks and vanilla topologies are described in Table 2. In preliminary experiments we found that, for SVHN and ImageNet, a small subset of the training data is sufficient for learning the structure (larger training set did not improve classification accuracy). As a result, for SVHN only the basic training data is used (without the extra data), i.e., 13% of the available training data, and for ImageNet 5% of the training data is used. Parameters were optimized using all of the training data.

benchmark

vanilla topology

dataset

topology

description

size

MNIST (LeCun et al., 1998)

None

learn a structure directly from pixels

MNIST-Man 32-64-FC:128

127K

SVHN (Netzer et al., 2011)

Maxout NiN (Chang & Chen, 2015) SVHN-Man 16-16-32-32-64-FC:256

1.6M 105K

CIFAR 10 (Krizhevsky & Hinton, 2009)

VGG-16-D WRN-40-4

(Simonyan & Zisserman, 2014) (Zagoruyko & Komodakis, 2016)

15M 9M

CIFAR 100 (Krizhevsky & Hinton, 2009) VGG-16-D (Simonyan & Zisserman, 2014)

15M

ImageNet (Deng et al., 2009)

AlexNet

(Krizhevsky et al., 2012)

61M

Table 2: Benchmarks and vanilla topologies. MNIST-Man and SVHN-Man topologies were manually created by us. MNIST-Man has two convolutional layer (32 and 64 filters each) and one dense layer with 128 neurons. SVHN-Man was created as a small network reference having reasonable accuracy compared to Maxout-NiN. In the first row we indicate that in one experiment a structure for MNIST was learned from the pixels and feature extracting convolutional layers were not used.
Convolutional layers are powerful feature extractors for images exploiting domain knowledge, such as spatial smoothness, translational invariance, and symmetry. We therefore evaluate our algorithm by using the first convolutional layers of the vanilla topologies as "feature extractors" (mostly below 50% of the vanilla network size) and learning a deep structure from their output. That is, the deepest

9

Under review as a conference paper at ICLR 2018

layers of the vanilla network (mostly over 50% of the network size; 64% on average) is removed and replaced by a structure learned by our algorithm in an unsupervised manner. Finally, a softmax layer is added and the entire network parameters are optimized.
First, we demonstrate the effect of replacing a different amount of the deepest layers and the ability of the learned structure to replace feature extraction layers. Table 3 describes classification accuracy achieved by replacing a different amount of the deepest layers in VGG-16-D. For example, column "conv.10" represents learning a structure using the activations of conv.10 layer. Accuracy and the normalized number of network parameters are reported for the overall network, e.g., up to conv.10 + the learned structure. Column "vanilla" is the accuracy achieved by the VGG-16-D network, after training under the exact same setting (a setting we found to maximize a validation-set accuracy for the vanilla topologies).

learned

vanilla

conv.5 conv.7 conv.10 classifier ­

CIFAR 10

accuracy

90.6 92.61 92.94

# parameters 0.10 0.15 0.52

92.79 0.98

92.32 1.00

CIFAR 100 accuracy 63.17 68.91 70.68 # parameters 0.10 0.13 0.52

69.14 0.98

68.86 1.00

Table 3: Classification accuracy (%) and overall network size (normalized number of parameters). VGG-16-D is the "vanilla" topology. For both, CIFAR 10/100 benchmarks, the learned structure achieves the highest accuracy by replacing all the layers that are deeper than layer conv.10. Moreover, accuracy is maintained when replacing the layers deeper than layer conv.7.

One interesting phenomenon to note is that the highest accuracy is achieved at conv. 10 rather than at the "classifier" (the last dense layer). This might imply that although convolutional layers are useful at extracting features directly from images, they might be redundant for deeper layers. By using our structure learning algorithm to learn the deeper layers, accuracy of the overall structure increases with the benefit of having a compact network. An accuracy, similar to that of "vanilla" VGG-16-D, is achieved with a structure having 85% less total parameters (conv. 7) than the vanilla network, where the learned structure is over 50X smaller than the replaced part.
Next, we evaluate the accuracy of the learned structure as a function of the number of parameters and compare it to a densely connected network (fully connected layers) having the same depth and size. For SVHN, we used the Batch Normalized Maxout Network in Network topology (Chang & Chen, 2015) and removed the deepest layers starting from the output of the second NiN block (MMLP-2-2). For CIFAR-10, we used the VGG-16-D and removed the deepest layers starting from the output of conv.10 layer. For MNIST, a structure was learned directly from pixels. Results are depicted in Figure 6. It is evident that accuracy of the learned structures is significantly higher (error bars represent 2 standard deviations) than a set of fully connected layers, especially in cases where the network is limited to a small number of parameters.

MNIST accuracy SVHN accuracy CIFAR-10 accuracy

99 98.5
98 97.5
97 96.5
96
[a]

fully connected learned structure

0.5 1 1.5 number of parameters

2 105

95 94 93 92 91 90
[b]

fully connected learned structure

1.09 1.1 1.11 1.12 1.13

number of parameters

106

90 85 80 75 70
7.66
[c]

fully connected learned structure

7.67 7.68 7.69

number of parameters

106

Figure 6: Accuracy as a function of network size. [a] MNIST, [b] SVHN. [c] CIFAR-10. Error bars represent 2 standard deviations.

10

Under review as a conference paper at ICLR 2018

Finally, in Table 4 we provide the highest classification accuracies, achieved without limiting the size of the learned structure (selected using a validation set). In the first row, a structure is learned directly from images; therefore, it does not have a "vanilla" topology as reference (a network with 3 fully-connected layers having similar size achieves 98.75% accuracy). In all the cases, the size of the learned structure is significantly smaller than the vanilla topology, and generally has an increase in accuracy.
Our structure learning algorithm runs efficiently on a standard desktop CPU, while providing structures with competitive classification accuracies. For example, the lowest classification error rate achieved by our unsupervised algorithm for CIFAR 10 is 4.58% with a network of size 6M (WRN40-4 row in Table 4). For comparison, the NAS algorithm (Zoph & Le, 2016) achieves error rates of 5.5% and 4.47% for networks of sizes 4.2M and 7.1M, respectively, and requires optimizing thousands of networks using hundreds of GPUs.

vanilla topology

learned structure

dataset

topology

size accuracy accuracy t-size replaced-size replaced/total

MNIST

None MNIST-Man 104K/127K

99.35

99.07 99.45 0.38 0.24 (4.2X)

SVHN

Maxout NiN 527K/1.6M SVHN-Man 88K/105K

98.10 97.10

97.70 0.70 0.10 (10X) 96.24 0.43 0.29 (3.4X)

CIFAR 10 VGG-16-D WRN-40-4

7.4M/15M 4.7M/9M

92.32 95.09

92.94 95.42

0.52 0.0179 (55X) 0.66 0.37 (2.7X)

CIFAR 100 VGG-16-D 7.4M/15M 68.86 70.68 0.52 0.0176 (57X)

ImageNet AlexNet

59M/61M 57.20 57.20 0.08 0.0438 (23X)

Table 4: A summary of the highest classification accuracy achieved by replacing the deepest layers of common topologies (vanilla) with a learned structure. For the vanilla network, "size" corresponds to the number of trainable parameters in the deepest layers-to-be-replaced/entire-network. For the learned structures, "t-size" is the total network size (vanilla - replaced layers size + learned structure) normalized by the vanilla network size; "replaced-size" is the size of the learned structure normalized by the size of the vanilla network section it replaced (size reduction ratio). The first row corresponds to learning a structure directly from the MNIST images.

6 CONCLUSIONS
We presented a principled approach for learning the structure of deep neural networks. Our proposed algorithm learns in an unsupervised manner and requires small computational cost. The resulting structures encode a hierarchy of independencies in the input distribution, where a node in one layer may connect another node in any deeper layer, and depth is determined automatically.
We demonstrated that our algorithm learns small structures, and provides high classification accuracies for common image classification benchmarks. It is also demonstrated that while convolution layers are very useful at exploiting domain knowledge, such as spatial smoothness, translational invariance, and symmetry, they are mostly outperformed by a learned structure for the deeper layers. Moreover, while the use of common topologies (meta-architectures), for a variety of classification tasks is computationally inefficient, we would expect our approach to learn smaller and more accurate networks for each classification task, uniquely.
As only unlabeled data is required for learning the structure, we expect our approach to be practical for many domains, beyond image classification, such as knowledge discovery, and plan to explore the interpretability of the learned structures.
11

Under review as a conference paper at ICLR 2018
REFERENCES
Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane´, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vie´gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.
Ryan Adams, Hanna Wallach, and Zoubin Ghahramani. Learning the structure of deep sparse graphical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 1­8, 2010.
Nuaman Asbeh and Boaz Lerner. Learning latent variable models by pairwise cluster comparison part ii- algorithm and evaluation. Journal of Machine Learning Research, 17(224):1­45, 2016.
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architectures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.
Jia-Ren Chang and Yong-Sheng Chen. Batch-normalized maxout network in network. arXiv preprint arXiv:1511.02583, 2015.
Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015.
David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 3(Nov):507­554, 2002.
Franois Chollet. keras. https://github.com/fchollet/keras, 2015.
R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, 2011a.
Ronan Collobert, Jason Weston, Le´on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493­2537, 2011b.
Gregory F Cooper and Edward Herskovits. A bayesian method for the induction of probabilistic networks from data. Machine learning, 9(4):309­347, 1992.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. IEEE, 2009.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, volume 32, pp. 647­655, 2014.
Gal Elidan, Noam Lotner, Nir Friedman, and Daphne Koller. Discovering hidden variables: A structure-based approach. In Advances in Neural Information Processing Systems, pp. 479­485, 2001.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for distribution estimation. In ICML, pp. 881­889, 2015.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 580­587, 2014.
Alex Graves and Ju¨rgen Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5):602­610, 2005.
12

Under review as a conference paper at ICLR 2018
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770­778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527­1554, 2006.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pp. 2146­2153. IEEE, 2009.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In AISTATS, volume 1, pp. 2, 2011.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 806­814, 2015.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pp. 97­105, 2015.
Thomas Miconi. Neural networks with differentiable structure. arXiv preprint arXiv:1606.06216, 2016.
Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Evolving deep neural networks. arXiv preprint arXiv:1703.00548, 2017.
K. Murphy. The Bayes net toolbox for Matlab. Computing Science and Statistics, 33:331­350, 2001.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807­814, 2010.
Radford M Neal. Connectionist learning of belief networks. Artificial intelligence, 56(1):71­113, 1992.
Renato Negrinho and Geoff Gordon. Deeparchitect: Automatically designing and training deep architectures. arXiv preprint arXiv:1704.08792, 2017.
13

Under review as a conference paper at ICLR 2018
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 5, 2011.
Brooks Paige and Frank Wood. Inference networks for sequential Monte Carlo in graphical models. In Proceedings of the 33rd International Conference on Machine Learning, volume 48 of JMLR, 2016.
Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge university press, second edition, 2009.
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc Le, and Alex Kurakin. Large-scale evolution of image classifiers. arXiv preprint arXiv:1703.01041, 2017.
Brian D Ripley. Pattern recognition and neural networks. Cambridge university press, 2007. Ricardo Silva, Richard Scheine, Clark Glymour, and Peter Spirtes. Learning the structure of linear
latent variable models. Journal of Machine Learning Research, 7(Feb):191­246, 2006. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014. Leslie N Smith, Emily M Hand, and Timothy Doster. Gradual dropin of layers to train very deep
neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4763­4771, 2016. Sean C Smithson, Guang Yang, Warren J Gross, and Brett H Meyer. Neural networks designing neural networks: Multi-objective hyper-parameter optimization. In Computer-Aided Design (ICCAD), 2016 IEEE/ACM International Conference on, pp. 1­8. IEEE, 2016. P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction and Search. MIT Press, 2nd edition, 2000. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929­1958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html. Andreas Stuhlmu¨ller, Jacob Taylor, and Noah Goodman. Learning stochastic inverses. In Advances in neural information processing systems, pp. 3048­3056, 2013. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1­9, 2015. Raanan Yehezkel and Boaz Lerner. Bayesian network structure learning by recursive autonomy identification. Journal of Machine Learning Research, 10(Jul):1527­1570, 2009. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. arXiv preprint arXiv:1707.07012, 2017.
14

Under review as a conference paper at ICLR 2018
APPENDIX
A PRESERVATION OF CONDITIONAL DEPENDENCE
Conditional dependence relations encoded by the genrative structure G are preserved by the discriminative structure GD conditioned on the class Y . That is, GD conditioned on Y can mimic G; denoted by preference relation G GD|Y . While the parameters of a model can learn to mimic conditional independence relations that are not expressed by the graph structure, they are not able to learn conditional dependence relations (Pearl, 2009). Proposition 1. Graph GInv preserves all conditional dependencies in G (i.e., G GInv).
Proof. Graph GInv can be constructed using the procedures described by Stuhlmu¨ller et al. (2013) where nodes are added, one-by-one, to GInv in a reverse topological order (lowest first) and connected (as a child) to existing nodes in GInv that d-separate it, according to G, from the remainder of GInv. Paige & Wood (2016) showed that this method ensures the preservation of conditional dependence G GInv. We set an equal topological order to every pair of latents (Hi, Hj) sharing a common child in G. Hence, jointly adding nodes Hi and Hj to GInv, connected by a bi-directional edge, requires connecting them (as children) only to their children and the parents of their children (Hi and Hj themselves, by definition) in G. That is, without loss of generality, node Hi is d-separated from the remainder of GInv given its children in G and Hj.
It is interesting to note that the stochastic inverse GInv, constructed without adding inter-layer connections, preserves all conditional dependencies in G. Proposition 2. Graph GD, conditioned on Y , preserves all conditional dependencies in GInv (i.e., GInv GD|Y ).
Proof. It is only required to prove that the dependency relations that are represented by bi-directional edges in GInv are preserved in GD. The proof follows directly from the d-separation criterion (Pearl, 2009). A latent pair {H, H }  H(n+1), connected by a bi-directional edge in GInv, cannot be d-separated by any set containing Y , as Y is a descendant of a common child of H and H . In Algorithm 2-line 12, a latent in H(n) is connected, as a child, to latents H(n+1), and Y to H(0).
We formulate GInv as a projection of another latent model (Pearl, 2009) where bi-directional edges represent dependency relations induced by latent variables Q. We construct a discriminative model by considering the effect of Q as an explaining-away relation induced by a class node Y . Thus, conditioned on Y , the discriminative graph GD preserves all conditional (and marginal) dependencies in GInv. Proposition 3. Graph GD, conditioned on Y , preserves all conditional dependencies in G (i.e., G GD).
Proof. It immediately follows from Propositions 1 & 2 that G GInv GD conditioned on Y .
Thus G GInv GD conditioned on Y .
15

