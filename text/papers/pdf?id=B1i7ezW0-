Under review as a conference paper at ICLR 2018
SEMI-SUPERVISED LEARNING VIA NEW DEEP NETWORK INVERSION
Anonymous authors Paper under double-blind review
ABSTRACT
We exploit a recently derived inversion scheme for arbitrary deep neural networks to develop a new semi-supervised learning framework that applies to a wide range of systems and problems. The approach outperforms current state-of-the-art methods on MNIST for small label set. Experiments with one-dimensional signals highlight the generality of the method. Importantly, our approach is simple, efficient, and requires no change in the deep network architecture.
1 INTRODUCTION
Deep neural networks (DNNs) have made great strides recently in a wide range of difficult machine perception tasks. However, the lionshare of systems are still trained in a fully supervised fashion using a large set of labeled data, which is tedious and costly to acquire.
Semi-supervised learning relaxes this requirement by leaning based on two datasets: a fully labeled set D of N training data pairs and an unlabeled set Du of Nu training inputs. Unlabeled training data is useful for learning, because the unlabeled inputs provide information on the statistical distribution of the data that can both guide the learning required to classify the supervised dataset and characterize the unlabeled samples in Du. Limited progress has been made on semi-supervised learning algorithms for DNNs Rasmus et al. (2015); Salimans et al. (2016); Patel et al. (2015); Nguyen et al. (2016), but today's methods suffer from a range of drawbacks, including training instability, lack of topology generalization, and computational complexity.
In this paper, we take two steps forward in semi-supervised learning for DNNs. First, we introduce an universal methodology to equip any deep neural net with an inverse that enables input reconstruction. Second, we introduce a new semi-supervised learning approach whose loss function features an additional term based on net's inverse that guides the weight updates such that information contained in unlabeled data are incorporated into the learning process. Our key insight is that the defined and general inverse function can be easily derived and computed; thus for unlabeled data points we can both compute and minimize the error between the input signal and the estimate provided by applying the inverse function to the network output without extra cost or change in the used model. The simplicity of this approach, coupled with its universal applicability promise to significantly advance the purview of semi-supervised and unsupervised learning.
1.1 RELATED WORK
The standard approach to DNN inversion was proposed in Dua & Gupta (2000), and the only DNN model with reconstruction capabilities is based on autoencoders Ng (2011). While more complex topologies have been used, such as stacked convolutional autoencoder Masci et al. (2011), there exists two main drawbacks. Firstly, the difficulty to train complex models in a stable manner even when using per layer optimization. Secondly, the difficulty to leverage supervised information into the learning of the representation.
1

Under review as a conference paper at ICLR 2018

The problem of semi-supervised learning with DNN has been attempted by several groups. The improved generative adversarial network (GAN) technique Salimans et al. (2016) couples two deep networks: a generative model that can create new signal samples, and a discriminative network. Both neural nets are trained jointly as in typical GAN framework. The main drawbacks to this approach include training instability Arjovsky et al. (2017), lack of portability to time series, lack of scalability to high-resolution images (e.g., ImagenetDeng et al. (2009)) Salimans et al. (2016), and the extra time and memory cost of training two deep networks. The semi-supervised with ladder network approach Rasmus et al. (2015) employs a per-layer denoising reconstruction loss, which enables the system to be viewed as a stacked denoising autoencoder which is a standard and until now only way to tackle unsupervised tasks. By forcing the last denoising autoencoder to output an encoding describing the class distribution of the input, this deep unsupervised model is turned into a semi-supervised model. The main drawback of this method is the lack of a clear path to generalize it to other network topologies, such as recurrent or residual networks. Also, the per-layer "greedy" reconstruction loss might be too restrictive unless correctly weighted pushing the need for a precise and large cross-validation of hyper-parameters. The probabilistic formulation of deep convolutional nets presented in Patel et al. (2015); Nguyen et al. (2016) natively supports semi-supervised learning. The main drawbacks of this approach lies in the requirement that the activation functions be ReLU and that the overall network topology follow a deep convolutional network.
1.2 CONTRIBUTIONS
This paper makes two main contributions: First, we propose a simple way to invert any piecewise differentiable mapping, including DNNs. We provide a formula for the inverse mapping of any DNN that requires no change to its structure. The mapping is computationally optimal, since the input reconstruction is computed via a backward pass through the network (as is used today for weight updates via backpropagation). Second, we develop a new optimization framework for semisupervised learning that features penalty terms that leverage the input reconstruction formula. A range of experiments validate that our method improves significantly on the state-of-the-art for a number of different DNN topologies.

2 DEEP NEURAL NETWORK INVERSION
In this section we review the work of Balestriero & Baraniuk (2017) aiming at interpreting DNNs as linear splines. This interpretation provides a rigorous mathematical justification for the need for reconstruction in the context of (semi-supervised or fully supervised) deep learning.

2.1 DEEP NEURAL NETS AS LINEAR SPLINE OPERATORS

Recent work in Balestriero & Baraniuk (2017) demonstrated that DNNs of many topologies are or can be approximated arbitrary closely by multivariate linear splines. The upshot of this theory for this paper is that it enables one to easily derive an explicit input-output mapping formula. As a result, DNNs can be rewritten as a linear spline of the form

f(x) = A[x]Xn + b[x],

(1)

where we denote by f a general DNN, x a generic input, A[x], b[x] the spline parameters conditioned on the input induced activations. Based on this interpretation, DNNs can be considered as template matching machines, where A[x] plays the role of an input-adaptive template.

To illustrate this point we provide for two common topologies the exact input-output mappings. For a standard deep convolutional neural network (DCN) with succession of convolutions, nonlinearities,

2

Under review as a conference paper at ICLR 2018

and pooling, one has

zC(LN) N (x) = W (L)

1
A( )A( )C( ) x+
=L-1

Template Matching



L-1

+1



W (L)



A(j)

A(j)C

(j)


=1 j=L-1

A( )A( )b( )

+ b(L),

(2)

Bias

where z( )(x) represents the latent representation at layer for input x. The total number of layers in a DNN is denoted as L and the output of the last layer z(L)(x) is the one before application of the softmax application. After application of the latter, the output is denoted by y^(x). The product terms are from last to first layer as the composition of linear mappings is such that layer 1 is applied on the input, layer 2 on the output of the previous one and so on. The bias term results simply from the accumulation of all of the per-layer biases after distribution of the following layers' templates. For a Resnet DNN, one has

zR(LE)S (x) = W (L)

1 =L-1

A( ,)inCi(n) + Co(u)t

x+

Template Matching

L-1 =1

+1
(A(,)inCi(n) + Co(u)t)
i=L-1

Bias

A( ,)inb(in) + b(ou)t + b(L) . (3)

We briefly observe the differences between the templates of the two topologies. The presence of an

extra term in

1 =L-1

A(,)inCi(n) + Co(u)t

as opposed to

1 =L-1

A(

)A(

)C(

)

provides

stability

and a direct linear connection between the input x and all of the inner representations z( )(x), hence

providing much less information loss sensitivity to the nonlinear activations.

Based on those findings, and by imposing a simple 2 norm upper bound on the templates, it has been shown that optimal DNNs should be able to generate templates proportional to their input,
positively for the belonging class and negatively for the others Balestriero & Baraniuk (2017).

Theorem 1. In the case where all inputs have identity norm ||Xn|| = 1, n and assuming all

templates denoted by A[Xn]c, c = 1, . . . , C have a norm constraint

C c=1

||A[Xn]c||2



K, Xn,

then the unique globally optimal templates are



 A[Xn]c =


C -1 C

K

Xn,



-

K C (C -1)

Xn

,

c = Yn

(4)

We now leverage the analytical optimal DNN solution to demonstrate that reconstruction is indeed implied by such an optimum.

2.2 OPTIMAL DNN LEADS TO INPUT RECONSTRUCTION
Based on the previous analysis, it is possible to draw implications based on the theoretical optimal templates of DNNs. This is formulated through the corollary below. First, we define the inverse of
3

Under review as a conference paper at ICLR 2018

a DNN as

f-1(Xn) =A[Xn]T (A[Xn]Xn + b[Xn])

C

= ( A[Xn]c, Xn + b[Xn]c)A[Xn]c.

(5)

c=1

Corollary 1. In the case of no or negligible b[x] or A[Xn]T b[Xn], the optimal templates defined in the previous Theorem lead to exact reconstruction, since we have by definition

C

A[Xn]c, x

A[Xn]c

=(C

-

1 1)K C(C -

1) ||Xn||2Xn

+

C- K
C

1 ||Xn||2Xn

=

Xn.

c=1

Hence optimal templates imply perfect reconstruction.

Hence, the case of no biases in the network corresponds to the absence of non-informative energy in the input Xn, exact reconstruction is implied by the optimal DNN in this case. In practice however, the biases and inputs are perturbed. For example, the presence of background, natural noise, or discontinuities are common in realistic datasets. Exact reconstruction is thus in practice not optimal. Only a subpart of the input energy is sufficient for the task at hand. Hence if we now consider the problem of reconstructing a noisy input, then the above framework follows schemes of standard thresholding and denoising such as done in wavelet thresholding. With minimization of the reconstruction error, one then has an adaptive filter-bank able to span the dataset. While not being sufficient for generalization it is necessary, since the absence of templates adapted to an input is synonym of false induced representation. We now present a particular application of these results in semi-supervised learning.

2.3 IMPLEMENTATION AND NEW LOSS FUNCTION FOR SEMI-SUPERVISED LEARNING

We now apply the above inverse strategy to a given task with an arbitrary DNN. As exposed earlier, all the needed changes to support semi-supervised learning happen in the objective training function by adding extra terms. In our application, we used automatic differentiation (as in TheanoBergstra et al. (2010) and TensorFlowAbadi et al. (2016)). Then it is sufficient to change the objective loss function, and all the updates are adapted via the change in the gradients for each of the parameters. The efficiency of our inversion scheme is due to the fact that any deep network can be rewritten as a linear mapping Balestriero & Baraniuk (2017). This leads to a simple derivation of a network inverse defined as f -1 that will be used to derive our unsupervised and semi-supervised loss function via

f -1(x, A[x], b[x]) =A[x]T A[x]x + b[x]

=A[x]T f (x; )

df (x; ) T = f (x; ).
dx The main efficiency argument thus comes from

(6)

df (x; )

A[x] =

,

dx

(7)

which enables one to efficiently compute this matrix on any deep network via differentiation (as it

would be done to back-propagate a gradient, for example). Interestingly for neural networks and

many common frameworks such as wavelet thresholding, PCA, etc., the reconstruction error as

(

df (x) dx

)T

f (x)

is

the

definition

of

the

inverse

transform.

For

illustration

purposes,

Tab.

1

gives

some

common frameworks for which the reconstruction error represents exactly the reconstruction loss.

We now describe how to incorporate this loss for semi-supervised and unsupervised learning. We

first define the reconstruction loss R as

R(Xn) =

df(Xn) dXn

T2
f(Xn) - Xn .

(8)

4

Under review as a conference paper at ICLR 2018

Table 1: Examples of frameworks with similar inverse transform definition.

Sparse Coding NMF PCA
Soft Wavelet Thres. Hard Wavelet Thres. Best Basis (WTA)
k-NN

i Learned
Learned f (x)i f (x)i f (x)i f (x)i
1

max

f (x)i
<x,Wi > ||Wi ||2
< x, Wi >
< x, Wi >
| < x, Wi > | - bi, 0 sig(< x, Wi >)

1|<x,Wi>|-bi>0 < x, Wi >

1
i=argmaxi

<x,Wi > ||Wi ||2

< x, Wi

>

1i=argmax<x,Wi>-||Wi||2/2 < x, Wi >

loss

||x -

i

i

df (x)i dx

||2

+

||||1

||x - ||x -

i

i

df (x)i dx

||2

s.t.

Jf (x)



0

i

i

df (x)i dx

||2

s.t.

Jf (x)

orth.

||x -

i

i

df (x)i dx

||2

||x - ||x -

i

i

df (x)i dx

||2

i

i

df (x)i dx

||2

||x -

i

i

df (x)i dx

||2

While we use the mean squared error, any other differentiable reconstruction loss can be used, such as cosine similarity. We also introduce an additional "specialization" loss defined as the Shannon entropy of the prediction

C
E(y^(Xn)) = - y^(Xn)c log(y^(Xn)c),
c=1

(9)

pushing the output distribution to have low entropy when minimized. The need for this loss is to make the unlabeled prediction with low-entropy also known as predicting a one-hot representation. As a result, we define our complete loss function as the combination of the standard cross entropy loss for labeled data denoted by LCE(Yn, y^(Xn)), the reconstruction loss, and the entropy loss as

L(Xn, Yn) = LCE(Yn, y^(Xn))1{Yn=}+(1-)[R(Xn)+(1-)E(Xn)], ,   [0, 1]2. (10)

The parameters ,  are introduced to form a convex combination of the losses,with  controlling the ratio between supervised and unsupervised loss and  the ratio between the two unsupervised losses. This weighting is important, because the correct combination of the supervised and unsupervised losses will guide learning toward a better optimum (as we now demonstrated via experiments).

3 EXPERIMENTAL VALIDATION
3.1 SEMI-SUPERVISED EXPERIMENTS
We now present results of our approach on a semi-supervised task on the MNIST dataset, where we are able to obtain state-of-the-art performance with different topologies. MNIST is made of 70000 grayscale images of shape 28 × 28 which is split into a training set of 60000 images and a test set of 10000 images. We present results for the case with NL = 50 which represents the number of samples from the training set that are labeled and fixed for learning. All the others samples form the training set are unlabeled and thus used only with the reconstruction and entropy loss minimization. We perform a search over (, )  {0.3, 0.4, 0.5, 0.6, 0.7} × {0.2, 0.3, 0.5}. In addition, 4 different topologies are tested and, for each, mean and max pooling are tested as well as inhibitor DNN (IDNN) as proposed in Balestriero & Baraniuk (2017). The latter proposes to stabilize training and remove biases units via introduction of winner-share-all connections. As would be expected based on the templates differences seen in the previous section, the Resnet topologies are able to reach the best performance. In particular, wide Resnet is able to outperform previous state-of-the-art results.
Running the proposed semi-supervised scheme on MNIST leads to the results presented in Tab. 2. We used the Theano and Lasagne libraries; and learning procedures and topologies are detailed in the appendix.The column 1000 corresponds to the accuracy after training of DNNs using only the supervised loss ( = 1,  = 0) on 1000 labeled data. Thus, one can see the gap reached with the same network but with a change of loss and 20 times less labeled data.
5

Under review as a conference paper at ICLR 2018

Table 2: Accuracy on MNIST test set for 50 labeled examples in the training. The column NL = X gives the accuracy of the same networks trained with the loss computed with only X labels. Our

Resnet2-32 with 50 labels is similar to the best ones with 50 or 100 labels.

NL

50, (, )

100

1000, ( = 1,  = 0)

SmallCNNmean (our)

99.07,(0.7, 0.2)

-

94.9

SmallCNNmax (our)

98.63,(0.7, 0.2)

-

95.0

SmallUCNN (our)

98.85,(0.5, 0.2)

-

96.0

LargeCNNmean (our)

98.63,(0.6, 0.5)

-

94.7

LargeCNNmax (our)

98.79,(0.7, 0.5)

-

94.8

LargeUCNN (our)

98.23,(0.5, 0.5)

-

96.1

Resnet2-32mean (our)

99.11,(0.7, 0.2)

-

95.5

Resnet2-32max (our)

99.14,(0.7, 0.2)

-

94.9

UResnet2-32 (our)

98.84,(0.7, 0.2)

-

95.6

Resnet3-16mean (our)

98.67,(0.7, 0.2)

-

95.4

Resnet3-16max (our)

98.56,(0.5, 0.5)

-

94.8

UResnet3-16 (our)

98.7,(0.7, 0.2)

-

95.5

Improved GAN Salimans et al. (2016) Auxiliary Deep Generative Model Maaløe et al. (2016)
LadderNetwork Rasmus et al. (2015) Skip Deep Generative Model Maaløe et al. (2016)
Virtual Adversarial Miyato et al. (2015) catGAN Springenberg (2015) DGN Kingma et al. (2014)

97.79 ± 1.36
-
-

99.07 ± 0.065
99.04 98.94 ± 0.37
98.68 97.88 98.61 ± 0.28 96.67 ± 0.14

-
-
-

DRMM Nguyen et al. (2016) DRMM +NN penalty Nguyen et al. (2016) DRMM+KL penalty Nguyen et al. (2016) DRMM+KL+NN pen.Nguyen et al. (2016)

78.27 77.9 97.54 99.09

86.59 87.72 98.64 99.43

-

6

Under review as a conference paper at ICLR 2018

Figure 1: Reconstruction of the studied DNN models for Resnet2-32 test set samples. The columns from left to right correspond to: the original image, mean-pooling reconstruction, max-pooling reconstruction, inhibitor connections.

3.2 SUPERVISED REGULARIZATION FOR IMPROVED GENERALIZATION

We now present and example of our approach on a supervised task on audio database (1D). It is the Bird10 dataset distributed online and described in Glotin et al. (2017). The task is to classify 10 bird species from their songs recorded in tropical forest. It is a subtask of the BirdLifeClef challenge. We train here networks based on raw audio using CNNs as detailed in the appendix. We vary (, ) to demonstrate that the non-regularized supervised model is not optimal. The maximum validation accuracies on the last 100 epochs (Fig. 2) show that the regularized networks tend to learn more slowly, but always generalize better than the not regularized baseline ( = 1,  = 0).

  train valid test 1 0 95.05 50.35 50.23 0.7 0.5 91.08 52.33 51.76 0.7 0.4 92.01 50.93 54.23 0.7 0.3 93.55 52.09 54.65 0.7 0.2 91.46 49.97 53.17 0.7 0.1 90.81 49.69 52.90 0.6 0.5 91.75 54.15 53.51 0.6 0.4 91.75 52.78 51.12 0.6 0.3 91.36 49.24 50.85 0.6 0.2 89.57 51.21 53.45

Accuracy on segments

1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.20

baseline
50 100 150 200 250 300 0 Epoch number

loss + reconstruction
Train set Validation set Test set 50 100 150 200 250 300 Epoch number

Figure 2: Left: Accuracy for Bird10 baseline ( = 1,  = 0), versus regularized networks. Middle: Learning curves of the baseline model. Right: Learning curve of the best test ( = 0.7,  = 0.3).

4 CONCLUSIONS AND FUTURE WORK
We have presented a well-justified inversion scheme for deep neural networks with an application to semi-supervised learning. By demonstrating the ability of the method to best current state-of-theart results on MNIST with different possible topologies support the portability of the technique as well as its potential. These results open up many questions in this yet undeveloped area of DNN inversion, input reconstruction, and their impact on learning and stability.
7

Under review as a conference paper at ICLR 2018

Among the possible extensions, one can develop the reconstruction loss into a per-layer reconstruction loss. Doing so, there is the possibility to weight each layer penalty bringing flexibility as well as meaningful reconstruction. Define the per layer loss as

L-1
L(Xn, Yn) = LCE(Yn, y^(Xn))1{Yn=} + E(Xn) + ( )R( )(Xn),

(11)

=0

with

R(

)(Xn)

=

||(

df(Xn) dz( )(Xn)

)T

f(Xn)

-

z(

)(Xn)||2.

(12)

Doing so, one can adopt a strategy in favor of high reconstruction objective for inner layers, close to the final latent representation z(L) in order to lessen the reconstruction cost for layers closer to

the input Xn. In fact, inputs of standard dataset are usually noisy, with background, and the object of interest only contains a small energy with respect to the total energy of Xn. Another extension would be to update the weighting while performing learning. Hence, if we denote by t the position

in time such as the current epoch or batch, we now have the previous loss becoming

L-1
L(Xn, Yn; ) = (t)LCE(Yn, y^(Xn))1{Yn=} + (t)E(Xn) + ( )(t)R( )(Xn). (13)
=0
One approach would be to impose some deterministic policy based on heuristic such as favoring reconstruction at the beginning to then switch to classification and entropy minimization. Finer approaches could rely on explicit optimization schemes for those coefficients. One way to perform this, would be to optimize the loss weighting coefficients , , ( ) after each batch or epoch by backpropagation on the updates weights. Define

(t + 1) = (t) -  dL(Xn, Yn) , d

(14)

as a generic iterative update based on a given policy such as gradient descent. One can thus adopt

the following update strategy for the hyper-parameters as

(

)(t

+

1)

=

(

)(t)

-

dL(Xn, Yn; (t d( )(t)

+

1)) ,

(15)

and so for all hyper-parameters. Another approach would be to use adversarial training to update those hyper-parameters where both update cooperate trying to accelerate learning.

EBGAN (Zhao et al. (2016)) are GANs where the discriminant network D measures the energy of a given input X. D is formulated such as generated data produce high energy and real data produce lower energy. Same authors propose the use of an auto-encoder to compute such energy function. We plan to replace this autoencoder using our proposed method to reconstruct X and compute the energy; hence D(X) = R(X) and only one-half the parameters will be needed for D.

Finally, our approach opens the possibility of performing unsupervised tasks such as clustering. In fact, by setting  = 0, we are in a fully unsupervised framework. Moreover,  can push the mapping f to produce a low-entropy, clustered, representation or rather simply to produce optimal reconstruction. Even in a fully unsupervised and reconstruction case ( = 0,  = 1), the proposed framework is not similar to a deep-autoencoder for two main reasons. First, there is no greedy (per layer) reconstruction loss, only the final output is considered in the reconstruction loss. Second, while in both case there is parameter sharing, in our case there is also "activation" sharing that corresponds to the states (spline) that were used in the forward pass that will also be used for the backward one. In a deep autoencoder, the backward activation states are induced by the backward projection and will most likely not be equal to the forward ones.

REFERENCES
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
8

Under review as a conference paper at ICLR 2018
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Randall Balestriero and Richard Baraniuk. A spline theory of deep learning, 2017. https://goo.gl/J54TxD.
James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: A cpu and gpu math compiler in python. In Proc. 9th Python in Science Conf, pp. 1­7, 2010.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248­255. IEEE, 2009.
Ashima Dua and Amar Gupta. Inversion of neural networks: A solution to the problems encountered by a steel corporation. 2000.
Herve Glotin, Julien Ricard, and Randall Balestriero. Fast chirplet transform injects priors in deep learning of animal calls and speech. In Proceedings of the 2017 ICLR workshop, 2017.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581­3589, 2014.
Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxiliary deep generative models. arXiv preprint arXiv:1602.05473, 2016.
Jonathan Masci, Ueli Meier, Dan Cires¸an, and Jürgen Schmidhuber. Stacked convolutional autoencoders for hierarchical feature extraction. Artificial Neural Networks and Machine Learning­ ICANN 2011, pp. 52­59, 2011.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing by virtual adversarial examples. stat, 1050:2, 2015.
Andrew Ng. Sparse autoencoder. CS294A Lecture notes, 72(2011):1­19, 2011. Tan Nguyen, Wanjia Liu, Ethan Perez, Richard G Baraniuk, and Ankit B Patel. Semi-supervised
learning with the deep rendering mixture model. arXiv preprint arXiv:1612.01942, 2016. Ankit B Patel, Tan Nguyen, and Richard G Baraniuk. A probabilistic theory of deep learning. arXiv
preprint arXiv:1504.00641, 2015. Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-
supervised learning with ladder networks. In Advances in Neural Information Processing Systems, pp. 3546­3554, 2015. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2226­2234, 2016. Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390, 2015. Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.
A MODELS AND TRAINING DESCRIPTION
A.1 MNIST
We trained the models in Tab. 2 by cross validation of ,  with step size of 0.1. The learning rate with Adam was optimized for MNIST semisup to 0.01 for SmallCNN and 0.01/3 for Resnet and LargeCNN.
9

Under review as a conference paper at ICLR 2018
A.2 BIRD10 We trained the models in Fig. 2 as follow: conv1d (with kernel size of 1024, stride of 512 and number of filters of 42) -> maxpool1d(4, 2) -> conv1d (3, 2, 42) -> Dense(32) -> dropout(0.5) -> Dense(10). For this experiment we measured the accuracy per segment using segments of 500ms with an overlap of 90 percent. Note that we did not used batch normalization in any of our models, because it would perturb the training as the update of the parameters will take effect before the update of normalization. We are working on a new procedure to update the parameters to overpass this issue. A.3 MNIST RECONSTRUCTION ON OTHER TOPOLOGIES We give below the figures of the reconstruction of the same test sample by four different nets : LargeUCNN ( = 0.5,  = 0.5), SmallUCNN (0.6,0.5), Resnet2-32 (0.3,0.5), Resnet3-16 (0.6,0.5). The columns from left to right correspond to: the original image, mean-pooling reconstruction, maxpooling reconstruction, and inhibitor connections. One can see that our network is able to correctly reconstruct the test sample.
10

Under review as a conference paper at ICLR 2018
Figure 3: Reconstruction of the studied DNN models, from top to right : LargeUCNN ( = 0.5,  = 0.5), SmallUCNN (0.6,0.5), Resnet2-32 (0.3,0.5), Resnet3-16 (0.6,0.5) on the same test set samples. The columns from left to right correspond to: the original image, mean-pooling reconstruction, maxpooling reconstruction, inhibitor connections.
11

