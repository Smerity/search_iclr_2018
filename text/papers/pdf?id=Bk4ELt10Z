Under review as a conference paper at ICLR 2018
TOWARDS QUANTUM INSPIRED CONVOLUTION NET-
WORKS
Anonymous authors Paper under double-blind review
ABSTRACT
Deep Convolution Neural Networks (CNNs), rooted by the pioneer work of Rumelhart et al. (1986); LeCun (1985); Krizhevsky et al. (2012), and summarized in LeCun et al. (2015), have been shown to be very useful in a variety of fields. The state-of-the art CNN machines such as image rest net He et al. (2016) are described by real value inputs and kernel convolutions followed by the local and non-linear rectified linear outputs. Understanding the role of these layers, the accuracy and limitations of them, as well as making them more efficient (fewer parameters) are all ongoing research questions. Inspired in quantum theory, we propose the use of complex value kernel functions, followed by the local non-linear absolute (modulus) operator square. We argue that an advantage of quantum inspired complex kernels is robustness to realistic unpredictable scenarios (such as clutter noise, data deformations). We study a concrete problem of shape detection and show that when multiple overlapping shapes are deformed and/or clutter noise is added, a convolution layer with quantum inspired complex kernels outperforms the statistical/classical kernel counterpart and a "Bayesian shape estimator" . The superior performance is due to the quantum phenomena of interference, not present in classical CNNs.
1 INTRODUCTION
The convolution process in machine learning maybe summarized as follows. Given an input f L-1(x)  0 to a convolution layer L, it produces an output
gL(y) = K(y - x)f L-1(x) dx
From gL(y) a local and non-linear function is applied, f L(y) = f (gL(y)), e.g., f = ReLu (rectified linear units) or f = |.|, the magnitude operator. This output is then the input to the next convolution layer (L+1) or simply the output of the whole process. We can also write a discrete form of these convolutions, as it is implemented in computers. We write giL = j wijfjL-1, where the continuous variables y, x becomes the integers i, j respectively, the kernel function K(y - x)  wij becomes the weights of the CNN and the integral over dx becomes the sum over j.
These kernels are learned from data so that an error (or optimization criteria) is minimized. The kernels used today a real value functions. We show how our understanding of the optimization criteria "dictate" the construction of the quantum inspired complex value kernel. In order to concentrate and study our proposal of quantum inspired kernels, we simplify the problem as much as possible hoping to identify the crux of the limitation of current use of real value kernels.
We place known shapes in an image, at any location, and in the presence of deformation and clutter noise. These shapes may have been learned by a CNN. Our main focus is on the feedforward performance, when new inputs are presented. Due to this focus, we are able to construct a Bayesian a posteriori probability model to the problem, which is based on real value prior and likelihood models, and compare it to the quantum inspired kernel method.
The main advantage of the quantum inspired method over existing methods is its high resistance to deviations from the model, such as data deformation, multiple objects (shapes) overlapping, clutter noise. The main new factor is the quantum interference phenomenon Feynman & Hibbs (2012);
1

Under review as a conference paper at ICLR 2018

Feynman (1971), and we argue it is a desired phenomena for building convolution networks. It can be carried out by developing complex value kernels driven by classic data driven optimization criteria. Here we demonstrate its strength on a shape detection problem where we can compare it to state of the art classical convolution techniques. We also can compare to the MAP estimator of the Bayesian model for the shape detection problem.
To be clear, we do not provide (yet) a recipe on how to build kernels for the full CNN framework for machine learning, and so the title of this paper reflects that. Here, we plant a seed on the topic of building complex value kernels inspired in quantum theory, by demonstrating that for a given one layer problem of shape detection (where the classic data optimization criteria is well defined), we can build such complex value kernel and demonstrate the relevance of the interference phenomena. To our knowledge such a demonstration is a new contribution to the field. We also speculate on how this process can be generalized.
1.1 THE SPECIFIC PROBLEM
We are given an image I with some known objects to be detected. The data is a set of N feature points, X = {x1, x2, ..., xN } in RD. Here we focus on 2-dimensional data, so D = 2. An image I may be described by the set of feature points, as shown for example in figure 1. Or, the feature points can be extracted from I, using for example, SIFT features, HOG features, or maximum of wavelet responses (which are convolutions with complex value kernels). It maybe be the first two or so layers of a CNN trained in image recognition tasks. The problem of object detection has been well addressed for example by the SSD machine Liu et al. (2016), using convolution networks. Here as we will demonstrate, given the points, we can construct a ONE layer CNN that solves the problem of shape detection and so we focus on this formulation. It allows us to study in depth its performance (including an analytical study not just empirical).

12 12

10 10

88

66

4 2 4 6 8 10 12 14
(a) A deformed circle

4 2 4 6 8 10 12 14
(b) Uniformly distributed noise added to (a)

Figure 1: (a) A circle centered at (8, 8), radius 3, and with 100 points, is deformed as follows: for each point xi a random value drawn from a uniform distribution with range (-i, i), i = 0.05, is added along the radius. (b) 1000 points of clutter are added by sampling from a uniform distribution inside a box of size 9 × 9, with corners at points: (4,4), (4,13), (13,4), (13,13).

1.2 PAPER ORGANIZATION
We organize the paper as follows. Section 2 presents a general description of shapes, which is easily adapted to any optimization method. Section 3 presents the Bayesian method and the Hough transform method (and a convolution implementation) to the shape detection problem. Section 4 lays out our main proposal of using quantum theory to address the shape detection problem. The theory also leads naturally to a classical statistical method behaving like a voting scheme, and we establish a connection to Hough transforms. Section 5 presents a theoretical and empirical analysis of the quantum method for shape detection and a comparison with the classical statistical method. We demonstrate that for large deformations or clutter noise scenarios the quantum method outperforms the classical statistical method. Section 6 concludes the paper.
2

Under review as a conference paper at ICLR 2018

2 SHAPE DETECTION

A shape S may be defined by the set of points x satisfying S(x) = 0, where  is a set of parameters describing S. Let µ be a shape's center (in our setting µ = (µx, µy)). The choice of µ is in general arbitrary, though frequently there is a natural choice for µ for a given shape, such as its "center of mass": the average position of its coordinates. We consider all the translations of S(x) to represent the same shape, so that a shape is translation invariant. It is then convenient to describe the shapes as S(x - µ), with the parameters  not including the parameters of µ. Thus we describe a shape by the set of points X such that

S(x - µ) = 0 for all x  X.

The more complex a shape is, the larger is the set of parameters required to describe it. For example,

to describe a circle, we use three parameters {µx, µy, r} representing the center and the radius of

the circle, i.e., S={r}(x - µ)

=

1-

(x-µ)2 r2

(see figure 1 a.)

An ellipse can be described by

S(x - µ) = 1 - (x - µ)T -1 (x - µ), where  = {} is the covariance matrix, specified by

three independent parameters.

We also require that a shape representation be such that if all the values of the parameters in  are 0, then the the set of points that belong to the shape "collapses" to just X = {µ}. This is the case
for the parameterizations of the two examples above: the circle and the ellipse.

Energy Model: Given a shape model we can create an energy model per data point x as

ES (x - µ) = |S(x - µ)|p

(1)

where the parameter p  0 defines the Lp norm (after the sum over the points is taken and the 1/p
root is applied). The smaller ES (x - µ), the more it is likely that the data point x belongs to the shape S with center µ. In this paper, we set p = 1 because of its simplicity and robust properties.

2.1 DEFORMATIONS
To address realistic scenarios we must study the detection of shapes under deformations. When deformations are present, the energy (1) is no longer zero for deformed points associated to the shape S(x - µ). Let each ideal shape data point xiS be deformed by adding i to its coordinates, so xi = xSi + i. Then the deformed points xi satisfy

0 = |S(xiS - µ)| = |S(xi - i - µ)|

Deformations of a shape are only observed in the directions perpendicular to the shape tangents, i.e.,

along the direction of xS(x - µ) , where x is the gradient operator.
xi

For

example,

for

a

(deformed)

circle

shape,



=

{r}

and

Sr (x - µ)

=

1-

(x-µ)2 r2

,

and

so

x Sr (x -

µ)

xi

=

2

(xi -µ) r2



r^i,

where r^i

is a

unit

vector pointing outwards in

the radius

direction at

point

xi. Thus, i = i r^i.

3 CLASSICAL METHODS: A BAYESIAN AND A HOUGH TRANSFORM
Given a set of data points X = {x1, x2, ..., xN } in RD originated from a shape S(xi - µ) = 0. We assume that each data point is independently deformed by i (a random variable, since the direction is along the shape gradient), conditional on being a shape point.

3.1 A BAYESIAN APPROACH

Based on the energy model (1), for p = 1 (for simplicity and robust properties), we can write the

likelihood

model

as

P(X|, µ)

=

1 C

N i=1

e-ES

(xi -µ)

=

1 C

N i=1

e-|S (xi -µ)| ,

where

C

is

a

normalization constant and  a constant that scale the errors/energies. The product over all points

3

Under review as a conference paper at ICLR 2018

is a consequence of the conditional independence of the deformations given the shape parameter (, µ). Assuming a prior distributions on the parameters to be uniform, we conclude that the a posteriori distribution is simply the likelihood model up to a normalization, i.e.,

P(, µ|X) = 1

N e-ES (xi-µ) = 1

N
e-|S (xi -µ)|

ZZ

i=1 i=1

where Z is a normalization constant (does not depend on the parameters). The parameters that

maximize the likelihood L(, µ) = log P(, µ|X) = -Z - 

N i=1

|S (xi

-

µ)|

also

minimize

the total energy E(, µ) =

N i=1

|S

(xi

-

µ)|.

3.2 HOUGH TRANSFORM

A Hough transform cast binary votes from each data point. The votes are for the shape parameter values that are consistent with the data point. More precisely, each vote is given by

v(, µ|xi) = u

1 

-

|S (xi

-

µ)|

(2)

where u(x) is the Heaviside step function, u(x) = 1 if x  0 and zero otherwise, i.e., u = 1 if

|S (xi

- µ)|



1 

and u

=

0 otherwise.

The parameter 

clearly defines the error tolerance for

a data point xi to belong to the shape S(x - µ), the larger is  the smaller is the tolerance. One

can carry out this Hough transform for center detection as a convolution process. More precisely,

create a kernel, KH (x) centered at (0, 0) and define it as KH (x) = u

1 

-

|S (x)|

for x in

a rectangular (or square) shape that includes all x for which u

1 

-

|S (x)|

= 1. The Hough

transform for center detection is then the convolution of the kernel with the input image. The result

of the convolution at each location is the Hough vote for that location to be the center.

3.3 A COMPARISON OF THE TWO METHODS

(a) Circle deformed with clutter noise.

(b) Bayesian estimation

(c) Hough method

Figure 2: (a) A circle centered at (8, 8), radius 3, and with 100 points, is deformed with  = 0.5. 100 clutter noise points are added uniformly to a box with diagonal corners at (0, 0) and (8, 8). (b) The Bayesian method (showing the probability value). The radius is fed to the method. The method mixes all data yielding the highest probability in the wrong place. increasing the parameter p can only improve a little as all data participate in the final estimation. (c) The Hough method with  = 2.769 estimated to include all circle points that have been deformed. The method is resistant to clutter noise.

When we have one circle with deformations (e.g., see figure 1a), the Bayesian approach is just the "perfect" model. Even if noise distributed uniformly across the image is added (e.g., see figure 1b), the Bayesian method will work very well. However, as one adds clutter noise to the data (noise that is not uniform and "may correspond to clutter" in an image) as shown in figure 2, the Bayesian method mix all the data, has no mechanism to discard any data, and the Hough method outperforms the Bayesian one. Even applying robust measures, decreasing p in the energy model, will have limited effect compared to the Hough method that can discard completely the data. Consider another

4

Under review as a conference paper at ICLR 2018
scenario of two overlapping and deformed circles, shown in figure 3. Again, the Bayesian approach does not capture the complexity of the data, two circles and not just one, and end up yielding the best one circle fit in the "middle", while the Hough method cope with this data by widening the center detection probabilities (bluring the center probabilities) and thus, including both true centers. Still, the Hough method is not able to suggest that there are two circles/two peaks. In summary, the Bayesian model is always the best one, as long as the data follows the exact model generation. However, it is weak at dealing with *real world uncertainty* on the data (clutter data, multiple figures), other scenarios that occur often. The Hough method, modeled after the same true positive event (shape detection) is more robust to these data variations and for the center detection problem can be carried out as a convolution.

(a) Two overlapping circles

(b) Bayesian estimation

(c) Hough method

Figure 3: (a) Two overlapping circles deformed with  = 0.5, radius r = 3 and with 100 points each. The circle centers are at (8, 8) and (7.2, 7.2). (b) The Bayesian method (showing the probability value). The radius is fed to the method. The method mixes all data yielding the highest probability approximately in the "middle" of both centers and no suggestion of two peaks/circles/centers exists. (c) The Hough method with  = 2.769 estimated to include all circle points that have been deformed. The method yields a probability that is more diluted and includes the correct centers, but does not suggest two peaks.

4 QUANTUM SHAPE DETECTION AND INTERFERENCE
Quantum theory was developed for system of particles that evolve over time. For us to utilize here the benefits of such a theory for the shape detection problem we invoke a hidden time parameter. We refer to this time parameter as hidden since the input is only one static picture of a shape. A hidden shape dynamics is not a new idea in computer vision, for example, scale space was proposed to describe shape evolution and allows for better shapes comparisons Witkin (1983); Lindeberg (1994). Hidden shape dynamics was also employed to describe a time evolution equation to produce shape-skeletons Siddiqi & Kimia (1996). Since our optimization criteria per point is given by "the energy" of (1), we refer to classic concept of action, the one that is optimized to produce the optimal path, as

Aµ0x (P0T ) = T |S(x - µ)|
where we are adopting for simplicity p = 1. The idea is that a shapes evolve from the center µ = x(t = 0) to the shape point x = x(t = T ) in a time interval T . During this evolutions all other parameters also evolve from (t = 0) = 0 to (t = T ) = . The evolution is reversible, so we may say equivalently, the shape point x contracts to the center µ in the interval of time T .
Following the path integral point of view of quantum theory Feynman & Hibbs (2012), we consider the wave propagation to evolve by the integral over all path

0(µ) = dP0T K(P0T ) (x)

(3)

where (t)(x(t)) is the probability amplitude that characterize the state of the shape, P0T is a path of shape contraction, from an initial state (x(0), (0)) = (x, ) to a final state (x(T ), (T )) =
(µ, 0). The integral is over all possible paths that initialize in (x(0), (0)) = (x, ) and end in (x(T ), (T )) = (µ, 0). The Kernel K is of the form

5

Under review as a conference paper at ICLR 2018

K(P0T )

=

1 C

ei 1

Axµ0(P0T )

=

1 C

ei T

|S (x-µ)|

(4)

where a new parameter, , is introduced. It has the notation used in quantum mechanics for the reduced Planck's constant, but here it will have its own interpretation and value (see section 5.1.3).

We now address the given image described by X = {x1, x2, ..., xN }  R2 (e.g., see figure 1). We

consider an empirical estimation of (x) to be given by a set of impulses at the empirical data set

X,

i.e.,

 (x)

=

1 N

N i=1

(x

-

xi),

where

(x)

is

the

Dirac

delta

function.

The

normalization

ensure the probability 1 when integrated everywhere. Note that (x) is a pure state, a superposition

of impulses. Then, substituting this state into equation (3), with the kernel provided by (4), yields

the evolution of the probability amplitude

N
(µ) 
i=1

dx

1 C

ei

T

|S (x-µ)|

1 N

(x - xi)

=

1 CN

N
ei T |S(xi-µ)| .
i=1

(5)

where C = ei T |S(x-µ)| dµ. Thus shape points with deformations, xi, are interpreted as evidence of different quantum paths, not just the optimal classical path (which has no deformation). Equation 5 is a convolution of the kernel K(x) = ei T |S(x)| throughout the center candidates,
except it is discretized at the locations where data is available. According to quantum theory, the probability associated with this probability amplitude (a pure state) is given by P() = |(µ)|2, i.e.,

P (µ)

=

(µ)  (µ)

=

1 C2 N

which can also be expanded as

N
ei T |S(xi-µ)|
i=1

N
e- i T |S(xj -µ)|
j=1

,



1 P(µ) = C2 N

NN
1 + 2 cos

T (|S(xi - µ)| - |S(xj - µ)|)  .

i=1 j>i

(6)

It is convenient to define the phase ij = T (|S(xi - µ)| - |S(xj - µ)|).

4.1 INTERFERENCE

Note the interference phenomenon arising from the cosine terms in the probability (6). More pre-

cisely, a pair of data points that belongs to the shape will have a small magnitude difference,

|ij| 1, and will produce a large cosine term, cos ij  1. Two different data points that be-

long to the clutter will likely produce different phases, scaled inversely according to , so that small

values of will create larger phase difference. Pairs of clutter data points, not belonging to the

shape, with large and varying phase differences, will produce quite different cosine terms, positive

and/or negative ones. If an image contains a large amount of clutter, the clutter points will end

up canceling each other. If an image contains little clutter, the clutter points will not contribute

much. This effect can be described by the following property for large numbers: if N 1 then

N i=1

N j=1

cos(

i

-

j) 

N /4

N , when each k is a random variable.

Figure 4 shows the performance of the quantum method on the same data as shown in figure 2a and figure 3a. The accuracy of the detection of the centers and the identification of two centers shows how the quantum inspired method outperforms the classical counterparts. In figure 4a, due to interference, clutter noise cancels out (negative terms on the probability equation 6 balance positive ones), and the center is peaked. We do see effects of the noise inducing some fluctuation. In figure 4b the two circle center peaks outperform both classical methods results as depicted in figure 3. A more thorough analysis is carried out in the next section to better understand and compare the performance of these different methods.

6

Under review as a conference paper at ICLR 2018
4.2 LINEAR-COMPLEXITY COMPUTATION IN THE SIZE OF THE DATA SET Note that even though the probability reflects a pair-wise computation as seen in (6), we evaluate it by taking the magnitude square of the probability amplitude (given by equation (5)), which is computed as a sum of N complex numbers. Thus, the complexity of the computations is linear in the data set size. After all, it is a convolution process.

(a) Quantum Probability on figure 2a

(b) Quantum Probability on figure 3a

Figure 4: Quantum Probability depicted for input shown in figure 2a and figure 3a, respectively. The parameters used where T = 1, = 0.12. The quantum method outperform the classical methods, as the center detection shown in (a) is more peaked than in the Hough method and in (b) the two peaks emerge. These are results of the interference phenomena, as cancellation of probabilities (negative terms on the probability equation 6) contribute to better resolve the center detection problem.

4.3 A CLASSICAL STATISTICAL VERSION OF THE QUANTUM CRITERION

we derive a classical probability from the quantum probability amplitude via the Wick rotation Wick
(1954). It is a mathematical technique frequently employed in physics, which transforms quantum physical systems into statistical physical systems and vice-versa. It consists in replacing the term i T
by a real parameter  in the probability amplitude. Considering the probability amplitude equation
(5), the Wick rotation yields

1 P(µ) = Z

N

e-  |S(xi-µ)| .

i=1

(7)

We can interpret this as follows. Each data point xi produces a vote v(, µ|xi) = e-  |S(xi-µ)|, with values between 0 and 1. The parameter  controls the weight decay of the vote. Interestingly, this probability model resembles a Hough transform with each vote (7) being approximated by the binary vote described by (2)

5 ANALYSIS FOR THE CIRCLE SHAPE

We analyze the quantum method described by the probability (6), derived from the amplitude (5), and compare it with the classical statistical method described by (7) and its approximation (2). This analysis and experiments is carried for a simple example, the circle.

We

consider

the

circle

shape,

Sr (x - µ)

=

1-

(x-µ)2 (r )2

of

radius

r

and

its

evaluation

not

only

at

the

true

center

µ

but

also

at

small

displacements

from

it

µ

=

µ

+

µ

where

µ r

<

1

with

µ

=

|µ|.

5.1 QUANTUM PATHS AS DEFORMATIONS

The points of an original circle are deformed to create the final "deformed" circle shape. Each point is moved by a random vector i pointing along the radius, i.e., i = ir^i with r^i being the unit

7

Under review as a conference paper at ICLR 2018

vector along the radius. Thus, we may write each point as xi = xiC + i so that xCi belong to

the original circle or Sr (xCi - µ) = 0 The deformation is assumed to vary independently and

uniformly

point

wise.

Thus,

i



(-, )

and

P(i)

=

1 2

.

Plugging

in

the

deformations

and

center

displacement into the shape representation, Si = Sr (xi - µ), we get

Si

=1 -

(x - µ)2 (r)2

=

1-

(xiC

+ i - µ - µ)2 (r)2

=

-[2(ai

+ bi) + (ai2

+ b2

- 2aibi)]

(8)

where µi

=

r^i · µ, ai

=

i r

and bi

=

µi r

,

b

=

µ r

=

maxi bi.

We are investigating small

deformations |ai| < 1 and small displacements from the true center |bi| < 1. We interpret |Si|

as a random variable and we assumed the sampling of the circle is such that the variation of bi is

uniform.

So Pb(bi)

=

1 2b

=

r 2µ

and with the deformations uniformly distributed we also have

Pa(ai)

=

1 2a

=

r 2

.

Finally,

since

the

deformations

are

independent

of

the

shape

position,

ai

and

bi

are independent and

PS (|Si|)

=

Pa(ai)Pb(bi)

=

1 4ab

=

(r)2 4 µ

For the special case of the evaluation of the shape at the true center, µ = 0 we obtain Si =

-2

i r

- i2
(r )2

=

-[2ai + ai2].

The action for each path is given by |S(xi - µ)| and we have

multiple paths samples from the data. Note that when we apply the quantum method, we interpret

data derived from shape deformation as evidence of quantum trajectories (paths) that are not optimal

while the classical interpretation for such data is a statistical error/deformation. Both are different

probabilistic interpretations that lead to different methods of evaluations of the optimal parameters

as we analyze next.

5.1.1 QUANTUM PROBABILITY AMPLITUDE

We interpret the probability amplitude in equation (5), the sum over i = 1, . . . , N , as a sum over

many independent samples. In general given a function f (|S|), then the sum over all points,

NC i=1

f

(|Si|),

can

be

interpreted

as

NC

times

the

statistical

average

of

the

function

f

over

the

random variable Si. In this case, the random variable Si(i, µi) represent two independent and

uniform random variables, (i, µi), or (ai, bi).

Inserting shape equation (8) into the quantum probability amplitude of equation (5) result in

r(µ

+

µ)



1 C NC

NC

ei

1

|Sr (xi-µ)|

=



NC C

Iab

(e)

i=1

where Iab(e)


=

1 4ab

r(µ) 

NC C

Ia

(e),

a -a

dai

b -b

dbi

where Ia(e) =

ei 1
1 2a

|2(ai+bi)+(a2i +b2-2aibi)| and at the

a -a

dai

ei

1

|2ai+ai2|.

The

ratio

of

the

true center we get probabilities (mag-

nitude square of the probability amplitudes) for the circle is then given by

QC (a, b,

)

=

Pr(µ) Pr(µ + µ)



|Ia(e)|2 |Iab(e)|2

(9)

These integrals can be evaluated numerically (or via integration of a Taylor series expansions, and then a numerical evaluation of the expansion).

5.1.2 CLASSICAL HOUGH TRANSFORM Inserting shape equation (8) into the vote for the Hough transform giving by equation (2) result in

v(r, µ + µ|xi) = u

1 

-

|2(ai

+

bi)

+

(ai2

+

b2

-

2aibi)|

8

Under review as a conference paper at ICLR 2018

and interpreting the Hough total vote, VrH (µ) =

NC i=1

v(r,

µ|xi),

as

an

average

over

a

function

of the random variable |Si| multiplied by the number of votes, we get

VrH (µ + µ) NC Iab(u)  VrH (µ)  NC Ia(u)

where

Iab(u)

=

1 4ab

a -a

d

ai

b -b

dbi

u

1 

-

2(ai + bi) + (a2i + b2 - 2aibi)

and at the true cen-

ter Ia(u)

=

1 2a

a -a

d

ai

u

1 

-

2ai + a2i

. The ratio of the votes at the true center and the dis-

placed center is then given by

H(a, b, )

=

VrH (µ) VrH (µ + µ)

=

Ia(u) Iab(u)

(10)

We now address the choice of hyper-parameters of the models, namely for the quantum model and  for the classical counterpart so that the detection of the true center is as accurate as possible.

5.1.3 EVALUATION OF AND  FOR CENTER DETECTION µ

In this section, without loss of generality, we set T = 1. In this way we can concentrate on understanding role (and not T ). The amplitude probability given by equation (5) has a parameter
where the inverse of scales up the magnitude of the shape values. The smaller is , the more the phase i = 1 |S(xi - µ)| reaches any point in the unit circle. A large can make the phase i
very small and a small can send each shape point to any point in the unit circle .

The parameter large can help in aligning shape points to similar phases. That suggests as large as possible. At the same time, should help in misaligning pair of points where at least one of them does not belong to the shape. That suggests small values of . Similarly, if one is evaluating a shape with the "wrong" set of parameters, we woulud want the shape points to cancel each other. In our example of the circle, we would like that shape points evaluated at a center displacement from the true center to yield some cancellation. That suggests small. One can explore the parameter
that maximize the ratio QC(a, b, ) given by equation (9). We can also attempt to analytically balance both requests (high amplitude at the true center and low amplitude at the displaced center) by choosing values of such that i = 1 |Sr (xi - µ)|   i = 1, . . . , NC . More precisely, by choosing

=

1 

max |Sr (xi
i

-

µ)|



2a

+ 

a2

(11)

Figure 5 suggests this choice of gives high ratios for Q(a, b, ).

Now

we

discuss

the

estimation

of

.

we

note

that

for

1 

>

2a

+ a2

=

2

 r

+

 r

2 all shape points

will vote

for

the true center.

Thus,

choosing 

so

that

largest value of

its

inverse

is

1 

=

2a + a2

will guarantee all votes and give a lower vote for shape points evaluated from the displaced center.

One could search for higher values of , smaller inverse values, so that reducing votes at the true

center and expecting to reduce them further at the displaced center, i.e., to maximize H(a, b, ) in

equation (10). Figure 5 suggests such changes do not improve the Hough transform performance.

Figure 5 demonstrates that the quantum method outperforms the classical Hough transform on accuracy detection.

We can also perform a similar analysis adding noise, to obtain similar results. This will require another two pages (a page of analysis and a page of graphs), and if the conference permits, we will be happy to add.

6 CONCLUSIONS
Deep Convolution Neural Networks (CNNs), rooted on the pioneer work of Rumelhart et al. (1986); LeCun (1985); Krizhevsky et al. (2012), and summarized in LeCun et al. (2015), have been shown to be very useful in a variety of fields.
Inspired in quantum theory, we investigated the use of complex value kernel functions, followed by the local non-linear absolute (modulus) operator square. We studied a concrete problem of

9

Under review as a conference paper at ICLR 2018

(a) Q ×

,H

× -,

b

=

1 2

a

=

1 12

.

(b) Q ×

,

H

×

-,

b

=

a

=

1 6

(c) Q ×

,

H

×

-,

b

=

2a

=

1 3

.

(d) ||2(µ) × and V (µ) × -

Figure 5: Quantum method vs classical Hough transform for accuracy of the detection of the center.

For

all

figures

we

fixed

the

radius

r

=

3

and

deformations



=

0.5,

thus,

a

=

1 6

.

For

each

of

the

figures

5a,

5b,5c

we

vary

we

vary

b

=

1 2

a,

a,

2a

(or

center

displacements

µ

=

(tirveedl)y.foTrhe-sefig(u2r2e.s7d2e7p,i2ct.7r6at9i)os(TQh(ea,rebv, er)se×arr(obwlueim) fpolries

 (0.047, 0.2802) the x-axis start at

0.25, 0.5, 1), respecand H(a, b, ) × - the maximum value

and decreases thereafter). All plots have 200 points, with uniform steps in their respective range.

Note that our proposed parameter value is = 0.1401, the solution to equation (11), and indeed

gives a high ratio. Also,  = 2.769 is the smallest value to yield all Hough votes in the cen-

ter. Clearly the quantum ratio outperforms the best classical Hough method, which does not vary

much across  values. As the center displacement increases, the quantum method probability, for

= 0.1401, decreases much faster than the Hough method probability. Final figure 5d display

values In red,

of V

(|µ|2)(×µ-) ×for

(-at

the true center) in blue, for  (22.727, 2.769), with 200

 (0.047, 0.2802), uniform steps.

with

200

uniform

steps.

shape detection and showed that when multiple overlapping shapes are deformed and/or clutter noise is added, a convolution layer with quantum inspired complex kernels outperforms the statistical/classical kernel counterpart and a "Bayesian shape estimator". It is worth to mention that the Bayesian shape estimator is the best method as long as the data satisfy the model assumptions. Once we add multiple shapes, or add clutter noise (not uniform noise), the Bayesian method breaks down rather easily, but not the quantum method nor the statistical version of it (the Hough method being an approximation to it). An analysis comparing the Quantum method to the Hough method was carried out to demonstrate the superior accuracy performance of the quantum method, due to the quantum phenomena of interference, not present in the classical CNN.
We have not focused on the problem of learning the shapes here. Given the proposed quantum kernel method, the standard techniques of gradient descent method should also work to learn the kernels, since complex value kernels are also continuous and differentiable. Each layer of the networks carries twice as many parameters, since complex numbers are a compact notation for two numbers,
10

Under review as a conference paper at ICLR 2018
but the trust of the work is to suggest that they may perform better and reduce the size of the entire network. These are just speculations and more investigation of the details that entice such a construction are needed. Note that many articles in the past have mentioned "quantum" and "neural networks" together. Several of them use Schro¨dinger equation, a quantum physics modeling of the world. Here in no point we visited a concept in physics (forces, energies), as Schro¨dinger equation would imply, the only model is the one of shapes (computer vision model). Quantum theory is here used as an alternative statistical method, a purely mathematical construction that can be applied to different models and fields, as long as it brings benefits. Also, in our search, we did not find an article that explores the phenomena of interference and demonstrate its advantage in neural networks. The task of brining quantum ideas to this field must require demonstrations of its utility, and we think we did that here.
REFERENCES
R. Feynman. The Feynman Lectures on Physics, volume 3. Addison Wesley, 1971. R. Feynman and A. Hibbs. Quantum Mechanics and Path Integrals: Emended by D. F. Steyer. Dover
Publications, 2012. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097­1105. 2012. Y. LeCun. A learning scheme for assymetric threshold network, 1985. Y. LeCun, Y. Bengio, and G. Hinton. Deep learning, 2015. T. Lindeberg. Scale-space theory: A basic tool for analysing structures at different scales. Journal of Applied Statistics (Supplement on Advances in Applied Statistics: Statistics and Images: 2), 2 (21):224270, 1994. W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, and A. Berg. SSD: Single Shot MultiBox Detector, pp. 21­37. Springer International Publishing, 2016. D. Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating error, 1986. K. Siddiqi and B. B. Kimia. A shock grammar for recognition. Computer Vision and Pattern Recognition, 1996. Proceedings CVPR '96, 1996. G. C. Wick. Properties of Bethe-Salpeter wave functions. Phys. Rev., 96:1124­1134, Nov 1954. A. P. Witkin. Scale-space filtering. Proc. 8th Int. Joint Conf. Art. Intell., pp. 10191022, 1983.
11

