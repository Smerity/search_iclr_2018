Under review as a conference paper at ICLR 2018
GENERALIZATION OF LEARNING USING RESERVOIR COMPUTING
Anonymous authors Paper under double-blind review
ABSTRACT
We investigate the methods by which a Reservoir Computing Network (RCN), trained to classify pairs of images as `similar', `transformed' or `different', learns the relationships between the images and generalizes these concepts to previously unseen types of data. One of our motivations is to explore how biologically realistic features, like recurrent relationships between neuron-like units and resemblance to neural dynamics (key features of RCNs), contribute to the learning capabilities of Artificial Neural Networks (ANNs). Specifically, we show that an RCN trained to identify strong similarities or transformations between image-pairs drawn from a subset of digits from the MNIST database generalizes the learned transformations to images of digits unseen during training. We observe that the high dimensional reservoir states generated from an input image pair with a specific transformation converge over time to a unique relationship. Thus, as opposed to training the entire high dimensional reservoir state, the reservoir only needs to train on these unique relationships, allowing the reservoir to perform well with very few training examples. The directions of the principal component of the time-projected reservoir states representing input image pairs with the same relationship are aligned closer together than those representing image pairs with different relationships. Thus, generalization of learning to unseen images is interpretable in terms of clustering of the reservoir state onto the attractor corresponding to the transformation in reservoir space. We find that RCNs can identify and generalize linear and nonlinear transformations, and combinations of transformations, naturally and be a robust and effective image classifier. Additionally, RCNs perform significantly better than state of the art neural network classification techniques such as deep siamese neural networks in generalization tasks both on the MNIST dataset and more complex depth maps of visual scenes from a moving camera. This helps bridge the gap between machine learning and the biological problem of human cognition, and points to new directions in the investigation of learning processes.
1 INTRODUCTION
Different types of Artificial Neural Networks (ANNs) have been used through time for the task of object recognition and classification. Feed-forward structures, such as convolutional neural networks (Krizhevsky et al., 2012), deep learning (LeCun et al., 2015), stacked auto encoders (Vincent et al., 2010) etc. have been extensively studied and are the state of the art for non-temporal problems of image classification. These architectures are well understood due to their feed-forward and non-dynamic nature. However, biological systems such as the brain are known to have recurrent connections.
In addition, biological systems learn visual concepts through analogies and comparisons, using only a handful of examples (Urcuioli et al., 2014). In particular, in Giurfa et al. (2001), bees were trained to fly towards the image from a pair of images that looked very similar to a previously displayed base image. On training bees to fly towards the visually similar image, the bees were presented with two scents, one very similar and one drastically different from a base scent they were exposed to. As a consequence of the visual training that induced preference to the very similar category, the bees flew towards the very similar scent. Thus, biological systems have been found to translate learning of concepts of similarity across sensory inputs, leading us to believe that the brain has a common and fundamental mechanism that helps with comprehension through analogies or through concepts
1

Under review as a conference paper at ICLR 2018
of `similarity'. Deriving inspiration from nature, we hope to develop a model that captures this generalization of learning.
In our framework, we refer to generalization as the ability of a system to learn the relationships or transformations, both linear and non-linear, between a pair of images and be able to recognize the same transformation in image pairs that it hasn't seen before. Feed-forward networks have, to the best of our knowledge, not been successful in developing an explainable model for this generalization of learning. In addition, neither is learning of individual stand-alone images without drawing comparisons biologically plausible nor are feed forward networks, in the absence of recurrent connections and dynamics suited to study the problem of generalization. Further, increasingly powerful graphical processing units (GPU) and efficient implementations are needed for feed-forward or deep neural networks and hence they do not scale well. In addition it seems reasonable to say that humans learn through relatively few training examples compared to deep learning. For instance, a child would learn the features of a horse and the difference between a horse and a donkey, simply by observing at a handful of examples, contrary to a deep neural network that would require a training set of size several orders of magnitude higher to learn with human-like accuracy. While research in learning from very few images, one shot learning (Wan et al., 2013; Fei-Fei et al., 2003) etc. has gained momentum in the past couple of years, integrating it with generalization of learning is a relatively unexplored area. Thus, in order to be biologically plausible, the model would need to have recurrent connections, train reasonably well on small datasets, and be able to generalize or extrapolate the relationship it learns between pairs of things to things it has never seen before.
Introducing recurrent connections in a system is one way to make learning more biological in an attempt to study the problem of generalization. In the ground-breaking work of Hopfield in Hopfield (1988), the success of the Recurrent Neural Network (RNN) depends on the existence of attractors. The system is trained offline, such that the dynamical system of the RNN is left running until it ends up in one of its several attractors. Thus an RNN can be interpreted as a dynamical systems model. However, training of RNNs is difficult due to changes in weights of the recurrent connections and problems like the vanishing gradient (Pascanu et al., 2013). Buonomano and Merzenich (1995) showed that much slower dynamics can be introduced in the RNN by using a random network of spiking neurons with short term plasticity, thus allowing the system to work with training of only the output weights. Exploiting this property, Echo State Networks (ESN) (Jaeger, 2001) and Liquid State Machine (LSM) (Maass et al., 2002), commonly falling under the term Reservoir Computing (RC) were introduced. RC is appealing because of its dynamical property and easy scalability since the recurrent connections in the network aren't trained. Applications of RC include many real world phenomena such as weather or stock market prediction, self driven cars, speech processing and language interpretation, gait generation and motion control in robots etc.
RCNs are known to perform better than feedforward networks for generating chaotic dynamics (Jaeger and Haas, 2004). Models of spontaneously active neural circuits typically exhibit chaotic dynamics, as do RNNs (Sompolinsky et al., 1988). For instance, chaotic dynamics is found in spiking models of spontaneous activity in cortical circuits (van Vreeswijk and Sompolinsky, 1996; J Amit and Brunel, 1997). Additionally, spatio-temporal `echo' (memory) properties in ESNs have also been found to resemble dynamics in the primary cortex of monkeys and cats (Bartlett and Wang, 2005; Brosch and Schreiner, 2001), and can be thought of as modeling the influence of past stimuli in the processing of subsequent stimuli. This biological plausibility coupled with their dynamical nature, leads us to believe that RCNs might work well to generalize learning, as bees and other animals do. In this work, we train RCNs on both the MNIST handwritten digit database as proof of concept and depth maps from a moving camera, to study generalization of the transformations between pairs of images learnt by the RCN. We use a RC approach to classify pairs of images into very similar, rotated, zoomed, blurred or different. The reservoir activity is then studied to reveal the underlying features of the activity that are responsible for classification. We find that the relationships between reservoir-state pairs corresponding to input image pairs converge for image pairs with a common relationship between them. In other words, the reservoir only learns relationships between the images, not features of the individual images themselves. This allows for generalization of the learnt relationships to all image pairs, seen and unseen by the reservoir. Additionally we compare its performance for a generalization task to a pair-based deep siamese neural network built on the keras implementation (Chollet, 2015) and show that the reservoir performs significantly better, both for simpler MNIST images as well as for depth maps of more complex visual scenes from a moving camera. We also show that the reservoir is able to recognize linear combinations of the individuals
2

Under review as a conference paper at ICLR 2018

transformations it has learned. This work can useful in the field of computer vision to identify similar transformations between images, even if they are non-linear as in a moving camera, in a biological plausible and computationally efficient way.

2 METHODS

2.1 NETWORK ARCHITECTURE

In this work we use the Echo State Network (ESN) class of RCNs for training and classification. RCNs are neural network with two layers: a hidden layer of recurrently interconnected non-linear nodes, driven both by inputs as well as by delayed feed-backs from other nodes in the reservoir layer and an output or readout layer (Fig. 1(a)). Our implementation of the ESN is guided by Lukosevicius (2012). The reservoir #ca,,n be though of as a dynamical system where the reservoir is described by a reservoir state vector r(t) at time t given by :

# r(t

+

,, 1)

=

tanh

(W

in

·

#,, u(t)

+

W

res

·

#,, r(t)

+

b)

(1)

The input weights matrix W in  RNR×Nu , where NR is number of nodes in the reservoir and Nu is

(a) (b)
#,, #,, Figure 1: (a) Reserv#oir,,architecture with input state at time t denoted by u(t), reservoir state by r(t) and output state by s(t). (b) shows one image from the MNIST data split vertically and fed into the reservoir in columns of 1 pixel width, shown to be larger here for ease of visualization.
#,, #,, the dimension of the input vector u(t). The activity of the reservoir at time t is given by r(t), of size NR. The recurrent connection weights W res  RNR×NR are initialized randomly between -1 and 1. b is a scalar bias. Only the output weights are trained. The reservoir, being a dynamical system, works particularly well for analyzing time-series input data. The input images are hence converted into a `time-series' by feeding the reservoir a column of the input image at each time point. We use hyperbolic tangent as the non-linear activation function. We set the spectral radius  (maximal absolute eigenvalue of W res to be < 1 to ensure that the reservoir is operating in the chaotic regime (Vengamoorthy G.K., 2009). The reservoir is a dynamical system that transforms the low dimensional input into a much higher dimensional reservoir space and reaches its optimal performance even when the W out and W res are sparse. We use a sparsity of 0.9 unless otherwise stated.

2.2 IMPLEMENTATION ON DATASET

In this section, we outline the steps for classification of images from the handwritten digit database MNIST. The MNIST database consists of 70000 images, each 28×28 pixels in size, of digits 0-9. In order to exploit the dynamical system properties of RCNs, the input must be a time series. Hence, we input our images column by column (Fig. 1(b)), allowing the time axis to run across the rows of the image. The size of the input vector is 28 and each image in input through 28 timesteps. The reservoir state for an image x is th#en,,formed by concatenating the reservoir state (the state of all reservoir nodes) at every timestep r(t) as follows:

x

=

#,, r(0)



# r(t

=

,, 1)



.

.

.



# r(t

=

,, 28).

(2)

x is a matrix of size NR × c where c is the number of columns in the image (number of time steps through which the entire image is input).

3

Under review as a conference paper at ICLR 2018
2.3 READOUT LAYER
Under this framework, images are always considered in pairs. We classify pairs of input images (base image and transformed image) into one of 5 labels: very similar, rotated, zoomed, blurred, and different. Very similar: Two different images of the same digit are taken directly from the MNIST database (Ex. Fig 2(a)). These images are related to a small non-linear transformation. To demonstrate that our method is robust to noise, we superimpose a random small noise on the transformed image with peak value given by 20 percent of the peak value of the base image. Rotated: Two different images of the same digit are taken. The transformed image is 90 rotated (Ex. Fig 2(b)) Zoomed: The transformed image is zoomed to twice its size and the central portion of size equal to size of the base pair (28 × 28 for MNIST) is selected (Ex. Fig 2(c)). Blurred: The transformed image is blurred (Ex. Fig 2(d)) by convolving every pixel of the image by a 5 × 5 convolution matrix with all values 1/25: Different: Two different images of different digits from the MNIST database (Ex. Fig 2(e)). All pairs are characterized by the relationship between the base and transformed image. For instance, we call a pair rotated if one of the images is rotated with respect to the other.

Figure 2: Pairs of images that are representative of the transformations classified into five labels: (a) very similar, (b) rotated, (c) zoomed, (d) blurred and (e) different.

The readout layer in our experiment is a vector with five elements. While training, the difference
between the reservoir states corresponding to a pair of images (differential reservoir state) is classified into one of the five labels. The differential reservoir state is given by Xk = Xk(i,j) = xi - xj, where xi is the reservoir state of the ith image. The readout layer representation for a very similar pair is (1, 0, 0, 0, 0), rotated pair is (0, 1, 0, 0, 0), zoomed pair is (0, 0, 1, 0, 0), blurred pair is (0, 0, 0, 1, 0) and different pair is (0, 0, 0, 0, 1). While testing, the reservoir allots a fractional probability to each
output label, and the image pair is classified into the label with the highest probability.

2.4 RIDGE REGRESSION AND TRAINING

Only weight matrix W out is optimized during training such that it minimizes the mean squared error E(y, Y ) between the output of the reservoir y and the target signal Y . The reservoir output is:

Y = W outX

(3)

W out  RNy×NR where Ny is the dimensionality of the readout layer.

X or the concatenated reservoir state is the matrix containing all differential reservoir states during
training phase, X = X0  X1  . . .  XM where M is the total number of training images and Y = Y0  Y1  . . .  YM is the matrix containing the corresponding readout layer for all images. The most common way to compute W out is to use Ridge Regression (or Thikonov regularization)
(Wyffels et al., 2008), which adds an additional small cost to least square error, thus making the system robust to overfitting and noise. Ridge regression calculates W out by minimizing squared error J(W out) while regularizing the norm of the weights as follows:

J (W out) = |W out|2 + ((W out)T Xi - Yi)2.

(4)

i

where X is the concatenated reservoir state over input image pairs, Y contains the corresponding

label representations and the summation is over all training image pairs. The stationary condition is

J W out

= W out

+

(XXT + I)W out = XY.

((W out)T Xi - Yi)X = 0.
i

W out = (XXT + I)-1XY.

(5)
(6) (7)

where  is a regularization constant and I is the identity matrix.

4

Under review as a conference paper at ICLR 2018
3 RESULTS
3.1 GENERALIZATION TO UNTRAINED IMAGES In this section we present the performance of the reservoir in identifying the relationships between test image pairs of digits 6-9 from the MNIST dataset when trained on the five relationships in section 2.3 on image-pairs of digits 0-5. A biologically reasonable system is expected to train with relatively few training examples. We use fraction correct (1- error rate) as a metric of performance.
Figure 3: Fraction correct against (a) training set size (b) reservoir size and (c) spectral radius . (a)  = 0.5, reservoir size=1000 nodes (b) training size=250 pairs,  = 0.5 and (c) reservoir size = 1000 nodes, training size=250 pairs. Training data: digits 0-5, testing data: digits 6-9, sparsity = 0.9. In Fig. 3(a), we see that the reservoir performance increases with training set size and the slope of performance improvement is inversely related to the training set size. A training set size of 250 image pairs gives a reasonable trade-off between performance and computational efficiency. This is significantly lower than the training set sizes typically used in deep learning. Fig. 3(b) shows that for a constant training data size (250 pairs) the performance increases as expected with reservoir size upto around 750 nodes after which it saturates at 0.85 with minor fluctuations. This is drastically better than random. Further, we look at the reservoir performance as a function of  in Fig. 3(b).  is varied from 0 to 1 while looking for the optimal performance region where the reservoir has memory or is in the `echo state' (Vengamoorthy G.K., 2009), however we find no indicative pattern. We find that even a sparse reservoir is able to generalize the transformations with high accuracy to untrained images with very few training image-pairs. For completion, Fig. 4 shows reservoir activity as well as single node activity for all labels. We see that the individual node itself doesn't encode any decipherable information. However each output label has a different signature in reservoir space.
Figure 4: (a), (b), (c), (d), (e) show the differential reservoir activity of 200 nodes over 28 timesteps for input transformations very similar, rotated, zoomed, blurred and different respectively. (f) shows the output weight matrix(W out) for 50 reservoir nodes. (g) shows activity of a random node for all output labels over 28 timesteps. Reservoir size: 1000 nodes.  = 0.5, sparsity= 0.9.
5

Under review as a conference paper at ICLR 2018

3.2 PRINCIPAL COMPONENT ANALYSIS
A possible explanation for the ability of the reservoir to learn relationships between pairs of images and generalize to unseen images comes from dynamical systems theory. The reservoir converts the image from the input image space to the reservoir space through a non-linear operation. In order to generalize, for a given relationship between the input image pairs, there must be a corresponding relationship between the reservoir activity, dependent only on the relationship between the input images and not on the input images themselves. From a non-linear dynamical perspective, this is analogous to there existing an attractor in reservoir space. The relationship between the reservoir state pair, also called the differential reservoir activity, can be thought to converge onto the attractor that represents the relationship between the input image pair.
In order to study how generalization occurs in the reservoir, we looked at the reservoir states for image-pairs with a common relationship. We calculated the differential reservoir states for a pair of images i and j (Xkp(i,j)) with the relationship (very similar, rotated, zoomed, blurred or different) between them denoted by p = 1, 2 . . . 5. As in section 2.2, xl(mn) is the differential reservoir state value of the mth reservoir node at time n for input image l; Xkp(i,j) = xi -xj. We then average over several over these differential states for a given transformation p to obtain the averaged differential reservoir matrix < Xp > of size NR× time (28 for MNIST). We are interested in studying whether the reservoir state corresponding to a particular input image transformation converges over time, analogous to an attractor. Hence, we study the correlation between the averaged differential reservoir activity for different transformations by looking at the time-projected reservoir state. The temporal projection of the average reservoir state is given by T p = Xp · (Xp)T . Matrix T p is of size 28 × 28 for MNIST images. The principal component (PC) of T p are calculated and the overlap between the directions of the most significant component for different relationships (different p) are found from their dot product.

(a) Ts1 Ts2 Ts3 Ts4 Ts5 Td1 0.917 0.703 0.670 0.877 0.669 Td2 0.781 0.906 0.688 0.724 0.733 Td3 0.743 0.609 0.898 0.662 0.875 Td4 0.904 0.392 0.628 0.906 0.630 Td5 0.714 0.312 0.856 0.631 0.907

(b) Ts1 Ts2 Ts3 Ts4 Ts5 Ts1 0.924 0.806 0.737 0.889 0.704 Ts2 0.797 0.924 0.803 0.731 0.788 Ts3 0.728 0.813 0.923 0.676 0.888 Ts4 0.910 0.770 0.695 0.933 0.663 Ts5 0.707 0.808 0.827 0.648 0.925

Table 1: Dot product between principal component of time-projected reservoir states averaged over 10 trials for (a, left) different digits and (b, right) same digits. Superscript numbering corresponds to very similar, rotated, zoomed, blurred, or different respectively. Reservoir size = 1000,  = 0.5.

Tables 1(a) and (b) show the dot product of the first principal component between the time-projected reservoir activity of image pairs of different digits and the same digit respectively over all transformations (denoted by the superscript), each averaged over 10 samples. Different subscripts in Ts and Td denote time-projected states obtained from input pairs of two different digits. We observe that, irrespective of the digit, the overlap in PC direction for the same transformation (diagonal) is higher than that for different transformations. In addition, the diagonal values (same transformation) are found to be slightly higher for the same digit as compared to different digits, as expected.
We interpret this from a dynamical systems perspective as the convergence of the differential reservoir state onto an attractor in the reservoir space. Each transformation corresponds to a specific attractor. Reservoir state of image pairs with the same transformation cluster together. Thus, we infer, that he reservoir is in fact, simply training these attractors as opposed to training the entire reservoir space. This explains why a much smaller training set performs fairly well. By identifying the attractor into which the reservoir converges for untrained image pairs, the reservoir is able to generalize.

3.3 COMBINING TRANSFORMATIONS
In the section we study the ability of the reservoir to identify all transformations involved in an image made from a linear combination of multiple transformations (Ex. rotated as well as blurred). To illustrate our result better, here we have restricted the output labels (categories) of our system to very similar, blurred and rotated, defined as in section 2.2. The training is done on the three

6

Under review as a conference paper at ICLR 2018
Figure 5: Label probability for images that are (a) both rotated and blurred, (b) rotated only. =0.8, reservoir size=1000. Training digits: 0-5, testing digits: 6-9. Fraction correct: (a) 0.986 (b) 0.989.
individual transformations (very similar, blurred, rotated) for digits 0-5. Testing is done on combined transformations (rotation and blurring) as well as pure rotation for digits 6-9. For image-pairs with two transformations applied simultaneously, we consider the reservoir to have classified correctly if the two highest probabilities correspond to the the two applied transformations. In Fig. 5 (a) we plot the probability of each label, over 500 iterations of image-pairs of digits 6-9 that are both rotated and blurred simultaneously. We observe that the reservoir categorizes them as rotated with a highest probability, followed by blurred, i.e., the reservoir learns rotation better than blurring. The performance of the reservoir in terms of fraction correct (where classification as either blurred or rotated is considered to be correct) is very high at 0.986. Fig. 5 (b) shows that performance is comparable at 0.989 when tested on purely rotated images of digits 6-9. We observe that in this case, the probabilities of the very similar and blurred labels are about the same and significantly lower than the rotated label, as expected. Thus, we conclude that while reservoir may be have a bias towards certain transformations, given an image with combined transformations, not only is the reservoir able to pick out the individual transformations involved, but also generalize this learning of transformations to image-pairs of digits unseen by the reservoir.
3.4 COMPARISON WITH DEEP SIAMESE NETWORK
The topic of generalized learning has, to the the best of our knowledge, not been addressed using a dynamical systems approach. To validate our model, we compare the performance of the reservoir with a deep Siamese Neural Network (SNN), a successful pair-based machine learning technique. Their performance is compared for two binary classification tasks. For these tasks, we use a reservoir of 1000 nodes with  = 0.5 and sparsity 0.9. We compare the performance of this reservoir with an equivalent with 8 layers of 128 nodes each, built on the keras (Chollet, 2015) framework. Training is done for 40 epochs on the SNN and once on the reservoir on 300 image pairs. The two tasks are:
3.4.1 GENERALIZED LEARNING OF THE ROTATION OPERATOR
We train the reservoir on a simple binary classification task, i.e., classify an image pair from the MNIST dataset as rotated or not. Our training set consists of rotated and not rotated images of digits 0-5. We then compare performance of both the reservoir and the SNN while classifying image pairs of the digits 6-9 as rotated or not rotated. Table 2(a) shows the relative performance of the reservoir and the deep SNN. We observe that, while their performance is comparable on digits that the training has occurred on (digits 0-5), the SNN seems to classify randomly for untrained digits (6-9). Performance didn't improve on increasing the network depth of the SNN. The reservoir performance remains equally good over trained digits (0-5) and untrained digits (6-9), showing that the reservoir is learning the underlying transformation in the pairs and not the individual digits themselves. As seen in section 3.2, the superior performance of the reservoir can be attributed to the convergence of the dynamical reservoir state for all rotated images, a concept analogous to that of an attractor in dynamical systems. In contrast, the deep SNN isn't a dynamical system, and training occurs explicitly on the images as opposed to the classes of transformations, leading to poorer performance while generalizing.
7

Under review as a conference paper at ICLR 2018

3.4.2 GENERALIZING SIMILARITIES IN DEPTH PERCEPTION FROM A MOVING CAMERA
Identifying similarities in scenes, objects in scenes or properties of scenes such as depth, style etc. from a moving camera is an important problem in the field of computer vision (Adachi et al., 2007; Chen and Lin, 2014). We are interested in studying how the reservoir could learn and generalize relationships between images from a moving camera, frames of which may be non-linearly transformed with respect to each other. To demonstrate the practicality of our method, we implement it on depth maps from 6 different visual scenes recorded indoors in an office setting. Each visual scene has depth maps from 300 images, recorded as the camera is moved within a small distance (30cm) and rotated within a small angle (30). We then train the systems to identify pairs of depth-maps as very similar (same visual scene) or different (different visual scenes). Training is done on 100 images each from the first three visual scenes. We study whether the systems are able to generalize, i.e., identify relationships between depth maps from the other three visual scenes. Table 2(b) shows the reservoir performs significantly better on untrained scenes than the SNN, which classifies randomly. Both systems have a comparable and very high performance on the trained scenes. Thus, the reservoir is able to identify frames with similar depth maps from scenes it hasn't seen before. This has potential applications in scene or object recognition using a moving camera.

Siamese deep NN Reservoir

Digits 6-9 (untrained) Digits 0-5 (trained)

0.52 0.90

0.84 0.86

Siamese deep NN Reservoir

Scenes 4-6 (untrained) Scenes 0-3 (trained)

0.50 0.99

0.96 0.98

Table 2: Performance (fraction correct) comparison between an SNN and an RCN for (a) rotated or not rotated MNIST images and (b) very similar or different depth maps from a moving camera.

4 CONCLUSION AND FUTURE WORK

In this paper we have used RCNs to solve a class of image classification problems that generalize learning of relationships between images using a rather small training data. While image classification has been studied extensively before, here we present a biologically plausible method that's unique in two ways. Not only does it generalize learning, but also allows us to interpret the results analytically through a dynamical systems lens. We see that the differential reservoir states obtained from input image-pairs with a common transformation have principal components that are aligned closer together. From a dynamical systems perspective, this can be interpreted as the existence of attractors in reservoir space, each corresponding to a given image transformation. Thus, by reducing the dimensionality of the reservoir space, the reservoir as a dynamical system allows us to train on a much smaller training dataset, whereas contemporary methods such as deep learning require much larger datasets due to the complexity of the hidden layers and lack of dynamics. This same property also allows the reservoir to generalizes the relationships learnt to images it hasn't seen during training.
In a reservoir, the image space is mapped onto the reservoir space in a way as to preserve the locality of common transformations in reservoir space. In addition, the reservoir performs significantly better than a deep SNN for the task of generalization. From a computation perspective, the reservoir is fast since only the output weights are being trained and the reservoir is sparsely connected. Further, we argue that our method is biologically plausible since the dynamics of the reservoir have been shown to resemble the neural activity in the primate cortex. Additionally, we use a training method that doesn't train a exploits similarities and differences in images, which has been shown to relate to how animals such as bees learn, i.e., by drawing analogies to things they already know. We conclude that although state of the art machine learning techniques such as deep neural networks work exceedingly well for image classification, they do not work as well for generalization of learning. In generalized learning, RCNs outperform them, due to their ability to function as a dynamical system with `memory'.
Thus, we see the strength of our work as lying in not only its ability to generalize to untrained images, but also our ability to explain this in terms of the reservoir dynamics and PCA. This relates to new ideas in explainable Artificial Intelligence, a topic that continues to receive traction. An interesting direction would be to explore different reservoir architectures that model the human brain better. Another interesting direction would be to use RCNs to study videos which are naturally temporal, and and investigate how the reservoir generalizes in the action domain. Finally, although we get a fairly good performance with a sparse reservoir and few training images, we predict that as the image complexity increases, a more sophisticated reservoir would be required to match performance.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Adachi, T., Kondo, K., Kobashi, S., and Hata, Y. (2007). Identification of a scene by estimating the position and pose of a moving camera.
Bartlett, E. and Wang, X. (2005). Long-lasting modulation by stimulus context in primate auditory cortex.
Brosch, M. and Schreiner, C. (2001). Sequence sensitivity of neurons in cat primary auditory cortex. 10:1155­67.
Buonomano, D. V. and Merzenich, M. (1995). Temporal information transformed into a spatial code by a neural network with realistic properties. Science, 267(1).
Chen, X.-W. and Lin, X. (2014). Big data deep learning: Challenges and perspectives.
Chollet, F. e. a. (2015). A keras implementation of siamese neural net.
Fei-Fei, L., Fergus, R., and Perona, P. (2003). A bayesian approach to unsupervised one-shot learning of object categories. In Proceedings of the Ninth IEEE International Conference on Computer Vision - Volume 2, ICCV '03.
Giurfa, M., Zhang, S., Jenett, A., Menzel, R., and V. Srinivasan, M. (2001). The concepts of `sameness' and `difference' in an insect. 410:930­933.
Hopfield, J. J. (1988). Neurocomputing: Foundations of research. chapter Neural Networks and Physical Systems with Emergent Collective Computational Abilities.
J Amit, D. and Brunel, N. (1997). Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex cerebral cortex 7 237-52. 7:237­52.
Jaeger, H. (2001). The" echo state" approach to analysing and training recurrent neural networks-with an erratum note'. 148.
Jaeger, H. and Haas, H. (2004). Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication. Science, 304 5667:78­80.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems Volume 1, NIPS'12, pages 1097­1105.
LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. 521:436­44.
Lukosevicius, M. (2012). A practical guide to applying echo state networks. In Neural Networks: Tricks of the Trade.
Maass, W., Natschläger, T., and Markram, H. (2002). Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural Comput., 14(11).
Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training recurrent neural networks.
Sompolinsky, H., Crisanti, A., and Sommers, H. J. (1988). Chaos in random neural networks. Phys. Rev. Lett., 61:259­262.
Urcuioli, P. J., Wasserman, E. A., and Zentall, T. R. (2014). Associative concept learning in animals: Issues and opportunities. Journal of the Experimental Analysis of Behavior, 101(1):165­170.
van Vreeswijk, C. and Sompolinsky, H. (1996). Chaos in neuronal networks with balanced excitatory and inhibitory activity. Science, 274(5293):1724­1726.
Vengamoorthy G.K., S. B. (2009). Effects of spectral radius and settling time in the performance of echo state networks. Neural Networks.
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Mach. Learn. Res., 11:3371­3408.
Wan, J., Ruan, Q., Li, W., and Deng, S. (2013). One-shot learning gesture recognition from rgb-d data using bag of features. J. Mach. Learn. Res., 14(1):2549­2582.
Wyffels, F., Schrauwen, B., and Stroobandt, D. (2008). Stable output feedback in reservoir computing using ridge regression. In Proceedings of the 18th International Conference on Artificial Neural Networks, Part I, ICANN '08, pages 808­817.
9

