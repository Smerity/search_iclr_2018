Under review as a conference paper at ICLR 2018
TRAINING CONFIDENCE-CALIBRATED CLASSIFIERS FOR DETECTING OUT-OF-DISTRIBUTION SAMPLES
Anonymous authors Paper under double-blind review
ABSTRACT
The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.
1 INTRODUCTION
Deep neural networks (DNNs) have demonstrated state-of-the-art performance on many classification tasks, e.g., speech recognition (Hannun et al., 2014), image classification (Girshick, 2015), video prediction (Villegas et al., 2017) and medical diagnosis (Caruana et al., 2015). Even though DNNs achieve high accuracy, it has been addressed (Lakshminarayanan et al., 2017; Guo et al., 2017) that they are typically overconfident in their predictions. For example, DNNs trained to classify MNIST images often produce high confident probability 91% even for random noise (see the work of (Hendrycks & Gimpel, 2016)). Since evaluating the quality of their predictive uncertainty is hard, deploying them in real-world systems raises serious concerns in AI Safety (Amodei et al., 2016), e.g., one can easily break a secure authentication system that can be unlocked by detecting the gaze and iris of eyes using DNNs (Shrivastava et al., 2017).
The overconfidence issue of DNNs is highly related to the problem of detecting out-of-distribution: detect whether a test sample is from in-distribution (i.e., training distribution by a classifier) or outof-distribution sufficiently different from it. Formally, it can be formulated as a binary classification problem. Let an input x  X and a label y  Y = {1, . . . , K} be random variables that follow a joint data distribution Pin (x, y) = Pin (y|x) Pin (x). We assume that a classifier P (y|x) is trained on a dataset drawn from Pin (x, y), where  denotes the model parameter. We let Pout (x) denote an out-of-distribution which is `far away' from in-distribution Pin (x). Our problem of interest is determining if input x is from Pin or Pout, possibly utilizing a well calibrated classifier P (y|x). In other words, we aim to build a detector, g (x) : X  {0, 1}, which assigns label 1 if data is from in-distribution, and label 0 otherwise.
There have been recent efforts toward developing efficient detection methods where they mostly have studied simple threshold-based detectors utilizing a pre-trained classifier (Hendrycks & Gimpel, 2016; Liang et al., 2017). For each input x, it measures some confidence score q(x) based on a pre-trained classifier, and compares the score to some threshold  > 0. Then, the detector assigns label 1 if the confidence score q(x) is above , and label 0, otherwise. Specifically, (Hendrycks & Gimpel, 2016) defined the confidence score as a maximum value of the predictive distribution,
1

Under review as a conference paper at ICLR 2018

and (Liang et al., 2017) further improved the performance by using temperature scaling (Guo et al., 2017) and adding small controlled perturbations to the input data. Although such inference methods are computationally simple, their performances highly depend on the pre-trained classifier. Namely, they fail to work if the classifier does not separate the maximum value of predictive distribution well enough with respect to Pin and Pout. Ideally, a classifier should be trained to separate all class-dependent in-distributions as well as out-of-distribution in the output space. As another line of research, Bayesian probabilistic models (Li & Gal, 2017; Louizos & Welling, 2017) and ensembles of classifiers (Lakshminarayanan et al., 2017) were also investigated. However, training or inferring those models are computationally more expensive. This motivates our approach of developing a new training method for the more plausible simple classifiers. Our direction is orthogonal to the Bayesian and ensemble approaches, where one can also combine them for even better performance.
Contribution. In this paper, we develop such a training method for detecting out-of-distribution Pout better without losing its original classification accuracy. First, we consider a new loss function, called confidence loss. Our key idea on the proposed loss is to additionally minimize the KullbackLeibler (KL) divergence from the predictive distribution on out-of-distribution samples to the uniform one in order to give less confident predictions on them. Then, in- and out-of-distributions are expected to be more separable. However, optimizing the confidence loss requires training samples from out-of-distribution, which are often hard to sample: a priori knowledge on out-of-distribution is not available or its underlying space is too huge to cover. To handle the issue, we consider a new generative adversarial network (GAN) (Goodfellow et al., 2014) for generating most effective samples from Pout. Unlike the original GAN, the proposed GAN generates `boundary' samples in the low-density area of Pin. Finally, we design a joint training scheme minimizing the classifier's loss and new GAN loss alternatively, i.e., the confident classifier improves the GAN, and vice versa, as training proceeds. Here, we emphasize that the proposed GAN does not need to generate explicit samples under our scheme, and instead it implicitly encourages training a more confident classifier.
We demonstrate the effectiveness of the proposed method using deep convolutional neural networks such as AlexNet (Krizhevsky, 2014) and VGGNet (Szegedy et al., 2015) for image classification tasks on CIFAR (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al., 2009), and LSUN (Yu et al., 2015) datasets. The classifier trained by our proposed method drastically improves the detection performance of all threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017) in all experiments. In particular, VGGNet with 13 layers trained by our method improves the true negative rate (TNR), i.e., the fraction of detected out-of-distribution (LSUN) samples, compared to the baseline: 14.0%  37.8% and 46.3%  99.9% on CIFAR-10 and SVHN, respectively, when 95% of in-distribution samples are correctly detected. We also provide visual understandings on the proposed method using the image datasets. We believe that our method can be a strong guideline when other researchers will pursue these tasks in the future.

2 TRAINING CONFIDENT NEURAL CLASSIFIERS

2.1 CONFIDENT CLASSIFIER FOR OUT-OF-DISTRIBUTION

We propose a new loss function to train a classifier which can map the samples from in- and outof-distributions into the output space separately. Without loss of generality, suppose that the cross entropy loss is used for training. Then, we define the following, termed confidence loss:

min


EPin (x,y )

- log P (y = y|x)

+ EPout(x) KL (U (y)

P (y|x)) ,

(1)

where KL denotes the Kullback-Leibler (KL) divergence and U (y) is the uniform distribution. It is highly intuitive as the new loss forces the predictive distribution on out-of-distribution samples to be closer to the uniform one, i.e., zero confidence, while that for samples from in-distribution still follows the label-dependent probability. In other words, the proposed loss is designed for assigning higher maximum prediction values to in-distribution samples than out-of-distribution ones. Here, a caveat is that adding the KL divergence term might degrade the classification performance. However, we found that it is not the case due to the high expressive power of deep neural networks, while inand out-of-distributions become more separable with respect to the maximum prediction value by optimizing the confidence loss (see Section 3.1 for supporting experimental results).

We remark that minimizing a similar KL loss was studied recently for different purposes (Lee et al., 2017; Pereyra et al., 2017). Training samples for minimizing the KL divergence term is explicitly

2

Under review as a conference paper at ICLR 2018

Class 0 Class 1

Class 0 Class 1

Class 0 Class 1
(a)

[0,0.2)

[0.2,0.8) [0.8,1)

(b)

Class 0 Class 1
(c)

[0,0.2)

[0.2,0.8) [0.8,1)

(d)

Figure 1: Illustrating the behavior of classifier under different out-of-distribution training datasets. We generate the out-of-distribution samples from (a) 2D box [-50, 50]2, and show (b) the corre-
sponding decision boundary of classifier. We also generate the out-of-distribution samples from (c) 2D box [-20, 20]2, and show (d) the corresponding decision boundary of classifier.

given in their settings while we might not. Ideally, one has to sample all (almost infinite) types of outof-distribution to minimize the KL term in (1), or require some prior information on testing out-ofdistribution for efficient sampling. However, this is often infeasible and fragile. To address the issue, we suggest to sample out-of-distribution close to in-distribution, which could be more effective in improving the detection performance, without any assumption on testing out-of-distribution.
In order to explain our intuition in details, we consider a binary classification task on a simple example, where each class data is drawn from a Gaussian distribution and entire data space is bounded by 2D box [-50, 50]2 for visualization. We apply the confidence loss to simple fully-connected neural networks (2 hidden layers and 500 hidden units for each layer) using different types of outof-distribution training samples. First, as shown in Figure 1(a), we construct an out-of-distribution training dataset of 100 (green) points using rejection sampling on the entire data space [-50, 50]2. Figure 1(b) shows the decision boundary of classifier optimizing the confidence loss on the corresponding dataset. One can observe that a classifier still shows overconfident predictions (red and blue regions) near the labeled in-distribution region. On the other hand, if we construct a training out-of-distribution dataset of 100 points from [-20, 20]2, i.e., closer to target, in-distribution space (see Figure 1(c)), a classifier produces confident predictions only on the labeled region and zero confidence on the remaining in the entire data space [-50, 50]2 as shown in Figure 1(d). This implies that training out-of-distribution samples nearby the in-distribution region could be more effective in improving the detection performance. Our underlying intuition is that the effect of boundary of indistribution region might propagate to the entire out-of-distribution space. Our experimental results in Section 3.1 also support this: realistic images are more useful as training out-of-distribution than synthetic datasets (e.g., Gaussian noise) for improving the detection performance when we consider an image classification task. This motivates us to develop a new generative adversarial network (GAN) for generating such effective out-of-distribution samples.

2.2 ADVERSARIAL GENERATOR FOR OUT-OF-DISTRIBUTION
In this section, we introduce a new training method for learning a generator of out-of-distribution inspired by generative adversarial network (GAN) (Goodfellow et al., 2014). We will first assume that the classifier for in-distribution is fixed, and also describe the joint learning framework in the next section.
The GAN framework consists of two main components: discriminator D and generator G. The generator maps a latent variable z from a prior distribution Ppri (z) to generated outputs G (z), and discriminator D : X  [0, 1] represents a probability that sample x is from a target distribution. Suppose that we want to recover the in-distribution Pin(x) using the generator G. Then, one can optimize the following min-max objective for forcing PG  Pin:

min max
GD

EPin (x)

log D (x)

+ EPpri(z)

log (1 - D (G (z))) .

(2)

3

Under review as a conference paper at ICLR 2018

However, unlike the original GAN, we want to make the generator recover an effective out-ofdistribution Pout instead of Pin. To this end, we propose the following new GAN loss:

min max
GD

EPG(x) KL (U (y) P (y|x)) -H (PG (x))
(a) (b)
+EPin(x) log D (x) + EPG(x) log (1 - D (x)) ,

(3)

(c)

where H (·) denotes the entropy and  is the model parameter of a classifier trained on indistribution. The above objective can be interpreted as follows: the first term (a) corresponds to a replacement of the out-of-distribution Pout in (1)'s KL loss with the generator distribution PG. The second term (b) discourages the generator from collapsing by maximizing its entropy. Finally, the last term (c) corresponds to the original GAN loss since we would like to have out-of-distribution samples close to in-distribution, as mentioned in Section 2.1. If one drops (c), the generator might draw purely noisy out-of-distribution samples which are not effective for training a classifier. Therefore, one can expect that proposed loss can encourage the generator to produce the samples which are on the low-density boundary of the in-distribution space. We also provide its experimental evidences in Section 3.2.

However, maximizing the entropy H (PG (x)) in the objective (3) is technically challenging since a GAN does not model the generator distribution explicitly. To handle the issue, we leverage the
pull-away term (PT) (Zhao et al., 2017):

-H (PG (x))

1M PT (PG (x)) = M (M - 1)
i=1 j=i

G (zi) G (zj) G (zi) G (zj)

2
,

where zi, zj  Ppri (z) and M is the number of samples. One can note that it corresponds to the squared cosine similarity of generated samples. By minimizing the squared cosine similarity, one
can increase the entropy.

We also remark that (Dai et al., 2017) consider a similar GAN generating samples from out-ofdistribution for the purpose of semi-supervised learning. The authors assume the existence of a pretrained density estimation model such as PixelCNN++ (Salimans et al., 2017) for in-distribution, but such a model might not exist and be expensive to train in general. Instead, we use much simpler confident classifiers for approximating the density. Hence, under our fully-supervised setting, our GAN is much easier to train and more suitable.

2.3 JOINT TRAINING METHOD OF CONFIDENT CLASSIFIER AND ADVERSARIAL GENERATOR

In the previous section, we suggest training the proposed GAN using a pre-trained confident classifier. We remind that the converse is also possible, i.e., the motivation of having such a GAN is for training the classifier. Under this relation between two models, we propose a joint training scheme where the confident classifier improves the proposed GAN, and vice versa, as training proceeds. Specifically, we suggest the following joint objective function:

min max min
GD 

EPin(x,y) - log P (y = y|x) +EPG(x) KL (U (y) P (y|x))
(d) (e)
+EPin(x) log D (x) + EPG(x) log (1 - D (x)) + PT (PG (x)).

(4)

(f)

The classifier's confidence loss corresponds to (d) + (e), and the proposed GAN loss corresponds
to (e) + (f), i.e., they share the KL divergence term (e) under joint training. To optimize the above objective efficiently, we propose an alternating algorithm, which optimizes model parameters {} of classifier and GAN models {G, D} alternatively as shown in Algorithm 1. Since the algorithm
monotonically decreases the objective function, it is guaranteed to converge.

4

Under review as a conference paper at ICLR 2018

Algorithm 1 Alternating minimization for detecting and generating out-of-distribution.
repeat / Update proposed GAN / Sample {z1, . . . , zM } and {x1, . . . , xM } from prior Ppri (z) and and in-distribution Pin (x),
respectively, and update the discriminator D by ascending its stochastic gradient of

1M M

log D (xi) + log (1 - D (G (zi))) .

i=1

Sample {z1, . . . , zM } from prior Ppri (z), and update the generator G by descending its stochastic gradient of

1M M

log (1 - D (G (zi)))

1M +
M (M - 1)

i=1 i=1 j=i

G (zi) G (zj) G (zi) G (zj)

2

1M +
M

KL (U (y)

P (y|G (zi))) .

i=1

/ Update confident classifier / Sample {z1, . . . , zM } and {(x1, y1) , . . . , (xM , yM )} from prior Ppri (z) and in-distribution Pin (x, y), respectively, and update the classifier  by descending its stochastic gradient of

1M M
i=1
until convergence

- log P (y = yi|xi) + KL (U (y)

P (y|G (zi))) .

3 EXPERIMENTAL RESULTS

We demonstrate the effectiveness of our proposed method using various datasets: CIFAR (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011), ImageNet (Deng et al., 2009), LSUN (Yu et al., 2015) and synthetic (Gaussian) noise distribution. We train convolutional neural networks (CNNs) including VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky, 2014) for classifying CIFAR-10 and SVHN datasets. The corresponding test dataset is used as the in-distribution (positive) samples to measure the performance. We use realistic images and synthetic noises as the out-of-distribution (negative) samples. For evaluation, we measure the following metrics using threshold-based detectors (Hendrycks & Gimpel, 2016; Liang et al., 2017): the true negative rate (TNR) at 95% true positive rate (TPR), the area under the receiver operating characteristic curve (AUROC), the area under the precision-recall curve (AUPR), and the detection accuracy, where larger values of all metrics indicate better detection performances. Due to the space limitation, more explanations about datasets, metrics and network architectures are given in Appendix A.

In-dist SVHN CIFAR-10

Out-of-dist
CIFAR-10 (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
SVHN (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)

Classification accuracy
93.82 / 94.23
80.14 / 80.56

TNR at TPR 95%

AUROC

Detection accuracy

AUPR in

Baseline (cross entropy loss) / Our (confidence loss)

47.4 / 99.9 49.0 / 100.0 46.3 / 100.0 56.1 / 100.0

62.6 / 99.9 64.6 / 100.0 61.8 / 100.0 72.0 / 100.0

78.6 / 99.9 79.6 / 100.0 78.2 / 100.0 83.4 / 100.0

71.6 / 99.9 72.7 / 100.0 71.1 / 100.0 77.2 / 100.0

13.7 / 99.8 13.6 / 10.1 14.0 / 10.8 2.8 / 3.5

46.6 / 99.9 39.6 / 31.8 40.7 / 34.8 10.2 / 14.1

66.6 / 99.8 62.6 / 58.6 63.2 / 60.2 50.0 / 50.0

61.4 / 99.9 58.3 / 55.3 58.7 / 56.4 48.1 / 49.4

AUPR out
91.2 / 99.4 91.6 / 99.4 90.8/ 99.4 92.8 / 99.4
73.5 / 99.8 71.0 / 66.1 71.5 / 68.0 39.9 / 47.0

Table 1: Performance of the baseline detector using VGGNet. All values are percentages and boldface values indicate relative the better results. For each in-distribution, we minimize the KL divergence term in (1) using training samples from an out-of-distribution dataset denoted by "seen", where other "unseen" out-of-distributions were only used for testing.

5

Under review as a conference paper at ICLR 2018

Fraction Fraction TPR on in-distribution (SVHN)

0.9 SVHN (in)
0.8 CIFAR-10 (out / unseen) 0.7 Gaussian (out / unseen)
LSUN (out / unseen) 0.6 TinyImageNet (out / unseen) 0.5
0.4
0.3
0.2
0.1
0 0.15 0.35 0.55 0.75 0.95 Maximum in softmax scores
(a) Cross entropy loss only

1.0

0.9

SVHN (in) CIFAR-10 (out / seen)

0.8 Gaussian (out / unseen)

0.7 LSUN (out / unseen) 0.6 TinyImageNet (out / unseen)

0.5

0.4

0.3

0.2

0.1

0 0.15 0.35 0.55 0.75 0.95

Maximum in softmax scores

(b) Confidence loss (1)

1.0

1.0
0.8 0.9

0.6
0.4
0.2 0

0.8
0 0.1 Cross entropy loss Confidence loss (KL on Gaussian) Confidence loss (KL on LSUN) Confidence loss (KL on TinyImageNet) Confidence loss (KL on CIFAR-10)
0.5 1.0

FPR on out-of-distribution (CIFAR-10)

(c) ROC curve

Figure 2: Fraction of the maximum prediction value in softmax scores trained by (a) cross entropy loss and (b) confidence loss: the x-axis and y-axis represent the maximum prediction value and the fraction of images receiving the corresponding score, respectively. The receiver operating characteristic (ROC) curves under different losses are reported in (c): the red curve corresponds to the ROC curve of a model trained by optimizing the naive cross entropy loss on SVHN data, whereas other ones correspond to the ROC curves of models trained by optimizing the confidence loss. For all experiments in (a), (b) and (c), we commonly use the SVHN dataset for in-distribution.

3.1 EFFECTS OF CONFIDENCE LOSS
We first verify the effect of confidence loss (1) trained by some explicit, say seen, out-of-distribution datasets. First, we compare the quality of confidence level by applying various training losses. Specifically, the softmax classifier is used and simple CNNs (two convolutional layers followed by three fully-connected layers) are trained by minimizing the standard cross entropy loss on SVHN dataset. We also apply the confidence loss to the models by additionally optimizing the KL divergence term using CIFAR-10 dataset (as training out-of-distribution). In Figure 2(a) and 2(b), we report distributions of the maximum prediction value in softmax scores to evaluate the separation quality between in-distribution (i.e., SVHN) and out-of-distributions. It is clear that there exists a better separation between the SVHN test set (red bar) and other ones when the model is trained by the confidence loss. Here, we emphasize that the maximum prediction value is also low on even untrained (unseen) out-of-distributions, e.g., TinyImageNet, LSUN and synthetic datasets. Therefore, it is expected that one can distinguish in- and out-of-distributions more easily when a classifier is trained by optimizing the confidence loss. To verify that, we obtain the ROC curve using the baseline detector (Hendrycks & Gimpel, 2016) that computes the maximum value of predictive distribution on a test sample and classifies it as positive (i.e., in-distribution) if the confidence score is above some threshold. Figure 2(c) shows the ROC curves when we optimize the KL divergence term on various datasets. One can observe that realistic images such as TinyImageNet (aqua line) and LSUN (green line) are more useful than synthetic datasets (orange line) for improving the detection performance. This supports our intuition that out-of-distribution samples close to in-distribution could be more effective in improving the detection performance as we discussed in Section 2.1.
We indeed evaluate the performance of the baseline detector for out-of-distribution using largescale CNNs, i.e., VGGNets with 13 layers, under various training scenarios, where more results on AlexNet and ODIN detector (Liang et al., 2017) can be found in Appendix B (the overall trends of results are similar). For optimizing the confidence loss (1), SVHN and CIFAR-10 training datasets are used for optimizing the KL divergence term for the cases when the in-distribution is CIFAR-10 and SVHN, respectively. Table 1 shows the detection performance for each in- and out-of-distribution pair. When the in-distribution is SVHN, the classifier trained by our method drastically improves the detection performance across all out-of-distributions without hurting its original classification performance. However, when the in-distribution is CIFAR-10, the confidence loss does not improve the detection performance in overall, where we expect that this is because the trained/seen SVHN outof-distribution does not effectively cover all tested out-of-distributions. Our joint confidence loss (4), which was designed under the intuition, resolves the issue of the CIFAR-10 (in-distribution) classification case in Table 1 (see Figure 4(b)).
6

Under review as a conference paper at ICLR 2018

Generated samples

Generated samples

(a) (b) (c) (d)
Figure 3: The generated samples from (a)/(c) original GAN and (b)/(d) proposed GAN. In (a)/(b), the grey area is the 2D histogram of training in-distribution samples drawn from a mixture of two Gaussian distributions and red points indicate generated samples by GANs.

Cross entropy loss

Confidence loss with original GAN

Joint confidence loss

Confidence loss (KL on CIFAR-10)

Out-of-distribution: CIFAR-10 100

90

80

70

60

50

40 TNR
at TPR 95%

AUROC

Detection accuracy

Out-of-distribution: TinyImageNet 100

90

80

70

60

50

40 TNR
at TPR 95%

AUROC

Detection accuracy

(a) In-distribution: SVHN

Out-of-distribution: LSUN 100

90

80

70

60

50

40 TNR
at TPR 95%

AUROC

Detection accuracy

Cross entropy loss

Confidence loss with original GAN

Joint confidence loss

Confidence loss (KL on SVHN)

Out-of-distribution: SVHN 100

90

80

70

60

50

40

30

20

10

0 TNR

AUROC

Detection

TPR 95%

accuracy

Out-of-distribution: TinyImageNet 100

90

80

70

60

50

40

30

20

10

0 TNR
at TPR 95%

AUROC

Detection accuracy

(b) In-distribution: CIFAR-10

Out-of-distribution: LSUN 100

90

80

70

60

50

40

30

20

10

0 TNR

AUROC

Detection

at TPR 95%

accuracy

Figure 4: Performances of the baseline detector under various training losses. For fair comparisons, we only plot the performances for unseen out-of-distributions, where those for seen out-ofdistributions (used for minimizing the KL divergence term in (1)) can be found in Table 1.

3.2 EFFECTS OF ADVERSARIAL GENERATOR AND JOINT CONFIDENCE LOSS
In this section, we verify the effect of the proposed GAN in Section 2.2 and evaluate the detection performance of the joint confidence loss (4). To verify that the proposed GAN can produce the samples nearby the low-density boundary of the in-distribution space, we first compare the generated samples by original GAN and proposed GAN on a simple example where the target distribution is a mixture of two Gaussian distributions. For both the generator and discriminator, we use fullyconnected neural networks with 2 hidden layers. For our method, we use a pre-trained classifier which minimizes the cross entropy on target distribution samples and the KL divergence on out-ofdistribution samples generated by rejection sampling on a bounded 2D box. As shown in Figure 3(a), the samples of original GAN cover the high-density area of the target distribution while those of proposed GAN does its boundary one (see Figure 3(b)). We also compare the generated samples of original and proposed GANs on MNIST dataset (LeCun et al., 1998), which consists of handwritten digits. For this experiment, we use deep convolutional GANs (DCGANs) (Radford et al., 2015). In this case, we use a pre-trained classifier which minimizes the cross entropy on MNIST

7

Under review as a conference paper at ICLR 2018

SVHN (in)

CIFAR-10 (in)

CIFAR-10 (out)

SVHN (out)

LSUN (out)

LSUN (out)

TinyImageNet (out) Cross entropy Confidence loss Joint confidence Confidence loss with original GAN loss (KL on CIFAR-10)
(a) In-distribution: SVHN

TinyImageNet (out) Cross entropy Confidence loss Joint confidence Confidence loss

with original GAN

loss

(KL on SVHN)

(b) In-distribution: CIFAR-10

Figure 5: Guided gradient (sensitivity) maps of the top-1 predicted class with respect to the input image under various training losses.

training samples and the KL divergence on synthetic Gaussian noises. As shown in Figure 3(c) and 3(d), samples of original GAN looks more like digits than those of proposed GAN. Somewhat interestingly, the proposed GAN still generates some new digit-like images.
We indeed evaluate the performance of our joint confidence loss utilizing the proposed GAN. To this end, we use VGGNets (as classifiers) and DCGANs (as GANs). We also test a variant of confidence loss which optimizes the KL divergence term on samples from a pre-trained original GAN (implicitly) modeling the in-distribution. One can expect that samples from the original GAN can be also useful for improving the detection performance since it may have bad generalization properties (Arora et al., 2017) and generate a few samples on the low-density boundary as like the proposed GAN. Figure 4 shows the performance of the baseline detector for each in- and out-of-distribution pair. First, observe that the joint confidence loss (blue bar) outperforms the confidence loss with some explicit out-of-distribution datasets (green bar). This is quite remarkable since the former is trained only using in-distribution datasets, while the latter utilizes additional out-of-distribution datasets. We also remark that our methods significantly outperform the baseline cross entropy loss (red bar) in all cases without harming its original classification performances (see Table 6 in Appendix B). Interestingly, the confidence loss with the original GAN (orange bar) is often (but not always) useful for improving the detection performance, whereas that with the proposed GAN (blue bar) still outperforms it in all cases.
Finally, we also provide visual interpretations of models using the guided gradient maps (Springenberg et al., 2014). Here, the gradient can be interpreted as an importance value of each pixel which influences on the classification decision. As shown in Figure 5, the model trained by the cross entropy loss shows sharp gradient maps for both samples from in- and out-of-distributions, whereas models trained by the confidence losses do only on samples from in-distribution. For the case of SVHN in-distribution, all confidence losses gave almost zero gradients, which matches to the results in Figure 4(a): their detection performances are almost perfect. For the case of CIFAR10 distribution, one can now observe that there exists some connection between gradient maps and detection performances. This is intuitive because for detecting samples from out-of-distributions better, the classifier should look at more pixels as similar importance and the KL divergence term forces it. We think that our visualization results might give some ideas in future works for developing better inference methods for detecting out-of-distribution under our models.
4 CONCLUSION
In this paper, we aim to develop a training method for neural classification networks for detecting out-of-distribution better without losing its original classification accuracy. In essence, our method jointly trains two models for detecting and generating out-of-distribution by minimizing their losses alternatively. Although we primarily focus on image classification in our experiments, our method can be used for any classification tasks using deep neural networks. It is also interesting future directions applying our methods for other related tasks: network calibration (Guo et al., 2017), ensemble method (Lakshminarayanan et al., 2017) and semi-supervised learning (Dai et al., 2017).
8

Under review as a conference paper at ICLR 2018
REFERENCES
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane´. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In International Conference on Machine Learning (ICML), 2017.
Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semisupervised learning that requires a bad gan. In Advances in neural information processing systems (NIPS), 2017.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), 2009.
Ross Girshick. Fast r-cnn. In International Conference on Computer Vision (ICCV), 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems (NIPS), 2014.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning (ICML), 2017.
Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations (ICLR), 2016.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2014.
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in neural information processing systems (NIPS), 2017.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Kimin Lee, Changho Hwang, KyoungSoo Park, and Jinwoo Shin. Confident multiple choice learning. In International Conference on Machine Learning (ICML), 2017.
Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-divergences. In International Conference on Machine Learning (ICML), 2017.
9

Under review as a conference paper at ICLR 2018
Shiyu Liang, Yixuan Li, and R Srikant. Principled detection of out-of-distribution examples in neural networks. arXiv preprint arXiv:1706.02690, 2017.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational bayesian neural networks. In International Conference on Machine Learning (ICML), 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 5, 2011.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations (ICLR), 2015.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. In International Conference on Learning Representations (ICLR), 2017.
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb. Learning from simulated and unsupervised images through adversarial training. 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Computer Vision and Pattern Recognition (CVPR), 2015.
Ruben Villegas, Jimei Yang, Yuliang Zou, Sungryull Sohn, Xunyu Lin, and Honglak Lee. Learning to generate long-term future via hierarchical prediction. In International Conference on Machine Learning (ICML), 2017.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. In International Conference on Learning Representations (ICLR), 2017.
10

Under review as a conference paper at ICLR 2018
A EXPERIMENTAL SETUPS IN SECTION 3
Datasets. We train deep models such as VGGNet (Szegedy et al., 2015) and AlexNet (Krizhevsky, 2014) for classifying CIFAR-10 and SVHN datasets: the former consists of 50,000 training and 10,000 test images with 10 image classes, and the latter consists of 73,257 training and 26,032 test images with 10 digits.1 The corresponding test dataset are used as the in-distribution (positive) samples to measure the performance. We use realistic images and synthetic noises as the out-ofdistribution (negative) samples: the TinyImageNet consists of 10,000 test images with 200 image classes from a subset of ImageNet images. The LSUN consists of 10,000 test images of 10 different scenes. We downsample each image of TinyImageNet and LSUN to size 32 × 32. The Gaussian noise is independently and identically sampled from a Gaussian distribution with mean 0.5 and variance 1. We clip each pixel value into the range [0, 1].
Detailed CNN structure and training. The simple CNN that we use for evaluation shown in Figure 2 consists of two convolutional layers followed by three fully-connected layers. Convolutional layers have 128 and 256 filters, respectively. Each convolutional layer has a 5 × 5 receptive field applied with a stride of 1 pixel each followed by max pooling layer which pools 2 × 2 regions at strides of 2 pixels. AlexNet (Krizhevsky, 2014) consists of five convolutitonal layers followed by three fullyconnected layers. Convolutional layers have 64, 192, 384, 256 and 256 filters, respectively. First and second convolutional layers have a 5×5 receptive field applied with a stride of 1 pixel each followed by max pooling layer which pools 3 × 3 regions at strides of 2 pixels. Other convolutional layers have a 3 × 3 receptive field applied with a stride of 1 pixel followed by max pooling layer which pools 2 × 2 regions at strides of 2 pixels. Fully-connected layers have 2048, 1024 and 10 hidden units, respectively. Dropout (Hinton et al., 2012) was applied to only fully-connected layers of the network with the probability of retaining the unit being 0.5. All hidden units are ReLUs. Figure 6 shows the detailed structure of VGGNet (Szegedy et al., 2015) with three fully-connected layers and 10 convolutional layers. Each ConvReLU box in the figure indicates a 3 × 3 convolutional layer followed by ReLU activation. Also, all max pooling layers have 2 × 2 receptive fields with stride 2. Dropout was applied to only fully-connected layers of the network with the probability of retaining the unit being 0.5. For all experiments, the softmax classifier is used, and each model is trained by optimizing the objective function using Adam learning rule (Kingma & Ba, 2014) with a mini-batch size of 128. The learning rate is chosen from {0.001, 0.0005, 0.0002}. We drop the learning rate by 0.2 at 60, 120 and 160 epochs and models are trained for total 200 epochs. The best test result is reported for each method.
Performance metrics. We measure the following metrics using threshold-based detectors:
· True negative rate (TNR) at 95% true positive rate (TPR). Let TP, TN, FP, and FN denote true positive, true negative, false positive and false negative, respectively. We measure TNR = TN / (FP+TN), when TPR = TP / (TP+FN) is 95%.
· Area under the receiver operating characteristic curve (AUROC). The ROC curve is a graph plotting TPR against the false positive rate = FP / (FP+TN) by varying a threshold.
· Area under the precision-recall curve (AUPR). The PR curve is a graph plotting the precision = TP / (TP+FP) against recall = TP / (TP+FN) by varying a threshold. AUPR-IN (or -OUT) is AUPR where in- (or out-of-) distribution samples are specified as positive.
· Detection accuracy. This metric corresponds to the maximum classification probability over all possible thresholds : 1 - min Pin (q (x)  ) P (x is from Pin) + Pout (q (x) > ) P (x is from Pout) , where q(x) is a confident score such as a maximum value of softmax. We assume that both positive and negative examples have equal probability of appearing in the test set, i.e., P (x is from Pin) = P (x is from Pout) = 0.5.
Note that AUROC, AUPR and detection accuracy are threshold-independent evaluation metrics.
Generating samples on a simple example. As shown in Figure 3(a) and Figure 3(b), we compare the generated samples by original GAN and proposed GAN on a simple example where the target distribution is a mixture of two Gaussian distributions. For both the generator and discriminator, we use fully-connected neural networks with 2 hidden layers and 500 hidden units for each layer. For
1We do not use the extra SVHN dataset for training.
11

ConvReLU (64 f.maps) ConvReLU (64 f.maps)
max pool ConvReLU (128 f.maps) ConvReLU (128 f.maps)
max pool ConvReLU (256 f.maps) ConvReLU (256 f.maps)
max pool ConvReLU (512 f.maps) ConvReLU (512 f.maps)
max pool ConvReLU (512 f.maps) ConvReLU (512 f.maps)
avg pool FC (512) FC (512) FC (10)

Under review as a conference paper at ICLR 2018
Figure 6: Detailed structure of VGGNet with 13 layers.
all layers, we use ReLU activation function. We use a 100-dimensional Gaussian prior for the latent variable z. For our method, we pre-train the simple fully-connected neural networks (2 hidden layers and 500 ReLU units for each layer) by minimizing the cross entropy on target distribution samples and the KL divergence on out-of-distribution samples generated by rejection sampling on bounded 2D box [-10, 10]2. We use ADAM learning rule (Kingma & Ba, 2014) with a mini-batch size of 400. The initial learning rate is set to 0.002, and we train for total 100 epochs. Generating samples on MNIST. As shown in Figure 3(c) and Figure 3(d), we compare the generated samples of original and proposed GANs on MNIST dataset, which consists of greyscale images, each containing a digit 0 to 9 with 60,000 training and 10,000 test images. We expand each image to size 3 × 32 × 32. For both the generator and discriminator, we use deep convolutional GANs (DCGANs) (Radford et al., 2015). The discriminator and generator consist of four convolutional and deconvolutional layers, respectively. Convolutional layers have 128, 256, 512 and 1 filters, respectively. Each convolutional layer has a 4 × 4 receptive field applied with a stride of 2 pixel. The second and third convolutional layers are followed by batch normalization (Ioffe & Szegedy, 2015). For all layers, we use LeakyReLU activation function. Deconvolutional layers have 512, 256, 128 and 1 filters, respectively. Each deconvolutional layer has a 4 × 4 receptive field applied with a stride of 2 pixel followed by batch normalization (Ioffe & Szegedy, 2015) and ReLU activation. For our method, we use a pre-trained simple CNNs (two convolutional layers followed by three fully-connected layers) by minimizing the cross entropy on MNIST training samples and the KL divergence on synthetic Gaussian noise. Convolutional layers have 128 and 256 filters, respectively. Each convolutional layer has a 5 × 5 receptive field applied with a stride of 1 pixel each followed by max pooling layer which pools 2 × 2 regions at strides of 2 pixels. We use ADAM learning rule (Kingma & Ba, 2014) with a mini-batch size of 128. The initial learning rate is set to 0.0002, and we train for total 50 epochs.
12

Under review as a conference paper at ICLR 2018

B MORE EXPERIMENTAL RESULTS

In this section, we provide more experimental results using AlexNet (Krizhevsky, 2014) (see Table 2) and ODIN detector (Liang et al., 2017) (see Table 3). We remark that they show similar trends. For the ODIN detector, the temperature is set to 1000 and perturbation magnitude is set to 0.0012. Also, we report the detection performances of the baseline and ODIN detectors when we optimize the KL divergence term using Gaussian noises and explicit out-of-distribution samples in Table 4 and 5. Finally, Table 6 reports the classification accuracy of VGGNets on CIFAR-10 and SVHN datasets under various training losses shown in Figure 4.

In-dist SVHN CIFAR-10

Out-of-dist
CIFAR-10 (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
SVHN (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)

Classification accuracy
92.14 / 93.77
76.58 / 76.18

TNR at TPR 95%

AUROC

Detection accuracy

AUPR in

Baseline (cross entropy loss) / Our (confidence loss)

42.0 / 99.9 45.6 / 99.9 44.6 / 100.0 58.6 / 100.0

88.0 / 100.0 89.4 / 100.0 89.8 / 100.0 94.2 / 100.0

83.4 / 99.8 84.3 / 99.9 84.5 / 99.9 88.8 / 100.0

88.7 / 99.9 90.2 / 100.0 90.8 / 100.0 95.5 / 100.0

12.8 / 99.6 10.3 / 10.1 10.7 / 8.1 6.7 / 1.0

71.0 / 99.9 59.2 / 52.1 56.3 / 51.5 49.6 / 13.5

73.2 / 99.6 64.2 / 62.0 64.3 / 61.8 61.3 / 50.0

74.3 / 99.9 63.6 / 59.8 62.3 / 59.5 58.5 / 43.7

AUPR out
87.3 / 99.3 88.6 / 99.3 88.4 / 99.3 92.5 / 99.3
70.7 / 99.6 64.4 / 62.3 65.3 / 61.6 59.5 / 32.0

Table 2: Performance of the baseline detector using AlexNet. All values are percentages and boldface values indicate relative the better results. For each in-distribution, we minimize the KL divergence term in (1) using training samples from an out-of-distribution dataset denoted by "seen", where other "unseen" out-of-distributions were also used for testing.

Model (In-dist)
AlexNet (SVHN)
AlexNet (CIFAR-10)
VGGNet (SVHN)
VGGNet (CIFAR-10)

Out-of-dist
CIFAR-10 (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
SVHN (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
CIFAR-10 (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
SVHN (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)

Classification accuracy
92.14 / 93.77
76.58 / 76.18
93.82 / 94.23
80.14 / 80.56

TNR at TPR 95%

AUROC

Detection accuracy

AUPR in

Baseline (cross entropy loss) / Our (confidence loss)

55.5 / 99.9 59.5 / 99.9 61.5 / 100.0 82.6 / 100.0

89.1 / 99.2 90.5 / 99.3 91.8 / 99.3 97.0 / 99.3

82.4 / 99.8 83.8 / 99.9 84.8 / 99.9 91.6 / 100.0

85.9 / 100.0 87.5 / 100.0 90.5 / 100.0 97.4 / 100.0

37.1 / 99.6 11.4 / 8.4 13.3 / 7.1 3.8 / 0.0

86.7 / 99.6 69.1 / 65.6 71.9 / 67.1 70.9 / 57.2

79.3 / 99.6 64.4 / 61.8 75.3 / 63.7 69.3 / 40.4

88.1 / 99.9 71.4 / 68.6 67.2 / 72.0 78.1 / 56.1

70.0 / 99.9 86.9 / 100.0 86.4 / 100.0 85.9 / 100.0

93.4 / 99.4 95.4 / 99.4 95.5 / 99.4 97.3 / 99.4

87.4 / 99.9 89.5 / 100.0 89.4 / 100.0 92.5 / 100.0

92.3 / 100.0 95.3 / 100.0 95.9 / 100.0 97.7 / 100.0

41.8 / 99.8 19.2 / 10.9 20.8 / 12.2 4.4 / 7.5

88.8 / 99.8 74.1 / 69.0 76.5 / 71.3 58.0 / 69.6

81.7 / 99.8 67.6 / 64.5 69.8 / 66.4 60.4 / 66.3

90.5 / 100.0 75.8 / 70.9 78.6 / 73.1 66.5 / 75.4

AUPR out
89.0 / 99.2 90.4 / 99.3 91.3 / 99.3 96.4 / 99.3
84.2 / 99.6 64.6 / 60.7 65.3 / 60.5 60.7 / 40.7
92.9 / 99.4 94.7 / 99.4 94.8 / 99.4 96.5 / 99.4
85.7 / 99.8 71.1 / 64.3 72.8 / 66.3 52.4 / 62.1

Table 3: Performance of the ODIN detector (Liang et al., 2017) using VGGNet and AlexNet. All values are percentages and boldface values indicate relative the better results. For each in-distribution, we minimize the KL divergence term in (1) using training samples from an out-of-distribution dataset denoted by "seen", where other "unseen" out-of-distributions were only used for testing.

13

Under review as a conference paper at ICLR 2018

Model (In-dist)
AlexNet (SVHN)
AlexNet (CIFAR-10)
VGGNet (SVHN)
VGGNet (CIFAR-10)

Out-of-dist
CIFAR-10 (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian
SVHN (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
CIFAR-10 (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
SVHN (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)

Classification accuracy
92.14 / 93.90
76.58 / 76.11
93.82 / 93.89
80.14 / 80.56

TNR at TPR 95%

AUROC

Detection accuracy

AUPR in

Baseline (cross entropy loss) / Our ( confidence loss)

42.0 / 99.9 45.6 / 99.9 44.6 / 100.0 58.6 / 100.0

88.0 / 100.0 89.4 / 100.0 89.8 / 100.0 94.2 / 100.0

83.4 / 99.8 84.3 / 99.8 84.5 / 100.0 88.8 / 100.0

88.7 / 100.0 90.2 / 100.0 90.8 / 100.0 95.5 / 100.0

12.8 / 99.5 10.3 / 13.1 10.7 / 13.1 6.7 / 100.0

71.0 / 99.9 59.2 / 61.6 56.3 / 65.9 49.6 / 100.0

73.2 / 99.5 64.2 / 64.5 64.3 / 67.4 61.3 /100.0

74.3 / 99.8 63.6 / 64.5 62.3 / 68.5 58.5 / 100.0

47.4 / 99.9 49.0 / 99.9 46.3 / 100.0 56.1 / 100.0

62.6 / 99.9 64.6 / 99.9 61.8 / 100.0 72.0 / 100.0

78.6 / 99.8 79.6 / 99.9 78.2 / 100.0 83.4 / 100.0

71.6 / 99.9 72.7 / 99.9 71.1/ 100.0 77.2 / 100.0

13.7 / 99.8 13.6 / 15.9 14.0 / 17.3 2.8 / 100.0

46.6 / 99.7 39.6 / 44.6 40.7 / 49.4 10.2 / 100.0

66.6 / 99.7 62.6 / 64.8 63.2 / 67.4 50.0 / 100.0

61.4 / 99.9 58.3 / 60.1 58.7 / 62.4 48.1 / 100.0

AUPR out
87.3 / 99.2 88.6 / 99.2 88.4 / 99.2 92.5 / 99.2
70.7 / 99.7 64.4 / 66.7 65.3 / 67.8 59.5 / 99.7
91.2 / 99.5 91.6 / 99.5 90.8/ 99.5 92.8 / 99.5
73.5 / 99.8 71.0 / 73.3 71.5 / 75.1 39.9 / 99.8

Table 4: Performance of the baseline detector using VGGNet and AlexNet. All values are percentages and boldface values indicate relative the better results. For each in-distribution, we minimize the KL divergence term in (1) using Gaussian noises and training samples from an out-of-distribution dataset denoted by "seen", where other "unseen" out-of-distributions were only used for testing.

Model (In-dist)
AlexNet (SVHN)
AlexNet (CIFAR-10)
VGGNet (SVHN)
VGGNet (CIFAR-10)

Out-of-dist
CIFAR-10 (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
SVHN (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
CIFAR-10 (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)
SVHN (seen) TinyImageNet (unseen)
LSUN (unseen) Gaussian (unseen)

Classification accuracy
92.14 / 93.90
76.58 / 76.11
93.82 / 93.89
80.14 / 80.56

TNR at TPR 95%

AUROC

Detection accuracy

AUPR in

Baseline (cross entropy loss) / Our ( confidence loss)

55.5 / 99.9 59.5 / 99.9 61.5 / 100.0 82.6 / 100.0

89.1 / 99.2 90.5 / 99.2 91.8 / 99.2 97.0 / 99.2

82.4 / 99.8 83.8 / 99.9 84.8 / 100.0 91.6 / 100.0

85.9 / 100.0 87.5 / 100.0 90.5 / 100.0 97.4 / 100.0

37.1 / 99.5 11.4 / 13.1 13.3 / 13.2 3.8 / 100.0

86.7 / 99.6 69.1 / 68.1 71.9 / 73.3 70.9 / 99.8

79.3 / 99.5 64.4 / 63.5 75.3 / 77.9 69.3 / 100.0

88.1 / 99.9 71.4 / 69.4 67.2 / 76.7 78.1 / 100.0

70.0 / 99.9 86.9 / 100.0 86.4 / 100.0 85.9 / 100.0

93.4 / 99.5 95.4 / 99.5 95.5 / 99.5 97.3 / 99.5

87.4 / 99.8 89.5 / 99.9 89.4 / 100.0 92.5 / 100.0

92.3 / 100.0 95.3 / 100.0 95.9 / 100.0 97.7 / 100.0

41.8 / 99.8 19.2 / 23.3 20.8 / 27.0 4.4 / 100.0

88.8 / 99.8 74.1 / 78.6 76.5 / 83.2 58.0 / 99.8

81.7 / 99.7 67.6 / 71.5 69.8 / 75.8 60.4 / 100.0

90.5 / 99.9 75.8 / 80.3 78.6 / 85.6 66.5 / 100.0

AUPR out
89.0 / 99.2 90.4 / 99.2 91.3 / 99.2 96.4 / 99.2
84.2 / 99.7 64.6 / 65.0 65.3 / 67.8 60.7 / 99.8
92.9 / 99.5 94.7 / 99.5 94.8 / 99.5 96.5 / 99.5
85.7 / 99.8 71.1 / 75.8 72.8 / 79.5 52.4 / 99.8

Table 5: Performance of the ODIN detector using VGGNet and AlexNet. All values are percentages and boldface values indicate relative the better results. For each in-distribution, we minimize the KL divergence term in (1) using Gaussian noises and training samples from an out-of-distribution dataset denoted by "seen", where other "unseen" out-of-distributions were only used for testing.

In-distribution
CIFAR-10 SVHN

Cross entropy
80.14 93.82

Confidence loss with original GAN
80.27 94.13

Joint confidence loss
80.38 93.80

Confidence loss with explicit out-of-distribution samples
80.56 94.23

Table 6: Classification test set accuracy of VGGNets on CIFAR-10 and SVHN datasets under various training losses.

14

