Under review as a conference paper at ICLR 2018
Pseudo sequence based deep neural network compression
Anonymous authors Paper under double-blind review
Abstract
Along with the performance increase of the neural network, both the number of layers and the number of parameters in each layer are becoming larger and larger. Therefore, there are more and more works trying to compress the neural network efficiently while keeping the performance. However, all of them have not taken the similarity among the kernels into consideration. In this paper, we try to organize the kernels in different channels into a frame and encode the frames using block-based video coding methods. First, we try to reshape the weights in different channels into a pseudo sequence. Second, after obtaining all the frames in the videos, we will convert the weight into the PCA domain to obtain a more compact representation. Then both the intra prediction and the inter prediction will be performed in the PCA domain to achieve better performance. Finally, the uniform quantization and entropy coding will be used to encode the residue blocks. The experimental results show that we can achieve 58 times compression for the classical VGG-16 model. Not only with very high compression ratio, the proposed method can also provide the benefits of getting a better balance between the bits per weight and the error in smaller granularity by adjusting the quantization parameters.
1 Introduction
Deep neural networks have found countless applications in both computer vision and image processing tasks such as image classification (Krizhevsky et al. (2012); Szegedy et al. (2015)), object detection (He et al. (2016)), image super resolution (Lim et al. (2017); Li et al. (2017)), and image compression artifacts removal (Dong et al. (2015); Dai et al. (2017)). However, along with the wide use and performance increase of the neural network, both the number of layers and the number of parameters in each layer are becoming larger and larger. It will be difficult for us to apply the deep neural networks to the light-weight platforms.
Therefore, there are more and more works trying to compress the neural network efficiently while keeping the performance. For example, Han et al. Han et al. (2015) proposed to prune the small-weight connections with no loss of accuracy. Then quantization and Huffman coding are used to reduce the number of bits required to represent each weight. This work is quite simple yet very effective. Besides, Zhang et al. Zhang et al. (2016) and Yu et al. Yu et al. (2017) introduced the low rank and sparse decomposition to effectively reduce the number of parameters and accelerate the neural network. Also, Matthieu et al. Courbariaux et al. (2015) and Yang et al. Yang et al. (2017) proposed to apply the so-called binary neural network to store the weight in their binarized format to achieve less storage and acceleration. However, all these works have not tried to take the mature video and image coding algorithms into account. The image and video coding standards have demonstrated its effectiveness in the compression society.
Therefore, in this paper, we try to organize all the weights into a pseudo sequence to make use of the technologies in modern video and image coding standards. Fig. 1 gives the basic coding process of the proposed algorithm. We will first reshape the weights of different layers into a pseudo sequence. After obtaining all the frames in the videos, we will convert the weight into the principal component analysis (PCA) domain to obtain a more compact representation. Then both the intra prediction and the inter prediction will
1

Under review as a conference paper at ICLR 2018

Reshape the weights into pseudo videos

Original weights

Reshape the weights

Pseudo video

Transform and prediction
PCA

Residue blocks

Quantization and Entropy Coding

Uniform quantization

Compressed weights

Enlarge the weights

Intra

Inter

prediction prediction

PAQ

Figure 1: The basic framework of proposed pseudo sequence based deep neural network compression
be performed in the PCA domain to achieve better performance. Finally, the uniform quantization and entropy coding will be used to encode the residue blocks. As far as we can see, the main insight of this paper is that it is the first work to involve the video coding based technologies such as motion estimation and motion compensation into the neural network compression framework. Also, quite high compression efficiency is achieved through the proposed algorithms.
This paper is organized as follows. In Section 2, we will give a detailed introduction of the three parts of the proposed algorithms. The detailed experimental results and discussions on the compression efficiency will be shown in Section 3. Section 5 concludes the whole paper and suggests some future works.
2 Proposed algorithms
2.1 Reshape the weights into a pseudo sequence
Under this step, we try to reshape the weights into video frames to beneficial the following coding process as shown in Fig. 2. According to the dimensions of the weights of different layers, we divide the weights of different layers into two kinds. The first kind can be naturally divided into various frames with different block sizes such as the FC6 layer and convolution layers of the VGG-16 network Simonyan & Zisserman (2014) as shown in Fig. 2 (a). The kernel size will be directly converted into the size of the block, the channel size can be directly converted into the number of blocks in one frame, and the number of outputs will be converted to the number of frames. Take the FC6 layer of VGG-16 as an example, the kernel size is 7, the number of channels is 512, and the number of outputs is 4096. Therefore, there are 7 × 7 × 512 × 4096 weights in the FC6 layer. It can be naturally considered as a pseudo sequence with 4096 frames, each frame with 512 blocks, and the size of each block is 7 × 7. Note that the number of blocks, the block size, and the number of frames can be adjusted to achieve even better performance.
The second kind cannot be divided into various frames directly since the kernel size is 1 such as the FC7 layer and FC8 layer of the VGG-16 network. Take the FC7 layer of VGG-16 as an example, the kernel size is 1, the number of channels is 4096, and the number of outputs is 4096. Since we cannot set the block size as 1×1, we need to divide the number of channels into the block size and the number of blocks as shown in Fig. 2 (b). For example, the block size can be 8 × 8 and the number of blocks can be 64. Under such a case, it will be more natural to adjust the number of blocks, the block size, and the number of frames to achieve even better performance.
Using the above mentioned methods, all the layers can be organized into a number of frames with some blocks. It should also be mentioned that since the weights in both the convolution layers and full connection layers are very small, we multiply all the weights by 4096 × 16 to make it beneficial to the following prediction and quantization processes.
2

Under review as a conference paper at ICLR 2018

Energy/%

(a) (b) Figure 2: The illustration of reshaping the weights into video frames

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3 123456789 NO. of dimension preserved
(a) Conv1_1

1

0.9

0.8

0.7

0.6

0.5

0.4 0

5 10 15 20 25 30 35 40 45 50

NO. of dimension preserved

(c) FC6

Energy/%

Energy/%

1 0.95
0.9 0.85
0.8 0.75
0.7 0.65
0.6 0.55
0.5 123456789 NO. of dimension preserved
(b) Conv4_1
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0 10 20 30 40 50 60 70 NO. of dimension preserved
(d) FC7

Figure 3: The energy compaction of different layers after PCA

Energy/%

2.2 Intra and inter prediction
Before performing the intra and inter prediction, for a specified layer, we first derive a PCA from all the blocks and apply PCA to all the blocks to obtain a more compact representation for each block. All the blocks are converted into vectors to calculate the PCA. For example, for a 7 × 7 block, the vector size is 1 × 49 and will be derived a PCA with 49 × 49 dimensions. The PCA is calculated by all the blocks in the current layer and will be transmitted to the decoder for reconstruction. In our scheme, the PCA is transmitted using 32 bits float numbers to guarantee the precision of the PCA. Fig. 3 shows energy compaction extent of some layers of the VGG-16 after the PCA. From Fig. 3, we can obviously see the extents of the energy compaction of various layers are totally different. The extent of the energy compaction will finally determine the compression ratio of a specified layer. The more compact layers will be easier to compress while the less compact layers will be harder to compress. Then in the following, we will do the prediction in the PCA domain.
For the first frame, since there are no previous coded frames, we can only use the intra prediction mode. According to our observations, the spatial correlations among various blocks inside one frame are quite low. No matter using the blocks in the current frame or the neighboring pixels, there will always be no obvious gains through the intra prediction. So we choose to directly encode the coefficients after PCA under the intra prediction mode.
3

Under review as a conference paper at ICLR 2018

Previous frame
0123 4567 8 9 10 11 12 13 14 15

Current block

Figure 4: The illustration of the block based motion estimation
For the other frames, besides the intra prediction mode, an inter prediction mode is also performed by searching all the previously coded blocks in the previous frames as shown in Fig. 4. Such a process is very similar to the block-based motion estimation in the video coding standard. However, since the prediction is performed in the PCA domain, we will obtain the best matching block through the motion estimation process block by block instead of the pixel. As can be obviously seen from Fig. 4, an index is needed to be transmitted to the decoder to indicate which block will be used to as best match. For example, if the total number of blocks in the previous frame is 16, 4 bits will be used to signal which block will be used. It should be emphasized that the larger the block size, the less number of bits will be spent on the number of bits to signal the block index while the prediction will be less accurate. The best performance can be obtained by achieving the optimal trade-off between the residue bits and block index bits. In the final, the difference of the current block and its best match will be coded under the inter prediction mode.
As mentioned above, in all the frames except the first frame, we can choose to use intra prediction mode or inter prediction mode. In essence, we need to use the rate distortion optimization by considering both the distortion and the block index bits to determine whether we should choose the intra prediction mode or inter prediction mode. However, in our case, we choose to only use the energy of the current blocks before inter prediction and after inter prediction as the criterion. If the energy of one block becomes less after inter prediction, we will choose inter prediction mode and vice versa. The energy is derived by calculating the sum of the square of all the coefficients.
2.3 Quantization and entropy coding
After obtaining the intra and inter residue blocks in the PCA domain, the uniform quantization will be used to quantize all the coefficients. Since we have already enlarged the coefficients in advance, here we can use relatively larger quantization parameters such as 256, 512, and 1024 while keeping the performance.
For seeking more efficient compression performance, we take advantage of PAQ (Knoll & d. Freitas (2012)), a compression scheme with machine learning perspectives for further compression of the residue blocks after quantization. Since the compression efficiency of the PAQ will be higher for the larger file sizes, we organize the residue blocks of all the frames for a specified layer into one file to compress the residue blocks more efficiently.
3 Experimental results
We try to use the proposed algorithms to compress two networks efficiently both on imageNet data-sets (Deng et al. (2009)): AlexNet (Krizhevsky et al. (2012)) and VGG-16. To save the verification time, we randomly choose 5000 samples from imageNet data-sets to verify the performance of the proposed algorithm. The overall network parameters and accuracy before and after compression can be seen from Table 1. In Table 1, the conv QP and FC QP mean the quantization parameter used for the convolution layer and full connection layer, respectively. The conv QP and FC QP equal to 1 means that the network has not
4

Under review as a conference paper at ICLR 2018

Table 1: The overall network parameters and accuracy before and after compression

Conv QP FC QP Top1-Error Top5-Error BPW Compression ratio

AlexNet

1

1

0.4336

0.2062

32

­

256 256

0.4402

0.2044 1.48

21.7

256 512

0.4394

0.2116 0.95

33.5

512 512

0.4426

0.2094 0.93

34.3

512 1024

0.4492

0.2196 0.51

62.0

VGG-16

1

1

0.3442

0.1342

32

­

256 256

0.3500

0.1320 0.89

36.2

256 512

0.3518

0.1356 0.55

58.3

512 512

0.3630

0.1452 0.49

64.9

512 1024

0.3788

0.1566 0.31

101.7

Layer
Conv1 Conv2 Conv3 Conv4 Conv5
FC6 FC7 FC8
Avg

Table 2: The compression ratio per layer of the AlexNet

The proposed algorithm

The state-of-the-art algorithm

QP BPW Compression ratio BPW

Compression ratio

256 2.24

14.3 6.57

4.9

256 2.41

13.3 3.02

10.6

256 2.80

11.4 2.70

11.9

256 2.94

10.9 2.92

11.0

256 2.93

10.9 3.02

10.6

512 0.77

41.3 0.76

41.8

512 0.98

32.6 0.79

40.7

512 1.44

22.2 1.87

17.1

­ 0.95

33.5 0.92

34.7

been compressed. The BPW in the table means the bits per weight, which is 32 for the uncompressed network. We tried different combinations of the quantization parameters for the convolution layers and the full connection layers to obtain a better trade-off to the overall performance.
From Table 1, we can see that the proposed algorithms can achieve quite significant compression ratios within very small Top-1 error and Top-5 error increase compared with the original network without compression. For the AlexNet, when both the convolution layers and the full connection layers are using the quantization parameters less or equal to 512, both the Top-1 error and the Top-5 error only increase a little. The VGG-16 network is more sensitive to the quantization parameters compared with the AlexNet. When the quantization parameter of the convolution layers reaches 512, there will be some obvious performance losses. From the table, we can also observe one of the advantages of the proposed algorithm is that we can have more precise granularity to adjust the balance between the errors and bits per weight by adjusting the quantization parameters.
3.1 Comparison of the proposed method with the state-of-the-art method
Besides the results of showing the overall compression efficiency with different quantization parameters. We also derive one representative quantization parameter (Conv QP 256 and FC QP 512) to show the compression efficiency of each layer and also to compare with the state-of-the-art network compression algorithm (Han et al. (2015)). The experimental results of the compression ratio per layer of the AlexNet and the VGG-16 network are shown in Table 2 and Table 3, respectively. The proposed algorithm achieves very similar Top-1 and Top-5 errors compared with the state-of-the-art algorithm. For the compression ratio, the proposed algorithm achieves similar or slightly worse compression ratio for the AlexNet. While for the VGG-16, the proposed algorithm achieves better compression ratio.
Although the overall compression ratios of both the AlexNet and the VGG-16 are similar, the compression ratios of each specified layer for each layer are totally different. For both the
5

Under review as a conference paper at ICLR 2018

Table 3: The compression ratio per layer of the VGG-16

The proposed algorithm

The state-of-the-art algorithm

Layer QP BPW Compression ratio BPW

Compression ratio

Conv1_1 256 3.75

8.5 9.59

3.3

Conv1_2 256 2.99

10.7 2.24

14.3

Conv2_1 256 2.95

10.9 2.85

11.2

Conv2_2 256 2.80

11.4 2.98

10.7

Conv3_1 256 2.54

12.6 3.57

9.0

Conv3_2 256 2.34

13.7 1.81

17.6

Conv3_3 256 2.38

13.5 2.87

11.2

Conv4_1 256 2.19

14.6 2.33

13.7

Conv4_2 256 2.03

15.8 1.90

16.9

Conv4_3 256 2.10

15.2 2.39

13.4

Conv5_1 256 2.18

14.7 2.56

12.5

Conv5_2 256 2.05

15.6 2.09

15.3

Conv5_3 256 1.97

16.3 2.56

12.5

FC6 512 0.27

119.6 0.35

90.9

FC7 512 1.73

44.0 0.40

80.0

FC8 512 1.23

26.1 1.68

19.1

Avg ­ 0.55

58.3 0.66

48.8

AlexNet and VGG-16, we can observe a quite different compression ratio for the first layer of the whole network. The proposed algorithm compresses the first convolution layer with much higher compression ratio compared with the state-of-the-art algorithm. According to our common sense, compressing the first layer of the whole network with a quite large quantization parameter may lead to bad performance for the whole network. This can partially reveal that different layers are with different importance for the whole network and thus should be applied to a smaller quantization parameter. Right now, we are just using different quantization parameters for the convolution layers and full connection layers. We can anticipate that the proposed algorithm will bring better compression ratio without influencing the errors if we can adjust the quantization parameters for each specified layer.
Also, for the FC7 layer of the VGG-16, we can see that the proposed algorithm achieves quite less compression ratio compared with that of the FC6 layer and also the FC7 layer of the previous algorithm. This phenomenon can be explained by Fig. 3. Through the comparison between FC6 and FC7, we can observe that the energy is much more compact for FC6 compared with FC7. The more compact the energy, the easier the high compression ratio will be achieved through the quantization process. You may also find out that the compression ratio of the convolution layers is even incomparable with the FC7. The reason is that the convolution layers is coded with smaller quantization parameters and with higher peak signal to noise ratio (PSNR).
4 Discussions
To better analyze the compression ratios of different layers, we also present the PSNR of each layer of the VGG-16 when coded using different quantization parameters as shown in Table 4. From Table 4, we can first obviously see that the PSNR of the full connection layer is lower than that of the convolution layer. Also, for the convolution layers, we can see an obvious trend of PSNR decrease from high to low. This is partially due to the reason that the values of the previous layer are generally larger than the following layers. Especially the difference between the values of the convolution layers and full connection layers is quite large. Therefore, there will be obvious PSNR differences for the convolution layers and full connection layers. It should be also noted that the convolution layers coded with higher PSNR are actually achieving a suitable bit allocation for the whole scheme since the precision of the convolution layers can propagate to more layers and have a large influence on the final classification accuracy.
6

Under review as a conference paper at ICLR 2018

Table 4: The quantization parameter and PSNR relationship of different layers

Layer QP PSNR(dB) QP PSNR(dB) QP PSNR(dB)

Conv1_1 256

46.24 512

40.20 1024

34.51

Conv1_2 256

40.55 512

34.84 1024

29.36

Conv2_1 256

36.80 512

30.94 1024

25.32

Conv2_2 256

37.21 512

31.42 1024

25.97

Conv3_1 256

34.20 512

28.43 1024

23.13

Conv3_2 256

32.90 512

27.31 1024

22.21

Conv3_3 256

33.64 512

28.01 1024

22.93

Conv4_1 256

30.63 512

25.11 1024

20.24

Conv4_2 256

29.37 512

24.04 1024

19.39

Conv4_3 256

30.05 512

24.61 1024

19.91

Conv5_1 256

30.45 512

24.95 1024

20.28

Conv5_2 256

30.24 512

24.95 1024

20.28

Conv5_3 256

30.30 512

25.08 1024

20.73

FC6 256

18.37 512

15.09 1024

13.42

FC7 256

20.00 512

14.01 1024

9.27

FC8 256

25.43 512

19.41 1024

13.42

Table 5: The performance when compressing FC6 of VGG-16 individually

FC6 QP Top1-Error Top5-Error BPW Compression ratio

VGG-16

1

0.3442

0.1342

32

­

256 0.3470 0.1338 0.61

52.4

512 0.3452 0.1348 0.27

119.6

1024

0.3488

0.1348 0.13

250.8

2048

0.3814

0.1514 0.06

581.8

4096

0.9044

0.7974 0.03

1167.9

As we have explained above, all the layers are not independent, the output of the current layer will be the input of the next layer. Therefore, compressing one layer is totally different from compressing all the layers. In Table 5, we also show the case when we only compress the FC6 layer of the VGG-16. We can obviously see that if we do not compress the other layers, we can compress the FC6 with 250 times or even 500 times without influencing the overall classification accuracy. It obviously demonstrates that the accuracy of one layer will have influence on the compression of the other layers. This can also be extended to the combination of some layers. If we keep some of the layers uncompressed, we can compress the other layers with higher compression ratio while maintaining the overall performance. For example, if we compress the layers with more weights while keeping the layers with fewer weights unchanged, it may lead to better compression efficiency. This is actually an extreme case of the bit allocation, in which some of the layers are losslessly coded.
5 Conclusion
In this paper, to improve the compression efficiency of the deep neural networks, we try to organize the kernels in different channels into a frame and encode the frames using blockbased video coding methods. First, we try to reshape the weights in different channels into a pseudo sequence. Second, after obtaining all the frames in the videos, we will convert the weight into the PCA domain and perform intra and inter prediction. Finally, the uniform quantization and entropy coding will be used to encode the residue blocks. After the above steps, we can compress the classical VGG-16 model by 58 times, which is better than the state-of-the-art method. In the future, we will try to further improve the compression performance by using more flexible encoding structure, involving more previously coded blocks for a better prediction, and assigning optimal quantization parameter to each layer.
7

Under review as a conference paper at ICLR 2018
References
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in Neural Information Processing Systems, pp. 3123­3131, 2015.
Yuanying Dai, Dong Liu, and Feng Wu. A convolutional neural network approach for postprocessing in hevc intra coding. In Multimedia Modeling, 2017.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009.
Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. Compression artifacts reduction by a deep convolutional network. In The IEEE International Conference on Computer Vision (ICCV), December 2015.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.
B. Knoll and N. d. Freitas. A machine learning perspective on predictive coding with paq8. In 2012 Data Compression Conference, pp. 377­386, April 2012. doi: 10.1109/DCC.2012.44.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097­1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/ 4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf.
Yue Li, Dong Liu, Houqiang Li, Li Li, Feng Wu, Hong Zhang, and Haitao Yang. Convolutional neural network-based block up-sampling for intra frame coding. IEEE Transactions on Circuits and Systems for Video Technology, 2017.
Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1­9, June 2015. doi: 10.1109/CVPR.2015.7298594.
Haojin Yang, Martin Fritzsche, Christian Bartz, and Christoph Meinel. Bmxnet: An open-source binary neural network implementation based on mxnet. arXiv preprint arXiv:1705.09864, 2017.
Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank and sparse decomposition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7370­7379, 2017.
Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional networks for classification and detection. IEEE transactions on pattern analysis and machine intelligence, 38(10):1943­1955, 2016.
8

