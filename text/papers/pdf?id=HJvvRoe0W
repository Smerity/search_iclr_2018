Under review as a conference paper at ICLR 2018
CONVOLVING DNA USING TWO-DIMENSIONAL HILBERT CURVE REPRESENTATIONS
Anonymous authors Paper under double-blind review
ABSTRACT
The folding structure of the DNA molecule combined with helper molecules, also referred to as the chromatin, is highly relevant for the functional properties of DNA. The chromatin structure is largely determined by the underlying primary DNA sequence, though the interaction is not yet fully understood. In this paper we develop a convolutional neural network that predicts key determinants of chromatin structure from primary DNA sequence. Key in our approach is the transformation of the input data from a sequence to an image using the Hilbert curve, which has several desirable properties from a biological point of view. Our experiments show that the method outperforms several existing methods both in terms of prediction accuracy and training time.
1 INTRODUCTION
DNA is often perceived as a sequence over the letters {A,C,G,T }, the alphabet of nucleotides. This sequence constitutes the code that acts as a blueprint for all processes taking place in a cell. But beyond merely reflecting primary sequence, DNA is a molecule, which implies that DNA assumes spatial structure and shape. The spatial organization of DNA is achieved by integrating ("recruiting") other molecules, the histone proteins, that help to assume the correct spatial configuration. The combination of DNA and helper molecules is called chromatin; the spatial configuration of the chromatin, finally, defines the functional properties of local areas of the DNA (de Graaf & van Steensel, 2013).
Chromatin can assume several function-defining epigenetic states, where states vary along the genome (Ernst et al., 2011). The key determinant for spatial configuration is the underlying primary DNA sequence: sequential patterns are responsible for recruiting histone proteins and their chemical modifications, which in turn give rise to or even define the chromatin states. The exact configuration of the chromatin and its interplay with the underlying raw DNA sequence are under active research. Despite many enlightening recent findings (e.g. Brueckner et al., 2016; The ENCODE Project Consortium, 2012; Ernst & Kellis, 2013), comprehensive understanding has not yet been reached. Methods that predict chromatin related states from primary DNA sequence are thus of utmost interest.
In machine learning, many prediction methods are available, of which deep neural networks have recently been shown to be very promising in many applications (LeCun et al., 2015). However, the immediate use of deep neural networks in predicting biologically (or clinically) relevant features from primary DNA sequence has remained less clear, despite many recent advances (see Angermueller et al. (2016) for a review). We reckon that the issue is threefold. First, existing convolutional neural network (CNN) based approaches for genome sequence data directly operate on the sequence (e.g. Nguyen et al., 2016); however, CNNs have proven to be most powerful when operating on multidimensional input, such as in image classification (Simonyan & Zisserman, 2014). Second, treating genome sequence data as just a sequence neglects its inherent and biologically relevant spatial configuration. Third, due to the lack of comprehensive understanding with respect to the structure of the chromatin, sensible suggestions for higher-dimensional representations of DNA do not exist. In summary, despite being highly desirable, higher-dimensional representations of DNA that cater to the particular power of CNN-based applications have not yet been presented.
1

Under review as a conference paper at ICLR 2018
To better take the spatial features of DNA sequences into account, this paper presents a novel kind of DNA representation by mapping DNA sequences to higher-dimensional images using space-filling Hilbert curves. Space-filling curves map a 1-dimensional line to a 2-dimensional space by mapping each element of the sequence to a pixel in the 2D image. In this work, we show that representing DNA-sequences with 2D images using Hilbert curves improves on the currently available methods from (Pahm et al., 2005; Higashihara et al., 2008; Nguyen et al., 2016) in terms of both prediction accuracy of epistatic state and efficiency.
To understand the benefits of our suggestion, we first have to realize that Hilbert curves are optimal as a space-filling technique with respect to a variety of criteria. (i): [Continuity] Hilbert curves optimally ensure that the pixels representing two sequence elements that are close within the sequence are also close within the image Bader (2016); Aftosmis et al. (2004). (ii): [Clustering property] Cutting out rectangular subsets of pixels yields a minimum amount of disconnected subsequences Moon et al. (2001). (iii): If a rectangular subimage cuts out two subsequences that are disconnected in the original sequence, chances are maximal that the two different subsequences are relatively far apart (see our analysis in Appendix A).
The combination of these points arguably renders Hilbert curves an optimal choice for representing DNA sequence as two-dimensional images. (i) is a basic requirement for mapping short-term sequential relationships, which are ubiquitous in DNA (such as codons, motifs or intron-exon structure). (ii) relates to the structure of the chromatin, which - without all details being fully understood - is tightly packaged and organized in general. Results from Elgin (2012) indicate that when arranging DNA sequence based on Hilbert curves, contiguous areas belonging to identical chromatin states cover rectangular areas. In particular, the combination of (i) and (ii) motivate the application of convolutional layers on Hilbert curves derived from DNA sequence: rectangular subspaces, in other words, submatrices encoding the convolution operations, contain a minimum amount of disconnected pieces of DNA. (iii) finally is beneficial insofar as long-term interactions affecting DNA can also be mapped. This in particular applies to so-called enhancers and silencers, which exert positive (enhancer) or negative (silencer) effects on the activity of regions harboring genes, even though they may be far apart from those regions in terms of sequential distance.
1.1 RELATED WORK
Since Watson and Crick first discovered the double-helix model of DNA structure in 1953 (Watson & Crick, 1953), researchers have attempted to interpret biological characteristics from DNA. DNA sequence classification is the task of determining whether a sequence S belongs to an existing class C, and this is one of the fundamental tasks in bio-informatics research for biometric data analysis (Z. Xing, 2010). Many methods have been used, ranging from statistical learning (Vapnik, 1998) to machine learning methods (Michalski et al., 2013). Deep neural networks (LeCun et al., 2015) form the most recent class of methods used for DNA sequence classification (R.R. Bhat, 2016; Salimans et al., 2016; Zhou & Troyanskaya, 2015; Angermueller et al., 2016).
The dominant DNA sequence related algorithms focus on understanding DNA from a sequential perspective. However, as discussed before, the spatial structure of DNA needs to be taken into consideration at the representational level, in particular since the performance of deep learning algorithms depends heavily on the representation of the data they are given.
To the best of our knowledge, only few other works aim to predict chromatin state from sequence. Both Pahm et al. (2005) and Higashihara et al. (2008) use support vector machines (SVM) to predict chromatin state from DNA sequence features. While Pahm et al. (2005) use the entire set of features as input to the SVM, Higashihara et al. (2008) use random forests to pre-select a subset of features that are expected to be highly relevant for prediction of chromatin state to use as input to the SVM. Only Nguyen et al. (2016) use a CCN as we do. There are two major differences between their approach and ours. First and foremost, while we use a space-filling curve to transform the sequence data into an image-like tensor, Nguyen et al. (2016) keep the sequential form of the input data. Second, the model architecture is different: the network in Nguyen et al. (2016) consists of two convolution layers followed by pooling layers, a fully connected layer and a sigmoid layer, while our model architecture is more complicated (see Methods). Residual connections are applied in our network to encourage the reuse of learned features, and the network is far deeper. The main difference is the format of the input: we use images as input for the network while their model uses
2

Under review as a conference paper at ICLR 2018
sequences. Apart from Elgin (2012), the only example we are aware of where Hilbert curves were used to map DNA sequence into two-dimensional space is from Anders (2009), who demonstrated the power of Hilbert curves for visualizing DNA. Beyond our theoretical considerations, these last two studies suggest there are practical benefits of mapping DNA using Hilbert curves.
1.2 CONTRIBUTION
Our contributions are twofold. First, we propose a method to transform DNA sequence patches into two-dimensional image-like arrays to enhance the strengths of CNNs using space-filling Hilbert Curves. Second, we predict chromatin state using a CNN that, in terms of architecture, resembles conventional CNNs for image classification. Our experiments demonstrate the benefits of our approach: we outperform all existing approaches for predicting the chromatin state from raw DNA sequence both in terms of prediction accuracy and runtime. In summary, we present a novel, powerful way to harness the power of CNNs in image classification for predicting biologically relevant features from primary DNA sequence.
2 METHODS
2.1 DNA SEQUENCE REPRESENTATION
We transform DNA sequences into images through three steps. First, we represent a sequence as a list of k-mers. Next, we transform each k-mer into a one-hot vector, which results in the sequence being represented as a list of one-hot vectors. Finally, we create an image-like tensor by assigning each element of the list of k-mers to a pixel in the image using Hilbert curves. Each of the steps is explained in further detail below.
From a molecular biology point of view, the nucleotides that constitute a DNA sequence do not mean much individually. Instead, nucleotide motifs play a key role in protein synthesis. In bioinformatics it is common to consider a sequence's k-mers, defined as the k-letter words from the alphabet {A,C,G,T} that together make up the sequence. In computer science the term q-gram is more frequently used, and is often applied in text mining (Tomovic et al., 2006). As an example, the sequence TGACGAC can be transformed into the list of 3-mers {TGA, GAC, ACG, CGA, GAC}. The first step in our approach is thus to transform the DNA sequence into a list of k-mers. Previous work has shown that 3-mers and 4-mers are useful for predicting epigenetic state (Pahm et al., 2005; Higashihara et al., 2008). Through preliminary experiments, we found that k = 4 yields the best performance: lower values for k result in reduced accuracy, while higher values yield a high risk of overfitting. Only for the Splice dataset (see experiments) we used k = 1 to prevent overfitting, as this is a small dataset.
In natural language processing, it is common to represent words as one-hot vectors (Goldberg, 2017). Each element of such a vector corresponds to a word, and a vector of length N can thus be used to represent N different words. A one-hot vector has a one in the position that corresponds to the word the position is representing, and a zero in all other positions. In order to represent all k-mers in a DNA sequence, we need a vector of length 4k, as this is the number of words of length k that can be constructed from the alphabet {A,C,G,T}. For example, if we wish to represent all 1-mers, we can do so using a one-hot vector of length 4, where A corresponds to [1 0 0 0], C to [0 1 0 0], G to [0 0 1 0] and T to [0 0 0 1]. In our case, the DNA sequence is represented as a list of 4-mers, which can be converted to a list of one-hot vectors each of length 44 = 256.
Our next step is to transform the list of one-hot vectors into an image. For this purpose, we aim to assign each one-hot vector to a pixel. This gives us a 3-dimensional tensor, which is very much like the tensor that serves as an input to image classification networks: the color of a pixel in an RGBcolored image is represented by a vector of length 3, while in our approach each pixel is represented by a one-hot vector of length 256.
What remains now is to assign each of the one-hot vectors in the list to a pixel in the image. For this purpose, we can make use of space-filling curves, as they can map 1-dimensional sequences to a 2-dimensional surface preserving continuity of the sequence (Bader, 2016; Aftosmis et al., 2004). Various types of space-filling curves are available. We have compared the performance of several such curves, and concluded that Hilbert curves yield the best performance (Appendix A).
3

Under review as a conference paper at ICLR 2018
This corresponds with our intuition: the Hilbert curve has several properties that are advantageous in the case of DNA sequences, as discussed in the introduction section. The Hilbert curve is a well-known space-filling curve that is constructed in a recursive manner (Fig. 1): in the first iteration, the curve is divided into four parts, which are mapped to the four quadrants of a square. In the next iteration, each quadrant is divided into four sub-quadrants, which, in a similar way, each hold 1/16th of the curve. The quadrants of these sub-quadrants each hold 1/64th of the curve, etc. By construction, the Hilbert curve yields a square image of size 2n × 2n, where n is the order of the curve (see Fig. 1). However, a DNA sequence does not necessarily have 2n  2n k-mers. In order to fit all k-mers into the image, we need to choose n such that 2n 2n is at least the number of k-mers in the sequence, and since we do not wish to make the image too large, we pick the smallest such n. In many cases, a large fraction of the pixels then remains unused, as there are fewer k-mers than pixels in the image. By construction, the used pixels are located in upper half of the image. Cropping the picture by removing the unused part of the image yields rectangular images, and increases the fraction of the image that is used (Figure 1e). In most of our experiments we used sequences with a length of 500 base pairs, which we convert to a sequence of 500 - 4 + 1 = 497 4-mers. We thus need a Hilbert curve of order 5, resulting in an image of dimensions 25 × 25 × 256 = 32 × 32 × 256 (recall that each pixel is assigned a one-hot vector of length 256). Almost half of the resulting 1024 pixels are filled, so we remove half of the image and end up with an image of size 16 × 32 × 256. The data now has the appropriate form to input in our model.
Figure 1: Hilbert Curve and cropped image
2.2 HILBERT-CNNS Modern CNNs or other image classification systems mainly focus on gray-scale images and standard RGB images, resulting in a full vector of length 1 or 3, respectively, for each pixel. In our approach, each pixel in the generated image is assigned a one-hot vector representing a k-mer. For increasing k, the length of the vector and thus the image dimension increases. Here, we use k = 4 resulting in 256 channels, which implies that each channel contains very sparse information. With the curse of dimensionality in full swing, standard network architectures applied to such images are prone to severe overfitting.
4

Under review as a conference paper at ICLR 2018
Figure 2: A simplified version of Hilbert-CNN with two computational block
Here, we design a specific Hilbert-CNN for the kind of high dimensional image that is generated from a DNA sequence using a space-filling curve. The network has L layers and each layer implements a non-linear function Fl(xl) where l is the index of the hidden layer with output xl+1 . The function Fl(xl) consists of various layers such as convolution (denoted by c), batch normalization (bn), pooling (p) and non-linear activation function (af ). The first part of the network has the objective to reduce the sparseness of the input image (Figure 2, and consists of the consecutive layers [c, bn, c, bn, af, p]. The main body of the network enhances the ability of the CNN to extract the relevant features from DNA space-filling curves. For this purpose, we designed a specific Computational Block inspired by the ResNet residual blocks (He et al., 2015). The last part of the network consists of 3 fully-connected layers, and softmax is used to obtain the output classification label. The complete model is presented in Table 1, and code will be provided on Github upon publication of the paper. A simplified version of our network with two Computational Blocks is illustrated in Figure 2.
Computation Block. In the Computation Block first the outputs of two Residual Blocks and one identity mapping are summed, followed by a bn and an af layer (Figure 2). In total, the computational block has 4 convolutional layers, two in each Residual Block (see Figure 3). The Residual Block first computes the composite function of five consecutive layers, namely [c, bn, af, c, bn], followed by the concatenation of the output with the input tensor. The residual block concludes with an af . The Residual Block can be viewed as a new kind of non-linear layer denoted by Residual l[kj,kj+1,dlink,dout], where kj and kj+1 are the respective filter sizes of the two convolutional layers. dlink and dout are the dimensions of the outputs of the first convolutional layer and the Residual Block, respectively, where dlink < dout. dlink; this condition simplifies the network architecture and reduces the computational cost. The Computational Block can be denoted as C[k1,k2,k3,k4] with the two residual blocks defined as Residual 1[k1,k2,dlink,dout] and Residual 2[k3,k4,dlink,dout]. Note that here we chose the same dlink and dout for both Residual Blocks in a Computational Block.
Implementation details. Most convolutional layers use small squared filters of size 2, 3 and 4, except for the layers in the first part of the network, where large filters are applied to capture long range features. We use Exponential Linear Units (ELU, Clevert et al. (2015)) as our activation function af to reduce the effect of gradient vanishing: preliminary experiments showed that ELU preformed significantly better than other activation functions (data not shown). For the pooling layers p we used Average pooling. Average pooling outperformed Max pooling in terms of prediction accuracy by more than 2% in general, as it reduces the high variance of the sparse generated images. Cross entropy was used as the loss function.
5

Under review as a conference paper at ICLR 2018

Figure 3: Residual Block

Table 1: Network Architecture

Layers
Input Conv1 Conv2 Activation Average-Pool ComputationBlock ComputationBlock Average-Pool ComputationBlock ComputationBlock ComputationBlock Average-Pooling BN,Activation Average-Pool
Classification layer1
Classification layer2
Classification layer3

Description
7 × 7 conv, BN 5 × 5 conv, BN
2 × 2 filter, stride 2 [8,4,4,3], dlink = 4 [3,3,3,3], dlink = 4 2 × 2 filter, stride 2 [2,4,4,3], dlink = 4 [2,2,2,2], dlink = 4 [3,2,2,3], dlink = 4 2 × 2 filter, stride 2
2 × 2 filter, stride 2 64D FC layer activation dropout with 0.5 100D FC layer activation dropout with 0.5 50D FC layer, softmax

Output size 16,32, 256 16,32, 64 16,32, 64
8, 16, 64 8, 16, 32 8, 16, 32 4, 8, 32 4, 8, 32 4, 8, 32 4, 8, 32 2, 4, 32
1, 2, 32

Table 2: DNA Datasets

Name H3 H4 H3K9ac
H3K14ac
H4ac
H3K4me1
H3K4me2
H3K4me3
H3K36me3
H3K79me3
Splice

Samples 14965 14601 27782
33048
34095
31677
30683
36799
34880
28837
3190

Description H3 occupancy H4 occupancy H3K9 acetylation relative to H3 H3K14 acetylation relative to H3 H4 acetylation relative to H3 H3K4 monomethylation relative to H3 H3K4 dimethylation relative to H3 H3K4 trimethylation relative to H3 H3K36 trimethylation relative to H3 H3K79 trimethylation relative to H3 Splice-junction Gene Sequences

3 EXPERIMENTS
We test the performance of our approach using ten public available datasets from Pokholok et al. (2005). The datasets contain DNA subsequences with a length of 500 base pairs. Each sequence is labeled either as "positive" or "negative", indicating whether or not the subsequence contains regions that are wrapped around a histone protein. The ten datasets each consider a different type of histone protein, indicated by the name of the dataset. Details can be found in Table 2
A randomly chosen 90% of the dataset is used for training the network, 5% is used for validation and early stopping, and the remaining (5%) is used for evaluation. We train the Hilbert-CNN network using the AdamOptimizer (Kingma & Ba, 2017)1. The learning rate is set to 0.003, the batch-size was set to 300 samples and the maximum number of epochs is 10. After each epoch the level of generalization is measured as the accuracy obtained on the validation set. We use early stopping to prevent overfitting. To ensure the model stops at the correct time, we combine the GL measurement (Prechelt, 1998) of generalization capability and the No-Improvement-In-N-Steps(Nii-N) method
1The LSTM model was implemented in Keras (Chollet et al., 2015), all other models were implemented in Tensorflow (Abadi et al., 2015).
6

Under review as a conference paper at ICLR 2018

(Prechelt, 1998). For instance, Nii-2 means the train process is terminated when generalization capability is not improved in two consecutive epochs.
To compare the Hilbert-CNN model to existing models, we reconstruct several state-of-the-art models and measure both performance and training time. In tight communication with the authors, we reconstructed the Seq-CNN model presented in (Nguyen et al., 2016) (the original software was no longer available), see Appendix C for detailed settings.
Additionally, we compared our method to two approaches commonly used in other applications. The first method employs Hilbert-CNN where the input is a one-dimensional sequence, which can be thought of as a "flattened" version of the Hilbert curve. The input sequence is obtained by first constructing the Hilbert curve, and then reading this curve row by row (Goldberg, 2017). The same model architecture is used as for Hilbert-CNN. The second approach is an LSTM where the socalled 4-mer profile of the sequence is used as input. A 4-mer profile is a list containing the number of occurrences of all 256 4-mers of the alphabet {A,C,G,T} in a sequence. Preliminary tests showed that using all 256 4-mers resulted in overfitting, and including only the 100 most frequent 4-mers is sufficient. Details of the LSTM architecture can be found in Table 6 in Appendix C.
In order to test whether our method is also applicable to other DNA sequence classification tasks than chromatin state prediction only, we performed additional tests on the splice-junction gene sequences dataset from (Lichman, 2013). Most of the DNA sequence is unused, and splice-junctions refer to positions in the genetic sequence where the transition from an unused subsequence (intron) to a used subsequence (exon) or vice versa takes place. The dataset consists of DNA subsequences of length 61 base pairs, and each of the sequences is known to be a intron-to-exon splice-junction, an exon-to-intron splice junction or neither. As the dataset is relatively small, we used 1-mers instead of 4-mers. Note that the sequences are much shorter than for the other datasets, resulting in smaller images (dimensions 8 × 8 × 4).

4 RESULTS

Test accuracy and runtimes obtained with the SVM-based approach by Higashihara et al. (2008), the Seq-CNN model from Nguyen et al. (2016), the 1-d sequence CNN, the LSTM method and HilbertCNN are listed in Table 3. The results for the SVM method are taken from Higashihara et al. (2008), all other results are obtained using our own implementations.
The results show that SVM and Seq-CNN, two methods available in the literature, were both outperformed by Hilbert-CNN, and Seq-CNN was also outperformed by the 1-d sequence CNN. LSTM shows poor performance. In general, Hilbert-CNN outperformed all other methods based on both prediction accuracy and treatment time.

Table 3: Prediction accuracy and runtime obtained with an SVM-based method, Seq-CNN, a 1-d sequence, an LSTM and Hilbert-CNN. The results for SVM are taken from Table 12 in Higashihara et al. (2008). Runtimes are presented as min:sec. In splice dataset, Seq-CNN performed best when using 4-mers, while for HilbertCNN 1-mers yielded the best performance.

Dataset
H3 H4 H3K9ac H3K14ac H4ac H3K4me1 H3K4me2 H3K4me3 H3K36me1 H3K79me1 Splice

SVM acc
86.47% 87.82% 75.08% 73.28% 72.06% 69.71% 68.97% 68.57% 75.19% 80.58% 94.70%

Seq-CNN acc time 79.25% 95:23 81.86% 95:53 68.76% 173:18 68.31% 180:56 64.80% 181:33 62.60% 192:20 62.38% 188:13 62.33% 162:32 72.20% 161:12 75.07% 158:34 91.82% 35:12

1-d sequence CNN

acc time

81.02%

3:45

87.12%

5:32

75.81%

6:12

72.52%

6:09

73.84%

6:05

67.49%

7:12

72.36%

7:04

70.65%

6:54

73.17%

6:34

78.13%

5:43

91.19%

2:32

LSTM acc time 64.13% 35:43 63.82% 45:32 63.07% 76:09 59.27% 81:21 60.63% 93:32 60.43% 93:44 61.45% 94:22 58.03% 96:03 60.78% 93:48 63.84% 64:28 96.23% 6:42

Hilbert-CNN acc time
87.98% 3:40 89.04% 4:02 79.05% 8:40 76.30% 10:01 78.73% 10:32 72.60% 11:12 74.18% 9:03 74.19% 9:54 77.69% 9:45 81.21% 9:13 96.86% 1:30

The good performance of Hilbert-CNN observed above may either be attributable to the conversion from DNA sequence to image, or to the use of the Hilbert curve. In order to address this question, we

7

Under review as a conference paper at ICLR 2018
results
Figure 4: Hilbert-CNN with different mapping strategies
adapted our approach by replacing the Hilbert curve with other space-filling curves and compared their prediction accuracy. Besides the Hilbert curve, other space-filling curves exist (Moon et al., 2001) (see Appendix A ). In Figure 4, we compare the performance of our model with different mapping strategies in various datasets as displayed. We find that the images generated by the spacefilling Hilbert curve provide the best accuracy on most datasets and the 1-d sequence performs worst.
5 DISCUSSION
In this paper we aimed at exploiting the spatial features of DNA sequences by combining a novel method to map the sequences into images using space-filling Hilbert Curves with ad-hoc designed CNN. We showed that representing DNA-sequences with 2D images using Hilbert curves is an effective way to improve prediction accuracy of epistatic state and training time. Indeed, Hilbert-CNN shows improved prediction accuracy and training time compared to currently available chromatin state prediction methods (Pahm et al., 2005; Higashihara et al., 2008; Nguyen et al., 2016) as well as an LSTM model and a CNN with a 1D input sequence. Hilbert curves have several properties that are desirable for DNA sequence classification. The intuitive motivation for the use of Hilbert curves is supported by good results when comparing Hilbert curves to other space-filling curves. Additionally, Hilbert curves have previously been shown to be useful for visualization of DNA sequences (Anders, 2009). We showed that Hilbert-CNN is not only capable of predicting chromatin state, but can also predict the presence or absence of splice-junctions in DNA subsequences. This suggests that Hilbert curves could be useful for DNA sequence classification problems in general; in fact, our approach may be applicable to other types of sequence prediction problems. The main limitation of Hilbert curves is their fixed length, which implies that the generated image contains some empty spaces. These spaces may occupy the computation resources and affect the classification accuracy.
8

Under review as a conference paper at ICLR 2018
REFERENCES
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G.S.Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https: //www.tensorflow.org/. Software available from tensorflow.org.
M. Aftosmis, M. Berger, and S. Murman. Applications of space-filling-curves to cartesian methods for cfd. 42nd AIAA Aerospace Sciences Meeting and Exhibit, May 2004. doi: 10.2514/6. 2004-1232.
S. Anders. Visualization of genomic data with the Hilbert curve. Bioinformatics, 25(10):1231­1235, 2009.
C. Angermueller, T. Pärnamaa, L. Parts, and O. Stegle. Deep learning for computational biology. Molecular Systems Biology, 12(7):878, 2016.
M. Bader. Space-filling curves: an introduction with applications in scientific computing. Springer, 2016.
L. Brueckner, J. van Arensbergen, W. Akhtar, L. Pagie, and B. van Steensel. High-throughput assessment of context-dependent effects of chromatin proteins. Epigenetics and Chromatin, 9(1): 43, 2016.
F. Chollet et al. Keras. https://github.com/fchollet/keras, 2015.
D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
C.A. de Graaf and B. van Steensel. Chromatin organization: form to function. Current Opinion in Genetics and Development, 23(2):185­190, 2013.
S.C.R. Elgin. http://slideplayer.com/slide/7605009/, 2012. Slides 55-57.
J. Ernst and M. Kellis. Interplay between chromatin state, regulator binding, and regulatory motifs in six human cell types. Genome Research, 23(7):1142­1154, 2013.
J. Ernst, P. Kheradpour, T.S. Mikkelsen, N. Shoresh, Ward L.D., et al. Mapping and analysis of chromatin state dynamics in nine human cell types. Nature, 473(7345):43­49, 2011.
Y. Goldberg. Neural network methods for natural language processing. Morgan & Claypool, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition, Dec 2015. URL https://arxiv.org/abs/1512.03385.
M. Higashihara, J.D. Rebolledo-Mendez, Y. Yamada, and K. Satou. Application of a feature selection method to nucleosome data: Accuracy improvement and comparison with other methods. WSEAS Transactions on Biology and Biomedicine, 2008.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, Jan 2017. URL https: //arxiv.org/abs/1412.6980.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436­444, 2015.
M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ ml.
R. S. Michalski, J. G. Carbonell, and T.M. Mitchell. Machine Learning: An Artificial Intelligence Approach. Springer Berlin, 2013.
B. Moon, H.V. Jagadish, C. Faloutsos, and J.H. Saltz. Analysis of the clustering properties of the Hilbert space-filling curve. IEEE Transactions on Knowledge and Data Engineering, 13(1): 124­141, 2001. doi: 10.1109/69.908985.
9

Under review as a conference paper at ICLR 2018
N.G. Nguyen, V.A. Tran, D.L. Ngo, D. Phan, F.R. Lumbanraja, et al. DNA sequence classification by convolutional neural network. Journal of Biomedical Science and Engineering, 9(5):280­286, 2016.
T.H. Pahm, D.H. Tran, T.B. Ho, K. Satou, and G. Valiente. Qualitatively predicting acetylation and methylation areas in dna sequences. Genome Informatics, 16(2):3­11, 2005.
D.K. Pokholok, C.T. Harbison, S. Levine, M. Cole, N.M. Hannett, T.I. Lee, G.W. Bell, K. Walker, P. A. Rolfe, and E. Herbolsheimer. Genome-wide map of nucleosome acetylation and methylation in yeast. Cell, 122(4):517­527, 2005. doi: 10.1016/j.cell.2005.06.026.
Lutz Prechelt. Automatic early stopping using cross validation: quantifying the criteria. Neural Networks, 11(4):761­767, 1998.
X. Li R.R. Bhat, V. Viswanath. Deepcancer: Detecting cancer through gene expressions via deep generative learning. arXiv, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans, Jun 2016. URL https://arxiv.org/abs/1606. 03498.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arxiv. org/abs/1409.1556, 2014. URL CoRRabs/1409.1556(2014).
The ENCODE Project Consortium. An integrated encyclopedia of DNA elements in the human genome. Nature, 489(7414):57­74, 2012.
A. Tomovic, P. Janicic, and V. Keselj. n-gram-based classification and unsupervised hierarchical clustering of genome sequences. Computer Methods and Programs in Biomedicine, 81(2): 137­153, 2006. doi: 10.1016/j.cmpb.2005.11.007.
V. N. Vapnik. Statistical learning theory., volume 1. Wiley, 1998. J.D. Watson and F.H. Crick. Molecular structure of nucleic acids. Nature, 171(4356):737­738,
1953. E. Keogh Z. Xing, J. Pei. A brief survey on sequence classification. ACM, 2010. J. Zhou and O.G. Troyanskaya. Predicting effects of noncoding variants with deep learning­based
sequence model. Nature methods, 12:931­934, 2015.
10

Under review as a conference paper at ICLR 2018
A COMPARISON OF SPACE-FILLING CURVES WITH REGARD TO LONG-TERM
INTERACTIONS As noted before, long-term interactions are highly relevant in DNA sequences. In this section we consider these long-term interactions in four space-filling curves: the reshape curve, the snake curve, the Hilbert curve and the diag-snake curve. See Fig. 5 for an illustration.

(a) Reshape curve

(b) snake curve

(c) Diag-Snake curve (d) Hilbert curve

Figure 5: Space-filling curves

As can be seen in Fig. 5, mapping a sequence to an image reduces the distance between two ele-
ments that are far from one another in the sequence, while the distance between nearby elements
does not increase. Each of the curves does have a different effect on the distance between far-
away elements. In order to assess these differences, we use a measure that is based on the dis-
tance between two sequence elements as can be observed in the image. We denote this distance by LC(x,y) where x, y  S, with S the sequence and C the curve under consideration. Then for the sequence S = {A,B,C,D,E, · · · ,P } we obtain

· Lseq(A,P ) = 15 for the sequence; 
· Lreshape(A,P ) = 3 2 for the reshape curve;

·

Lsnakesnake(A,P

)

=

3 for 

the

snake

curve;

· Ldiag-snake(A,P ) = 3 2 for the diagonal snake curve.

· Lhilbert(A,P ) = 3 for the Hilbert curve;

We now introduce the following measure:
(C) = mean((C)) , max((C))
where the (C) is the set of the weighted distances between all pairs of the elements in the sequence. Here, (C) is a set containing the distance between any two sequence elements, weighted by their distance in the sequence:
(C) = {Lseq(x,y) · LC(x,y) | x, y  C, x = y}.
Note that a low max((C)) relative to mean((C)) implies that long-term interactions are strongly accounted for, so a high (C) is desirable.
(C) is evaluated for the four space-filling curves as well as the sequence representation for sequences of varying lengths.The results show that the Hilbert curve yields the highest values for (C) 4 and thus performs best in terms of retaining long-distance interactions.

11

Under review as a conference paper at ICLR 2018

Curve

Sequence Length : bits 16 64 256 1024 4096

Reshape 0.22 0.18 0.16 0.16 0.15

Snake

0.28 0.20 0.17 0.16 0.16

Diag-snake 0.22 0.17 0.16 0.15 0.15

Hilbert

0.30 0.24 0.22 0.21 0.21

Table 4: (C) for four space-filling curves,

evaluated in sequences of varying lengths.

B FROM SEQUENCE TO IMAGE
Figure 6 shows the conversion from DNA sequence to image.

Figure 6: Sequence to Image

12

Under review as a conference paper at ICLR 2018

C DETAILS OF ALTERNATIVE NEURAL NETWORKS

Table 5: Sequence Convolutional Model

Layer Convolution 1 Activation Max pooling Convolution 2 Activation Max pooling Dropout, 0.5 FC layer Activation Dropout, 0.5 Classifier

filter size 7
3 5
3

stride 2
3 2
2

output dim 60 60 30 30 100
2

Table 6: Bir-Direction LSTM

Layer Embedding
Conv 1
Max pooling Bir-LSTM 1 Bir-LSTM 2 Dropout Classifier

Description
1-by-3 convolution layer with 32 filter size activation function is RELU 1 × 2 max pooling layer 100 units 128 units 0.3 dropout rate sigmoid

13

