Under review as a conference paper at ICLR 2018
MELODY GENERATION FOR POP MUSIC VIA WORD REPRESENTATION OF MUSICAL PROPERTIES
Anonymous authors Paper under double-blind review
ABSTRACT
Automatic melody generation for pop music has been a long-time aspiration for both AI researchers and musicians. However, learning to generate euphonious melody has turned out to be highly challenging due to a number of factors. Representation of multivariate property of notes has been one of the primary challenges. It is also difficult to remain in the permissible spectrum of musical variety, outside of which would be perceived as a plain random play without auditory pleasantness. Observing the conventional structure of pop music poses further challenges. In this paper, we propose to represent each note and its properties as a unique `word,' thus lessening the prospect of misalignments between the properties, as well as reducing the complexity of learning. We also enforce regularization policies on the range of notes, thus encouraging the generated melody to stay close to what humans would find easy to follow. Furthermore, we generate melody conditioned on song part information, thus replicating the overall structure of a full song. Experimental results demonstrate that our model can generate auditorily pleasant songs that are more indistinguishable from human-written ones than previous models. 1
1 INTRODUCTION
Recent explosion of deep learning techniques has opened up new potentials for various fields of multimedia. Vision and language have been its primary beneficiary, particularly with rising interest in generation task. Considerable amount of recent works on vision and language have hinged beyond mere generation onto artistic aspects, often producing works that are indistinguishable from human works (Goodfellow et al. (2014); Radford et al. (2016); Potash et al. (2015)). On the other hand, it is only recently that deep learning techniques began to be applied to music, and the quality of the results are yet far behind those in other domains, as there are few works that demonstrate both euphonious sound and structural integrity that characterize the human-made musical contents. This unfortunate status holds true for both music in its physical audio format and its abstraction as notes or MIDI (Musical Instrument Digital Interface). Such lagging of deep learning-enabled music generation, particularly in music as abstraction, can be attributed to a number of factors. First, a note in a musical work contains various properties, such as its position, pitch, length, and intensity. The overall tendency of each property and the correlation among them can significantly vary depending on the type of music, which makes it difficult to model. Second, the boundary between musical creativity and plain clumsiness is highly indefinite and difficult to quantify, yet exists. As much as musical creativity cannot be limited, there is yet a certain aspect about it that makes it sound like (or not sound like) human-written music. Finally, music is not merely a series of notes, but entails an overall structure of its own. Classical music pieces are well-known for their high structural complexity, and much of pop music follows the general convention of verse - pre-chorus - chorus structure. This structure inevitably necessitates different modeling of musical components; for example, notes in the chorus part generally tend to be more high-pitched. It goes without saying that these structure-oriented variations further complicate the modeling of music generation.
1Code and dataset will be publicly available prior to the publication of this paper. Check the following URL for our demos: https://drive.google.com/open?id=0B7FM2yuGVreASmw3b2YxM2xtdUk
1

Under review as a conference paper at ICLR 2018
In this paper, we propose a new model for music generation, specifically symbolic generation of melodies for pop music in MIDI format. The term "pop music" can have different meanings depending on the context, but we use the term in this paper to refer to its musical characteristics as conventionally accepted. Specifically, it refers to the songs of relatively short lengths, mostly around 3 minutes, with simple and memorable melodies that have relatively low structural complexity, especially in comparison to classical music. Music in MIDI format (or, equivalently, in notes) can be considered a discrete abstraction of musical sound, analogous to the relationship between text and speech. Just as understanding text is not only essential in its own merit, but provides critical clues to speech and language in general, understanding music at its abstraction can provide an ample amount of insights as to music and sound as a physical format, while being fun and significant per se. We address each of the challenges described above in our proposed model. First, we propose to treat a note and its varying properties as a unique `word,' as opposed to many previous approaches that took each property into consideration separately, by implementing different layers for generation. In our model, it suffices to train only one model for generation, as each `word' is an incarnation of all of its properties, thus forming a melody as a `sentence' consisting of those notes and the properties. This approach was inspired by recent successes in image captioning task (Karpathy & Li (2015); Vinyals et al. (2015); Xu et al. (2015)), in which a descriptive sentence is generated with one word at a time in a recurrent manner, while being conditioned on the image features. Likewise, we generate the melody with one note at a time in a recurrent manner. The difference is that, instead of image features obtained via convolutional neural networks (CNN), we condition the generation process on simple two-hot vectors that contain information on chords sequences and the part within the song. Chord sequences and part annotations are automatically generated using multinomial hidden markov model (HMM) whose state transition probabilities are computed from our own dataset. Combining Bayesian graphical models with deep neural netweorks (DNN) has become a recent research interest (Gal & Ghahramani (2016)), but our model differs in that HMM is purely used for feature input generation that is processed by neural networks. Second, we enforce regularization policy on the range of notes. Training with a large amount of data can lead to learning of excessively wide range of pitches, which may lead to generation of melodies that are not easy to sing along. We alleviate these problem by assigning a loss function for the range of notes. Finally, we train our system with part annotation, so that more appropriate melody for the corresponding part can be generated, even when the given chord sequences are identical with other parts of the song. Apart from the main model proposed, we also perform additional experiments with generative adversarial networks (Goodfellow et al. (2014)) and with multi-track songs. Our main contributions can be summarized as following:
· proposal of a model to generate euphonious melody for pop music by treating each note and its properties as single unique "word", which alleviates the complexity of learning
· implementation of supplementary models, such as chord sequence generation and regularization, that refine the melody generation
· construction of dataset with chord and part annotation that enables efficient learning and is publicly available.
2 RELATED WORKS
Most of the works on automatic music composition in the early days employed rule or templatebased approach (Jacob (1996); Papadopoulos & Wiggins (1999)). While such approaches made important contributions and still continue to inspire the contemporary models, we mainly discuss recent works that employed neural networks as a model of learning to compose music, to make close examination and comparison to our model. DeepBach (Hadjeres & Pachet (2017)) aims to generate Bach-like chorale music pieces by employing pseudo-Gibbs sampling. They discretize time into sixteenth notes, generating notes or intervals at each time step. This marks a contrast to our model that does not have to be aware of each discrete time step, since positional information is already involved in the note representation. They also assume that only one note can be sung per instrument at a given time, dividing chords into different layers of generation. On the other hand, our model can handle multiple notes at the same position, since sequential generation of notes does not imply sequential positioning of notes. As we will see in
2

arXiv:1411.4555v2 [cs.CV] 20 Apr 2015

Show and TellU: AndNeerurraelvIimewageaCsaaptcioonnGfeerneenracteorpaper at ICLR 2018

Oriol Vinyals Google
vinyals@google.com

Alexander Toshev Google
toshev@google.com

Samy Bengio Google
bengio@google.com

Dumitru Erhan Google
dumitru@google.com

Abstract

Deep CNN

a group of people

#END

Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep re-

Vision! Deep CNN

Language ! Generating!
RNN

A group of people shopping at an outdoor market.
! There are many vegetables at the

LSTM

LSTM

LSTM

LSTM ... LSTM

current architecture that combines recent advances in com-

fruit stand.

puter vision and machine translation and that can be used to generate natural sentences describing an image. The

a

group

of

market

model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the

Figure 1. NIC, our model, is based end-to-end on a neural net-
RNN-based Image Captioningwork consisting of a vision CNN followed by a language gener-
ating RNN. It generates complete sentences in natural language

fluency of the language it learns solely from image descrip- from an input image, as shown on the example above.

... ...

tions. Our model is often quite accurate, which we verify

Multinomial HMM

6bttbsCcohhue6oOeotrt,bthrCwaeecernuOnqcB)drtuoLordasomenEtalnnapitUttttaSahae-sBsret-1eietoUvatPdsf,et-,aceltwtfosyoh-rcoeroehaaef-ma-unlatichmmdrd1htea9.paiq-etrnatauvoorsaepvte2neeat8Btmrii.fLBtsoeaLELnr2tamiUEt5vss,Uaet-ol1olny-ynuc,4.seroFconFaloafioprrtc2hoerpk7eurr(i.o3nnn7tha0dse,ecktww6ah,l9hnhfyyr.iciogicreeWemhh,lledeeiw5rssaa6hs5ttlihehst9l{oodeee, verCswtlSedfaierPhxikookca,i=eomTseutlr.itslrioh.it{d{hdn.sneaaCSo,gnlasnmoic1r,ksdy,iihao.emm,S.ilp.otntu2a,aho(rt,ggASiain.poeuet|.smrnIdsp.eIts)}eosi}}sroeaowacinfsfttrshtitiipohbenidrnneerpoeesuoatdshetbtcfuh,iarocoseicavipuhwnnietrmgdiwosowauraniokogsbrtre[-daatk6prraaS,rsgcdoiieot1nnebtm6qegclse]doule.eemmasqttjoseIuefo,nrlseimyoinnnf.mctrcaoooexmnrmrieotdmorcfedaaeireszwngtlte,tooitvatrhwgheddanoees-t

4

&4 oe



E4;0;1/4 F4;1/4;1/2

3



& LSTM

LSTM

5

j

 oe

oeoeoe

G4;7/8;1/8


#END

LSTM

...

LSTM

vances inBmarac1hineBtararn2slatiBona,rw3here the task is to transform

E4;0;1/4

F4;1/4;1/2

C4;1/2;1/2

1. Introduction

a sentence S written in a source language, into its translation T in the target language, by maximizing p(T |S). For

&





Being able to automatically describe the content of an many years, machine translation was also achieved by a se-

image using properly formed English sentences is a very challenging task, but it could have great impact, for instance

Our Modelries of separate tasks (translating words individually, align-
ing words, reordering, etc), but recent work has shown that

f7or

Melody

Generation 



by helping visually impaired people better understand the translation can be done in a much simpler way using Re-

&

content of images on the harder, for example, than

Figure 1:web. This task is significantly
the well-studied image classifi-

Visual analogy between image captioningcurrent Neural Networks (RNNs) [3, 2, 30] and still reach state-of-the-art performance. An "encoder" RNN reads the

9

task

and

our

model.

By

grouping

a

note

and

its

cation or object recognition tasks, which have been a main
properties as a `word', we generate melody as a `sentence'.focus in the computer vision community [27]. Indeed, a

source sentence and transforms it into a rich fixed-length vector representation, which in turn in used as the initial

&





description must capture not only the objects contained in hidden state of a "decoder" RNN that generates the target

an image, but it also must express how these objects relate
Section 4.3, our model can generate simultaneousto each other as well as their attributes and the activities

sentence. Here, we propose to follow this elegant recipe, replac-

11

notes

for

a

single

instrument.

Huang

et

al.

(2017)

they are involved in. Moreover, the above semantic knowl-
also take a similar approach of applying Gibbs sampling toedge has to be expressed in a natural language like English, mostly share the same drawbacks that make a contrast to ourwhich means that a language model is needed in addition to

ing the encoder RNN by a deep convolution neural network (CNN). Over the last few years it has been convincingly shown that CNNs can produce a rich representation of the

&



generate model.

Bach-like chorale

music,

but

visual understanding.

input image by embedding it to a fixed-length vector, such 13

Most previous attempts have proposed toJsatitqchutoegestheer t alt.ha(t 2thi0s 1rep7re)sepntartioonpcoansbeeduseRd fLor aTvaurinetyeorf vtiosiosn upp&lement recur rent neural networks with reinforcement

learning b1y imposing cross-entropy reward function along with off-policy methods from KL control.

Note RNN trained on MIDI files is implement15ed to assign rewards based on the log probability of

a note given a melody. They defined a numb&er of music-theory based rules to set up the reward

function. Our model, on the other hand, does not require any pre-set rules, and the outcome can be

easily controlled with simple regularizations. 17
&





Chu et al. (2017) proposed a hierarchical recurrent neural network model to produce multi-track

songs, where the bottom layers generate the m19 elody and the higher levels generate the drums and

chords. They built separate layers for pitch an&d duration th at generate an output at each time step,

whereas our model needs only one layer for pitch and duration and does not have to be aware of

time step. They also conditioned their model on scale types, whereas we condition our model on

chord sequence and part information.

While generating music as physical audio format is out of scope of this paper, we briefly discuss one of the recent works that demonstrated promising results. Originally designed for text-to-speech conversion, WaveNet (van den Oord et al. (2016)) models waveform as a series of audio sample xt conditioned on all previous timesteps, whose dependence is regulated by causal convolutional layers that prevent the violations in ordering. When applied to music, it was able to reconstruct the overall characteristics of corresponding music datasets. While only for a few seconds with frequent inconsistency, it was able to generate samples that often sound harmonic and pleasant.

3 GENERATION MODEL

3.1 MELODY REPERESENTATION

Our model for melody generation can be best illustrated by making an analogy to image captioning

task. In image captioning, most popular model is to generate each word sequentially via recurrent

networks such as long short-term memory (LSTM) (Hochreiter & Schmidhuber (1997)), conditioned

on the image representation. In our model, we treat each note and its properties as a unique `word,'

so that melody becomes the `sentence' to be generated. In other words, a pitch pi with duration li

located at ti within the current chord sequence will be represented as a single word wi = (pi, ti, li).

Accordingly, a melody will be a LSTM for word generation part,

sequence of we condition

witoorndsm, ussjic=-re(lwev0a,n..t.,fewamtuir)e

 xi

S. 

While we X, instead

also use of CNN

3

Under review as a conference paper at ICLR 2018

Recurrent network

Overfit to « no-note »

Recurrent network

Independent predictions E4 - F4 - - - -
1/4 - 1/2 - - - -

G4 Pitch layer 1/8 Duration layer

w1 =(E4;0;1/4)

w2 =(F4;1/4;1/2)

w3 =(G4;7/8;1/8)

4
&4 oe



j
 oe

4
&4 oe



j
 oe

(a) I3ndependent representation of each property at (b) Timestep-independent word representation of

3

ident&ical time intervals (previous)

multiple properties (proposed)



&

Figu5re 2: Comparison of our model for musical representation with previous model. Most of the previous models used a frame-level time granularity, w5 hich can easily lead to a model over-fitting
& on the repetition of the previous time step. Our propo&sed model of word representation alleviates this7problem by encoding the time information (duration and position).

 &

7

image features; namely, max9imum log likelihood

cehstoimrda-stieoqnubenycfienxdcinhogrtdhieapnadrap&marettearnsnsoettationsuxcphatrhtia.t

Thus,

we

perform

a

 &


N9

 = arg max
11 

log p(si|xi; ) = arg max


&log p(w0, ..., wmi |xchordi , xparti ; )

(1)

(X,S)

1

 whe&re N is the number of training samples. Figure 1 1m1 akes a visual analogy between image cap-

tioning task and our model. 13

 &

Our &model of melody representation makes a strong contrast with widely used approach of implementing separate layers for each property as described13in Section 2. Previous approach essentially trea1t5s every 1/16 segment equally. Because this appro&ach encounters a substantial number of segmen&ts that are repeated over several time steps, it isvery likely that a statistically trained model will simply learn to repeat the previous segment, particula1r5ly for segments with no notes. It also complic1a7tes the learning by having to take the correlations a&mong the different properties into consideration&. On the other hand, our model does not have to consider intervals that do not contain notes, since our word representation already contains positional information. This puts us at advantage
17
part1i9cularly when simultaneous notes are involved; even though notes are generated sequentially, 
they&can be positioned at the same position, forming c&hords, which is difficult to implement with previous models based on time step. It also suffices to implement only one layer of generation, since the representation contains both pitch and lengt1h9 information. Moreover, considering pitch, position, and its length simultaneously is more concurr&ent with how humans would write melodies (Levitin (2006)). Visual description of our model and previous model is shown in Figure 2.

Melody generation through outputting a sequence of `words' is performed by LSTM with musical input features that will be described in Section 3.2. Following Karpathy & Li (2015), word vectors were randomly initialized. We used the conventionally used gate functions for LSTM as following:

it = (Wix xt + Wih ht-1 + bi)

ft = (Wfx xt + Wfh ht-1 + bf ) ot = (Wox xt + Woh ht-1 + bo)

(2)

gt = tanh(Wgx xt + Wgh ht-1 + bg)

where  previous

indicates timestep

sigmoid function that is fed back to

LfoSrTnMon,-bliiniesabriiatys,aacntidvaitt,ifotn,,oht tc1orisretshpeomndemtooirnypuotu,tfpougteftr,oomutpthuet

gates respectively.

         

3.2 CHORD SEQUENCE & PART GENERATION Since our melody generation model is conditioned on musical input features, namely chord sequence and part information, we now examine how to automate the input generation part. We employ twofold multinomial Hidden Markov Model (HMM), in which each chord and each part is a state whose state transition probabilities are computed according to our dataset. It works in a two-fold way, in which chord states are treated as latent variables whose transitions are dependent on the part states
4

Under review as a conference paper at ICLR 2018

Algorithm 1 Regularization for pitch range

1: Inputs: W =initially empty weight matrix, P = softmax predictions, S = generated melody

2:

with pitches for pi in S

(p0,

...,

pn),

pre-set

minimum

and

maximum

pitches

pmin

and pmax, coefficient µ

3: if pi > pmin

4: append max(pi - pmax, 0) to W

5: else

6: append pmin - pi to W 7: Sum up the products of P and W to get cost C = WjPj

8:

Compute derivative

dE dpi

= Pi(Wi - C)

9:

Update softmax cost by adding

dE dpi

µ

Table 1: List of chord sequences over 2 continuous bars used in our dataset. Scale for all sequences has been adjusted to C Major.

Chord Sequences (C-Em), (A#-F), (Dm-Em), (Dm-G), (Dm-C), (Am-Em), (F-C), (F-G), (Dm-F), (C-C), (C-E), (Am-G), (F-F), (G-G), (Am-Am), (Dm-Dm), (C-A#), (Em-F), (C-G), (G#-A#), (F-Am), ( G#-Fm), (Am-Gm), (F-E), (Dm-Am), (Em-Em), (G#-G#), (Em-Am), (C-Am), (F-Dm), (G#-G), (F-A#), (Am-G#), (C-D), (G-Am), (Am-C), (Am-A#), (A#-G), (Am-F), (A#-Am), (E-Am), (Dm-E), (A-G), (Am-Dm), (Em-Dm), (C-F#m),
(Am-D), (G#-Em), (C-Dm), (C-F), (G-C), (A#-A#), (Am-Caug), (Fm-G), (A-A), (F-Em)

as observed variables. Thus,

NN

p(x1, ..., xm, z1, ..., zN ) = p(z1) p(zN |zN-1) p(xn|zN )

n=2

n=1

where xi are part states and zi are chord states. Viterbi algorithm was used for decoding.

(3)

3.3 REGULARIZATION Training with a large amount of data can lead the learning process to encounter a wide range of pitches, particularly when scale shifts are involved in the training data as in Chu et al. (2017) or in our dataset. Such problem can lead to generation of unnatural melody whose pitch range deviates from what would be expected from a single song. We enforce regularization on the pitch range, so that the generated melody stays in a pitch range that humans would find easy to follow. We assign regularization cost to the learning process, so that a penalty is given in proportion to the absolute distance between the generated note and the nearest note in the pre-determined pitch range. Algorithm 1 describes the procedure of our regularization on pitch range, whose outcome will be back-propagated to get gradients. We set minimum and maximum pitch as 60 (C4) and 72 (C5) respectively, but it can be easily adjusted depending on the desirable type of song or gender of target singer. We set regularization coefficient as 0.0001.
4 EXPERIMENT

4.1 SETTING
We collected 46 songs in MIDI format, most of which are unpublished materials from semiprofessional musicians. Unofficial MIDI files for published songs were obtained on the internet, and we were granted the permission to use the songs for training from the organization owning the copyrights of the corresponding songs. It is very common in computer vision field to restrict a task to a certain domain so that the learning becomes more feasible. We also restricted our domain to pop music of major scale to make the learning more efficient. Some of the previous works (Hadjeres & Pachet (2017)) have employed data augmentation via scale shift. Instead, we adjusted each song's scale to C Major, thus eliminating the risk of mismatch between scale and generated melody. This adjustment has a side effect of widening the pitch range of melody beyond singable one, but this effect can be lessened by the regularization scheme over pitch range as described in Section 3.

5

Under review as a conference paper at ICLR 2018

Table 2: Statistics of our dataset.

# songs 46
min pitch 53

# samples 1912
max pitch 86

avg # notes 9.33
median pitch 69

max # notes 24
min length 1/16

min # notes 1
max length 1

std dev 3.60
median length 1/8

Figure 3: Visualization of songs generated with GAN. We manually annotated chord and part for each bar in the songs collected. We restricted our chord annotation to only major and minor chords with one exception of C augmented 2. Note, however, that this does not prevent the system from generating songs of more complex chords. For example, melodies in training data that are conditioned on C Major still contain notes other than the members of the conditioning triad, namely C, E, and G. Thus, our system may generate a non-member note, for example, B, as part of the generated melody when conditioned on C Major, thus indirectly forming C Major 7th chord. Part annotation consisted of 4 possible choices that are common in pop music structure; verse, pre-chorus, chorus, and bridge. We experimented with n=1,2,4 continuous bars of chord sequences. Conditioning on only one bar generated melody that hardly displays any sense of continuity or theme. On the other hand, using chord progression over 4 bars led to data sparsity problem, which leads to generated songs simply copying the training data. Chord sequences over 2 bars thus became our natural choice, as it was best balanced in terms of both thematic continuity and data density. Check our demo for example songs conditioned on n=1,2,4 continuous bars. We annotated non-overlapping chord sequences only; for example, given a sequence C - Am - F - G, we sample C - Am and F -G, but not the intermediate Am - F. This was our design choice to better retain the thematic continuity. As for the length of notes, we discretized by 16 if the length was less than 1/2, and by 8 otherwise. Table 2 shows some of the statistics from our dataset. Throughout our dataset construction, prettymidi 3 framework was used to read, edit, and write MIDI files. Our dataset is publicly available with permissions from the owners of the copyright. We ended up having 2082 unique `words' in our vocabulary. Learning rate was set to 0.001. Total number of learnable parameters was about 1.6M, and we applied dropout (Srivastava et al. (2014)) with 50% probability after encoding to LSTM.
4.2 EVALUATION We make comparison to some of the recent works that employed deep learning to generate music in MIDI format. We performed two kinds of human evaluation tasks on Amazon Mechanical Turk, making comparison between outcomes from our model and two baseline models; Chu et al. (2017) and Jaques et al. (2017). We deliberately excluded Hadjeres & Pachet (2017) as it belongs to a different domain of classical music. In task 1, we first asked the participants how much expertise they have in music. We then played one song from our model and another song from one of the baseline models. After listening to both songs, participants were asked to answer which song has melody that sounds more like human-written one, which song is more well-structured, and which one they like better. In task 2, we performed a type of Turing test (Turing (1950)) in which the participants were asked to determine whether the song was written by human or AI.
2Note that, since all the songs have been adjusted to C major scale, we are using the tabular notation with root notes for convenience, instead of the conventional Roman numerals that are scale-invariant.
3https://github.com/craffel/pretty-midi
6

Under review as a conference paper at ICLR 2018

Table 3: Results from evaluation task 1. Numbers indicate the proportion in which our model was preferred over the baseline model.

vs. Model vs. Chu et al. (2017)
vs. Jaques et al. (2017)

Expertise low
middle high all low middle high all

Melody .598 .719 .687 .691 .583 .570 .598 .577

Structure .545 .692 .712 .684 .458 .427 .511 .447

Preference .542 .619 .712 .654 .583 .567 .565 .567

Overall .576 .684 .704 .678 .542 .521 .558 .530

Table 4: Results from evaluation task 2. Deception rate indicates the proportion in which the song was believed to be made by human.

Model

Ours Chu et al. (2017) Jaques et al. (2017) Human

Deception rate .680

.599

.620 .777

Table 3 shows the results from task 1 for each question and each expertise level of the participants. 973 workers participated. Against Chu et al. (2017), our model was preferred in all aspects, suggesting our model's superiority over their multi-layer generation. Against Jaques et al. (2017), our model was preferred in all aspects except in structure. Lower score in structure is most likely due to their musical formality enabled by pre-defined set of theoretical rules. Yet, our model, without any pre-defined rule, was considered to have more natural melodies and was more frequently preferred. Interestingly, even when participants determined that one song has more human-like melody with clearer structure, they frequently answered that they preferred the other song, implying that humanness may not always correlate to musical taste. 2 statistic is 75.69 against Chu et al. (2017) and 31.17 against Jaques et al. (2017), with p-value less than 1e-5 in both cases. Against either baseline model, people with intermediate or high expertise in music tended to prefer our model than those with low expertise. Table 4 shows the results from task 2. 986 workers participated. Understandably, songs actually written by humans had the largest proportion of being judged as humans. Our model had the best deception rate among the artificial generation models. Consistency of the results with task 1 implies that generating natural melody while preserving structure is a key for human-like music generation.

4.3 ADDITIONAL EXPERIMENTS
Generative Adversarial Networks (GANs) (Goodfellow et al. (2014)) have proven to be a powerful technique to generate visual contents, to the extent where the generated results are frequently indistinguishable from human-made contents or actual pictures (Radford et al. (2016); Reed et al. (2016)). Since the musical score can be regarded as a one-dimensional image with the time direction as the x axis and the pitch as the channel, we hypothesized that GAN may be able to generate music as image. GANs consist of a generator G and a discriminator D. The generator G receives random noise z and condition c as inputs, and outputs contents G(z, c). The discriminator D distinguishes between real data x in the dataset and the outputs of the generator G(z, c). The discriminator D also receives the condition c. D is trained to minimize - log(D(x, c)) - log(1 - D(G(x, c), c)) while G is trained to minimize - log(D(G(x, c), c)). We used the two-hot feature vector described in Section 3 as condition c. We used down-sampling & up-sampling architecture, Adam optimizer (Kingma & Ba (2015)), and batch normalization (Ioffe & Szegedy (2015)) as suggested in Radford et al. (2016). Listening to the generated results, it does have its moments, but is frequently out of tune and the melody patterns sound restricted. GAN does have advantage particularly with chords, as it can `visually' capture the harmonies, as opposed to sequential generation in our proposed model, or stacking different layers of single notes as in Hadjeres & Pachet (2017). Also, melody generation with GAN can potentially avoid the problem of overfitting due to elongated training. On the other hand, the same melody frequently appears for the same input. This is likely due to the problem known as GAN's mode collapse, in which the noise input is mostly ignored. In addition, it is difficult to know whether a line corresponds to a single note or consecutive notes of smaller lengths.

7

Under review as a conference paper at ICLR 2018
Many of the problems seem to fundamentally stem from the difference in modalities; image and music. See Figure 3 for visualization of the songs generated with GAN. We also examined generating other instrument tracks on top of the melody track using the same model. We extracted bass tracks, piano tracks, and string tracks from the dataset, and performed the same training procedure as described in Section 3. Generated instruments sound fairly in tune individually, confirming that our proposed model is applicable to other instruments as well. Moreover, we were able to generate instrument tracks with simultaneous notes (chords), which is difficult to implement with previous generation model based on time step. However, combining the generated instrument tracks to make a 4-track song resulted in dissonant and unorganized songs. This implies that generating a multi-track song requires a more advanced model for learning that reflects the interrelations among the instruments, which will be our immediate future work. Check our demo for songs generated with GAN and multi-track song generated with our model. 4.4 DISCUSSION Although our model was inspired by the model used in image captioning task, its task objective has a fundamental difference from that of image captioning. In image captioning task, more resemblance to human-written descriptions reflects better performance. In fact, matching human-written descriptions is usually the evaluation scheme for the task. However, in melody generation, resembling human-written melody beyond certain extent becomes plagiarism. Thus, while we need sufficient amount of training to learn the patterns, we also want to avoid overfitting to training data at the same time. This poses questions about how long to train, or essentially how to design the loss function. We examined generations with parameters learned at different epochs. Generated songs started to stay in tune roughly after 5 epochs. However, after 20 epochs and on, we could frequently observe the same melodies as in the training data, implying overfitting (check our demo). So there seems to exist a `safe zone' in which it learns enough from the data but not exceedingly to copy it. Previous approaches like Jaques et al. (2017) have dealt with this dilemma by rewarding for following the pre-determined rules, but encouraging off-policy at the same time. Since we aim for learning without pre-determined rules, alternative would be to design a loss function where matching the melody in training data over n consecutive notes of threshold is given penalty. Designing a more appropriate loss function remains as our future work. On the other hand, generating songs with parameters obtained at different stages within the `safe zone' of training leads to diversity of melodies, even when the input vectors are identical. This property nicely complements our relatively low-dimensional input representation.
5 CONCLUSION & FUTURE WORKS
In this paper, we proposed a novel model to generate melody for pop music. We generate melody with word representation of notes and their properties, instead of training multiple layers for each property, thereby reducing the complexity of learning. We also proposed a regularization model to control the outcome. Finally, we implemented part-dependent melody generation which helps the generated song preserve the overall structure, along with a publicly available dataset. Experimental results demonstrate that our model can generate songs whose melody sounds more like human-written ones, and is more well-structured than previous models. Moreover, people found it more difficult to distinguish the songs from our model from human-written songs than songs from previous models. On the other hand, examining other styles such as music of minor scale, or incorporating further properties of notes, such as intensity or vibrato, has not been examined yet, and remains as future work. As discussed in Section 4, learning to model the correlations among different instruments also remains to be done, and designing an appropriate loss function for the task is one of the most critical tasks to be done. We plan to constantly update our dataset and repository, addressing the future works.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Hang Chu, Raquel Urtasun, and Sanja Fidler. Song from PI: A musically plausible network for pop music generation. In ICLR Workshop, 2017.
Yarin Gal and Zoubin Ghahramani. Bayesian convolutional neural networks with bernoulli approximate variational inference. In ICLR Workshop, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Gae¨tan Hadjeres and Franc¸ois Pachet. Deepbach: a steerable model for bach chorales generation. In ICML, 2017.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): 1735­1780, 1997.
Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, and Douglas Eck. Counterpoint by convolution. In ISMIR, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.
Bruce L. Jacob. Algorithmic composition as a model of creativity. Organised Sound, 1(3), 1996. Natasha Jaques, Shixiang Gu, Richard E. Turner, and Douglas Eck. Tuning recurrent neural net-
works with reinforcement learning. In ICLR Workshop, 2017. Andrej Karpathy and Fei-Fei Li. Deep Visual-Semantic Alignments for Generating Image Descrip-
tions. In CVPR, 2015. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. D.J. Levitin. This is Your Brain on Music: The Science of a Human Obsession. Dutton, 2006. George Papadopoulos and Geraint Wiggins. Ai methods for algorithmic composition: A survey, a
critical view and future prospects. In AISB Symphosium on Musical Creativy, 1999. Peter Potash, Alexey Romanov, and Anna Rumshisky. Ghostwriter: Using an LSTM for automatic
rap lyric generation. In EMNLP, 2015. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In ICLR, 2016. Scott Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. Learn-
ing what and where to draw. In NIPS, 2016. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1), 2014. A. M. Turing. Computing machinery and intelligence. Mind, 59(236), 1950. Aa¨ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. CoRR, abs/1609.03499, 2016. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and Tell: A Neural Image Caption Generator. In CVPR, 2015. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Richard Zemel, and Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In ICML, 2015.
9

