Under review as a conference paper at ICLR 2018

CNNS AS INVERSE PROBLEM SOLVERS AND DOUBLE NETWORK SUPERRESOLUTION
Anonymous authors Paper under double-blind review

ABSTRACT
In recent years Convolutional Neural Networks (CNN) have been used extensively for Superresolution (SR). In this paper we use inverse problem and sparse representation solutions to form a mathematical basis for CNN operations. We prove a single neuron is able to provide the optimum solution for inverse problems. Intoducing a new concept called Representation Dictionary Duality we show that CNN layers act as sparse representation solvers. In the light of theoretical work we propose a new algorithm which uses two networks with different structures that are separately trained with low and high coherency image patches and prove that it performs faster compared to the state-of-the-art algorithms while not sacrificing from performance.

1 INTRODUCTION
Recent years have witnessed an increased demand for superresolution (SR) algorithms. Increased number of video devices boosted the need for displaying high quality videos online with lower bandwidth. In addition, the social media required the storage of videos and images with lowest possible size for server optimization. Other areas include 4K video displaying from Full HD broadcasts, increasing the output size for systems that have limited sized sensors, such as medical imaging, thermal cameras and surveillance systems.
SR algorithms aim to generate high-resolution (HR) image from single or ensemble of lowresolution (LR) images. The observation model of a real imaging system relating a high resolution image to the low resolution observation frame can be given as

g = SHy + n

(1)

where H models the blurring effects, S models the downsampling operation, and n models the system noise. The solution to this problem seeks a minimal energy of an energy functional comprised of the fidelity of the estimated image y^ to the observational image y.
State-of-the art algorithms that are addressing SR problem can be collected under Dictionary learning based methods (DLB) and Deep learning based methods (DLM) categories. Although SR problem is an inverse problem by nature, performance of other methods such as Bayesian and Example based methods have been surpassed which is the reason why they are not included in this work. Also the SR problem has never been directly dealt with inverse problem solutions as in Combettes & Wajs (2005) Daubechies et al. (2004) DLB are generally solving optimization problems with sparsity constraints such as Yang et al. (2008) Yang et al. (2010) and L2 norm regularization as in Timofte et al. (2013). The main concern of DLB is creation of a compact dictionary for reconstruction of high resolution (HR) image. Although useful, DLB methods become heavy and slow algorithms as reconstruction performance increases. Recent advances on GPUs have fueled the usage of convolutional neural networks (CNNs) for SR problem. CNN based algorithms such as Dong et al. (2014) and Kim et al. (2016) have used multi-layered networks which have successfully surpassed DLB methods in terms of run speed and performance. Though CNNs are successful for SR problem experimentally, their mathematical validation is still lacking. The relation of inversion of observation model and neuron activations in CNNs is missing. In this paper we prove that the neurons act as optimum inverse problem solvers during training. The reason we are interested in DLB methods is because the learning procedure of a dictionary is similar to training of neuron filters which act as

1

Under review as a conference paper at ICLR 2018

dictionary elements. We describe a new concept namely Representation Dictionary Duality (RDD) and show that neuron filters which act as representation vectors during training switch roles while testing and start acting as dictionaries for a sparse representation solution at each layer. After analyzing a network with inverse problem and DLB solutions, we propose a new network structure which is able to recover certain details better, faster without sacrificing overall performance.

2 RELATED WORK

2.1 INVERSE PROBLEM SOLUTIONS
Solution to 1 is inherently ill-posed since a multiplicity of solutions exist for any given LR pixel. Thus proper prior regularization for the high resolution image is crucial. The regularization of the inversion is provided with a function, reg, which promotes the priori information from the desired output. reg takes different forms ranging from L0 norm, Tikhonov regularization to orthogonal decomposition of the estimate. Denoting the SH matrix in equation 2 by K, the regularized solution is given by

y^

=

argmin
y

1 2

||K

y

-

g||22

+

reg(y)

(2)

In Daubechies et al. (2004) Daubechies have used sparsity promoting regularization and Combettes & Wajs (2005) have inspected various proximity mapping functions for solutions of inverse problems with projection onto convex sets. The application of convex analysis results (Combettes & Wajs (2005)) to the linear inverse problem, involves iterations which result in so called Iterative Shrinkage/Thresholding (IST). So, the solution to the inversion of equation 2 with an L1 norm regularization function can be obtained by the help of Moreau proximity operator as

f n = proxb||.||(f n-1 + b.KT ||g - Kf n-1||)

(3)

Where a class of proximity operators are defined, the special function for the case of L1 regularization is soft thresholding function also known as shrinkage operator.

proxb||.||f =

(1

-

1 ||f ||

)f

0

if ||f ||  b otherwise

= sign(f ). max(|f | - b, 0) = sof t(b)(f )

(4)

Notice that KT ||g - Kf n-1|| is the negative gradient of data fidelity term in the original formulation. Therefore the solution for the inverse problem using IST iterations is obtained in a gradient descent type method thresholded by Moreau proximity mapping which is also named as Proximal Landweber Iterations. Daubechies et al. (2004) have proposed the usage of non-quadratic regularization constraints that promote sparsity by the help of an orthonormal basis l of a Hilbert space. For the problem defined in equation 2 it is proposed to use a functional b,p as

b,p(f ) = ||Kf - g||2 + bl| f, l |p
l

(5)

For the case when p = 1, a straightforward variational equation can be obtained in an iterative way.

f n, l = sof tb( f n-1, l + KT (g - Kf n-1), l )

(6)

Iterations over the set of basis functions can be carried out in one formula

f n = Zb(f n-1 + KT (g - Kf n-1))

(7)

where

Zb(x) =. ( x, l )l

(8)

l

which can be seen as a method to file the elements of x in the direction of l. Daubechies et. al. have proven that the solution obtained by iterating f is the global minimum of the solution space.

2

Under review as a conference paper at ICLR 2018

2.2 DICTIONARY LEARNING BASED SUPERRESOLUTION
Instead of approaching the superresolution problem as an inverse problem, DLB learn mappings from LR to HR training images based on a dictionary. The algorithms jointly solve for a compact dictionary and a representation vector. Following a division of input LR image into patches, a Markov Random Field framework is used for dictionary search and mapping. However direct usage of a dictionary is computationally cumbersome. Recently sparse representation has been applied to the dictionary learning based SR problem.
The K-SVD algorithm Aharon et al. (2006) is one of the keystones of dictionary learning for the purpose of sparse representation. Aharon et. al. have proposed the usage of a compact dictionary D, from which a set of atoms (columns or dictionary elements) are to be selected via a vector f and the combination of these atoms is constrained to be similar to a patch (or image) g via ||g - Df ||p  . If the dimension of g is less that that of matrix D and if D is full-rank matrix then there are infinitely many solutions to the problem therefore a sparsity constraint is introduced.

min||f ||0 s.t. ||g - Df ||2  
f

(9)

The L0 norm gives the number of entries in f that are non-zero. The usage of compact dictionaries for SR problem is introduced in Yang et al. (2008). The authors have used the approach of K-SVD.
The optimization of L0 norm regularized equation is hard and a closed form solution might not be available. For the case when f is sufficiently sparse, equation 1 can be approximated by L1 norm. The solution of such an equation can be obtained by Lagrange multipliers.

min||f ||1
f

+

1 ||Df 2

-

g||22





(10)

Due to the computationally demanding nature of the L1 norm in the regularization, Timofte et al. (2013) have proposed the usage of L2 norm instead of L1 norm. Although usage of L2 norm eliminated the sparsity constraint from the equation it will play a role in understanding how CNNs work.

2.3 CONVOLUTIONAL NEURAL NETWORKS
The mapping between the high and low resoultion images can also be found by convolutional networks (Dong et al. (2014), Kim et al. (2016)).
The activation function plays an important role in neural network training. In many state-of-the-art algorithms major functions such as tanh and softmax have been replaced by rectified linear units Maas et al. (2013) that are linear approximations of mathematically complex and computationally heavy functions. Glorot et al. (2011) has empirically shown that by using rectified activations the network can learn sparse representations easier. For a given input, only a subset of hidden neurons are activated, leading to better gradient backpropagation for learning and better representations during forward pass. Especially sparse representation has been shown Glorot et al. (2011) to be useful. Sparsity constraint provides information disentagling which allows the representation vectors to be robust against small changes in input data.
Romano et al. (2017) uses gradient information to separate image pixels during interpolation. Separation is done according to three properties namely, strength, coherence and angle. A low strength and coherence signifies as lack of content inside the patch. A high strength but low coherence signifies corner or multi directional edge information. High strength and high coherence signifies a strong edge. Especially the coherence information will play an important role in section 3.
Dong et al. (2014) have provided the earliest relation of CNNs to Sparse Representation. In their view outputs of the first layer constitute a representation vector for a patch around each pixel in LR image, second layer maps LR representations to HR representation vectors and the last layer reconstructs HR image using 5x5 sized filters (or atoms if we have used the jargon of sparse representations). Although this idea qualitatively maps CNNs as a solution method for sparse representation problem, we will now show a more complete understanding with mathematical background.

3

Under review as a conference paper at ICLR 2018
3 RELATION OF INVERSE PROBLEM, SPARSE REPRESENTATION AND CNNS
Even though CNNs yield very good estimates of superesolved images, the connection between inversion of observation model and activation of neurons in CNNs is missing. In this section, we will prove the relation between the inverse problem solutions and sparse representation to CNNs. Figure 1 summarize how we are connecting all previous work to CNNs. Considering single neuron we are going to generalize the solution.
Figure 1: Connecting the Dots
For the training phase of CNNs, LR images are fed into the network for forward pass. The resulting image from the network is compared against a ground truth HR image and the error is backpropagated. Since the input image is convolved by the neuron filter, its size should be larger than the size of the output to prevent boundary conditions. The convolution operation can be carried out in an algebraic manner. Let us assume that we are operating on a patch of LR image, that is named as superpatch. The superpatch is divided into chunks, that are named as subpatches, which have the same support as the filter. Filter and each subpatch is vectorized in column stack method. Vectorized subpatches are concatenated to form a matrix. For the CNN analysis we are going to change the perspective of the linear inverse problem definition. During training, the known values of the system are HR image and LR image. The network solves the mapping of LR input to HR training set. The product of the network is going to be a mapping, f, from LR superpatch, that is collected under D, to HR patch, g. Therefore the vector f will be the neuron filter, i.e. the only variable for the training phase, D will be concatenated subpatch matrix, each subpatch in vectorized form will be named as l. The vector g will be a patch from HR image. In neither inverse problems the distortion kernel K nor in sparse representation the dictionary D is bound to obey certain rules that might prevent them from being used interchangeably. Therefore equations 2 and 5 can be used for a solution for CNNs. To complete the mathematical mapping of a neuron learning process to the solution given by 7 we modify gradient descent type learning process. Convolution of subpatches with the filter can be algebraically written as Df . Output of a neuron is given as Sof tb(Df ), where Soft is the soft thresholding and b is the bias of a neuron. Then the error at the output of a neuron is given as g - Sof tb(Df ) where g is the objective of the neuron, HR image patch in our case. The MSE of neuron output is therefore given as ||g - Sof tb(Df )||2. Taking the gradient of MSE with respect to f is tricky. When an element of Df vector lies below the bias, the result will be zero causing the gradient to be zero. We modify the equation by changing the bias vector to b to enable us to use MSE gradient formulation in Softb (DT (g - Df )). This is a valid insertion since the addition of g and multiplication of DT are linear operators that can be used to scale elements of original bias vector b. The neuron filter f is updated by subtracting the gradient, generally with a factor which is omitted for simplicity f + Sof tb (DT (g - Df )). We can again change the biases to include additive f vector into the soft thresholding as Softb (f+DT (g-Df)). Now assume that we are using a CON or overcomplete basis set to decompose the updated filter value. For each l we will take a
4

Under review as a conference paper at ICLR 2018

new bias vector b leading to the equation Softb (f + DT (g - Df )), l ). 1 Then the summation of thresholded decompositions is given as Zb(f + DT (g - Df ))which is the same as 7. Therefore gradient descent type learning of a single neuron is guarantied to reach an optimal solution.

For the testing phase, a new representation - dictionary duality (RDD) concept is proposed. Considering the initial condition of equation 3 for the training phase, the filters, or the representations, will be obtained by projecting HR information to the domain of LR image through the D matrix. Assuming w.l.o.g. if the random initialization of a filter of interest matches a set of features of training set to a certain degree, then the filter will converge to the matching feature by the course of iterations. See Appendix A for detailed explanation of RDD.

To extend the understanding of single neuron to the entire network Theorem 1 will be used from Papyan et al. (2016).

Theorem 1 Suppose y = g + n where n is noise whose the power of noise is bounded by 0 and g is

a noiseless signal. Considering a convolutional sparse coding structrure

g = D1f1

f1 = D2f2

.

.

fN-1 = DN fN
Let f^i be a set of solutions obtained by running a convolutional neural network, or layered soft thresholding algorithm with biases bi as f^i = sof ti{DiT f^i-1}wheref^0 = y. Denote |f max| and |f min| as absolute maximum and minimum entries of representation vectors. Then assuming for

1  i  N

||fi||0

<

1 2

(1

+

1 µ(Di )

)

-

1

µ(Di

)

i-1 |f max|

where µ(Di) is the mutual coherence of the dictionary then

1. The support of the solution f^i is equal to the support of fi

2. ||fi - f^i||2  i where i = ||fi||0(i-1 + µ(Di))(||fi||0 - 1)|f max| + bi

The theorem shows that a network consisting of layered neurons could yield the same result as a layered sparse coding algorithm. Therefore a network of neurons, whose optimality for inverse problem solutions has been proven individually, is now proven to reach to a solution for sparse coding. Let us now recall the Landweber equation f = (DT D + µI)-1DT g In order to be able to use insights from this equation assume that all neurons in the network are activated for the inputs. For that un-realistic case, the network filters can be convolved among themselves to produce an end point filter, f. This is feasible because when all neurons are activated, their linear unit outputs are going to be the convolution results minus a bias that can be added up at the end, simply enabling the convolution of all filters to be applied in a single instant. A similar work is done by Mallat (2016) to analyze linearization, projection and separability properties of sparse representations for deep neural networks.

The vector f is going to be a normalized projection of g onto LR image domain. Considering the rows of DT matrix, each row is a vectorized subpatch, thus each multiplication result from DT g is going to be subpatch, g meaning the projection of HR patch onto an LR subpatch. DT D matrix
have elements of inner products of subpatches such as subpatchi, subpatchj . The diagonals of DT D matrix, therefore, are normed square of each subpatch. The inverted matrix is going to be
mostly composed of diagonals that are inverted normed square values of subpatches. This means
that the entire equation calculates the projection of HR patch, g, onto the LR image domain. In
other words, the result, f, consists of scores which measure how similar g vector is to each subpatch
from the entire superpatch. If the HR image has content that cannot be recovered by using certain
region of LR image, the reconstructed image is going to be inferior. This is due to the violation of
overcompleteness assumption. Selection of a larger area for the reconstruction of certain HR patches

1Notice that the bias vector b that is used for each decomposition is taken as the same vector which might look like a problem. This problem is addressed by taking each basis as a column from the D matrix which are subpatches. By doing so the bias value that was chosen according to the D matrix is used to threshold same vectors by eluding the problem.

5

Under review as a conference paper at ICLR 2018
proves useful because of increased information included into the system that brings the subpatches, or bases, closer to being overcomplete. This insight provides a method for determining how deep a network should be for certain features. For example when the superpatch and corresponding HR region contains only texture, which can be modeled as gaussian noise, the D matrix becomes linearly independent, meaning easily invertible. Consequently when the training set consists solely of textured images, shallow networks should do as good as deep networks. Then for the testing phase, same filters are used to construct the D matrix and the result of the network is obtained by the same equation without normalization (without the inverse term) this time (since it is already normalized) as in equation 11, i.e. projecting LR image onto filters domain. Notice that the error is generally not completely orthogonal to the LR images because of iterative nature of equations. Therefore this is not going to be a meaningless operation. The training set contains various features with different variances. Therefore the generalization of the new concepts that are introduced here are difficult. Training with different structures enables the constant evolution of neuron filters during training. However to have an activating branch for each feature either the network should have increased number of filters or the neuron filters should converge to very generic features which in the end decreases the quality of the reconstruction.
4 PROPOSED NETWORK
The discussions from section 3 revealed that using a single training set for a single network is complicating the training process. Because we expect the neuron filters to assume predominant forms from the training set, training a single network either leads to a heavy network with a lot of memory requirement or leads to insufficiently learned filters. We have proposed using double network SR (DNSR) for two different data. The data separation is done according to gradient information, dividing data set into low and high coherence sets. High coherence data contains edge and corner information therefore we trained a deep network to reconstruct edge information. For low coherence data which is mainly texture we have trained a shallower network as in Figure 2. This is, to the best of our knowledge, the first time proposition of separation of neural network for the purpose of recovering different contents for superresolution. In order to satisfy the assumption made in Theorem 1, which concerns the coherence of dictionary elements, we have used skip connections between layers to correlate the outcomes. This is only done in low coherence network due to inherent lack of correlation of dictionary elements which are input LR images, as our RDD explains. Since output of each layer of neurons is an input to next layer, acting as a new dictionary, skip layers qualitatively provide the required increase in coherence. We also used cross entropy loss besides the MSE loss for low coherence network. We have used bicubically upsampled inputs to the network which is the only pre-processing before neural networks. The aggregation of two separate network are done in a post-processing block because the training operation uses separate validation data for error gradient calculations. We have backprojected the results to upsampled input images and then simply added two outputs.
Figure 2: DNSR Structure
6

Under review as a conference paper at ICLR 2018
5 EXPERIMENTAL RESULTS
5.1 EXPERIMENTAL SETUP We have used the same data set for training and testing as used in Kim et al. (2016). We have used an Intel i7-4770K CPU Nvidia GeForce GTX 760 GPU computer to run the training and testing operations. Since we do not readily have a GPU implementation, we have given individual run times for each image while comparing speed. Training is completed in 16 hours for high coherence network and 8 hours for low coherence network which is significantly less then the requirement of state of the art algorithms. The run times are as fast as twice the speed of reference model Kim et al. (2016) as reported in Table 1 5.2 REPRESENTATION-DICTIONARY DUALITY We have conducted experiments to test out the RDD proposition which states that the learned filters for neurons resemble to the training data. We have created two separate training set which contained high coherence data with edges of 0-20 degrees and 40-60 degrees. The results were proving that the learned filters for the first layer resemble the predominant features of the training set as in Figure 3 and Figure 4
Figure 3: Trained network filters 40-60 degrees
Figure 4: Trained network filters 0-20 degrees
5.3 SEPARATE NETWORKS The advantage of separate network training is to be able to recover details that otherwise might be dropped out during training due to stronger data. Barbara image details can be clearly seen in Figure 5 low coherence network output. 5.4 RESULTS Numerical comparisons are done only with the referenced algorithm Kim et al. (2016) in Table 1. A comparison with previous DLB and DLM can be found in the reference paper Kim et al. (2016).
7

Under review as a conference paper at ICLR 2018
Figure 5: Recovery of texture from Barbara image VDSR(left) DNSR(middle) GT(right)
Table 1: Results
6 CONCLUSIONS
We have proven that a neuron is able to solve an inverse problem optimally. By introducing RDD we have shown that CNN layers act as sparse representation solvers. We have proposed a method that addresses the texture recovery better. Experiments have shown that RDD is valid and proposed network recovers some texture components better and faster than state of the art algorithms while not sacrificing performance and speed. In the future we plan to investigate a content-aware aggregation method which might perform better than simple averaging. In parallel we are investigating a better network structure for texture recovery. Also we are going to incorporate the initial upsampling step into the network by allowing the network to learn its own interpolation kernels.
8

Under review as a conference paper at ICLR 2018
REFERENCES
M. Aharon, M. Elad, and A. Bruckstein. Svdd: An algorithm for designing overcomplete dictionaries for sparse representation. Trans. Sig. Proc., 54(11):4311­4322, November 2006. ISSN 1053-587X. doi: 10.1109/TSP.2006.881199. URL http://dx.doi.org/10.1109/TSP. 2006.881199.
Patrick L. Combettes and Vale´rie R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale Modeling & Simulation, 4(4):1168­1200, 2005.
I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Communications on Pure and Applied Mathematics, 57(11): 1413­1457, 2004. ISSN 1097-0312.
Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV (4), volume 8692 of Lecture Notes in Computer Science, pp. 184­199. Springer, 2014.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In AISTATS, volume 15 of JMLR Proceedings, pp. 315­323. JMLR.org, 2011.
Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional networks. In CVPR, pp. 1646­1654. IEEE Computer Society, 2016.
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In in ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013.
Ste´phane Mallat. Understanding deep convolutional networks. CoRR, abs/1601.04920, 2016. Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via
convolutional sparse coding. CoRR, abs/1607.08194, 2016. Vardan Papyan, Jeremias Sulam, and Michael Elad. Working locally thinking globally: Theoretical
guarantees for convolutional sparse coding. IEEE Trans. Signal Processing, 65(21):5687­5701, 2017. Yaniv Romano, John Isidoro, and Peyman Milanfar. RAISR: rapid and accurate image super resolution. IEEE Trans. Computational Imaging, 3(1):110­125, 2017. Radu Timofte, Vincent De Smet, and Luc J. Van Gool. Anchored neighborhood regression for fast example-based super-resolution. In ICCV, pp. 1920­1927. IEEE Computer Society, 2013. Jianchao Yang, John Wright, Yi Ma, and Thomas Huang. Image superresolution as sparse representation of raw image patches, 2008. Jianchao Yang, John Wright, Thomas S. Huang, and Yi Ma. Image super-resolution via sparse representation. Trans. Img. Proc., 19(11):2861­2873, November 2010. ISSN 1057-7149. doi: 10. 1109/TIP.2010.2050625. URL http://dx.doi.org/10.1109/TIP.2010.2050625.
9

Under review as a conference paper at ICLR 2018
A REPRESENTATION-DICTIONARY DUALITY CONCEPT
RDD concept states that the representation vectors learned during the training phase can be used as atoms of a dictionary for the testing phase. A similar idea is proposed by Papyan et al. (2016) and Papyan et al. (2017) stating that each layer output which is a representation for inputs of previous layer, can also be seen as an input to be represented by the next layer. In that the authors have argued that each layer output will contain a structure which can be represented by a convolutional sparse coding (CDC) layer. A CDC layer is essentially a CNN layer. The difference of our RDD is that we use the idea that dictionaries and representations swap roles during training and testing (forward pass). Also during training, inputs to each layer is perceived as a dictionary for the next layer, contrary to previously proposed perception of Papyan et al. (2016). Following the idea of RDD, the neuron filter, previously named as f, can be viewed as an atom of a dictionary consisting of many other neuron filters. During testing period, the filters are vectorized and concatenated to form the dictionary matrix D, the vector g will be the input image this time and the f vector will be the neuron outputs, which will be the representation vector of input image in terms of the dictionary atoms, i.e. the neuron filters. The mathematical insight for this is again given in equation 3. Considering the initial condition for the equation, during testing phase, f0 can be assumed as zero and the f1 is going to be the representation vector provided with
f 1 = proxb||.||(f 0 + b.Df 0||) = proxb||.||(DT g) = sof tb(DT g) = max(DT g - b, 0) (11) Again we reach to the conclusion that the ReLU operators provide the representation vector, f, for the input image, g, given the trained filter values collected under D. This concludes the proof of the optimality of neurons for the solution of inverse problems. Using the new RDD, the joint solution for network training and testing is shown. This conclusion might appear incomplete. We have solved for and g and obtained a representation vector f but the whole point of CNN was to obtain an image output. The answer to this contingency again lies in the RDD. Observe that we are obtaining a representation vector that is the non-linear combination (ReLU) of multiple neuron outputs which yields a result that can both be seen as a representation and a feature. This feature in the case of full image execution will be the superresolved image itself.
10

