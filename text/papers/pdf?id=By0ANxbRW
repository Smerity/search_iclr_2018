DNN Model Compression Under Accuracy Constraints
Anonymous Author(s) Affiliation Address email
Abstract
1 The growing interest to implement Deep Neural Networks (DNNs) on resource2 bound hardware has motivated the innovation of compression algorithms. Using 3 these algorithms, DNN model sizes can be substantially reduced, with little-to-no 4 accuracy degradation. This is achieved by either eliminating components from 5 the model, or penalizing complexity during training. While both approaches 6 demonstrate considerable compressions, the former often ignores the loss function 7 during compression while the latter produces unpredictable compressions. In this 8 paper, we propose a technique that directly minimizes both the model complexity 9 and the changes in the loss function. In this technique, we formulate compression 10 as a constrained optimization problem, and then present a solution for it. We will 11 show that using this technique, we can achieve competitive results.
12 1 Introduction
13 Deep Neural Networks have been rapidly improving in many classification tasks, even surpassing 14 human accuracy for some problems (Russakovsky et al. 2015). This high accuracy, however, is 15 obtained by employing wider (Zagoruyko and Komodakis 2016) or deeper (He et al. 2016) networks. 16 Some prominent networks today even surpass 100 layers, store hundreds of millions of parameters, 17 and require billions of operations per input sample (He et al. 2016; Srivastava et al. 2015). Such 18 large networks are not well-suited for resource-bound, embedded and mobile platforms which are 19 dominating the consumer market. This mismatch between computational requirements and available 20 resources has motivated efforts to compress DNN models.
21 Compression techniques exploit the redundancy inherent to neural networks that emerges due to the 22 considerable number of parameters in them. These many parameters help learn highly informative 23 features during training. However, they simultaneously learn multitudes of unnecessary, ineffectual 24 ones. Han et al. (2015) reduces these redundancies by pruning the network and quantizing the 25 remaining parameters. Using these techniques, they were able to reduce the model size by more 26 than an order of magnitude. Their success inspired other methodical approaches such as the soft 27 weight-sharing (Ullrich et al. 2017). This method encodes model parameters using Bayesian prior 28 and then penalizes this prior during training. As a result, it performs both pruning and quantization 29 simultaneously and achieves superior compressions with negligible loss in accuracy.
30 In this work, we take a similar, integrated approach with the twist that we directly minimize the 31 complexity. Unlike soft weight-sharing, however, we encode the parameters using the k-means 32 objective which imposes less computations. We further apply a hard constraint on the training loss 33 to maintain sufficient accuracy. Such a constraint takes advantage of the fact that we have already 34 obtained some information about the loss function during training. We then present a straightforward 35 solution for this constrained optimization problem. Our solution iteratively minimizes the k-means 36 objective, while satisfying the loss constraint. Consequently, it can compress the model progressively
Submitted to 6th Conference on Learning Representations (ICLR 2018). Do not distribute.

37 where the compression rate can be adjusted. Finally, we test the proposed technique on three popular 38 datasets and show that this method can achieve state-of-the-art compression with minimal loss of 39 accuracy.

40 2 Related Work
41 Robustness of DNNs to noisy parameters is a byproduct of their redundancy and has been utilized 42 frequently to facilitate and accelerate their implementations. Limited precision representations which 43 introduce quantization noise, for example, have been widely used to simplify computations (Gupta 44 et al. 2015) and design neural network hardware (Jouppi et al. 2017). Other works, take a different 45 approach and eliminate most parameters (pruning) (Molchanov et al. 2016), aiming to reduce the total 46 number of computations instead. Finally, weight-sharing techniques (Han et al. 2015) encode the 47 remaining parameters of the networks in a small dictionary, reducing the storage requirement of the 48 network. In this paper, we will be focusing on the last two techniques (pruning and weight-sharing), 49 which were proposed first by Han et al. (2015). Our goal is to present a simple, unified method for 50 compressing a trained DNN model.
51 Han et al. (2015) first proposed Deep Compression which introduced pruning and weight-sharing as 52 techniques to compress DNN models. Pruning eliminates unnecessary network connections identified 53 by their magnitude. Weight-sharing then encodes parameters into a small dictionary, using the k54 means algorithm. This technique also employs an intermediate tuning step which offsets the effect of 55 pruning on accuracy. However, the changes in the training loss function are, for the most part, ignored 56 in this technique. Later studies demonstrated that taking this effect into account, either in pruning or 57 quantization, can improve the compression. Guo et al. (2016), for example, formulated pruning as an 58 optimization problem. Their method essentially performs pruning and tuning simultaneously, but 59 does not perform weight-sharing. Likewise, Choi et al. (2016) incorporate this effect by augmenting 60 the k-means objective by the training loss Hessian. However, they only consider the Hessian at one 61 point, and their method requires prediction of the compression ratio beforehand.
62 Soft weight-sharing, proposed by Ullrich et al. (2017), minimized both training loss and model size 63 simultaneously. Unlike the previous attempts, they encoded the network parameters using a Bayesian 64 prior. This prior is then penalized when optimizing the training loss function and the mixing ratio 65 controls the trade-off between accuracy and model size. This is then optimized using a SGD method. 66 The limitation of this technique is the complexity of the Bayesian prior. Due to this limitation, in 67 this paper, we will focus on k-means encoding of the weights, even though our method can be easily 68 extended to Bayesian encoding as well.

69 3 Method

70 The proposed algorithm compresses a trained neural network model by minimizing its complexity 71 while constraining its loss function. Here, we first review the optimization process for the loss 72 function during the training process. Then, we discuss the k-means encoding, which we use to 73 measure the model complexity. Finally, we will present our compression algorithm.

74 3.1 Training Neural Networks

75 Training neural networks involves minimizing a loss function that quantifies the dissimilarity between

76 the model output distribution and the ideal distribution. This optimization problem is formally written

77 as:

W = arg min L(W, X , y)

(1)

W

78 Here, X and y define a set of samples and ideal output distributions we use for training. W is the 79 learnt model and is the concatenation of all model parameters, (W = [w1 w2 . . . wN ]T  RN
80 where N is the number of parameters). W is the final, trained model. Also, L(W, X , y) represents 81 the average loss over the dataset ({X , y}) corresponding to the model W . This function is typically 82 chosen as Cross entropy or hinge square loss. In the rest of this paper, we represent L(W, X , y) as 83 L(W ) for simplicity. The local optimum for W in this problem is usually found iteratively, using a
84 SGD method.

2

85 3.2 K-means Encoding

86 The weight-sharing techniques proposed by Han et al. (2015) and Guo et al (2016) use the k-means 87 algorithm to encode the network parameters. Each parameter is then replaced by the nearest of the k 88 centroids. This procedure can also be defined as the following optimization problem:

N
C = arg min wi - mi 2
C i=1
i : mi = arg min wi - m
mC

(2) (3)

89 where M = [m1 m2 . . . mN ]T  RN is the concatenated array of all centroids mi corresponding 90 to the parameters wi. The set C = {c1, c2, . . . , ck}  R also defines the set of cluster centroids. 91 The commonly used heuristic solution to this problem is to iteratively find M and update centroids
92 accordingly. In each iteration t this update is performed by:

cjt+1

=

1 |J |

wi

iJ

J = {i|mi = ctj}

(4) (5)

93 3.3 Proposed Compression Method

94 Similar to weight-sharing in Deep Compression (Han et al. 2015), we minimize the k-means objective 95 function to reduce the model complexity. Further, similar to soft weigh-sharing (Ullrich et al. 2017), 96 we minimize the changes in the loss function. However, we control these changes through a hard 97 constraint to take advantage of the information already learnt through the training phase. In the rest of 98 this section, we will formulate the problem of minimizing the complexity of the model and propose a 99 solution for it.
100 As previously discussed, we use the k-means objective to represent the complexity of the model. 101 The goal is to learn the network parameters as well as the centroids that minimize this objective. 102 Specifically, we would like to learn a small set of centroids and encourage the network parameters to 103 populate a small region about these centroids only.

min (W, M )
W,M

N

(W, M ) =

wi - mi 2

i=1

(6) (7)

104 We note that the centroids mi are correlated to wi by equation 2 with the difference that the number 105 of centroids k is also obtained through the compression process.
106 In the absence of any additional constraints, this optimization problem reverts back to one solved by 107 Han et al. (2015) for Deep Compression. The solution to this problem is simply wi = mi. However, 108 this might result in significant changes in the accuracy. We address this issue by introducing a 109 constraint on the loss function.

L(W ) 

(8)

110 Here, is the hard constraint derived based on the optimal value of the loss function. We will discuss 111 how we determine this hyper-parameter later in this section. While solving our optimization problem, 112 we will make sure that this condition is always satisfied.
113 Next, we present our method for solving the problem of minimizing the complexity of a trained 114 model. This method iteratively solves the equation 6 for W and then calculates M based on equation 115 2, as presented in algorithm 1.

3

Algorithm 1 Compression
t0 Initialize Randomly initialize k centroids while Convergence is not achieved do
Calculate Mt using equation 3. Solve equation 6 for Wt. Update C using equation 2. Eliminate unnecessary centroids. tt+1 end while

116 This algorithm first initializes k random centroids. The value of k is provided to the algorithm as 117 a hyper-parameter and can be large. During the compression this algorithm eliminates or merges 118 unnecessary centroids and reduces k. At the end of each iteration, if a centroid has not been assigned 119 to any network parameter or is close to another centroids, it is eliminated.

120 Each iteration t of the compression, finds the nearest centroid to each network parameter and solves 121 the constrained optimization problem by finding a displacement for the model parameters like Wt 122 which solves:

min (Wt + W, Mt)
W
L(Wt + W ) -  0

(9) (10)

123 This way, the model update for the next iteration would be: Wt+1 = Wt + Wt.

124 The solution of this constrained optimization problem should satisfy the KKT condition. W (Wt + W , Mt) + µW L(Wt + W ) = 0 L(Wt + W ) -  0 µ0

(11) (12) (13)

125 This system has two solutions. When µ = 0, the solution is i : wi = mi. If W  satisfies the 126 constraint in equation 8, this solution is valid. Otherwise:

W (Wt + W , Mt) + µW L(Wt + W ) = 0  2(Wt - Mt) + µW L((Wt + W )) = 0

(14) (15)

L(Wt + W ) - = 0

(16)

µ>0

(17)

127 The difficulty of solving this system is the unknown value and gradient of the loss function at the 128 solution which we want to obtain. We get around this issue by locally estimating the loss function 129 using its linear Taylor expansion and replacing that in the system.

L~(W + W ) = L(W ) + W L(W )T W

(18)

130 We then approximately calculate the displacement of the weights (W ) with regards to W0, repre131 senting the trained parameters, as below:

2(W0 + W - M ) + µW L~(W ) = 0 L~(W ) = L(W0) + W L(W0)T W
L~(W0 + W ) - = 0  µ > 0

(19)

132 This system has a closed form solution:

W

=

-

µ 2

W

L(W0)

+M

-W

µ

=

2

L(W0)

+

W W

LT (M L(W0)

-
2

W0)

-

(20) (21)

4

133 This solution is of course valid only in a neighborhood around W0 where ~W is an accurate 134 estimation of L(W0 + W ). We denote this region by the radius . That is, when W  . This 135 technique is similar to local modeling of objective functions as done in Trust Region optimization
136 methods. We also adopt a similar method as trust region techniques to initialize and update the value 137 of  in each iteration of the algorithm. Thus, we summarize the algorithm to solve this problem in
138 algorithm 2.

Algorithm 2 Calculate the displacement of W for iteration t

n0

Initialize t0 while True do

Solve the constrained optimization problem using equation 20 and find W n

  min(1,

tn W n

)

if < L(Wt +   W n) then

nt +1



1 2

nt

else

Wt+1  Wt +   W n 0t+1  2  tn break

end if

end while

139 In this algorithm, each iteration receives a trust region radius as input and finds the displacement 140 for Wt based on equation 20. Then, if this displacement is larger than the trust region radius, the 141 algorithm scales it. Finally, it checks if the loss function corresponding to this displacement satisfies 142 the loss constraint. If not, it will shrink the trust region radius and retries the condition. Otherwise, it 143 will update the parameters and expands the trust region radius. This new radius will be provided to 144 the next iteration of Algorithm 1.
145 Lastly, we discuss how we obtain the value of . We initialize this value to the loss value corresponding 146 to the original model = L(W0). After the compression is done, we can test the accuracy of the 147 compressed model. If this accuracy is sufficient, we can increase it. In our technique, each time we 148 achieved a sufficient accuracy for the compressed model, we increase by 20%.

4149 Models
150 We test our compression technique on three datasets: MNIST (LeCun et al. 1998a), CIFAR-10 151 (Krizhevsky and Hinton 2009), and SVHN (Netzer et al. 2011). MNIST is a collection of 6000 152 28 × 28 black-and-white images of handwritten digits. CIFAR-10 is a set of 60000 images of object 153 from 10 different classes. Finally, SVHN contains 60000 images of digits captured by google street 154 view. We train a LeNet-5 on the MNIST dataset and a smaller version of the VGG network (Hubara 155 et al. 2016) for both CIFAR-10 and SVHN. The model size and accuracy of these baseline networks 156 are presented in table 1.

Dataset

Table 1: Baseline models

Model

Model Size Error Rate

MNIST LeNet-5 (LeCun et al. 1998b)

CIFAR-10 (Hubara et al. 2016)

SVHN

(Hubara et al. 2016)

12M b 612M b 110M b

0.65% 13.07% 2.73%

5

157 5 Experiments
158 In this section, we experimentally study the convergence of the proposed algorithm and show that it 159 achieves state-of-the-art compression rates. We confirm that the parameters of the compressed model 160 cluster about the learnt centroids and evaluate the effect of this clustering on the accuracy. We then 161 present a more in-depth study of the trade-off between accuracy and the compressed model size on 162 our benchmarks. Finally, we compare the optimal model obtained through the trade-off analysis with 163 the previous techniques.
164 The proposed compression algorithm encourages parameters to gravitate towards a set of centroids, 165 which it learns simultaneously. We verify this on MNIST by initializing 256 random centroids. These 166 centroids are eliminated or optimized according to Algorithm 6. Figure 1a illustrates the optimization 167 of the centroids. In each iteration t, this figure shows the accuracy gap between the current model, 168 Wt, and its compression using the current centroids, Ct. We observe that this gap decreases as the 169 centroids are adjusted. In addition, we observe little uncompressed model accuracy degradation, 170 despite the parameter updates. We confirm these updates cause meaningful changes to the parameters 171 in Figure 1b. This figure depicts the distribution of the parameters before and after the compression. 172 We can see that the parameters approach 4 centroids, with 86% of them gathered around 0. Overall, 173 the compression results in 112× reduction in the model size with only 0.53% drop in accuracy. We 174 note that the number of pruned parameters in this result are lower compared to previous works such 175 as Deep Compression. However, the model size reduction is higher due to the smaller dictionary.

(a) Effect of reducing the complexity and learning the centroids on the uncompressed and compressed model accuracies.

(b) Final distribution of network parameters.

Figure 1: Compression of LeNet-5 using the proposed method

176 The loss bound (section 2) controls the final model size, introducing a trade-off between accuracy 177 and compression. We study this trade-off by incrementing the loss bound and optimizing the model 178 size using Algorithm 1. In the analysis for each of the benchmark datasets we initialize 256 random 179 centroids and depict the results in Figure 2. We can see that for small reductions in size, the accuracy 180 remains generally the same. During this stage, most clusters are quickly eliminated due to being 181 empty. As the model size becomes small however, the error starts to increase quickly. We also 182 observe clusters being eliminated less often. These eliminations are a result of merging clusters. 183 Using these analyses, we can find the optimal trade-off between the accuracy and model size.
184 Table 2 summarizes the optimal trade-offs obtained from the previous analysis and compares them 185 with previous works. With little drop in accuracy, the proposed method achieves state-of-the art 186 accuracy for the MNIST dataset. It also achieves similar reductions in model size for the SVHN and 187 CIFAR-10 datasets using the much larger network model.
6

(a) MNIST

(b) CIFAR-10

(c) SVHN

Figure 2: The trade-off between accuracy and model compression for MNIST, SVHN, and CIFAR-10, using the constrained compression method.

Dataset

Table 2: Baseline models Compression Technique Compression Ratio Accuracy Loss

Constrained Compression

MNIST

Deep Compression Iterative ECSQ

Soft Weight-Sharing

CIFAR-10 Constrained Compression

SVHN

Constrained Compression

112 39 49 64 42 128

0.53 -0.06 -0.02 -0.05 0.78 2.37

188 6 Conclusion
189 In this paper, we presented a method for compressing trained neural network models that directly 190 minimizes its complexity while maintaining its accuracy. For simplicity of calculations, we chose to 191 represent the complexity using the k-means objective which is frequently used in previous works. 192 In doing so, we maintain the accuracy by introducing a hard constraint on the loss function. This 193 constraint incorporates the information learnt during the training process into the optimization. We 194 then present an algorithm that iteratively finds the optimal complexity. We test this solution on several 195 datasets and show that it can provide state-of-the-art compression, with little accuracy loss.
196 References
197 O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, 198 M. Bernstein et al., "Imagenet large scale visual recognition challenge," International Journal of 199 Computer Vision, vol. 115, no. 3, pp. 211­252, 2015.
200 S. Zagoruyko and N. Komodakis, "Wide residual networks," arXiv preprint arXiv:1605.07146, 2016.
201 K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings 202 of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770­778.
203 R. K. Srivastava, K. Greff, and J. Schmidhuber, "Training very deep networks," in Advances in neural 204 information processing systems, 2015, pp. 2377­2385.
205 S. Han, H. Mao, and W. J. Dally, "Deep compression: Compressing deep neural networks with 206 pruning, trained quantization and huffman coding," arXiv preprint arXiv:1510.00149, 2015.
7

207 K. Ullrich, E. Meeds, and M. Welling, "Soft weight-sharing for neural network compression," arXiv 208 preprint arXiv:1702.04008, 2017. 209 S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, "Deep learning with limited numerical 210 precision," in Proceedings of the 32nd International Conference on Machine Learning (ICML-15), 211 2015, pp. 1737­1746. 212 N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia, N. Boden, 213 A. Borchers et al., "In-datacenter performance analysis of a tensor processing unit," arXiv preprint 214 arXiv:1704.04760, 2017. 215 P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, "Pruning convolutional neural networks for 216 resource efficient inference," 2016. 217 Y. Guo, A. Yao, and Y. Chen, "Dynamic network surgery for efficient dnns," in Advances In Neural 218 Information Processing Systems, 2016, pp. 1379­1387. 219 Y. Choi, M. El-Khamy, and J. Lee, "Towards the limit of network quantization," arXiv preprint 220 arXiv:1612.01543, 2016. 221 Y. LeCun, C. Cortes, and C. J. Burges, "The mnist database of handwritten digits," 1998. 222 A. Krizhevsky and G. Hinton, "Learning multiple layers of features from tiny images," 2009. 223 Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, "Reading digits in natural images 224 with unsupervised feature learning," in NIPS workshop on deep learning and unsupervised feature 225 learning, vol. 2011, no. 2, 2011, p. 5. 226 I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, "Binarized neural networks," in 227 Advances in Neural Information Processing Systems, 2016, pp. 4107­4115. 228 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document 229 recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278­2324, 1998.
8

