Under review as a conference paper at ICLR 2018
DEEP BOOSTING OF DIVERSE EXPERTS
Anonymous authors Paper under double-blind review
ABSTRACT
In this paper, a deep boosting algorithm is developed to learn more discriminative ensemble classifier by seamlessly combining a set of base deep CNNs (base experts) with diverse capabilities, e.g., these base deep CNNs are sequentially trained to recognize a set of object classes in an easy-to-hard way according to their learning complexities. Our experimental results have demonstrated that our deep boosting algorithm can significantly improve the accuracy rates on largescale visual recognition.
1 INTRODUCTION
The rapid growth of computational powers of GPUs has provided good opportunities for us to develop scalable learning algorithms to leverage massive digital images to train more discriminative classifiers for large-scale visual recognition applications, and deep learning (Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016) has demonstrated its outstanding performance because highly invariant and discriminant features and multi-way softmax classifier are learned jointly in an end-to-end fashion.
Before deep learning becomes so popular, boosting has achieved good success on visual recognition (Viola & Jones, 2004). By embedding multiple weak learners to construct an ensemble one, boosting (Schapire, 1999) can significantly improve the performance by sequentially training multiple weak learners with respect to a weighted error function which assigns larger weights to the samples misclassified by the previous weak learners. Thus it is very attractive to invest whether boosting can be integrated with deep learning to achieve higher accuracy rates on large-scale visual recognition.
By using neural networks to replace the traditional weak learners in the boosting frameworks, boosting of neural networks has received enough attentions (Zhou & Lai, 2009; Mosca & Magoulas, 2017a; Kuznetsov et al., 2014; Moghimi et al., 2016). All these existing deep boosting algorithms simply use the weighted error function (proposed by Adaboost (Schapire, 1999)) to replace the softmax error function (used in deep learning ) that treats all the errors equally. Because different object classes may have different learning complexities, it is more attractive to invest new deep boosting algorithm that can use different weights over various object classes rather than over different training samples.
Motivated by this observation, a deep boosting algorithm is developed to generate more discriminative ensemble classifier by combining a set of base deep CNNs with diverse capabilities, e.g., all these base deep CNNs (base experts) are sequentially trained to recognize different subsets of object classes in an easy-to-hard way according to their learning complexities. The rest of the paper is organized as: Section 2 briefly reviews the related work; Section 3 introduce our deep boosting algorithm; Section 4 reports our experimental results; and we conclude this paper at Section 5.
2 RELATED WORK
In this section, we briefly review the most relevant researches on deep learning and boosting.
Even deep learning has demonstrated its outstanding abilities on large-scale visual recognition (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016; Huang et al., 2016), it still has room to improve: all the object classes are arbitrarily assumed to share similar learning complexities and a multi-way softmax is used to treat them equally. For recognizing large numbers of object classes, there may have significant differences on their learning complexi-
1

Under review as a conference paper at ICLR 2018
ties, e.g., some object classes may be harder to be recognized than others. Thus learning their deep CNNs jointly may not be able to achieve the global optimum effectively because the gradients of their joint objective function are not uniform for all the object classes and such joint learning process may distract on discerning some object classes that are hard to be discriminated. For recognizing large numbers of object classes with diverse learning complexities, it is very important to organize them in an easy-to-hard way according to their learning complexities and learn their deep CNNs sequentially. By assigning different weights to the training samples adaptively, boosting (Schapire, 1999; Freund & Schapire, 1997; Schapire et al., 1998) has provided an easy-to-hard approach to train a set of weak learners sequentially. Thus it is very attractive to invest whether we can leverage boosting to learn a set of base deep CNNs sequentially for recognizing large numbers of object classes in an easy-to-hard way.
Some deep boosting algorithms have been developed by seamlessly integrating boosting with deep neural networks to improve the performance in practice. Schwenk & Bengio (1997; 2000) proposed the first work to integrate Adaboost with neural networks for online character recognition application. Zhou & Lai (2009) extended the Adaboosting neural networks algorithm for credit scoring. Recently, Mosca & Magoulas (2017b) developed a deep incremental boosting method which increases the size of neural network at each round by adding new layers at the end of the network. Moreover, Mosca & Magoulas (2017a) integrated residual networks with incremental boosting and built an ensemble of residual networks via adding one more residual block to the previous residual network at each round of boosting. All these methods combine the merits of boosting and neural networks; they train each base network either using a different training set by resampling with a probability distribution derived from the error weight, or directly using the weighted cost function for the base network.
Alternatively, Saberian & Vasconcelos (2011) proposed a margin enforcing loss for multi-class boosting and presented two ways to minimize the resulting risk: the one is coordinate descent approach which updates one predictor component at a time, the other way is based on directional functional derivative and updates all components jointly. By applying the first way, i.e., coordinate descent, Cortes et al. (2014) designed ensemble learning algorithm for binary-class classification using deep decision trees as base classifiers and gave the data-dependent learning bound of convex ensembles, and Kuznetsov et al. (2014) furthermore extended it to multi-class version. By applying the second way, i.e., directional derivative descent, Moghimi et al. (2016) developed an algorithm for boosting deep convolutional neural networks (CNNs) based on least squares between weights and directional derivatives, which differs from the original method based on inner product of weights and directional derivative in (Saberian & Vasconcelos, 2011). All above algorithms focus on seeking the optimal ensemble predictor via changing the error weights of samples; they either update one component of the predictor per boosting iteration, or update all components simultaneously.
On the other hand, our deep boosting algorithm focuses on combining a set of base deep CNNs with diverse capabilities: (1) large numbers of object classes are automatically organized in an easyto-hard way according to their learning complexities; (2) all these base deep CNNs (base experts) are sequentially learned to recognize different subsets of object classes; and (3) these base deep CNNs with diverse capabilities are seamlessly combined to generate more discriminative ensemble classifier.
3 DEEP BOOSTING OF DIVERSE EXPERTS
In this paper, a deep boosting algorithm is developed by seamlessly combining a set of base deep CNNs with various capabilities, e.g., all these base deep CNNs are sequentially trained to recognize different subsets of object classes in an easy-to-hard way according to their learning complexities. Our deep boosting algorithm uses the base deep CNNs as its weak learners, and many well-designed deep networks (such as AlexNet (Krizhevsky et al., 2012), VGG (Simonyan & Zisserman, 2015), ResNet (He et al., 2016), and huang2016densely), can be used as its base deep CNNs. It is worth noting that all these well-designed deep networks [] optimize their structures (i.e., numbers of layers and units in each layer), their node weights and their softmax jointly in an end-to-end manner for recognizing the same set of object classes. Thus our deep boosting algorithm is firstly implemented for recognizing 1,00 object classes, however, it is straightward to extend our current implementation
2

Under review as a conference paper at ICLR 2018

Algorithm 1 Deep Boosting of Diverse Experts

Require: Training set from C classes: {(xi, yi) | yi  {1, ..., C}, i = 1, ..., N }; Initial significance

distribution over categories: [D1(1), ..., D1(C)]; Number of base deep CNNs: T .

1: for t = 1, . . . , T do

2:

Normalization:

Dt(l)

=

CDt
j=1

(l) Dt (j

)

,

(l

=

1, ..., C)

3: Training the tth base deep CNNs ft(x) via Losst with respect to the importance distribution

over C categories [Dt(1), ..., Dt(C)];

4: 5:

Calculating Computing

the the

error per weighted

category error for

ffot(rxf)t:(x)t:=t(l)Cl,=(1l

= 1, ..., C); Dt(l)t(l);

6:

Setting

t

=

t 1-t

;

7: Updating Dt+1(l) as Dt+1(l) = Dt(l)t1-t(l), (l = 1, ..., C), so that hard object classes

misclassified by ft(x) can receive larger weights (importances) when training the (t + 1)th base

deep CNNs at the next round;

8: 9:

end for Ensembling:

g(x) =

1 Z

T
t=1

log

(
1 t

)

ft(x)

when huge deep networks (with larger capacities) are available in the future and being used as the base deep CNNs.

3.1 ALGORITHM

As illustrated in Algorithm 1, our deep boosting algorithm contains the following key components: (a) Training the tth base deep CNNs (base expert) ft(x) by focusing on achieving higher accuracy rates for some particular object classes; (b) Estimating the weighted error function for the tth base
deep CNNs ft(x) according to the distribution of importances Dt for C object classes; (c) Updating the distribution of importances Dt+1 for C object classes to train the (t + 1)th base deep CNNs by spending more efforts on distinguishing the hard object classes which are not classified very well by
the previous base deep CNNs; (d) Such iterative training process stops when the maximum number
of iterations is reached or a certain level of the accuracy rates is achieved.

For the tth base expert ft(x), we firstly employ deep CNNs to map x into more separable feature space ht(x; t), followed by a fully connected discriminant layer and a C-way softmax layer. The output of the tth base expert is the predicted multi-class distribution, denoted as ft(x) = [pt(1|x), ..., pt(C|x)], whose each component pt(l|x) is the probability score of x assigned to the object class l, (l = 1, ..., C):

pt(l|x) = Cje=x1pe{xwp{ltwhjt(thxt;(xt);}t)}

(1)

where t and wlt, (l = 1, ..., C) are the model parameters for the tth base expert ft(x). Based on the above probability score, the category label of x can be predicted by the tth base expert as follows:

y^t = arg max pt(l|x)
l

(2)

Suppose that training set consists of N labeled samples from C classes: {(xi, yi) | yi  {1, ..., C}}Ni=1. To train the tth base expert ft(x), the model parameters can be learned by maximizing the objective function in the form of weighted margin as follows:

where

C Ot(t, {wlt}Cl=1) = Dt(l)lt
l=1

lt

=

1 Nl

N 1(yi
i=1

=

l) log pt(l|xi) -

N

1 - Nl

N 1(yi
i=1

=

l) log pt(l|xi)

3

(3) (4)

Under review as a conference paper at ICLR 2018

Herein the indicator function 1(yi = l) is equal to 1 if yi = l; otherwise zero. Nl denotes the number of samples belonging to the lth object class. Dt(l) is the normalized importance score for class l in the tth base expert ft(x). By using the distribution of importances [Dt(1), ..., Dt(C)] to approximate the learning complexities for C object classes, our deep boosting algorithm can push the current base deep CNNs to focus on distinguishing the object classes which are hard classified by the previous base deep CNNs, thus it can support an easy-to-hard solution for large-scale visual recognition.

lt measures the margin between the average confidence on correctly classified examples and the

average confidence on misclassified small enough and negligible, lt 

eN1xl ampiN=le1s1fo(yr ith=e

lth object class. If the second item in Eq.(4) is l) log pt(l|xi), then maximizing the objective

function in Eq.(3) is equivalent to maximizing the weighted likelihood.

For the tth base expert ft(x), the classification error rate over the training samples in lth object class is as follows:

t(l)

=

1 2

N {1(yi
i=1

=

1 l)

-

pt(l|xi) Nl

+

1(yi

=

l)

pt(l|xi) N - Nl

}

(5)

This error rate is used to update category weight and the loss function of the next weak learner,

and above definition encourages predictors with large margin to improve the discrimination between

correct class and incorrect classe competing with it. Error rate calculated by Eq.(6) is in soft decision

wN1ilthpNir=o1ba1b(iyliity=;

alternatively, l  y^it = l).

we can also simply compute the error rate in The criterion for hard object classes is t(l)

hard decision

>

1 2

where

as t(l) = the hyper-

parameter



controls

the

threshold,

and

we

constrain



>

1 2

(i.e.,

1 2

<

1

)

such

that

the

threshold

makes sense. The larger the hyper-parameter  is, the more strict the precision requirement becomes.

We then compute the weighted error rate t over all classes for ft(x) such that hard object classes

are focused on by the next base expert.

C t = Dt(l)t(l)
l=1

(6)

The distribution of importances is initialized equally for all C

object classes:

D1(l)

=

1 C

,

(l

=

1, ..., C), and it is updated along the iterative learning process by emphasizing the object classes

which are heavily misclassified by the previous base deep CNNs:

Dt+1(l) = Dt(l)t1-t(l)

(7)

where t should be an increasing function of t, and its range should be 0 < t < 1. It should be pointed out that t(l) denotes the product of  and t(l). Such update of distribution encourages the next base network focusing on the categories that are hard to classify. As shown in Section
4, to guarantee the upper boundary of ratio (the number of heavily misclassified categories over the
number of all classes) to be minimized, we set

t

=

t 1 - t

(8)

Normalization of the updated importances distribution can be easily carried out:

Dt+1(l) = CjD=1t+D1t(+l)1(j) ,

(l = 1, ..., C)

(9)

The distribution of importances is used to: (a) separate the hard object classes (heavily misclassified
by the previous base deep CNNs) from the easy object classes (which have been classified correctly by the previous base deep CNNs); (b) estimate the weighted error function for the (t + 1)th base
deep CNNs ft+1(x), so that it can spend more efforts on distinguishing the hard object classes misclassified by the previous base deep CNNs.

4

Under review as a conference paper at ICLR 2018

After T iterations, we can obtain T base deep CNNs (base experts) {f1, · · · , ft, · · · , fT }, which are sequentially trained to recognize different subsets of C object classes in an easy-to-hard way

according to their learning complexities. All these T base deep CNNs are seamlessly combined to

generate more discriminative ensemble classifier g(x) for recognizing C object classes:

1 T

() 1

where

Z

=

T
t=1

log

(

1 t

)

is

a

g(x) =

log

Z

t=1

normalization factor.

t By

ft(x) diversifying

a

set

of

base

deep

(10) CNNs on

their capabilities (i.e., they are trained to recognize different subsets of C object classes in an easy-

to-hard way), our deep boosting algorithm can obtain more discriminative ensemble classifier g(x)

to significantly improve the accuracy rates on large-scale visual recognition.

To apply such ensembled classifier for recognition, for a given test sample xtest, it firstly goes

through all these base deep CNNs to obtain T deep representations {h1, · · · , hT } and then its final

probability score p(l|xtest) to be assigned into the lth object class is calculated as follows:

p(l|xtest)

=

1 Z

T

() 1

log

t=1 t

jCe=x1pe{xwp{ltwhjt(thxtte(sxtt;estt);}t)}

(11)

3.2 SELECTION OF t

In our deep boosting algorithm, t is selected to be an increasing function of error rate t, with its range [0, 1]. t is employed in two folds: (i) As seen in Eq.(7), t helps to update the importance of different categories such that hard object classes are emphasized; (ii) As seen in Eq.(10) and
Eq.(11), reciprocals of t are the combination coefficients for the final ensemble classifier such that those base experts with low error rate have large weight.

The criterion of hard object classes for the tth expert is t(l)

>

1 2

.

Denote min(l) 

min{1(l), ..., T (l)}.

If min(l) >

1 2

,

then

t(l)

>

1 2

for

each

t,

(t

= 1, ..., T ); it implies

that the lth object class is hard for all T experts.

Let

{l

:

min(l)

>

1 2

}

denote

the

the

number

of

hard

object

classes

for

all

T

experts.

Inspired

by

Freund & Schapire (1997), we now show that the selection of t as in Eq.(8) guarantees the upper

boundary of ratio (the number of heavily misclassified categories over the number of all classes) to

be minimized.

It can be shown that for 0 < x < 1 and 0 <  < 1, we have x  1 - (1 - x). According to Eq.(7):
Dt+1(l) = Dt(l)t1-t(l),
we get

C Dt+1(l) = C Dt(l)t1-t(l)  C Dt(l)(1 - (1 - t)(1 - t(l)))

l=1 l=1

l=1

C C = ( Dt(l))(1 - (1 - t)) + (1 - t) Dt(l)t(l)

l=1 l=1

According to Eq.(6) and Eq.(9), we get:

(12)

Therefore

C C Dt(l)t(l) = ( Dt(l))t
l=1 l=1

C C

C

Dt+1(l)  Dt(l)(1 - (1 - t)) + (1 - t)( Dt(l))t

l=1 l=1

l=1

C = ( Dt(l))[1 - (1 - t)(1 - t)]

l=1

(13)

5

Under review as a conference paper at ICLR 2018

Since

C
l=1

D1(l)

=

1,

we

have

C C D2(l)  ( D1(l))[1 - (1 - 1)(1 - 1)] = 1 - (1 - 1)(1 - 1)
l=1 l=1

Thus

C DT +1(l)  tT=1[1 - (1 - t)(1 - t)]
l=1

By substituting Eq.(7) into Eq.(14), we get

(14)

tT=1[1

-

(1

-

t)(1

-

t)]



C

DT +1(l)

=

C (D1

(l)tT=1

t1-t

(l)

)

l=1 l=1

=

1 C

C (Tt=1

t1-t

(l)

)



1 C

 (tT=1t1-t(l))

l=1

l:min

(l)>

1 2

(15)

Due

to

min(l)

>

1 2

,

it

holds

that

t(l)

>

1 2

and

1 - t(l)

<

1 2

for

all

l.

Recall

the

constraint

that

0 < t < 1, thus

1 C



(Tt=1t1-t(l))



1 C



(tT=1

1
t2

)

l:min

(l)>

1 2

l:min

(l)>

1 2

(16)

=

{l

:

min(l) C

>

1 2

}

tT=1

1
t2

Combining Eq.(15) with Eq.(16), we get

{l

:

min(l) C

>

1 2

}



tT=1[1 - (1 - t)(1
1
tT=1t2

-

t)]

=

tT=1

1

-

(1

-

t)(1
1

-

t)

t2

To minimize the rightside, we set its partial derivative with respect to t to zero:

 t

(tT=1

1

-

(1

-

t)(1
1
t2

-

t)

)

=

0

Since t only exists in the tth factor, above equation is equivalent to

 t

(

1

-

(1

-

t)(1
1
t2

-

t)

)

=

0

Solving it, we find that t can be optimally selected as:

t

=

1

t - t

(17)

3.3 SELECTION OF 

We substitute t

=

t 1-t

into Eq.(17), and get the upper boundary of ratio (the number of hard

object categories over the number of all classes):

{l

:

min(l) C

>

1 2

}



 2T Tt=1 t(1 - t)

(18)

Now we discuss the range for the hyper-parameter . Recall that the criterion of hard object classes

for the tth expert is t(l)

>

1 2

and  controls the threshold.

When we are to raise the precision

6

Under review as a conference paper at ICLR 2018

requirements,  can be set large.

We

constrain



>

1 2

such that

1 2

<

1 and

the threshold of error

rate

makes

sense.

On

the

other

hand,

the

range

of

t

=

t 1-t

is

0

<

t

<

1,

so

it

is

required

that

t

<

1 2

,

i.e.,



<

1 2t

.

To

conclude,



should

be

selected

between

the

interval

[

1 2

,

1 2t

].

From the relation between t and t(1 - t), as illustrated in Fig.1, we can see the effect of  on the upper boundary of ratio (the number of hard object categories over the number of all classes)
in Eq.(18).

·

In the yellow shaded region, 



[

1 2

,

1 2t

],

i.e.,

t 2

<

t

<

1 2

,

the

condition

0

<

t

<

1

is satisfied, and the upper boundary of hard category percentage in Eq.(18) increases with

 increasing, the reason for which is that when  increases, the precision requirement

increases, thus the number of hard categories increases too.

·

On the right side of the yellow shaded region, 

>

1 2t

,

i.e.,

t

>

1 2

.

In this case, the

condition 0

<

t

=

t 1-t

<

1 is not satisfied, thus the update of importance distribution

in Eq.(7) can not effectively emphasize the object classes which are heavily misclassified

by the previous experts. In hard classification task, large error rates t tend to result in t

larger

than

or

approaching

1 2

,

and

t

larger

than

or

approaching

1.

The

value

of



should

be

set

smaller

to

alleviate

large

t

such

that

t

<

1 2

and

0

<

t

<

1.

·

On the left side of the yellow shaded region,  <

1 2

,

i.e.,

1 2

> 1, then it can not be used as

the

the

criterion

of

hard

object

classes

that

t(l)

>

1 2

.

Figure

1:

The

relation

between

t

and

t(1 - t).

The

domain

with

respect

to



is

1 2

<



<

1 2t

,

so

t 2

<

t

<

1 2

(yellow shaded region).

3.4 BACK PROPAGATION

The procedure of learning the tth base expert repeatedly adjusts the parameters of the corresponding deep network so as to maximize the objective function Ot in Eq.(3). To maximize Ot, it is necessary to calculate its gradients with respect to all parameters, including the weights {wlt}lC=1 and the set of model parameters t.

For clearance, we denote

zlt(x) = wlt ht(x; t)  zlt(ht, wlt)

Thus, the probability score of x assigned to the object class l, (l = 1, ..., C), in Eq.(1) can be written

as

pt(l|x) =

ezlt (x)

C
j=1

ezj t (x)

 ptl (z1t, ..., zCt)

Then, the objective function in Eq.(3) can be denoted as Ot(t, {wlt}lC=1)  Ot(pt1, ..., ptC )

From above presentations, it can be more clearly seen that the objective is a composite function. Ot is firstly a function of multiple variables p1t , ..., ptC , whose each component plt is a function of

7

Under review as a conference paper at ICLR 2018

multiple variables z1t, ..., zCt, and each zlt is a function of multiple variables ht, wlt, furthermore, each ht is a function of multiple variables t. By chain rule, the gradients for the objective function Ot with respect to {wlt}Cl=1 and t are computed from the top layer to the bottom one in backward pass.

where and

Ot wlt

=

C
j=1

Ot pjt

pjt zlt

zlt , wlt

Ot t

=

C
j,l=1

Ot pjt

ptj zlt

zlt ht ht t

Ot ptj

=

1 Dt(j)[ Nj

N 1(yi
i=1

=

1 j)
pt(j|xi)

-

N

1 - Nj

N 1(yi
i=1

=

j) log pt(j|xi)]

pjt zlt

{ =

plt(1 - ptl ) -ptl ptj

if j = l if j = l

zlt wlt

=

ht,

zlt ht

=

wlt,

ht = J t

Herein, J is Jacobi matrix. Such gradients are back-propagated [] through the tth base deep CNNs to fine-tune the weights {wlt}Cl=1 and the set of model parameters t simultaneously.

3.5 GENERALIZATION ERROR BOUND

Denote X as the instance space, denote  as the distribution over X , and denote S as a training set

of N examples chosen i.i.d according to . We are to investigate the gap between the generalization

error on  and the empirical error on S.

{

Suppose that F is the set from which the base deep experts are chosen, and let G = x 

 }

be

f F af f viewed

(x)|af  0, fF af as a distribution over F

= 1 . The combination coefficients A

.

Define G^

=

{ x



1 


t=1

ft(x)|ft



=}[a1, ..., af , F , and each

...] can ft  F

may appear multiple times in the sum. For any g  G, there exists a distribution A = [a1, ..., af , ...], so we can select the base deep experts from F for  times independently according to A and obtain

g^  G^, denote g^  A.

Note that g is a C-dim vetor, and each component of g is the category confidence, i.e., gy(x) = p(y|x), (y = 1, ..., C). Based on Eq.(11), the category label of test sample can be predicted by arg maxy gy(x) = p(y|x). The ensembled classifier g predicts wrong if gy(x)  maxy¯= y gy¯(x). The generalization error rate for the final ensembled classifier can be measured by the probability P[gy(x)  maxy¯=y gy¯(x)].
According to probability theory, for any events B1 and B2, P(B1)  P(B2) + P(B¯2|B1), therefore

P [gy (x)



max
y¯= y

gy¯(x)]

P,g^A [g^y (x)



max
y¯= y

g^y¯(x)

+

 ]+
2

P,g^A [g^y (x)

>

max
y¯=y

g^y¯(x)

+

 2

|gy

(x)



max
y¯= y

gy¯(x)]

(19)

where  > 0 measures the margin between the confidences from ground-truth and incorrect categories. Using Chernoff bound (Schapire et al., 1998), the the second term in the right side of Eq.(19)

is bounded as:

P,g^A [g^y (x)

>

max g^y¯(x)
y¯=y

+

 2

|gy

(x)



max gy¯(x)]
y¯= y



e-2/8

(20)

8

Under review as a conference paper at ICLR 2018

Assume that the base-classifier space F is with VC-dimension d, which can be approximately estimated by the number of neurons  and the number of weigths  in the base deep network, i.e., ndouf =mhybpOeor(tohfesh)e.yspRoovethecreaslSlestfhofaortrG^SF=iso{vaexrsaSmips1leatsmett=oo1sftftN(xid)e=|xf1at(mCpiNFle)s}=firso(ametNdmCCo)scdta.t(eTegNhdouCrise),st.dh.eTehfefnectthiveeenffuemctbiveer
Applying Devroye Lemma as in (Schapire et al., 1998), it holds with probability at least 1 -  that

P,g^A [g^y (x)



max
y¯=y

g^y¯(x)

+

 ]
2



PS ,g^A [g^y (x)



max
y¯= y

g^y¯(x)

+

 ]
2

+





where  =

1 2N

[d

log

eN 2C d

+ log

4e8

(+1) 

].

(21)

Likewise, in probability theory for any events B1 and B2, P(B1)  P(B2) + P(B1|B¯2), thus

PS ,g^A [g^y (x)



max
y¯= y

g^y¯(x)

+

 ]
2

PS [gy(x)



max
y¯=y

gy¯(x)

+

]+

PS ,g^A [g^y (x)



max
y¯= y

g^y¯(x)

+

 2

|gy

(x)

>

max
y¯=y

gy¯(x)

+

]

(22)

Because

PS ,g^A [g^y (x)



max
y¯= y

g^y¯(x)

+

 2

|gy

(x)

>

max
y¯=y

gy¯(x)

+

]





PS ,g^A [g^y (x)



g^y¯(x)

+

 2

|gy

(x)

>

gy¯(x)

+

]



(C

-

1)e-2/8

y¯=y

(23)

So, combining Eq.(19-23) together, it can be derived that

P[gy(x)  max gy¯(x)]  PS [gy(x)  max gy¯(x) + ] + Ce-2/8 + 

y¯=y

y¯=y

(24)

As can be seen from above, large margin  over the training set corresponds to narrow gap between the generalization error on  and the empirical error on S, which leads to the better upper bound of generalization error.

4 EXPERIMENTAL RESULTS
In this section we evaluate the proposed algorithms on three real world datasets MNIST (LeCun, 1998), CIFAR-100 (Krizhevsky & Hinton, 2009), and ImageNet (Russakovsky et al., 2015). For MNIST and CIFAR-100, we train all networks from scrach in each AdaBoost iteration stage. On ImageNet, we use the pretrained model as the result of iteration #1 and then train weighted models sequentially. The pretrained model is available in TorchVision1. In each iteration, we adopt the weight initialization menthod proposed by He et al. (2015). All the networks are trained using stochastic gradient descent (SGD) with the weight decay 0f 10-4 and the momentum of 0.9 in the experiments.
EXPERIMENTAL RESULTS ON MNIST
MNIST dataset consists of 60,000 training and 10,000 test handwritten digit samples. Schwenk & Bengio (2000) showed the accuracy improvement of MLP via AdaBoost on MNIST dataset by updating sample weights according to classification errors. For fair comparison, we firstly use the similar network architecture (MLP) as the base experts in experiments. We train two sets of networks with the only difference that one updates weights w.r.t the class errors on training datasets while the other one updates weights w.r.t the sample errors on training datasets. The former is the proposed
1https://github.com/pytorch/vision

9

Under review as a conference paper at ICLR 2018

method in this paper, and the latter is the traditional AdaBoost method. In the two sets of weak learners, we share the same weak learner in iteration #1 and train other two weak learners seperately. For data pre-processing, we normalize data via subtracting means and dividing standard deviations. In the experiment on MNIST, we simply train the network with learning rate 0.01 through out the whole 120 epoches.

Method

Iteration #1 Iteration #2 Iteration #3

update weights w.r.t sample errors

4.73

2.53

2.23

update weights w.r.t class errors

4.73

2.22

1.87

Table 1: Comparison of test error rate(%) with boosted model on MNIST datasets.

With our proposed method, the top 1 error on test datasets decreases from 4.73% to 1.87 % after three iterations (table 1). After the interation #1, the top 1 error of our method drops more quickly than the method which update weights w.r.t sample errors. Our method, which updates weights w.r.t the class errors, leverages the idea that different class should have different learning comlexity and should not be treated equally. Through the iterations, our method trains a set of classifiers in an easy-to-hard way.
Class APs vary from each weak learner in each iteration to others, increasing for marjor weighted classes while decreasing for minor weighted classes(fig. 2-left). Therefore, in each iteration, the weighted learner classifier behaves like a expert different from the classfier in the previous iteration. Though some APs for certain classes may decrease in some degree with each weak learner, the boosting models improve the accuracy for hard classes while preservering the accuracy for easy classes (fig. 2-right). Our method cordinates the set of weak learners trained sequeentially with diversified capabilities to improve the classfication capability of boosting model.

AP AP

0.99 weak learner #1 weak learner #2
0.98 weak learner #3 0.97 0.96 0.95 0.94 0.93
0 1 2 3 4Class ID5 6 7 8 9
(a) Individual Expert Performance

0.99

iteration #1 iteration #2

iteration #3

0.98

0.97

0.96

0.95

0.94

0.93

0 1 2 3 4Class ID5 6 7 8 9

(b) Ensembled Experts Performance

Figure 2: The comparison of class AP(average precision) on MNIST with MLP. The x-axis (class ID) is sorted according to the test APs (average pricision) per class in the weak learner #1. AP(average pricision) on the left plot is calculated on validation darasets with single model. AP (average pricision) on the right plot is calcucalted on test datasets with boosted model as in eq. (11) with t={1,2,3} respectively.

EXPERIMENTAL RESULTS ON CIFAR-100
We also carry out experiments on CIFAR-100 dataset. CIFAR-100 dataset consists of 60,000 images from 100 classes. There are 500 training images and 100 testing images per class. We adopt padding, mirroring, shifting for data augumentation and normalization as in (He et al., 2016; Huang et al., 2016). In training stage, we hold out 5,000 training images for validation and leave 45,000 for training. Because the error per class on training datasets approaches zero and training errors could be all zeros with even simple networks (Zhang et al., 2016), we update the category distribution w.r.t
10

Under review as a conference paper at ICLR 2018

the class errors on validation datasets. We do not use any sample of validation datasets to update parameters of the networks itself. When training networks on CIFAR-100, the initial learning rate is set to 0.1 and divided by 0.1 at epoch [150, 225]. Similar to (He et al., 2015; Huang et al., 2016), we train the network for 300 epoches. We show the results with various models including ResNet56( = 0.7) and DenseNet-BC(k=12)(Huang et al., 2016) on test set. The performances of emsembled classifier with different number of base networks are shown in the middle two rows of (table 2).
As illustrated in section 3.3,  controls the weight differences among classes. In comparison, we use ={0.7, 0.5, 0.1}. As shown in fig. 3-left, with smaller lambda, the weitht differences become bigger. We use ResNet model in (He et al., 2016) with 56 layers on CIFAR-100 datasets.

Weight Top 1 error

5 4 3 2 1
0

weak learner #1 weak learner #2 (lambda=0.7) weak learner #2 (lambda=0.5) weak learner #2 (lambda=0.1) 20 40 60 80 100 Class ID

32 30 28 26 24
1

lambda=0.7 lambda=0.5 lambda=0.1

23 Iteration #

4

Figure 3: Comparison of lambda selection on CIFAR-100 datsets with ResNet56 model. Class IDs are sorted w.r.t the class APs of iteration #1. Top 1 errors (left plot) are generated on test datasets.
Overall, the models with lambda=0.7 performs the best, resulting in 24.15% test error after four iterations. Comparing with lambda=0.5 and lambda=0.7, we find that both model performs well in the initial several iterations, but the model with lambda=0.7 would converge to a better optimal(fig. 3right). However, with lambda=0.1 which weights classes more discriminately, top 1 error fluctuates along the iterations (fig. 3-right). We conclude that lambda should be merely used to insure that the value of  is below 0.5 and may harm the performance in the ensemble models if set to a low value.

Class AP Class AP

iteration #1 0.7 iteration #2

0.6

0.5

0.4

0.3

0.2 0

5 10 Clas1s5ID

20

25

0.60 0.55 0.50 0.45 0.40 0.35
0

iteration #1 iteration #2

5 Class10ID

15

Figure 4: The change of validation APs from the classes with low APs in the iteration # 1 for ResNet56 (left) and DenseNet-BC(k=12) (right) on CIFAR-100. APs are all calculated from validation sets which would be used for the update of weights in the proceeding iteration. Class IDs are sorted by the APs of weak learner #1 and selected for those with small class IDs.

In fig. 4, we show the comparison of weak leaner #1 and weak learner #2 without boosting. Though with minor exeptions, most classes with low APs improve their class APs in the proceeding weak

11

Under review as a conference paper at ICLR 2018

learner. Our method is based on the motivation that different classes should have different learning comlexity. Thus, those classes with higher learning complexity should be paid more attention along the iterations. Based on the class AP result of the privious iteration, we suppose those classes with lower APs should have higher learning complexity and be paid for attention in the subsequent iterations.

EXPERIMENTAL RESULTS ON IMAGENET

We furthermore carry out experiments on ILSVRC2012 Classification dataset(Russakovsky et al., 2015) which consists of 1.2 million images for training, and 50,000 for validation. There are 1,000 classes in the dataset. For data augmentation and normalization, we adopt scaling, ramdom cropping and horizontal flipping as in (He et al., 2016; Huang et al., 2016). Similar to the experiments on CIFAR-100, the error per class on training datasets approaches zero, we update the category distribution w.r.t the class errors on validation datasets. Since the test dataset of ImageNet are not available, we just report the results on the validation sets, following He et al. (2016); Huang et al. (2016) for ImageNet. When we train ResNet50 networks on ImageNet, the initial learning rates are set to 0.1 and divided by 0.1 at epoch [30, 60]. Similar to (He et al., 2015; Huang et al., 2016) again, we train the network for 90 epoches. The performances of emsembled classifier with different number of base networks are shown in the bottom rows of (table 2). These base ResNet networks with diverse capabilities are combined to generate more discriminative ensemble classifier.

Datasets Network

T=1 T=2 T=3 T=4

MNIST MLP

4.73 2.22 1.87 1.86

CIFAR-100

ResNet56(He et al., 2016)* DenseNet-BC(k=12)(Huang

et

al.,

2016)*

29.53 30.78

26.65 28.95

24.97 27.60

24.15 26.64

ImageNet ResNet50 (He et al., 2015)

24.18(7.49) 23.28(6.98) 22.96(6.81) 22.96(6.79)

Table 2: Single crop test error rate(%) along iterations with boosted models. * indicates updating weights with  = 0.7, while the others  = 1. Blue indicates the use of pre-trained model from TorchVision. () indicates top 5 error rate.

CONCLUSIONS
In this paper, we develop a deep boosting algorithm is to learn more discriminative ensemble classifier by combining a set of base experts with diverse capabilities. The base experts are from the family of deep CNNs and they are sequentially trained to recognize a set of object classes in an easy-to-hard way according to their learning complexities. As for the future network, we would like to investigate the performance of heterogeneous base deep networks from different families.
ACKNOWLEDGMENTS
REFERENCES
Corinna Cortes, Mehryar Mohri, and Umar Syed. Deep boosting. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 1179­1187, 2014.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119­139, 1997.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 1026­1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016.

12

Under review as a conference paper at ICLR 2018
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012. Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Multi-class deep boosting. In Advances in Neural Information Processing Systems, pp. 2501­2509, 2014. Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. Mohammad Moghimi, Serge J Belongie, Mohammad J Saberian, Jian Yang, Nuno Vasconcelos, and Li-Jia Li. Boosted convolutional neural networks. In BMVC, 2016. Alan Mosca and George D Magoulas. Boosted residual networks. In International Conference on Engineering Applications of Neural Networks, pp. 137­148. Springer, 2017a. Alan Mosca and George D Magoulas. Deep incremental boosting. arXiv preprint arXiv:1708.03704, 2017b. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211­252, 2015. doi: 10.1007/s11263-015-0816-y. Mohammad J Saberian and Nuno Vasconcelos. Multiclass boosting: Theory and algorithms. In Advances in Neural Information Processing Systems, pp. 2124­2132, 2011. Robert E Schapire. A brief introduction to boosting. In Ijcai, volume 99, pp. 1401­1406, 1999. Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. Boosting the margin: A new explanation for the effectiveness of voting methods. The annals of statistics, 26(5):1651­1686, 1998. Holger Schwenk and Yoshua Bengio. Adaboosting neural networks: Application to on-line character recognition. In International Conference on Artificial Neural Networks, pp. 967­972. Springer, 1997. Holger Schwenk and Yoshua Bengio. Boosting neural networks. Neural Computation, 12(8):1869­ 1887, 2000. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1­9, 2015. Paul Viola and Michael J Jones. Robust real-time face detection. International journal of computer vision, 57(2):137­154, 2004. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016. Ligang Zhou and Kin Lai. Adaboosting neural networks for credit scoring. In The Sixth International Symposium on Neural Networks (ISNN 2009), pp. 875­884. Springer, 2009.
13

