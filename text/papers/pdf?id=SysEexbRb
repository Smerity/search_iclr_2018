Under review as a conference paper at ICLR 2018
CRITICAL POINTS OF NEURAL NETWORKS: ANALYTICAL FORMS AND LANDSCAPE PROPERTIES
Anonymous authors Paper under double-blind review
ABSTRACT
Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect. Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms. In this paper, we provide full (necessary and sufficient) characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for various neural networks. We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum. Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of these neural networks. One particular conclusion is that: The loss function of linear networks has no spurious local minimum, while the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum.
1 INTRODUCTION
In the past decade, deep neural networks Goodfellow et al. (2016) have become a popular tool that has successfully solved many challenging tasks in a variety of areas such as machine learning, artificial intelligence, computer vision, and natural language processing, etc. As the understandings of deep neural networks from different aspects are mostly based on empirical studies, there is a rising need and interest to develop understandings of neural networks from theoretical aspects such as generalization error, representation power, and landscape (also referred to as geometry) properties, etc. In particular, the landscape properties of loss functions (that are typically nonconex for neural networks) play a central role to determine the iteration path and convergence performance of optimization algorithms.
One major landscape property is the nature of critical points, which can possibly be global minima, local minima, saddle points. There have been intensive efforts in the past into understanding such an issue for various neural networks. For example, it has been shown that every local minimum of the loss function is also a global minimum for shallow linear networks under the autoencoder setting and invertibility assumptions Baldi & Hornik (1989) and for deep linear networks Kawaguchi (2016); Lu & Kawaguchi (2017); Yun et al. (2017) respectively under different assumptions. The conditions on the equivalence between local minimum or critical point and global minimum has also been established for various nonlinear neural networks Yu & Chen (1995); Gori & Tesi (1992); Nguyen & Hein (2017); Soudry & Carmon (2016); Feizi et al. (2017) under respective assumptions.
However, most previous studies did not provide characterization of analytical forms for critical points of loss functions for neural networks with only very few exceptions. In Baldi & Hornik (1989), the authors provided an analytical form for the critical points of the square loss function of shallow linear networks under certain conditions. Such an analytical form further helps to establish the landscape properties around the critical points.
The focus of this paper is on characterizing the analytical forms of critical points for much broader neural network scenarios, i.e., shallow and deep linear networks with no assumptions on data matrices and network dimensions, and shallow nonlinear networks over certain parameter space. In particular, such analytical forms of critical points capture the corresponding loss function values
1

Under review as a conference paper at ICLR 2018
and the necessary and sufficient conditions to achieve global minimum. This further enables us to establish new landscape properties around these critical points for the loss function of neural networks under general settings, and provides alternative (yet simpler and more intuitive) proofs for existing understanding of the landscape properties of neural networks.
OUR CONTRIBUTION
We summarize our contributions in detail as follows.
1) For the square loss function of linear networks with one hidden layer, we provide a full (necessary and sufficient) characterization of the analytical forms for its critical points and global minimizers. These results generalize the characterization in Baldi & Hornik (1989) to arbitrary network parameter dimensions and any data matrices. Such a generalization further enables us to establish the landscape property, i.e., every local minimum is also a global minimum and all other critical points are saddle points, under no assumptions on parameter dimensions and data matrices. From a technical standpoint, we exploit the analytical forms of critical points to provide a new proof for characterizing the landscape around the critical points under full relaxation of assumptions, where the corresponding approaches in Baldi & Hornik (1989) are not applicable. As a special case of linear networks, the matrix factorization problem satisfies all these landscape properties.
2) For the square loss function of deep linear networks, we establish a full (necessary and sufficient) characterization of the analytical forms for its critical points and global minimizers. Such characterizations are new and have not been established in the existing art. Furthermore, such analytical form divides the set of non-global-minimum critical points into different categories. We identify the directions along which the loss function value decreases for two categories of the critical points, for which our result directly implies the equivalence between the local minimum and the global minimum. For these cases, our proof generalizes the result in Kawaguchi (2016) under no assumptions on the network parameter dimensions and data matrices.
3) For the square loss function of one-hidden-layer nonlinear neural networks with ReLU activation function, we provide a full characterization of both the existence and the analytical forms of the critical points in certain types of regions in the parameter space. Particularly, in the case where there is one hidden unit, our results fully characterize the existence and the analytical forms of the critical points in the entire parameter space. Such characterization were not provided in previous work on nonlinear neural networks. Moreover, we apply our results to a concrete example to demonstrate that both local minimum that is not a global minimum and local maximum do exist in such a case.
RELATED WORK
Analytical forms of critical points: Characterizing the analytical form of critical points for loss functions of neural networks dates back to Baldi & Hornik (1989), where the authors provided an analytical form of the critical points for the square loss function of linear networks with one hidden layer.
Properties of critical points: Baldi & Hornik (1989); Baldi (1989) studied the linear autoencoder with one hidden layer and showed the equivalence between the local minimum and the global minimum. Moreover, Baldi & Lu (2012) generalized these results to the complex-valued autoencoder setting. The deep linear networks were studied by some recent work Kawaguchi (2016); Lu & Kawaguchi (2017); Yun et al. (2017), in which the equivalence between the local minimum and the global minimum was established respectively under different assumptions. Particularly, Yun et al. (2017) established a necessary and sufficient condition for a critical point of the deep linear network to be a global minimum. A similar result was established in Freeman & Bruna (2017) for deep linear networks under the setting that the widths of intermediate layers are larger than those of the input and output layers. The effect of regularization on the critical points for a two-layer linear network was studied in Taghvaei et al. (2017).
For nonlinear neural networks, Yu & Chen (1995) studied a nonlinear neural network with one hidden layer and sigmoid activation function, and showed that every local minimum is also a global minimum provided that the number of input units equals the number of data samples. Gori & Tesi (1992) considered a class of multi-layer nonlinear networks with a pyramidal structure, and showed that all critical points of full column rank achieve the zero loss when the sample size is less than the
2

Under review as a conference paper at ICLR 2018

input dimension. These results were further generalized to a larger class of nonlinear networks in Nguyen & Hein (2017), in which they also showed that critical points with non-degenerate Hessian are global minimum. Choromanska et al. (2015a;b) connected the loss surface of deep nonlinear networks with the Hamiltonian of the spin-glass model under certain assumptions and characterized the distribution of the local minimum. Kawaguchi (2016) further eliminated some of the assumptions in Choromanska et al. (2015a), and established the equivalence between the local minimum and the global minimum by reducing the loss function of the deep nonlinear network to that of the deep linear network. Soudry & Carmon (2016) showed that a two-layer nonlinear network has no bad differentiable local minimum. Feizi et al. (2017) studied a one-hidden-layer nonlinear neural network with the parameters restricted in a set of directions of lines, and showed that most local minima are global minima. Tian (2017) considered a two-layer ReLU network with Gaussian input data, and showed that critical points in certain region are non-isolated and characterized the critical-point-free regions.
Geometric curvature: Hardt & Ma (2017) established the gradient dominance condition of deep linear residual networks, and Zhou & Liang (2017) further established the gradient dominance condition and regularity condition around the global minimizers for deep linear, deep linear residual and shallow nonlinear networks. Li et al. (2016) studied the property of the Hessian matrix for deep linear residual networks. The local strong convexity property was established in Soltanolkotabi et al. (2017) for overparameterized nonlinear networks with one hidden layer and quadratic activation functions, and was established in Zhong et al. (2017) for a class of nonlinear networks with one hidden layer and Gaussian input data. Zhong et al. (2017) further established the local linear convergence of gradient descent method with tensor initialization. Soudry & Hoffer (2017) studied a one-hidden-layer nonlinear network with a single output, and showed that the volume of sub-optimal differentiable local minima is exponentially vanishing in comparison with the volume of global minima. Dauphin et al. (2014) investigated the saddle points in deep neural networks using the results from statistical physics and random matrix theory.
Notation: The pseudoinverse, column space and null space of a matrix M are denoted by M , col(M ) and ker(M ), respectively. For any index sets I, J  N, MI,J denotes the submatrix of M formed by the entries with the row indices in I and the column indices in J. For positive integers i  j, we define i : j = {i, i + 1, . . . , j - 1, j}. The projection operator onto a linear subspace V is denoted by PV .

2 LINEAR NEURAL NETWORKS WITH ONE HIDDEN LAYER

In this section, we study linear neural networks with one hidden layer. Suppose we have an input data matrix X  Rd0×m and a corresponding output data matrix Y  Rd2×m, where there are in total m data samples. We are interested in learning a model that maps from X to Y via a linear
network with one hidden layer. Specifically, we denote the weight parameters between the output layer and the hidden layer of the network as A2  Rd2×d1 , and denote the weight parameters between the hidden layer and the input layer of the network as A1  Rd1×d0 . We are interested in the square loss function of this linear network, which is given by

L

:=

1 2

A2A1X - Y

2 F

.

Note that in a special case where X = I, L reduces to a loss function for the matrix factorization problem, to which all our results apply. The loss function L has been studied in Baldi & Hornik (1989) under the assumptions that d2 = d0  d1 and the matrices XX , Y X (XX )-1XY are invertible. In our study, no assumption is made on either the parameter dimensions or the invertibility
of the data matrices. Such full generalization of the results in Baldi & Hornik (1989) turns out to be
critical for our study of nonlinear shallow neural networks in Section 4.

We further define  := Y XXY and denote its full singular value decomposition as U U . Suppose that  has r distinct positive singular values 1 > · · · > r > 0 with multiplicities m1, . . . , mr, respectively, and has m¯ zero singular values. Recall that (A1, A2) is defined to be a critical point of L if A1 L = 0, A2 L = 0. Our first result provides a full characterization of all critical points of L.
Theorem 1 (Characterization of critical points). All critical points of L are necessarily and sufficiently characterized by a matrix L1  Rd1×d0 , a block matrix V  Rd2×d1 and an invertible

3

Under review as a conference paper at ICLR 2018

matrix C  Rd1×d1 via
A1 = C-1V U Y X + L1 - C-1V V CL1XX A2 = U V C.

(1) (2)

Specifically, V = [diag(V1, . . . , Vr, V ), 0d2×(d1-rank(A2))], where both Vi  Rmi×pi and V 

Rm¯ ×p¯ consist of orthonormal columns with pi  mi, i = 1, . . . , r, p¯  m¯ such that

r i=1

pi

+

p¯

=

rank(A2), and L1, V , C satisfy

Pcol(UV ) Y X L1C Pker(V ) = 0.

(3)

Theorem 1 characterizes the necessary and sufficient forms for all critical points of L. The analytical

forms in eqs. (1) and (2) allow one to construct a critical point of L by specifying a choice of

L1, V , C that fulfill the condition in eq. (3). For example, choosing L1 = 0 guarantees eq. (3), in which case eqs. (1) and (2) yield a critical point (C-1V U Y X, U V C) for any invertible

matrix C and any block matrix V that takes the form specified in Theorem 1. Intuitively, the matrix

C captures the invariance of the product A2A1 under an invertible transform, and L1 captures the degree of freedom of the solution set for linear systems. Thus, the block pattern parameters pi, i = 1, . . . , r, p¯ of V contain all useful information of the critical points that determine the function value

of L as presented in the following proposition.

Proposition 1.

Any critical point (A1, A2) of L satisfies L(A1, A2)

=

1 2

(Tr(Y

Y

)-

r i=1

pi

i

).

Proposition 1 evaluates the function value L at a critical point using the parameters {pi}ri=1. To explain further, recall that the data matrix  has each singular value i with multiplicity mi. For
each i, the critical point captures pi out of mi singular values i. Hence, for a i with larger value
(i.e., a smaller index i), it is desirable that a critical point captures a larger number pi of them.
In this way, the critical point captures more important principle components of the data so that
the value of the loss function is further reduced as suggested by Proposition 1. In summary, the parameters {pi}ir=1 characterize how well the learned model fits the data in terms of the value of the loss function. Moreover, the parameters {pi}ir=1 also determine a full characterization of the global minimizers as given below.

Proposition 2 (Characterization of global minimizers). A critical point (A1, A2) of L is a global minimizer if and only if it falls into the following two cases.

1. Case 1: min{d2, d1} 

r i=1

mi,

A2

is

full

rank,

and

p1

=

m1, . . . , pk-1

=

mk-1, pk

=

2.

rCbaeansnkeo(n2A-:f2um)ll-inra{ndk2ki,=w-d1i11th}mr>iankm(Air=k21f)omr=is,opmiire==k1

 r; mi for mi.

i

=

1, . . . , r, and p¯ 

0.

In particular, A2

can

The analytical form of any global minimizer can be obtained from Theorem 1 with further specification to the above two cases.

Proposition 2 establishes the neccessary and sufficient conditions for any critical point to be a global
minimizer. If the data matrix  has a large number of nonzero singular values, i.e., the first case,
one needs to exhaust the representation budget (i.e., rank) of A2 and capture as many large singular values as the rank allows to achieve the global minimum; Otherwise, A2 of a global minimizer can be non-full rank and still captures all nonzero singular values. Furthermore, the parameters {pi}ir=1 naturally divide all non-global-minimum critical points (A1, A2) of L into the following two categories.

· (Non-optimal order): The matrix V specified in Theorem 1 satisfies that there exists 1  i < j 

r such that pi < mi and pj > 0.

· (Optimal order): rank(A2) < min{d2, d1} and the matrix V specified in Theorem 1 satisfies

that p1 = m1, . . . , pk-1 = mk-1, pk = rank(A2) -

k-1 i=1

mi



mk

for

some

1



k



r.

To understand the above two categories, note that a critical point of L with non-optimal order cap-
tures a smaller singular value j (since pj > 0) while skipping a larger singular value i with a lower index i < j (since pi < mi), and hence cannot be a global minimizer. On the other hand, although a critical point of L with optimal order captures the singular values in the optimal (i.e.,
decreasing) order, it does not fully utilize the representation budget of A2 (because A2 is non-full

4

Under review as a conference paper at ICLR 2018

rank) to further capture nonzero singular values and reduce the function value, and hence cannot be a global minimizer either. Next, we show that these two types of non-global-minimum critical points
have different landscape properties around them. Throughout, a matrix M is called the perturbation of M if it lies in an arbitrarily small neighborhood of M .
Proposition 3 (Landscape around critical points). The critical points of L have the following landscape properties.
1. A non-optimal-order critical point (A1, A2) has a perturbation (A1, A2) with rank(A2) = rank(A)2, which achieves a lower function value;
2. An optimal-order critical point (A1, A2) has a perturbation (A1, A2) with rank(A2) = rank(A)2 + 1, which achieves a lower function value;
3. Any point in X := {(A1, A2) : A2A1X = 0} has a perturbation (A1, A2), which achieves a higher function value;
As a consequence, items 1 and 2 imply that any non-global-minimum critical point has a descent direction, and hence cannot be a local minimizer. Thus, any local minimizer must be a global minimizer. Item 3 implies that any point has an ascent direction whenever the output is nonzero. Hence, there does not exist any local/global maximizer in X . Furthermore, item 3 together with items 1 and 2 implies that any non-global-minimum critical point in X has both descent and ascent directions, and hence must be a saddle point. We summarize these facts in the following theorem.
Theorem 2 (Landscape of L). The loss function L satisfies: 1) every local minimum is also a global minimum; 2) every non-global-minimum critical point in X is a saddle point.
From a technical point of view, the proof of item 1 of Proposition 3 applies that in Baldi (1989) and generalizes it to the setting where  can have repeated singular values and may not be invertible. To further understand the perturbation scheme from a high level perspective, note that non-optimalorder critical points capture a smaller singular value j instead of a larger one i with i < j. Thus, one naturally perturbs the singular vector corresponding to j along the direction of the singular vector corresponding to i. Such a perturbation scheme preserves the rank of A2 and reduces the value of the loss function.
More importantly, the proof of item 2 of Proposition 3 introduces a new technique. As a comparison, Baldi & Hornik (1989) proves a similar result as item 2 using the strict convexity of the function, which requires the parameter dimensions to satisfy d2 = d0  d1 and the data matrices to be invertible. In contrast, our proof completely removes these restrictions by introducing a new perturbation direction and exploiting the analytical forms of critical points in eqs. (1) and (2) and the condition in eq. (3). The accomplishment of the proof further requires careful choices of perturbation parameters as well as judicious manipulations of matrices. We refer the reader to the supplemental materials for more details. As a high level understanding, since optimal-order critical points capture the singular values in an optimal (i.e., decreasing) order, the previous perturbation scheme for non-optimal-order critical points does not apply. Instead, we increase the rank of A2 by one in a way that the perturbed matrix captures the next singular value beyond the ones that have already been captured so that the value of the loss function can be further reduced.

3 DEEP LINEAR NEURAL NETWORKS

In this section, we study deep linear networks with  2 layers. We denote the weight parameters between the layers as Ak  Rdk×dk-1 for k = 1, . . . , , respectively. The input and output data are denoted by X  Rd0×m, Y  Rd ×m, respectively. We are interested in the square loss function

of deep linear networks, which is given by

LD

:=

1 2

A · · · A2A1X - Y

2 F

.

Denote k := Y (A(k,1)X)A(k,1)XY for k = 0, . . . , with the full singular value decom-

position UkkUk. Suppose that k has r(k) distinct positive singular values 1(k) > · · · > r(k)(k) > 0 with multiplicities m1(k), . . . , mr(k)(k), respectively, and m¯ (k) zero singular values. Our first result provides a full characterization of all critical points of LD, where we denote A(i,j) := AiAi-1 · · · Aj+1Aj for notational convenience1.

1Here, A(0,1) should be understood as identity matrix I.

5

Under review as a conference paper at ICLR 2018

Theorem 3 (Characterization of critical points). All critical points of LD are necessarily and sufficiently characterized by matrices Lk  Rdk×dk-1 , block matrices Vk  Rdl×dk+1 and invertible matrices Ck  Rdk+1×dk+1 for k = 0, . . . , -2 such that A1, . . . , A can be individually expressed out recursively via the following two equations:

Ak+1 = Ck-1Vk UkY (A(k,1)X) + Lk+1 - Ck-1Vk VkCkLk+1A(k,1)X(A(k,1)X), (4)

A( ,k+2) = UkVkCk.

(5)

Specifically, Vk = [diag(V1(k), . . . , Vr((kk)), V (k)), 0dl×(dk+1-rank(A( ],,k+2))) where Vi(k) 

Rmi(k)×pi(k), V (k)  Rm¯ (k)×p¯(k) consist of orthonormal columns with pi(k)  mi(k) for

i = 1, . . . , r(k), p¯(k)  m¯ (k) such that

r(k) i=1

pi

(k)

+

p¯(k)

=

rank(A

,k+1),

and

Lk, Vk, Ck

satisfy for k = 2, . . . , - 1

A( ,k) = A( ,k+1)Ak, (I - Pcol(A( )Y,k+1)) X A( -1,1) = 0.

(6)

Note that the forms of the individual parameters A1, . . . , A can be obtained as follows by recursively applying eqs. (4) and (5). First, eq. (5) with k = 0 yields the form of A( ,2). Then, eq. (4) with k = 0 and the form of A( ,2) yield the form of A1. Next, eq. (5) with k = 1 yields the form of A( ,3), and then, eq. (4) with k = 1 and the forms of A( ,3), A1 further yield the form of A2. Inductively, one obtains the expressions of all individual parameter matrices. Furthermore, the first
condition in eq. (6) is a consistency condition that guarantees that the analytical form for the entire
product of parameter matrices factorizes into the forms of individual parameter matrices.

Similarly to shallow linear networks, the parameters {pi(0)}ir=(01), p¯(0) determine the value of the loss function at the critical points and further specify the analytical form for the global minimizers,
as we present in the following two propositions.

Proposition 4. Any critical point (A1, . . . , A ) of LD satisfies

LD(A1, . . . , A

)

=

1 2

Tr(Y Y

)-

r(0) i=1

pi(0)i

(0)

.

Proposition 5 (Characterization of global minimizers). A critical point (A1, . . . , A ) of LD is a global minimizer if and only if it falls into the following two cases.

1. Case 1: min{d , . . . , d1} 

r(0) i=1

mi(0),

A( ,2)

achieves

the

maximal

rank,

and

p1(0)

=

m1(0), . . . , pk-1(0) = mk-1(0), pk(0) = rank(A( ,2)) -

k-1 i=1

mi(0)



mk (0)

for

some

k  r(0);

2. Case 2: min{d , . . . , d1} >

r(0) i=1

mi(0),

pi(0)

=

mi(0)

for

all

i

=

1,

.

.

.

,

r(0)

and

p¯(0)



0.

In particular, A( ,2) can be non-full rank with rank(A( ,2)) =

r(0) i=1

mi

(0).

The analytical form of any global minimizer can be obtained from Theorem 3 with further specification to the above two cases.

We next exploit the analytical forms of the critical points to further understand the landscape of the loss function LD. It has been shown in Kawaguchi (2016) that every local minimum of LD is also a global minimum, under certain conditions on the parameter dimensions and the invertibility of the data matrices. Here, our characterization of the analytical forms for the critical points allow us to understand such a result from an alternative viewpoint. The proofs for certain cases (that we discuss below) are simpler and more intuitive, and no assumption is made on the data matrices and dimensions of the network.

Similarly to shallow linear networks, we want to understand the local landscape around the critical
points. However, due to the effect of depth, the critical points of LD are more complicated than those of L. Among them, we identify the following subsets of the non-global-minimum critical
points (A1, · · · , A ) of LD.

· (Deep-non-optimal order): There exist 0  k  - 2 such that the matrix Vk specified in

Theorem 3 satisfies that there exist 1  i < j  r(k) such that pi(k) < mi(k) and pj(k) > 0.

· (Deep-optimal order): (A , A -1) is not a global minimizer of LD with A( -2,1) being fixed,

rank(A ) < min{d , d -1}, and the matrix V -2 specified in Theorem 3 satisfies that p1(l-2) =

m1(l -2), . . . , pk-1(l -2) = mk-1(l -2), pk(l -2) = rank(Al)-

k-1 i=1

mi(l

- 2)



mk (l

-

2)

for some 1  k  r(l - 2).

6

Under review as a conference paper at ICLR 2018

The following result summarizes the landscape of LD around the above two types of critical points.
Theorem 4 (Landscape of LD). The loss function LD has the following landscape properties.
1. A deep-non-optimal-order critical point (A1, . . . , A ) has a perturbation (A1, . . . , Ak+1, . . . , A ) with rank(A ) = rank(A ), which achieves a lower function value.
2. A deep-optimal-order critical point (A1, . . . , A ) has a perturbation (A1, . . . , A -1, A ) with rank(A ) = rank(A ) + 1, which achieves a lower function value.
3. Any point in XD := {(A1, . . . , A ) : A( ,1)X = 0} has a perturbation (A1, . . . , A ) that achieves a higher function value.
Consequently, 1) every local minimum of LD is also a global minimum for the above two types of critical points; and 2) every critical point of these two types in XD is a saddle point.
Theorem 4 implies that the landscape of LD for deep linear networks is similar to that of L for shallow linear networks, i.e., the pattern of the parameters {pi(k)}ir=(k1) implies different descent directions of the function value around the critical points. Our approach does not handle the remaining set of non-global minimizers, i.e., there exists q  -1 such that (A , . . . , Aq) is a global minimum point of LD with A(q-1,1) being fixed, and A( ,q) is of optimal order. It is unclear how to perturb the intermediate weight parameters using their analytical forms for deep networks , and we leave this as an open problem for the future work.

4 NONLINEAR NEURAL NETWORKS WITH ONE HIDDEN LAYER

In this section, we study nonlinear neural networks with one hidden layer. In particular, we consider nonlinear networks with ReLU activation function  : R  R that is defined as (x) := max{x, 0}. Our study focuses on the set of differentiable critical points. The weight parameters between the layers are denoted by A2  Rd2×d1 , A1  Rd1×d0 , respectively, and the input and output data are denoted by X  Rd0×m, Y  Rd2×m, respectively. We are interested in the square loss function which is given by

LN

:=

1 2

A2(A1X) - Y

2 F

,

(7)

where  acts on A1X entrywise. Existing studies on nonlinear networks characterized the sufficient conditions for critical points being global minimum Gori & Tesi (1992); Nguyen & Hein (2017),

established the equivalence between local minimum and global minimum under the condition that

d0 = m Yu & Chen (1995), and provided understanding of geometric properties of the critical points Tian (2017). In comparison, our results below provide a full characterization of the critical points of

LN with d1 = 1 and critical points of LN over certain parameter space with d1 > 1, and show that local minimum of LN that is not global minimum can exist.

Since the activation function  is piecewise linear, the entire parameter space can be partitioned
into disjoint cones. In particular, we consider the set of cones KI×J where I  {1, . . . , d1}, J  {1, . . . , m} that satisfy

KI×J := {(A2, A1) : (A1)I,:X:,J  0, other entries of A1X < 0},

(8)

where "" and "<" represent entrywise comparisons. Within KI×J , the term (A1X) activates only the entries (A1X)I:J , and the corresponding loss function LN is equivalent to

(A2, A1)  KI×J ,

LN

:=

1 2

(A2):,I (A1)I,:X:,J - Y:,J

2 F

+

1 2

Y:,J c

2 F

.

(9)

Hence, within KI×J , LN reduces to the loss of a shallow linear network with parameters

((A2):,I , (A1)I,:) and input & output data pair (X:,J , Y:,J ). Note that our results on shallow

linear networks in Section 2 are applicable to all parameter dimensions and data matrices. Thus,

Theorem 1 fully characterizes the forms of critical points of LN in KI×J . Moreover, the exis-

tence of such critical points can be analytically examined by substituting their forms into eq. (8). In

summary, we obtain the following result, where we denote J := Y:,J X:,J X:,J Y:,J with the full

singular value decomposition UJ J UJ, and suppose that J has r(J) distinct positive singular values 1(J) > · · · > r(J)(J) with multiplicities m1, . . . , mr(J), respectively, and m¯ (J) zero

singular values.

7

Under review as a conference paper at ICLR 2018

Proposition 6 (Characterization of critical points). All critical points of LN in KI×J for any I  {1, . . . , d1}, J  {1, . . . , m} are necessarily and sufficiently characterized by an L1  R|I|×d0 , a block matrix V  Rd2×|I| and an invertible matrix C  R|I|×|I| such that

(A1)I,: = C-1V UJY:,J X:,J + L1 - C-1V V CL1X:,J X:,J , (A2):,I = UJ V C.

(10) (11)

Specifically, V = [diag(V1, . . . , Vr(J), V ), 0d2×(|I|-rank((A2):,I ))], where Vi  Rmi×pi , V 

Rm¯ ×p¯ consist of orthonormal columns with pi  mi for i = 1, . . . , r(J), p¯  m¯ such that

r(J ) i=1

pi

+

p¯

=

rank((A2):,I ),

and

L1,

V

,

C

satisfy

Pcol(UJ V ) Y:,J X:,JL1C Pker(V ) = 0. Moreover, a critical point in KI×J exists if and only if there exists such C, V , L1 that

(12)

(A1)I,:X:,J = C-1V UJY X:,J X:,J + C-1(I - Pker(V ))CL1X:,J  0, Other entries of A1X < 0.

(13) (14)

To further illustrate, we consider a special case where the nonlinear network has one unit in the
hidden layer, i.e., d1 = 1, in which case A1 and A2 are row and column vectors, respectively. Then, the entire parameter space can be partitioned into disjoint cones taking the form of KI×J , and I = {1} is the only nontrivial choice. We obtain the following result from Proposition 6.

Proposition 7 (Characterization of critical points). Consider LN with d1 = 1 and any J  {1, . . . , m}. Then, any nonzero critical point of LN within K{1}×J can be necessarily and suffi-
ciently characterized by an 1  R1×d0 , a block unit vector v  Rd2×1 and a scalar c  R such that

A1 = c-1v UJY:,J X:,J + 1 - 1X:,J X:,J , A2 = cUJ v.

(15)

Specifically, v is a unit vector that is supported on the entries corresponding to the same singular value of J . Moreover, a nonzero critical point in K{1}×J exists if and only if there exist such
c, v, 1 that satisfy

A1X:,J = c-1v UJY:,J X:,J X:,J  0, A1X:,Jc = c-1v UJY:,J X:,J X:,Jc + 1X:,Jc - 1X:,J X:,J X:,Jc < 0.

(16) (17)

We note that Proposition 7 characterizes both the existence and the forms of critical points of LN over the entire parameter space for nonlinear networks with a single hidden unit. The condition in eq. (12) is guaranteed because Pker(v) = 0 for v = 0.

To further understand Proposition 7, suppose that there exists a critical point in K{1}×J with v

being supported on the entries that correspond to the i-th singular value of J . Then, Proposition 1

implies

that

LN

=

1 2

Tr(Y

Y

)

-

1 2

i(J

).

In

particular,

the

critical

point

achieves

the

local

minimum

1 2

Tr(Y

Y

)

-

1 2

1(J

)

in

K{1}×J

with

i

=

1.

This is because in this case the critical point is full

rank with an optimal order, and hence corresponds to the global minimum of the linear network in

eq. (9). Since the singular values of J may vary with the choice of J, LN may achieve different

local minima in different cones. Thus, local minimum that is not global minimum can exist for LN .

The following proposition concludes this fact by considering a concrete example.

Proposition 8. For one-hidden-layer nonlinear neural networks with ReLU activation function, there exists local minimum that is not global minimum, and there also exists local maximum.

The above proposition is demonstrated by the following example.
Example 1. Consider the loss function LN of the nonlinear network with d2 = d0 = 2, and d1 = 1. The input and output data are set to be X = diag(1, 1), Y = diag(2, 1).

First, consider the cone KI×J with I = {1}, J = {1}. Calculation yields that J = diag(4, 0), and the conditions for existence of critical points in eqs. (16) and (17) hold if 2c-1(v)1,:  0, ( 1)2,: < 0. Then choosing c = 1, v = (1, 0) , 1 = (1, -1) yields a local minimum in KI×J , because

8

Under review as a conference paper at ICLR 2018

the nonzero entry in v corresponds to the largest singular value of J . Then, calculation shows

that the local minimum achieves LN

=

1 2

.

On the other hand, consider the cone KI×J

with

I = {1}, J = {2}, in which J = diag(0, 1). The conditions for existence of critical points

in eqs. (16) and (17) hold if c-1(v)1,:  0, ( 1)1,: < 0. Similarly to the previous case, choosing

c = 1, v = (1, 0) , 1 = (-1, 0) yields a local minimum that achieves the function value Ln = 2.

Hence, local minimum that is not global minimum does exist. Moreover, in the cone KI×J with

I = {1}, J

=

,

the

function

LN

remains

to

be

the

constant

5 2

,

and

all

points

in

this

cone

are

local

minimum or local maximum. Thus, the landscape of the loss function of nonlinear networks is very

different from that of the loss function of linear networks.

CONCLUSION
In this paper, we provide full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks, namely, shallow linear networks, deep linear networks, and shallow ReLU nonlinear networks. We show that such analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points. As a consequence, the loss function for linear networks has no spurious local minimum, while such point does exist for nonlinear networks with ReLU activation. In the future, it is interesting to further explore nonlinear neural networks. In particular, we wish to characterize the analytical form of critical points for deep nonlinear networks and over the full parameter space. Such results will further facilitate the understanding of the landscape properties around these critical points.

REFERENCES
P. Baldi. Linear learning: Landscapes and algorithms. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 65­72. 1989.
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2(1):53 ­ 58, 1989.
P. Baldi and Z. Lu. Complex-valued autoencoders. Neural Networks, 33:136­147, September 2012.
A. Choromanska, M. Henaff, M. Mathieu, G. Arous, and Y. LeCun. The loss surfaces of multilayer networks. Journal of Machine Learning Research, 38:192­204, 2015a. ISSN 1532-4435.
A. Choromanska, Y. LeCun, and G. Arous. Open problem: The landscape of the loss surfaces of multilayer networks. In Proc. Conference on Learning Theory (COLT), volume 40, pp. 1756­ 1760, Jul 2015b.
Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Proc. Advances in Neural Information Processing Systems (NIPS). 2014.
S. Feizi, H. Javadi, J. Zhang, and D. Tse. Porcupine neural networks: (almost) all local optima are global. ArXiv: 1710.02196, October 2017.
C. D. Freeman and J. Bruna. Topology and geometry of half-rectified network optimization. Proc. International Conference on Learning Representations (ICLR), 2017.
I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016. http://www. deeplearningbook.org.
M. Gori and A. Tesi. On the problem of local minima in backpropagation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(1):76­86, Jan 1992.
M. Hardt and T. Ma. Identity matters in deep learning. Proc. International Conference on Learning Representations (ICLR), 2017.
K. Kawaguchi. Deep learning without poor local minima. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 586­594. 2016.

9

Under review as a conference paper at ICLR 2018
S. Li, J. Jiao, Y. Han, and T. Weissman. Demystifying resnet. Arxiv: 1611.01186, 2016. URL https://arxiv.org/pdf/1611.01186.
H. T. Lu and K. Kawaguchi. Depth creates no bad local minima. ArXiv: 1702.08580, February 2017.
Q. Nguyen and M. Hein. The loss surface of deep and wide neural networks. In Proc. International Conference on Machine Learning (ICML), volume 70, pp. 2603­2612, Aug 2017.
M. Soltanolkotabi, A. Javanmard, and J. D. Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. ArXiv:1707.04926, 2017.
D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. ArXiv: 1605.08361, May 2016.
D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks. ArXiv:1702.05777, February 2017.
A. Taghvaei, J. W. Kim, and P. Mehta. How regularization affects the critical points in linear networks. In Proc. International Conference on Neural Information Processing Systems (NIPS), 2017.
Y. Tian. An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis. In Proc. International Conference on Machine Learning (ICML), volume 70, pp. 3404­3413, 06­11 Aug 2017.
X. H. Yu and G. A. Chen. On the local minima free condition of backpropagation learning. IEEE Transactions on Neural Networks, 6(5):1300­1303, Sep 1995.
C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. ArXiv: 1707.02444, 2017.
K. Zhong, Z. Song, P. Jain, P. L. Bartlett, and I. S. Dhillon. Recovery guarantees for one-hidden-layer neural networks. In Proc. International Conference on Machine Learning (ICML), volume 70, pp. 4140­4149, Aug 2017.
Y. Zhou and Y. Liang. Characterization of gradient dominance and regularity conditions for neural networks. ArXiv: 1710.06910v2, October 2017.
10

Under review as a conference paper at ICLR 2018

Supplementary Materials

PROOF OF THEOREM 1

Notations: For any matrix M , denote vec(M ) as the column vector formed by stacking its columns. Denote the Kronecker product as "". Then, the following useful relationships hold for any dimension compatible matrices M , U , V , W :

vec(U M V ) = (V U )vec(M ) , (U V ) = U  V , (M W )(U V ) = (M U )(W V ), (M M )M = M (M M ) = M , MMM = M, MMM = M.

(18) (19) (20) (21) (22)

Recall that a point (A1, A2) is a critical point of L if it satisfies

A1 L = A2(A2A1X - Y )X = 0, A2 L = (A2A1X - Y )X A1 = 0.

(23) (24)

We first prove eqs. (1) and (2). Lemma 1. Let (A2, A1) be a critical point of L. Then it must satisfy, for some L1  Rd1×d0 , that

A1 = A2Y X + L1 - A2A2L1XX, Pcol(A2) Pcol(A2) =  Pcol(A2) = Pcol(A2).

(25) (26)

Proof of Lemma 1. Since (A2, A1) is a critical point of L, eq. (23) implies that

A2A2A1XX = A2Y X .
Applying the vectorizing operator on both sides of the above equation and use the property in eq. (18), we conclude that

(XX A2A2)vec(A1) = (X A2)vec(Y ) .
Since vec(A1) is a solution of the above linear equation, it must take the form of the solution of linear systems, i.e., for some L1  Rd1×d0 , we have
vec(A1) = (XX A2A2)(X A2)vec(Y ) + [I - (XX A2A2)(XX A2A2)]vec(L1) (=i) (XX )X (A2A2)A2 vec(Y ) + [I - (XX )XX (A2A2)A2A2]vec(L1) = vec (A2A2)A2Y X (XX ) + L1 - (A2A2)A2A2L1XX (XX ) (=ii) vec A2Y X + L1 - A2A2L1XX

where (i) uses eqs. (19) and (20) and (ii) uses eq. (21). Then, eq. (25) follows by reshaping the vector into a matrix.
Next we prove eq. (26). Multiplying both sides of eq. (25) by A2 on the left and by X on the right and then using eq. (22), we obtain

A2A1X = A2A2Y XX = Pcol(A2)Y XX.

(27)

Also, multiplying both sides of eq. (24) by A2 on the right yields that A2A1XX A1A2 = Y X A1A2. This equation, together with the above expression of A2A1X, further implies that
Pcol(A2) Pcol(A2) =  Pcol(A2).
Note that Pcol(A2) Pcol(A2) is symmetric. Thus, we conclude that  Pcol(A2) = Pcol(A2).

11

Under review as a conference paper at ICLR 2018

Next, we derive the form of A2. Recall the full singular value decomposition  = U U , where  is a diagonal matrix with distinct singular values 1 > . . . > r > 0 and multiplicities m1, . . . , mr, respectively. We also assume that there are m¯ number of zero singular values in . Using the fact that Pcol(A2) = U Pcol(U A2)U , the last equality in eq. (26) reduces to

 Pcol(U A2) = Pcol(U A2).

By the multiplicity pattern of the singular values in , Pcol(U A2) must be block diagonal. Specifically, we can write Pcol(U A2) = diag( P1, . . . , Pr, P), where Pi  Rmi×mi and P  Rm¯ ×m¯ .

Also, since Pcol(U A2) is a projection, P1, . . . , Pr, P must all be projections. Note that Pcol(U A2)

has rank we must

rank(A2), and suppose that P1, . . . , have pi  mi for i = 1, . . . , r, p¯  m¯

Pr , and

P

rih=a1vepira+nkp¯s=p1r,a.n.k. ,(Apr2,)p¯.,

respectively. Then, Also, note that each

projection can be expressed as Pi = ViVi with Vi  Rmi×pi , V  Rm¯ ×p¯ consisting of orthonor-

mal columns. Hence, we can write Pcol(U A2) = V V where V = diag(V1, . . . , Vr, V ). We then

conclude that Pcol(A2) = U Pcol(U A2)U = U V V U . Thus, A2 has the same column space

as U V , and there must exist an invertible matrix C  Rd1×d1 such that A2 = U [V , 0]C, where

0  Rd2×(d1-rank(A2)) is a zero matrix. Denoting V = [V , 0], we conclude that A2 = U V C. Then, plugging A2 = C-1V U into eq. (25) yields the desired form of A1.

We now prove eq. (3). Note that the above proof is based on the equations A1 L = 0, (A2 L)A2 =

0. Hence, the forms and the form of A2,

of A1, A2 we obtain

in eqs. (1) and that A2A1X

(2) =

need to Pcol(A2 )

further satisfy Y XX = U

A2 L = V (U V )

0. By eq. Y XX.

(27) This

expression, together with the form of A1 in eq. (1), implies that

A2A1XX A1 = U V (U V ) Y XXX A1
(=i) U V (U V ) Y X A1 = U V (U V ) U V (C )-1 + U V (U V ) Y X L1
- U V (U V ) Y X L1C V V (C )-1 = U V V V (C )-1 + U V (U V ) Y X L1(I - C V V (C )-1)
(=ii) U V (C )-1 + U V (U V ) Y X L1(I - C V V (C )-1),

where (i) uses the fact that XXX = X , (ii) uses the fact that the block pattern of V is compatible with the multiplicity pattern of the singular values in , and hence V V V = V . On the other hand, we also obtain that

Y X A1 = U V (C )-1 + Y X L1(I - C V V (C )-1) = U V (C )-1 + Y X L1(I - C V V (C )-1).

Thus, to satisfy A2 L = 0 in eq. (24), we require that

(I - U V (U V ) )Y X L1(I - C V V (C )-1) = 0,

which is equivalent to

(I - U V (U V ) )Y X L1C (I - V V ) = 0.
Lastly, note that (I - U V (U V ) ) = Pcol(UV ) , and (I - V V ) = Pker(V ), which concludes the proof.

PROOF OF PROPOSITION 1

By

expansion

we

obtain

that

L

=

1 2

Tr(Y

Y

) - Tr(A2A1XY

)

+

1 2

Tr(A2 A1 X X

A1A2). Con-

sider any (A1, A2) that satisfies eq. (23), we have shown that such a point also satisfies eq. (27),

12

Under review as a conference paper at ICLR 2018

which further yields that

L

=

1 2

Tr(Y

Y

) - Tr(A2A1XY

)

+

1 2

Tr(A2A1

X

X

A1A2)

=

1 2

Tr(Y

Y

)

-

Tr( Pcol(A2))

+

1 2

Tr(

Pcol(A2

)



Pcol(A2

)

)

(=i)

1 2

Tr(Y

Y

)

-

1 2

Tr(

Pcol(A2

)

)

(=ii)

1 2

Tr(Y

Y

)-

1 2

Tr(

Pcol(U

A2))

(28)

where (i) follows from the fact that Tr( Pcol(A2) Pcol(A2)) = Tr( Pcol(A2)), and (ii) uses the
fact that Pcol(A2) = U Pcol(U A2)U . In particular, a critical point (A1, A2) satisfies eq. (28). Moreover, using the form of the critical point A2 = U V C, eq. (28) further becomes

L

=

1 2

Tr(Y

Y

)

-

1 2

Tr(

Pcol(V

C))

(=i)

1 2

Tr(Y

Y

)

-

1 2

Tr(V

V )

r

(=ii)

1 2

Tr(Y

Y

)

-

1 2

pii,

i=1

where (i) is due to Pcol(V C) = Pcol(V ) = V V , and (ii) utilizes the block pattern of V and the multiplicity pattern of  that are specified in Theorem 1.

PROOF OF PROPOSITION 2

(1): Consider a critical point (A1, A2) with the forms given by Theorem 1. By choosing L1 = 0,

the condition in eq. (3) is guaranteed. Then, we can specify a critical point with any V that satisfies

the block pattern specified in pi  mi for i = 1, . . . , r, p¯
global minimizer, Proposition
condition that min{d2, d1} 

1Thgirmei¯=vo1eraesmnmtdih,a1tt,hLeiri=.(eg1A.l,op1bwi,a+eAl mc2p¯a)inn==imchru21oamTonsrkev((YaAalunY2ey).ip)sSi-a,uciph21pi=eovsee1ird=,t.1hb.ayp.ti,a(rAif,.up¯1lUl,sArunacd2nhe)kr

that is a the A2

with rank(A2) = min{d2, d1} and p1 = m1, . . . , pk-1 = mk-1, pk = rank(A2) -

k-1 i=1

mi



mk for some k  r. That is, the singular values are selected in a decreasing order to minimize the

function value.

(2): If (A2, A1) is a global minimizer and min{dy, d} >

r i=1

mi,

the

global

minimum

can

be

achieved rank A2

by choosing pi = mi for all i = to achieve the global minimum.

1, . For

. . , r and p¯  example, we

0. In particular, we do not can choose rank(A2) =

need a full

r i=1

mi

<

min{dy, d} with pi = mi for all i = 1, . . . , r and p¯ = 0.

PROOF OF PROPOSITION 3

We first prove item 1. Consider a non-optimal-order critical point (A1, A2). By Theorem 1, we
can write A2 = U V C where V = [diag(V1, . . . , Vr, V ), 0] and Vi, i = 1, . . . , r, V consist of orthonormal columns. Define the orthonormal block diagonal matrix

S := diag

V1 O1

,··· ,

Vr Or

,

V O

,

(29)

where the matrices O1, · · · , Or, O are such that each diagonal block forms an orthonormal submatrix. By construction we have S V = [diag(Im1×p1 , . . . , Imr×pr , Im¯ ×p¯), 0], where Imk×pk corresponds to the first pk columns of the identity matrix Imk×mk . Then, A2 can be alternatively written as A2 = U SS V C. Also, denote the columns of U S as

U S = [us11, . . . , us1p1 , . . . , urs1, . . . , usrpr , u¯s1, . . . , u¯ps¯].

Since (A1, A2) is a non-optimal-order critical point, there exists 1  i < j  r such that pi < mi and pj > 0. Then, consider the following perturbation of U S for some > 0.

M=

us11, . . . , us1p1 , . . . ,

ujs1

+ 

usi(pi

1+ 2

+1)

,

.

.

.

u¯ 1s

,

.

.

.

,

u¯ sp¯

,

13

Under review as a conference paper at ICLR 2018

with which we further define the perturbation matrix A2 = M S V C. Also, let the perturbation matrix A1 be generated by eq. (1) with U  M and V  S V . Note that with
this construction, (A1, A2) satisfies eq. (25), which further implies eq. (27) for (A1, A2), i.e., A2A1X = Pcol(A2)Y XX. Thus, eq. (28) holds for the point (A1, A2), and we obtain that

L(A2, A1)

=

1 2

Tr(Y

Y

)-

1 2

Tr(

Pcol(U

A2))

=

1 2

Tr(Y

Y

)-

1 2

Tr(

Pcol(S

U

A2)S

S)

=

1 2

Tr(Y

Y

)-

1 2

Tr(

Pcol(S

U

MS

V )S

S)

=

1 2

Tr(Y

Y

)

-

1 2

Tr(

Pcol(S

U

MS

V )),

where the last equality uses the fact that S S = , as can be observed from the block pattern of
S and the multiplicity pattern of . Also, by the construction of M and the form of S V , a careful calculation shows that only the i, j-th diagonal elements of Pcol(S U MS V ) have changed, i.e.,

Pcol(S U M S V ) k =

2

1+ 2 , if k = i

1 1+

2

,

if k = j

As the index i, j correspond to the singular values i, j, respectively, and i > j, one obtain that

L(A2,

A1)

=

L(A2,

A1)

-

2
1+

2

(i

-

j )

<

L(A2,

A1).

Thus, the construction of the point (A2, A1) achieves a lower function value for any > 0. Letting  0 and noticing that M is a perturbation of U S, the point (A2, A1) can be in an arbitrary
neighborhood of (A2, A1). Lastly, note that rank(A2) = rank(A2). This completes the proof of item 1.

Next, we prove item 2. Consider an optimal-order critical point (A1, A2). Then, A2 must be

non-full rank, since otherwise a full rank A2 with optimal order corresponds to a global minimizer

by Proposition 2. Since there exists some k  r such that p1 = m1, . . . , pk-1 = mk-1, pk =

rank(A2) -

k-1 i=1

mi



mk ,

the necessary form

of A2

gives

that A2

=

UV C

with V

=

[diag(V1, . . . , Vk), 0] := [Vdiag, 0]. Using this expression, eq. (1) yields that

A1 = C-1

(U Vdiag) Y X 0

+ CL1 -

(CL1)1:rank(A2),:X X 0

.

We now specify our perturbation scheme. Recalling the orthonormal matrix S defined in eq. (29). Then, we consider the following matrices for some 1, 2 > 0

k-1

A2 = [U Vdiag, 2U S:,q, 0]C, where q = mi + pk + 1,

i=1

A1 = C-1

(U Vdiag) Y X 0

(C L1 )1:rank(A2 ),: X X   + CL1 -  1(U S:,q) Y X 
0

.

Our goal is to show that L(A1, A2) < L(A1, A2) for 1, 2  0. For this purpose, we need to utilize the condition of critical points in eq. (3), which can be equivalently expressed as

(I - U V (U V ) )Y X L1C (I - V V ) = 0

(i)

0 (C L1 )(rank(A2 )+1):d1 ,: X

Y

(I - U V (U V ) ) = 0

 (CL1)(rank(A2)+1):d1,:XY (I - U V (U V ) ) = 0

(ii) (CL1)(rank(A2)+1):d1,:XY (I - U S:,1:(q-1)(U S:,1:(q-1)) ) = 0

(30) (31)

14

Under review as a conference paper at ICLR 2018

where (i) follows by taking the transpose and then simplifying, and (ii) uses the fact that V = SS V = S:,1:(q-1) in the case of optimal-order critical point. Calculating the function value at
(A1, A2), we obtain that

L(A1, A2)

=

1 2

U Vdiag(U Vdiag) Y XX

Q

+

2U S:,q(CL1)(rank(A2)+1),:X +

1 2U S:,q(U S:,q) Y XX -Y

2 F

P

=

L(A1,

A2)

+

1 2

[Tr(P

P

) + 2Tr(P Q ) - 2Tr(P Y

)].

We next simplify the above three trace terms using eq. (31). For the first trace term, observe that

Tr(P P

) = Tr(

2 2

U

S:,q

(C

L1

)(rank(A2

)+1),:

X

X

(CL1)(rank(A2)+1),:(U S:,q)

)

+ 2Tr(

1

2 2

U

S:,q

(C

L1

)(rank(A2

)+1),:

X

Y

U S:,q(U S:,q) )

+ Tr(

2 1

2 2

U

S:,q

(U

S:,q

)

U S:,q(U S:,q)

)

(=i) Tr(

2 2

U

S:,q

(C

L1

)(rank(A2

)+1),:

X

X

(CL1)(rank(A2)+1),:(U S:,q)

)

+ Tr(

2 1

2 2

U

S:,q

(U

S:,q

)

U S:,q(U S:,q)

)

=

2 2

Tr((C

L1

)(rank(A2

)+1),:

X

X

(CL1)(rank(A2)+1),:) +

2 1

2 2

Tr(S:,qS:,q

)

where (i) follows from eq. (31) as S:,q is orthogonal to the columns of S:,1:(q-1). For the second trace term, we obtain that

2Tr(P Q ) = 2Tr( 2U S:,q(CL1)(rank(A2)+1),:XY U Vdiag(U Vdiag) ) + 2Tr( 1 2U S:,q(U S:,q) U Vdiag(U Vdiag) )
= 2Tr( 2U S:,q(CL1)(rank(A2)+1),:XY U Vdiag(U Vdiag) ) + 2Tr( 1 2U S:,qS:,qSS Vdiag(U Vdiag) )
(=i) 2Tr( 2U S:,q(CL1)(rank(A2)+1),:XY U Vdiag(U Vdiag) ) + 2Tr( 1 2kU S:,qeqS Vdiag(U Vdiag) )
(ii)
= 2Tr( 2U S:,q(CL1)(rank(A2)+1),:XY U Vdiag(U Vdiag) ),

where (i) follows from S:,qS = keq, and (ii) follows from eqS Vdiag = 0. For the third trace term, we obtain that

2Tr(P Y ) = 2Tr( 2U S:,q(CL1)(rank(A2)+1),:XY ) + 2Tr( 1 2U S:,q(U S:,q) ) = 2Tr( 2U S:,q(CL1)(rank(A2)+1),:XY ) + 2Tr( 1 2S:,qS:,q).
Combining the expressions for the three trace terms above, we conclude that

1 2

[Tr(P

P

) + 2Tr(P Q ) - 2Tr(P Y

)]

=

1 2

2 2

Tr((C

L1

)(rank(A2

)+1),:

X

X

(C L1 )(rank(A2 )+1),:)

+

(

1 2

2 1

2 2

-

1 2)Tr(S:,qS:,q)

+ 2 2Tr(U S:,q(CL1)(rank(A2)+1),:XY [U Vdiag(U Vdiag) - I])

(=i)

1 2

2 2

Tr((C

L1

)(rank(A2

)+1),:

X

X

(C L1 )(rank(A2 )+1),:)

+

(

1 2

2 1

2 2

-

1 2)Tr(S:,qS:,q)

=

1 2

2 2

Tr((C

L1

)(rank(A2

)+1),:

X

X

(C

L1)(rank(A2)+1),:)

+

(

1 2

2 1

2 2

-

1 2)k,

where (i) follows from eq. (30). Note that the first term in the last equation is nonnegative. Now,

letting

2=

2 1



0,

the

overall

perturbation

of

the

function

value

becomes

1 2

[Tr(P

P

) + 2Tr(P Q ) - 2Tr(P Y

)] = O( 14) - O( 13) < 0.

15

Under review as a conference paper at ICLR 2018

Thus, the constructed perturbation (A1, A2) achieves a lower function value, and it can be in an

arbitrary neighborhood of (A1, A2) as

2=

2 1



0.

Lastly,

note

that

rank(A2)

=

rank(A2)

+

1.

Next, we prove item 3. We first introduce a technical lemma. Consider row vectors a , y with

a

=

0.

Define the scalar function f ()

=

1 2

a - y

22. Then, f () is strongly convex as

f

() =

a

2 2

> 0.

Thus, the following fact holds due to strong convexity.

Fact 1. For any   R, we can identify a perturbation  such that f () > f ().

Now consider any point (A1, A2)  X . Since A2A1X = 0, then there exists a certain row, say, the i-th row (A2)i,:A1X, that is nonzero. We then apply the above fact with  = 1, a = (A2)i,:A1X, y = Yi,:, and conclude that one can find a perturbation  that achieves a higher function value. Equivalently, one can treat the perturbation of  as the perturbation of (A2)i,:, i.e.,
define the perturbation (A2)i,: := (A2)i,:.

PROOF OF THEOREM 3

We first derive the forms of the parameter matrices. Consider a critical point (A1, . . . , A ) of LD. By definition of the critical point, we have Ak LD = 0 for all k = 1, . . . , , which implies that

A( ,k+1)A( ,k+1)AkA(k-1,1)X(A(k-1,1)X) = A( ,k+1)Y X A(k-1,1).

(32)

Solving this linear system of Ak, we obtain that, for some Lk  Rdk×dk-1 Ak = A( ,k+1)Y (A(k-1,1)X) + Lk - A( ,k+1)A( ,k+1)LkA(k-1,1)X(A(k-1,1)X)

(33)

Multiplying eq. (33) on both sides by A( ,k+1) on the left and A(k-1,1)X on the right and then simplifying, we obtain that for all k = 1, . . . , - 1

A( ,1)X = Pcol(A( Y,k+1)) (A(k-1,1)X )A(k-1,1)X .

(34)

On the other hand, applying eq. (32) with k = and multiplying both sides by A on the right, one obtains that

A( ,1)XX A( ,1) = Y X A( ,1), which, together with eq. (34), further implies that for all k = 1, . . . , - 1

(35)

Pcol(A( ,k+1)) k-1 Pcol(A( ,k+1)) = k-1 Pcol(A( .,k+1))

(36)

Following the same argument as that in the proof of Theorem 1, we conclude that A( ,k+1) = Uk-1Vk-1Ck-1, where Uk-1, Vk-1, Ck-1 satisfy the conditions that are stated in the theorem. Then, plugging the expression of A( ,k+1) in eq. (33), one obtains the form of Ak in eq. (4).

We next prove the conditions in eq. (6). Note that the first condition is simply a consistency con-
dition on the matrix products. This is because eq. (36) only provides the forms of the matrices A( ,k+1), k = 1, . . . , l - 1, which must factorize into the product of individual matrices. For the
other condition in eq. (6), note that the proof of eq. (35) uses the weaker condition (A LD)A = 0 than the original condition A LD = 0 of the critical point. Thus, the forms of the parameter matrices must also satisfy A LD = 0, i.e., A( ,1)X(A( -1,1)X) = Y X A( -1,1). Then, plugging eq. (34) in the above condition and simplifying, one obtains eq. (6).

PROOF OF PROPOSITION 4

Note that by expansion LD =

1 2

Tr(Y

Y

) - Tr(A( ,1)XY

)

+

1 2

Tr(A(

,1) X X

A(

,1)).

For

any

A1, . . . , A that satisfy eq. (32), we have shown that they must satisfy eq. (34) with k = 1, with

which we further obtain that

L

=

1 2

Tr(Y

Y

)

-

1 2

Tr(

Pcol(A(

,2) ) 0 )

=

1 2

Tr(Y

Y

)

-

1 2

Tr(

Pcol(U0

A(

,2))0).

(37)

16

Under review as a conference paper at ICLR 2018

Consider a critical point (A1, . . . , A ) so that eq. (37) holds. Using the form of critical points A( ,2) = U0V0C0, eq. (37) further becomes

L

=

1 2

Tr(Y

Y

)

-

1 2

Tr(

Pcol(V0

C0

)

0

)

=

1 2

Tr(Y

Y

)

-

1 2

Tr(V0

0V0)

r(0)

(=i)

1 2

Tr(Y

Y

)

-

1 2

pi(0)i(0),

i=1

where (i) utilizes the block pattern of V0 and the multiplicity pattern of 0 that are specified in Theorem 3.

PROOF OF PROPOSITION 5

Observe that the product matrix A( ,2) is equivalent to the class of matrices B2  Rmin{d ,...,d2}×d1 .

Consider a critical point (B2, A1) of the shallow linear network L

:=

1 2

B2A1X - Y

2 F

.

By

Proposition 2, we conclude that

· If min{d , . . . , d1} 

r(0) i=1

mi

(0),

then

(A1,

B2

)

is

a

global

minimizer

if

and

only

if

B2

is full rank and p1 = m1(0), . . . , pk-1 = mk-1(0), pk = rank(B2) -

k-1 i=1

mi(0)



mk(0) for some k  r(0);

· If min{d , . . . , d1} >

r(0) i=1

mi(0),

then

(A1,

B2)

is

a

global

minimizer

if

and

only

if

pi = mi(0) for all i = 1, . . . , r(0) and p¯(0)  0. In particular, B2 can be non-full rank

with rank(B2) =

r(0) i=1

mi

(0).

Note that LD achieves the same global minimum as L. Hence Proposition 1 and Proposition 4 must

match, which yields that

r(0) i=1

pii(0)

=

r(0) i=1

pi(0)i(0).

We

then

conclude

that

pi(0)

=

pi

for

i = 1, . . . , r(0) as 1(0) > · · · > r(0)(0). This proves the proposition.

PROOF OF THEOREM 4

The proof is similar to that for shallow linear networks. Consider a deep-non-optimal-order critical

point (A1, . . . , A ), and define the orthonormal block matrix Sk using the blocks of Vk in a similar

way as eq. (29). Then, A(l,k+2) takes the form A(l,k+2) = UkSkSkVkCk. Since A(l,k+2) is of

non-optimal order, there exists i < j < r(k) such that pi(k) < mi(k) and pj(k) > 0. Thus,

we

perturb

the

j-th

column

of

Uk Sk

to

be

,usj 1 +usi(pi (k)+1)
1+ 2

and

denote

the

resulting

matrix

as

Mk .

Then, we perturb A to be A = Mk(UkSk) A so that A A( -1,k+2) = MkSkVkCk. Moreover,

we generate Ak+1 by eq. (4) with Uk  Mk, Vk  SkVk. Note that such construction satisfies eq. (32), and hence also satisfies eq. (34), which further yields that

A A( -1,k+2)Ak+1A(k,1)X = Pcol(A A( Y-1,k+2)) (A(k,1)X )A(k,1)X . With the above equation, the function value at this perturbed point is evaluated as

L(A

, . . . , Ak+1, . . .)

=

1 2

Tr(Y

Y

)

-

1 2

Tr(

Pcol(SkUkMk SkVk ) k ).

Then, a careful calculation shows that only the i, j-th diagonal elements of Pcol(SkUkMkSkVk)

have

changed,

and

are

2
1+ 2 ,

1 1+

2,

respectively.

We

then

conclude

that

L(A1, . . . , Ak+1, . . . , A

)

=

L(A1, . . . , A

)

-

2
1+ 2 (i(k)

-

j (k))

<

L(A1, . . . , A

).

Now consider a deep-optimal-order critical point (A1, . . . , A ). Note that with A( -2,1) fixed to be
a constant, the deep linear network reduces to a shallow linear network with parameters (A , A -1). Since (A , A -1) is not a non-global minimum critical point of this shallow linear network and A

17

Under review as a conference paper at ICLR 2018 is of optimal-order, we can apply the perturbation scheme in the proof of Proposition 3 to identify a perturbation (A , A -1) with rank(A ) = rank(A ) + 1 that achieves a lower function value. Consider any point in XD. Since A( ,1)X = 0, we can scale the nonzero row, say, the i-th row (A )i,:A( -1,1)X properly in the same way as that in the proof of Proposition 3 to increase the function value. Lastly, item 1 and item 2 imply that every local minimum is a global minimum for these two types of critical points. Moreover, combining items 1,2 and 3, we conclude that every critical point of these two types in XD is a saddle point.
18

