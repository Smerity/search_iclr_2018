Under review as a conference paper at ICLR 2018
JOINTLY LEARNING TO CONSTRUCT AND CONTROL AGENTS USING DEEP REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
The physical design of a robot and the policy that controls its motion are inherently coupled. However, existing approaches largely ignore this coupling, instead choosing to alternate between separate design and control phases, which requires expert intuition throughout and risks convergence to suboptimal designs. In this work, we propose a method that jointly optimizes over the physical design of a robot and the corresponding control policy in a model-free fashion, without any need for expert supervision. Given an arbitrary robot morphology, our method maintains a distribution over the design parameters and uses reinforcement learning to train a neural network controller. Throughout training, we refine the robot distribution to maximize the expected reward. This results in an assignment to the robot parameters and neural network policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel robot designs and walking gaits for several different morphologies, achieving performance comparable to or better than that of hand-crafted designs.
1 INTRODUCTION
An agent's ability to navigate through and interact with its environment depends not just on its skill at planning and controlling its motion, but also on its physical design. Different physical designs are inherently better suited to different tasks and environments. By making appropriate choices during fabrication, mechanical elements can be designed to improve robustness to non-idealities such as errors in perception, delays in actuation, etc., and indeed, make control problem an easier one to solve. At the same time, robots that take different forms may find completely different control strategies to be optimal to complete the same task. Therefore, the physical and computational design of an agent are inherently coupled, and must ideally be jointly optimized if the robot is to successfully complete a task in a particular environment. Consider the development of a legged robot for locomotion. Variations in physical design will require changes to the joint torques in order to preserve a particular locomotion behavior (e.g., heavier feet require greater torque at the ankle), and will likely result in completely different walking gaits, even when the morphology is preserved. In fact, some changes to design may render locomotion impossible for the target operating environment (e.g., a robot with long feet may be unable to locomote up an incline). It is therefore beneficial to not simply consider the robot's design or gait to be fixed, but to optimize both jointly for the target environment and task. Similar co-design can be beneficial in other settings--for example for the control policy and physical characteristics of digits in robotic grippers for grasping. While a robot's physical design and the corresponding control policy are inherently coupled, most existing methods ignore this coupling, instead choosing to alternate between separate design and control phases. Existing approaches that jointly reason over design and control (Digumarti et al., 2014; Ha et al., 2017; Spielberg et al., 2014) assume knowledge of an accurate model of the robot dynamics and require expert supervision (e.g., to provide a suitable initial design and guide the optimization process). However, these restrictive assumptions limits their applicability to a handful of specific settings, and often yield solutions heavily influenced by expert intuition. In this work, we seek a general approach--one that can optimize a robot's physical characteristics jointly with controllers of a desired complexity (Fig. 1), that can be applied to general tasks in some given environment, and that can explore the joint search space of physical design and computational
1

Under review as a conference paper at ICLR 2018

Default Initial

Final Default Initial

Final

Default

Initial

Final

Figure 1: Our algorithm learns a robot's physical design jointly with the control policy. Here we show the learned designs evolving over time for the Hopper (top left), the Walker2d (top right) and the Ant (bottom), each with the default Roboschool design for comparison. Scale is fixed for each robot. Note that these designs correspond to modes of the distribution over robot designs that our algorithm maintains during training.

control in a purely data-driven way, without a model of the robot dynamics and independent of the biases of expert intuition. We develop this approach in the context of determining the physical parameters of an articulated agent--the lengths and thicknesses of each limbs in a given morphology-- through joint training with a neural network for control, with the objective of achieving locomotion. Our method maintains a distribution over these physical parameters, and simultaneously trains the parameters of this distribution with those of a neural network controller, using deep reinforcement learning. In this way, we pursue a design distribution and control policy that are jointly optimal for the given task and environment. Experimental results show that starting from random initializations, our approach is able to find novel designs and walking gaits that match or exceed the performance of manually designed agents. To do the best of our knowledge, our method is the first to successfully carry out such a joint optimization of design and control in a completely model-free manner.
2 RELATED WORK
Similar to our objective, attention has been paid recently to the problem of jointly optimizing the parameters that specify robot structure and a parameterization of its motion (e.g., gaits). Particularly relevant is the work of Ha et al. (2017), who relate design and motion parameters via a set of implicit functions that express robot dynamics, desired trajectories, and actuation limits. These functions encode a manifold that is then linearized to model the relationship between design and motion via the implicit function theorem. The method then solves for the desired parameters in a local fashion via constraint-based optimization. Similarly, Spielberg et al. (2014) describe an approach that jointly reasons over physical design and motion parameters for robots with articulated degrees of freedom (e.g., legged robots). They formulate the problem in the framework of trajectory optimization by incorporating parameters involved in the design of articulated robots, including dimensions, masses, mass distribution, and moments of inertia, together with the contact force and torque variables typically associated with trajectory optimization. They use weighted actuation cost as the objective, subject to a regularization penalty on parameters. Their method is guaranteed to find a feasible design so long as the problem is initialized within a small neighborhood of a feasible solution. Unlike our approach, their method requires that the user provide an estimate of the robot configuration at each time step, as well as an accurate analytic model of constraint dynamics (e.g., foot contact), which is computationally expensive to enforce. Meanwhile, Digumarti et al. (2014) similarly describe an evolutionary method that jointly reasons over design and motion parameters for legged robots, while also tuning the parameters of a robust controller that tracks these motions. Their method is limited to biologically inspired quadrupedal foothold patterns, and does not account for contact dynamics. In contrast, our approach applies to arbitrary morphologies and does not require that we model contact dynamics. While the focus on simultaneous optimization over robot design and motion parameters is relatively new, there is a long history of research focused on the related problem of co-design of physical structure and control (Park & Asada, 1994; Pil & Asada, 1996), (Reyer & Papalambros, 2002),
2

Under review as a conference paper at ICLR 2018
(Ravichandran et al., 2006; Villarreal-Cervantes et al., 2013). Park & Asada (1994) jointly optimize the link geometry of a high-speed robot arm along with the parameters of a PD joint controller. These methods often rely upon access to an accurate model of the robot dynamics in order to design the controller. In order to reduce the dependence upon a detailed analytical model, Pil & Asada (1996) use on-robot experiments to refine their model as part of the iterative design process. More recently, Villarreal-Cervantes et al. (2013) allow for some degree of uncertainty in the model by including the sensitivity of the design and control parameters to model uncertainty along with the task-specific optimization objectives. However, existing methods still rely upon access to an analytical model of the robot dynamics and are typically limited to simple (e.g., linear) control designs. Our method assumes only that the controller can be modeled via a convolutional neural network, and can thereby learn complex, highly nonlinear control policies with no a priori knowledge of the robot dynamics. Far more attention has been paid to the individual problem of task-driven optimization of robot motion and control. Given an arbitrary robot design chosen by novice users, Megaro et al. (2015) describe an interactive design framework that solves for a realizable walking gait that results in stable locomotion. Similarly, Mordatch et al. (2015) synthesize emergent behaviors (motion) for arbitrary morphologies and tasks from high-level specifications, by jointly optimizing over contact and motion. Mordatch et al. (2015) build upon this work by training recurrent neural networks to serve as feedback controllers capable of producing complex, stable behaviors for a variety of dynamical systems. Their method interleaves supervised learning with trajectory optimization, and incorporates noise to improve generalizability. Meanwhile, there is a large body of literature that formulates motion synthesis as a trajectory optimization problem. This approach has proven effective at respecting contract constraints (e.g., between the foot and ground), which make controlling dynamic motion particularly challenging (Dai & Tedrake, 2016; Posa et al., 2016; Griffin & Grizzle, 2016). These approaches have been shown to generate sophisticated behaviors for complex robot designs (e.g., humanoids) (Tassa et al., 2012), and for robots of arbitrary morphologies using only a high-level specification of the robot's shape, gait, and task (Wampler et al., 2013). Related, a number of methods interleave trajectory optimization and supervised learning with neural network regression (Levine & Abbeel, 2014; Levine & Koltun, 2014; Mordatch & Todorov, 2014; Mordatch et al., 2015). Unlike our method, the use of trajectory optimization makes these approaches reliant upon knowledge of the model. A great deal of attention of-late has focused on the problem of learning complex control policies directly from low-level sensory input, without any knowledge of the system dynamics. Methods that have proven particularly effective combine neural networks that learn representations of the highdimensional raw sensor input with deep reinforcement learning (Riedmiller, 2005; Mnih et al., 2013; 2015; Schulman et al., 2015a). While much of the work in this area focuses on low-dimensional, discrete action spaces, several methods have been recently proposed that learn continuous control policies through deep reinforcement learning. These techniques have been applied to control simple, simulated robots (Wawrzyn¥ski, 2009; Wawrzyn¥ski & Tanwani, 2013; Watter et al., 2015; Lillicrap et al., 2015), perform robot manipulation Levine et al. (2016a;b); Gu et al. (2017), and control legged robots (Schulman et al., 2015b; Peng et al., 2016). Black-box optimization (Schmidhuber et al., 2007; Risi & Togelius, 2017) is an alternative to using reinforcement learning to training the control policy. These approaches have the advantage that they do not involve backpropagating gradients, are insensitive to reward sparsity, and can handle long time horizons. While black-box optimization strategies have traditionally been thought of as illsuited to difficult reinforcement learning problems, Salimans et al. (2017) recently showed that they perform similarly to state-of-the-art RL methods on difficult continuous control problems, including those that involve locomotion. Closely related is the policy-gradient method Sehnke et al. (2010), who define the policy as a distribution over the parameters of a controller, which they then sample over. This results in gradient estimates that are far less noisy than is typical of policy gradient algorithms. These approaches are similar to the way in which we learn the robot design, which we formulate as a Gaussian mixture model over design parameters. Indeed, the two aforementioned methods result in the same gradient estimate in the event that the parameter distribution is Gaussian Salimans et al. (2017). Meanwhile, much work has focused on the problem of determining robot designs that meet the requirements of a particular task. Given a user demonstration of the desired behaviors, Coros et al. (2013) learn optimum kinematic linkages that are capable of reproducing these motions. Mehta et al. (2014) synthesize electromechanical robot designs in a compositional fashion based upon a
3

Under review as a conference paper at ICLR 2018

complete user-specified structural specification of the robot. Mehta et al. (2016) build upon this work, allowing the user to specify functional objectives via structured English, which is parsed to a formal specification using linear temporal logic. Censi (2017) describes a theory for co-design that includes the ability to select discrete robot parts according to functional constraints, but do not reason over geometry or motion. Related to our approach is recent work that jointly optimizes sensor design and inference algorithms for perception systems. Chakrabarti (2016) considers the problem of jointly learning a camera sensor's multiplexing pattern along with reconstruction methods for imaging tasks. They model inference as a neural network together and use stochastic gradient descent to backpropagate the loss to a neural layer representation of the multiplexing pattern. Related, Schaff et al. (2017) jointly learn design and inference for beacon-based localization. They encode beacon allocation (spatially and across transmission channels) as a differential neural layer that interfaces with a neural network for inference. Joint optimization then follows from standard techniques for training neural networks.

3 APPROACH
In this section, we begin by describing the standard reinforcement learning framework for training agent policies, and then describe our approach of extending this framework to also learn the physical design of the agent.

3.1 REINFORCEMENT LEARNING BACKGROUND

In the standard reinforcement learning setting, an agent interacts with its environment, usually a

Markov Decision Process, over a number of discrete timesteps. At each time step t, the agent

receives a state st 2 S and takes action at 2 A according to a policy  : S ! A. Then, the agent

receives a scalar reward rt and the next state st+1 from the environment. This process continues

until a terminal state is reached. The goal of maximizes the expected return E[Rt], where

Rretin=forPce1im=e0ntilreta+rinianngd

is

to 2

then find [0, 1) is a

a policy  that discount factor.

Policy gradient methods are a class of algorithms often used to optimize reinforcement learning problems, due to their ability to optimize cumulative reward and the ease with which the can be used with neural networks and other nonlinear function approximators. Consequently, they are commonly used for reinforcement learning problems that involve complex, continuous action spaces. Policy gradient methods directly parameterize a stochastic policy (at|st) and perform stochastic gradient ascent on the expected return. "Vanilla" policy gradient methods compute an estimate of the gradient rE[Rt] using a sample-based mean computed over r log (at|st)Rt (Williams, 1992), which yields an unbiased gradient estimate (Sutton et al., 2000). While the variance in the resulting estimate decreases with the number of samples, sampling is computationally expensive. Another way to reduce variance, while maintaining an unbiased estimate, is to estimate the gradient by comparing the reward to a "baseline" reward b(st).

While effective, these methods can be unstable, especially in the context of deep reinforcement

learning. Small changes to the policy parameters may cause large changes in the distribution of vis-

ited states. Several methods have been proposed to mitigate these effects. Among them, Schulman

et al. (2015a) introduce Trust Region Policy Optimization (TRPO), which imposes a constraint on

the KL-divergence between policies before and after an update. Recently, Schulman et al. (2017)

proposed proximal policy optimization (PPO), a first-order class of methods similar to TRPO. PPO

alternates between sampling data through interaction with the environment and optimizing the ob-

jective

hi

E^t min(rt()A^t, clip(rt(), 1 , 1 + )) ,

(1)

where rt() = objective seeks

tomold(aa(xati|tsm|st)ti)zeatnhde

E^t represents an empirical average expected return while encouraging a

over a finite small step in

sample set. policy space.

This The

clipping within the objective removes any incentive for moving rt() outside the interval [1 , 1+]. The net effect of this objective is a simple, robust policy gradient algorithm that attains or matches

state-of-the-art results on a wide array of tasks (Schulman et al., 2017).

4

Under review as a conference paper at ICLR 2018

Algorithm 1 Joint Optimization Initialize (a|s, r), p (r) while True do for i 2 [1..np] do Sample r1, ..., rn from p Collect rollouts with each robot Update  to maximize PPO's surrogate objective. end for for i 2 [1..nr] do Sample r1, ..., rm from p Compute returns R1, ..., Rm. Update to maximize PPO's surrogate objective. end for end while

3.2 JOINT OPTIMIZATION OF DESIGN AND CONTROL

In our approach, we extend the standard reinforcement learning formulation by considering a space

of possible robot designs . Specifically, we assume that for every design in E 2 , we can define

a common state and action spaces S and A, reward function r(s, a), and initial state distribution

p0(s), that is meaningful to all designs. The designs differ only in the transition dynamics they

induce, denoted by pE (s0|s, a). In this setting, our goal is to find the jointly optimal design E and

pagoelinctys,eEachthdaetsmiganxriempirzeeseenxtpsetchteedprroebwleamrd.ofIncothnetroclolinntgexat

of joint specific

design and control for robotic robot in order to accomplish a

given task. Due to the constraint that all designs shares a common action space A, we assume that

there is a single morphology that is shared by all possible robot designs.

This optimization is effectively a joint search over the spaces of all possible designs and policies. Without any more information about the transition dynamics, the best approach would be to train separate policies for every possible design, and pick the best performing design-policy pair. However, it is often reasonable to assume that robots that are "nearby" in parameter space exhibit similar dynamics. In other words, there is some notion of local smoothness to the transition dynamics as a function of E. Therefore, if the two robots are similar enough, a good policy for one will perform reasonably well for the other. This suggests that features learned while training the policy associated with one robot will be useful for other robots as well, so long as the diversity in robot parameters is consistent with the generalizability of the control policy. With this in mind, we propose to train a single, shared policy to maximize the expected return over a distribution of robots. The use of a single controller has two primary advantages. First, it permits efficient training across all designs. Second, it allows us to quickly evaluate the potential of unseen robots by using the policy as a baseline for performance.

Formally, let p(r; ) be a distribution over robots parameterized by . Additionally, let (at|st, r; ) be a stochastic policy parameterized by . Note that in addition to the current state st, the policy  also depends on the robot r it is acting on. Our goal is to solve the following optimization problem:

,  = arg max Er,t [Rt] ,
,

(2)

where Er,t[∑] is the expectation over robots and trajectories those robots induce.

Our method (Algorithm 3.2) employs an alternating minimization of the parameters of the policy and robot distribution,  and , respectively. We optimize the policy parameters  using Proximal Policy Optimization. However, instead of collecting data from a single robot, we sample a robot r after a fixed number T of timesteps according to the robot distribution p(r; ). After n iterations of PPO, we freeze the policy parameters, and proceed to optimize the parameters of the robot distribution. Without knowledge of the model, we optimize the robot parameters via policy gradient over parameter space, similar to Sehnke et al. (2010). This is equivalent to black-box optimization methods that have proven effective for complex learning problems (Salimans et al., 2017). We sample m different robots and compute their returns for a single episode acting under policy (∑; ). We then use this data to shift the robot policy p(∑; ) in a direction maximizes expected return. These iterations are repeated until convergence.

5

Under review as a conference paper at ICLR 2018
Figure 2: From left to right: the default Hopper, Walker2d, and Ant robots. The Hopper and Walker2d are constrained to walk in a line, while the Ant can walk anywhere in the plane.
4 RESULTS
We validate our approach with three commonly used robots provided in OpenAI's Roboschool (Schulman et al., 2017). Environments in Roboschool are built on top of Bullet Physics, a popular open-source physics engine. In a series of locomotion experiments, we show that our approach is not only able to discover novel robot designs and gaits, but also outperforms two out of three of the given robots trained on the same task, and yields performance comparable to the third. 4.1 EXPERIMENT DETAILS We evaluate our approach on three legged locomotion tasks within Roboschool: RoboschoolHopper, RoboschoolWalker2d, and RoboschoolAnt (note that we subsequently drop the Roboschool prefix for brevity). Figure 2 depicts the default Roboschool design for each robot. These environments describe a locomotion task that has the robots moving along the positive x-axis (to the right in the figures show) towards a distant goal. The reward function defined in these environments is a weighted sum of: rewards for forward progress and staying upright, and penalties for applying torques and for joints that reach their rotational limits. The episode ends when the robot falls over or reaches a maximum number of timesteps. We learn robot designs using the morphologies specified by the standard Roboschool designs. For each morphology, we parameterize the robot in terms of the length and radius (e.g., mass) of each link (or just the radius in the case of the sphere for the ant body). We impose symmetry, and share parameters across each leg for the Walker2d and Ant designs. This parameterization permits a wide variety of robots of different shapes and sizes, some of which are better suited to the locomotion objective than others, and some designs that do not permit a control policy that results in locomotion. We model the control policy (at|st, E; p) as a convolutional neural network consisting of three fully-connected layers with 64 units and tanh activation functions. A final layer maps the output to the robot action space. With the exception of the last robot-specific layer, the architecture is the same for all experiments. Meanwhile, we represent the distribution over the parameterized robot design p(∑; r) using a Gaussian mixture model with four mixture components. Empirically, we find that having a multimodal distribution has two primary advantages. First, it promotes exploration in the space of robot designs by allowing the framework to simultaneously maintain several distinct hypotheses. Second, a mixture model encourages the policy network to generalize across a wider array of robots. We initialize the means of each component randomly within a wide range, and initialize the variances in order to span the range of each parameter. We train our method with eight threads of Proximal Policy Optimization (Schulman et al., 2017) for a total of 300M environment timesteps across all threads. We use the publicly available implementation of PPO provided in OpenAI Baselines (Hesse et al., 2017). We tune all hyperparameters on the Hopper, and subsequently fix them across all experiments.
6

Under review as a conference paper at ICLR 2018

Hopper

Walker2d

Ant

Figure 3: Training curves that show the evolution of reward across training iterations for the Hopper, Walker2d, and Ant environments over eight different random seeds. The black dashed line corresponds to the highest achievable performance of the corresponding baseline robot. In other figures, we display the best-performing checkpoint of each run. Therefore, we make each training curve transparent after it stops improving.

4.2 EXPERIMENTS We evaluate the performance of our approach to joint optimization over robot design and control on the Hopper, Walker2d, and Ant robots, and compare against a policy trained according to the standard Roboschool design. We evaluate the consistency and robustness of our approach relative to the initial robot parameter distribution using several different random seeds. For each robot morphology, we find that our method learns robot designs and the corresponding control policy that are either comparable (Walker2d) or exceed the performance (Hopper and Ant) achievable using the default designs (Fig. 3). Further, we see that our method achieves these levels of performance by discovering unique robot designs (Fig. 1) together with novel walking gaits (Fig. 4) that can not be achieved with the default designs. We stress the fact that our method learned these designs and control policies from a random initialization, without access to a dynamics model or any expert supervision. Our method learns a joint design and controller for the Hopper that outperforms the baseline by as much as 50%. Our learned robot exploits the small size of the foot to achieve faster, more precise walking gaits. Meanwhile, the longer torso of the learned robot improves stability, allowing it to maintain balance while locomoting at a faster pace. We found this behavior to be consistent across several different Hopper experiments, with the method converging to designs with a small foot and long, thin torso. Our method learns a physical design of the Ant that is significantly different from the default design (Fig. 2). Consequently, the learned Ant drastically outperforms the baseline, improving reward by up to 116%. Our method learns a design that has a small, lightweight body and extremely long legs. The long legs enable the ant to apply large torque at contact, allowing it to move at a fast pace. Our framework learned different design-control pairs for the Walker that perform similarly to the default design. Across several different experiments, we see two distinct, successful designs and walking gaits. Interestingly, neither agent walks primarily on its feet. The first design has small, thick legs and long feet. The controller is then able to exploit the thick middle leg link, which protrudes slightly past the foot, to push off the ground. The long foot then provides additional balance. The second design is similar in geometry to the baseline Walker2d, but moves in a very different way. By lowering the knee joint and lengthening the foot, the Walker2d is able to efficiently balance on its knees and toes. This low stance allows the Walker2d to fully extend its leg backwards, creating a long, powerful stride, similar to that of a sprinter using starting blocks.
5 CONCLUSION
We proposed what is, to the best of our knowledge, the first model-free algorithm that jointly optimizes over the physical design of a robot and the corresponding control policy, without any need for expert supervision. Given an arbitrary morphology, our robot maintains a distribution over the
7

Under review as a conference paper at ICLR 2018
Figure 4: Here, we compare locomotion of default and learned robots, visualizing both their physical design and corresponding learned gaits. We pick the best results for Hopper and Ant, and two of the best results for Walker2d (due to diversity in gaits). Note that for each robot type, we show a blend of a fixed number of frames in the same time interval, allowing direct comparison between the speed with which different designs are able to locomote. robot design parameters and learns these parameters together with a neural network controller using policy gradient-based reinforcement learning. This results in an assignment to the policy over robot parameters and the control policy that are jointly optimal. We evaluated our approach on a series of different legged robot morphologies, demonstrating that it results in novel robot designs and walking gaits, achieving performance that either matches or exceeds that of manually defined designs. This work provides several opportunities for future work. Among them is to explore the ability to learn legged robot designs and controllers that are able to locomote despite uneven terrain, the presence of obstacles, changes in slope, and variations in friction. Further, we are currently working on relaxing the assumption that the morphology is pre-defined, permitting our algorithm to search both over different morphologies as well as their particular parameterization. It is worth stressing our joint approach to design and control is not specific to legged robots. Thus, we are also investigating applications applications to different types of agents (e.g., robot end-effectors). This will require that we revisit the use of a Gaussian mixture model as the representation of the robot distribution for agents with higher degrees-of-freedom and, in turn, more complex design distributions.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Andrea Censi. A class of co-design problems with cyclic constraints and their solution. Robotics and Automation Letters, 2(1):96≠103, 2017.
Ayan Chakrabarti. Learning sensor multiplexing design through backpropagation. In Advances in Neural Information Processing Systems (NIPS), December 2016.
Stelian Coros, Bernhard Thomaszewski, Gioacchino Noris, Shinjiro Sueda, Moira Forberg, Robert W. Sumner, Wojciech Matusik, and Bernd Bickel. Computational design of mechanical characters. Trans. on Graphics, 32(4):83, 2013.
Hongkai Dai and Russ Tedrake. Planning robust walking motion on uneven terrain via convex optimization. In Proc. Int'l Conf. on Humanoid Robots (Humanoids), 2016.
Krishnamanaswi M. Digumarti, Christian Gehring, Stelian Coros, J. Hwangbo, and R. Siegwart. Concurrent optimization of mechanical design and locomotion control of a legged robot. In Proc. IEEE Int'l Conf. on Robotics and Automation (ICRA), July 2014.
Brent Griffin and Jessy Grizzle. Nonholonomic virtual constraints and gait optimization for robust walking control. Int'l J. of Robotics Research, pp. 895≠922, 2016.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In Proc. IEEE Int'l Conf. on Robotics and Automation (ICRA), 2017.
Sehoon Ha, Stelian Coros, Alexander Alspach, Joohyung Kim, and Katsu Yamane. Joint optimization of robot design and motion parameters using the implicit function theorem. In Proc. Robotics: Science and Systems (RSS), June 2017.
Christopher Hesse, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/openai/baselines, 2017.
Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems (NIPS), 2014.
Sergey Levine and Vladlen Koltun. Learning complex neural network policies with trajectory optimization. In Proc. Int'l Conf. on Machine Learning (ICML), 2014.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. J. Machine Learning Research, 17(39):1≠40, 2016a.
Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. Int'l J. of Robotics Research, 2016b.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
Vittorio Megaro, Bernhard Thomaszewski, Maurizio Nitti, Otmar Hilliges, Markus Gross, and Stelian Coros. Interactive design of 3d-printable robotic creatures. Trans. on Graphics, 34(6): 216, 2015.
Ankur M. Mehta, Joseph DelPreto, Benjamin Shaya, and Daniela Rus. Cogeneration of mechanical, electrical, and software designs for printable robots from structural specifications. In Proc. IEEE/RSJ Int'l Conf. on Intelligent Robots and Systems (IROS), 2014.
Ankur M. Mehta, Joseph DelPreto, Kai Weng Wong, Scott Hamill, Hadas Kress-Gazit, and Daniela Rus. Robot creation from functional specifications. In Int'l Symp. of Robotics Research (ISRR), 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
9

Under review as a conference paper at ICLR 2018
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529≠533, 2015.
Igor Mordatch and Emo Todorov. Combining the benefits of function approximation and trajectory optimization. In Proc. Robotics: Science and Systems (RSS), 2014.
Igor Mordatch, Kendall Lowrey, Galen Andrew, Zoran Popovic¥, and Emanuel V Todorov. Interactive control of diverse complex characters with neural networks. In Advances in Neural Information Processing Systems (NIPS), 2015.
Jahng-Hyon Park and Haruhiko Asada. Concurrent design optimization of mechanical structure and control for high speed robots. J. Dynamic systems, Measurement, and Control, 116(3):344≠356, 1994.
Xue Bin Peng, Glen Berseth, and Michiel Van de Panne. Terrain-adaptive locomotion skills using deep reinforcement learning. ACM Trans. on Graphics (TOG), 35(4):81, 2016.
Anton C. Pil and Harry H. Asada. Integrated structure/control design of mechatronic systems using a recursive experimental optimization method. Trans. on Mechatronics, 1(3):191≠203, 1996.
Michael Posa, Scott Kuindersma, and Russ Tedrake. Optimization and stabilization of trajectories for constrained dynamical systems. In Proc. IEEE Int'l Conf. on Robotics and Automation (ICRA), 2016.
Thambirajah Ravichandran, David Wang, and Glenn Heppler. Simultaneous plant-controller design optimization of a two-link planar manipulator. Mechatronics, 16(3):233≠242, 2006.
Julie A. Reyer and Panos Y. Papalambros. Combined optimal design and control with application to an electric dc motor. J. of Mechanical Design, 124(2):183≠191, 2002.
Martin Riedmiller. Neural fitted Q iteration-first experiences with a data efficient neural reinforcement learning method. In Proc. European Conf. on Machine Learning (ECML), volume 3720. Springer, 2005.
Sebastian Risi and Julian Togelius. Neuroevolution in games: State of the art and open challenges. IEEE Trans. on Computational Intelligence and AI in Games, 9(1):25≠41, 2017.
Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Charles Schaff, David Yunis, Ayan Chakrabarti, and Matthew R. Walter. Jointly optimizing placement and inference for beacon-based localization. In Proc. IEEE/RSJ Int'l Conf. on Intelligent Robots and Systems (IROS), September 2017.
Ju®rgen Schmidhuber, Daan Wierstra, Matteo Gagliolo, and Faustino Gomez. Training recurrent networks by evolino. Neural Computation, 19(3):757≠779, 2007.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. arXiv preprint arXiv:1502.05477, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Frank Sehnke, Christian Osendorfer, Thomas Ru®ckstieﬂ, Alex Graves, Jan Peters, and Ju®rgen Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551≠559, 2010.
Andrew Spielberg, Brandon Araki, Cynthia Sung, Russ Tedrake, and Daniela Rus. Functional co-optimization of articulated robots. In Proc. Int'l Conf. of Climbing and Walking Robots (CLAWAR), May 2014.
10

Under review as a conference paper at ICLR 2018 Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems (NIPS), 2000. Yuval Tassa, Tom Erez, and Emanuel Todorov. Synthesis and stabilization of complex behaviors through online trajectory optimization. In Proc. IEEE/RSJ Int'l Conf. on Intelligent Robots and Systems (IROS), 2012. Miguel G. Villarreal-Cervantes, Carlos A. Cruz-Villar, Jaime Alvarez-Gallegos, and Edgar A. Portilla-Flores. Robust structure-control design approach for mechatronic systems. Trans. on Mechatronics, 18(5):1592≠1601, 2013. Kevin Wampler, Jovan Popovic¥, and Zoran Popovic¥. Animal locomotion controllers from scratch. In Computer Graphics Forum, volume 32, 2013. Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in Neural Information Processing Systems (NIPS), 2015. Pawel Wawrzyn¥ski. Real-time reinforcement learning by sequential actor-critics and experience replay. Neural Networks, 22(10):1484≠1497, 2009. Pawel Wawrzyn¥ski and Ajay Kumar Tanwani. Autonomous reinforcement learning with experience replay. Neural Networks, 41:156≠167, 2013. Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3≠4), May 1992.
11

