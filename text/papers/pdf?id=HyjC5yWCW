Under review as a conference paper at ICLR 2018
META-LEARNING AND UNIVERSALITY:
DEEP REPRESENTATIONS AND GRADIENT DESCENT CAN
APPROXIMATE ANY LEARNING ALGORITHM
Anonymous authors Paper under double-blind review
ABSTRACT
Learning to learn is a powerful paradigm for enabling models to learn from data more effectively and efficiently. A popular approach to meta-learning is to train a recurrent model to read in a training dataset as input and output the parameters of a learned model, or output predictions for new test inputs. Alternatively, a more recent approach to meta-learning aims to acquire deep representations that can be effectively fine-tuned, via standard gradient descent, to new tasks. In this paper, we consider the meta-learning problem from the perspective of universality, formalizing the notion of learning algorithm approximation and comparing the expressive power of the aforementioned recurrent models to the more recent approaches that embed gradient descent into the meta-learner. In particular, we seek to answer the following question: does deep representation combined with standard gradient descent have sufficient capacity to approximate any learning algorithm? We find that this is indeed true, and further find, in our experiments, that gradient-based meta-learning consistently leads to learning strategies that generalize more widely compared to those represented by recurrent models.
1 INTRODUCTION
Deep neural networks that optimize for effective representations have enjoyed tremendous success over human-engineered representations. Meta-learning takes this one step further by optimizing for a learning algorithm that can effectively acquire representations. A common approach to metalearning is to train a recurrent or memory-augmented model such as a recurrent neural network to take a training dataset as input and then output the parameters of a learner model (Schmidhuber, 1987; Bengio et al., 1992; Li & Malik, 2017a; Andrychowicz et al., 2016). Alternatively, some approaches pass the dataset and test input into the model, which then outputs a corresponding prediction for the test example (Santoro et al., 2016; Duan et al., 2016; Wang et al., 2016; Mishra et al., 2017). Such recurrent models are universal learning algorithm approximators, in that they have the capacity to approximately represent any mapping from dataset to algorithm. However, depending on the form of the model, it may lack statistical efficiency.
In contrast to the aforementioned approaches, more recent work has proposed methods that include the structure of optimization problems into the meta-learner (Ravi & Larochelle, 2017; Finn et al., 2017a; Husken & Goerick, 2000). In particular, model-agnostic meta-learning (MAML) optimizes only for the initial parameters of the learner model, using standard gradient descent as the learner's update rule (Finn et al., 2017a). Then, at meta-test time, the learner is trained via gradient descent. By incorporating prior knowledge about gradient-based learning, MAML improves on the statistical efficiency of black-box meta-learners and has successfully been applied to a range of meta-learning problems (Finn et al., 2017a;b; Li et al., 2017). But, does it do so at a cost? A natural question that arises with purely gradient-based meta-learners such as MAML is whether it is indeed sufficient to only learn an initialization, or whether representational power is in fact lost from not learning the update rule. Intuitively, we might surmise that learning an update rule is more expressive than simply learning an initialization for gradient descent. In this paper, we seek to answer the following question: does simply learning the initial parameters of a deep neural network have the same representational power as arbitrarily expressive meta-learners that directly ingest the training data at meta-test time? Or, more concisely, does representation combined with standard gradient descent have sufficient capacity to constitute any learning algorithm?
1

Under review as a conference paper at ICLR 2018

In this paper, we analyze this question from the standpoint of the universal function approximation theorem. We compare the theoretical representational capacity of the two meta-learning approaches: a deep network updated with one gradient step, and a meta-learner that directly ingests a training set and test input and outputs predictions for that test input (e.g. using a recurrent neural network). In studying the universality of MAML, we find that there exists a form of learner model for which MAML has the same theoretical representational power as recurrent meta-learners. We therefore conclude that, when using deep, expressive function approximators, there is no theoretical disadvantage in terms of representational power to using MAML over a black-box meta-learner represented, for example, by a recurrent network.
Since MAML has the same representational power as any other universal meta-learner, the next question we might ask is: what is the benefit of using MAML over any other approach? We study this question by analyzing the effect of continuing optimization on MAML performance. Although MAML optimizes a network's parameters for maximal performance after a fixed small number of gradient steps, we analyze the effect of taking substantially more gradient steps at meta-test time. We find that initializations learned by MAML are extremely resilient to overfitting to tiny datasets, in stark contrast to more conventional network initialization, even when taking many more gradient steps than were used during meta-training. We also find that the MAML initialization is substantially better suited for extrapolation beyond the distribution of tasks seen at meta-training time, when compared to meta-learning methods based on networks that ingest the entire training set. We analyze this setting empirically and provide some intuition to explain this effect.

2 PRELIMINARIES
In this section, we review the universal function approximation theorem and its extensions that we will use when considering the universal approximation of learning algorithms. We also overview the model-agnostic meta-learning algorithm and an architectural extension that we will use in Section 4.

2.1 UNIVERSAL FUNCTION APPROXIMATION
The universal function approximation theorem states that a neural network with one hidden layer of finite width can approximate any continuous function on compact subsets of Rn up to arbitrary precision (Hornik et al., 1989; Cybenko, 1989; Funahashi, 1989). The theorem holds for a range of activation functions, including the sigmoid (Hornik et al., 1989) and ReLU (Sonoda & Murata, 2017) functions. A function approximator that satisfies the definition above is often referred to as a universal function approximator (UFA). Similarly, we will define a universal learning algorithm approximator to be a UFA with input (D, x ) and output y , where (D, x ) denotes the training dataset and test input, while y denotes the desired test output. Furthermore, Hornik et al. (1990) showed that a neural network with a single hidden layer can simultaneously approximate any function and its derivatives, under mild assumptions on the activation function used and target function's domain. We will use this property in Section 4 as part of our meta-learning universality result.

2.2 MODEL-AGNOSTIC META-LEARNING WITH A BIAS TRANSFORMATION

Model-Agnostic Meta-Learning (MAML) is a method that proposes to learn an initial set of parameters  such that one or a few gradient steps on  computed using a small amount of data for one task leads to effective generalization on that task (Finn et al., 2017a). Tasks typically correspond to supervised classification or regression problems, but can also correspond to reinforcement learning problems. The MAML objective is computed over many tasks {Tj} as follows:

min


LTj (Tj ) =

LTj ( - LTj ()),

jj

where the inner loss corresponds to a training set for task Tj and the outer loss evaluates generalization on test data from Tj. The inner optimization to compute Tj can use multiple gradient steps; though, in this paper, we will focus on the single gradient step setting. After meta-training on a
wide range of tasks, the model can quickly and efficiently learn new, held-out test tasks by running
gradient descent starting from the meta-learned representation .

While MAML is compatible with any neural network architecture and any differentiable loss function, recent work has observed that some architectural choices can improve its performance. A particularly effective modification, introduced by Finn et al. (2017b), is to concatenate a vector of

2

Under review as a conference paper at ICLR 2018

parameters, b, to the input. As with all other model parameters, b is updated in the inner loop via gradient descent, and the initial value of b is meta-learned. This modification, referred to as a bias transformation, increases the expressive power of the error gradient without changing the expressiv-
ity of the model itself. While Finn et al. (2017b) report empirical benefit from this modification, we
will use this architectural design as a symmetry-breaking mechanism in our universality proof.

3 PROBLEM STATEMENT

We can broadly classify RNN-based meta-learning methods into two categories. In the first approach (Santoro et al., 2016; Duan et al., 2016; Wang et al., 2016; Mishra et al., 2017), there is a meta-learner model g with parameters  which takes as input the dataset DT for a particular task T and a new test input x , and outputs the estimated output y^ for that input:
y^ = g(DT , x ; ) = g((x, y)1, ..., (x, y)K , x ; )
The meta-learner g is typically a recurrent model that iterates over the dataset D and the new input x . For a recurrent neural network model that satisfies the UFA theorem, this approach is maximally expressive, as it can represent any function on the dataset DT and test input x .

In the second approach (Hochreiter et al., 2001; Bengio et al., 1992; Li & Malik, 2017b; Andrychowicz et al., 2016; Ravi & Larochelle, 2017; Ha et al., 2017), there is a meta-learner g that takes as input the dataset for a particular task DT and the current weights  of a learner model f , and outputs new parameters T for the learner model. Then, the test input x is fed into the learner model to produce the predicted output y^ . The process can be written as follows:
y^ = f (x ; T ) = f (x ; g(DT ; )) = f (x ; g((x, y)1:K ; ))
Note that, in the form written above, this approach can be as expressive as the previous approach, since the meta-learner could simply copy the dataset into some of the predicted weights, reducing to a model that takes as input the dataset and the test example.1 Several versions of this approach, i.e. Ravi & Larochelle (2017); Li & Malik (2017b), have the recurrent meta-learner operate on order-invariant features such as the gradient and objective value averaged over the datapoints in the dataset, rather than operating on the individual datapoints themselves. This induces a potentially helpful inductive bias that disallows coupling between datapoints, ignoring the ordering within the dataset. As a result, the meta-learning process can only produce permutation-invariant functions of the dataset.

In model-agnostic meta-learning (MAML), instead of using an RNN to update the weights of the learner f , standard gradient descent is used. Specifically, the prediction y^ for a test input x is:

y^ = f(x ; T ) = f(x ;  - L(DT , )) = f

x

;



-



1 K

K

L(yk, f(xk; ))

,

k=1

where  denotes the initial parameters of the model f and also corresponds to the parameters that

are meta-learned. Since the RNN approaches can approximate any update rule, they are clearly at

least as expressive as gradient descent. It is less obvious whether or not the MAML update imposes

any constraints on the set of update functions that can be learned.

The first goal of this paper is to show that fMAML(x ; T ) is a universal function approximator of (DT , x ) in the one-shot setting, where the dataset DT consists of a single datapoint (x, y). Then, we will consider the case of K-shot learning, showing that fMAML(x ; T ) is universal in the set of functions that are invariant to the permutation of datapoints. In both cases, we will discuss meta
supervised learning problems with both discrete and continuous labels and the loss functions under
which universality does or does not hold.

4 UNIVERSALITY OF THE ONE-SHOT GRADIENT-BASED LEARNER
We first introduce a proof of the universality of gradient-based meta-learning for the special case with only one training point, corresponding to one-shot learning. We denote the training datapoint as (x, y), and the test input as x . A universal learning algorithm approximator corresponds to the ability of a meta-learner to represent any function ftarget(x, y, x ) up to arbitrary precision.
1For this to be possible, the model f must be a neural network with at least two hidden layers, since the dataset can be copied into the first layer of weights and the predicted output must be a universal function approximator of both the dataset and the test input.

3

Under review as a conference paper at ICLR 2018

nonlinear layers linear layer

Figure 1: Diagram of constructed neural network policy f^ including a scalar bias transformation variable b.

We will proceed by construction, showing that there exists a neural network function f^(·; ) such that f^(·;  ) approximates ftarget(x, y, x ) up to arbitrary precision, where  =  - L(y, f (x)) and  is the learning rate. As we discuss in Section 6, the loss function L cannot be any loss function,
but the standard cross-entropy and mean-squared error objectives are both suitable. In this proof, we will start by presenting the form of f^ and deriving its value after one gradient step. Then, to show
universality, we will construct a setting of the weight matrices that enables independent control of
the information flow coming forward from x and x , and backward from y.

We will start by constructing f^. Note that, for a particular weight matrix Wi at layer i, a single

gradient step Wi - Wi L can only represent a rank-1 update to the matrix Wi. That is because the gradient of Wi is the outer product of two vectors, Wi L = aibiT-1, where ai is the error gradient with respect to the pre-synaptic activations at layer i, and bi-1 is the forward post-synaptic

activations at layer i - 1. The expressive power of a single gradient update to a single weight matrix

is therefore quite limited. However, if we sequence N weight matrices as

N i=1

Wi

,

corresponding

to multiple linear layers, it is possible to acquire a rank-N update to the linear function represented

by W =

N i=1

Wi.

Motivated

by

this

reasoning,

we

will

construct

f^(·;

)

as

the

following:

f^(·; ) = fout

N
Wi (·; ft, b); out .

i=1

(·; ft, b) represents an input feature extractor with parameters ft and a scalar bias transformation

variable b,

N i=1

Wi

is

a

product

of

square

linear

weight

matrices,

fout(·,

out)

is

a

readout

function

at the output, and the learned parameters are  := {ft, b, {Wi}, out}. The network structure is

visualized in Figure 1. The input feature extractor and readout function can be represented with

fully connected neural networks with one or more hidden layers, which we know are universal

function approximators, while

N i=1

Wi

corresponds to a set of linear layers.

Since linear layers

are no more expressive than nonlinear layers, this model design is generic, in the sense that such a

network can be represented by a sufficiently deep standard fully connected network.

Next, we derive the form of the post-update prediction f^(x ;  ). Let z =

N i=1

Wi

(x; ft, b),

and the error gradient zL = e(x, y). Then, the gradient with respect to any of the weight matrices Wi is given by

 T
i-1


N

T

Wi L(y, f^(x, )) =  Wj e(x, y)(x; ft, b)T 

Wj .

j=1

j=i+1

Therefore, the post-update value of

N i=1

Wi

=

N i=1

(Wi

-

Wi

)

is

given

by

   T

N

N i-1

i-1


N

T 
N

Wi-  Wj  Wj e(x, y)(x; ft, b)T 

Wj 

 Wj -O(2 ),

i=1 i=1 j=1

j=1

j=i+1

j=i+1

where we will disregard the last term, assuming that  is comparatively small such that 2 and all higher order terms vanish. In general, these terms do not necessarily need to vanish, and likely would further improve the expressiveness of the gradient update, but we disregard them here for the sake of the simplicity of the derivation. Ignoring these terms, we now note that the post-update value of z when x is provided as input into f^(·;  ) is given by
N

z = Wi(x ; ft, b)

(1)

i=1

  T

N i-1

i-1


N

T
N



-  Wj Wj e(x, y)(x; ft, b)T 

Wj 

Wj(x ; ft, b),

i=1 j=1

j=1

j=i+1

j=i+1

4

Under review as a conference paper at ICLR 2018

and f^(x ;  ) = fout(z ; out).
Our goal is to show that that there exists a setting of Wi, fout, and  for which the above function, f^(x ,  ), can approximate any function of (x, y, x ). To show universality, we will aim to independently control information flow from x, from y, and from x by multiplexing forward information from x and backward information from y. We will achieve this by decomposing Wi, , and the error gradient into three parts, as follows:

 W~ i 0

 0

Wi :=  0 W i 0 

0 0 wi

 ~(·; ft, b)  (·; ft, b) :=  0 
b

0 zL(y, f^(x; )) := e(y)
e(y)

(2)

where the initial value of b will be 0. The top components all have equal numbers of rows, as do the middle components. As a result, we can see that z will likewise be made up of three components, which we will denote as z~, z, and z. Lastly, we construct the top component of the error gradient to be 0, whereas the middle and bottom components, e(y) and e(y), can be set to be any linear (but not affine) function of y. We will discuss how to achieve this gradient in the latter part of this section when we define fout and in Section 6.
In Appendix A.3, we show that we can choose a particular form of W~ i, W i, and wi that will simplify the products of Wj matrices in Equation 1, such that we get the following form for z :

N
z = - Aie(y)~(x; ft, b)T BiT Bi~(x ; ft, b),
i=1

(3)

where A1 = I, BN = I, Ai can be chosen to be any symmetric positive-definite matrix, and Bi can be chosen to be any positive definite matrix.

Finally, we need to define the function fout at the output. When the training input x is passed in, we need fout to propagate information about the label y as defined in Equation 2. And, when the test input x is passed in, we need a different function defined only on z . Thus, we will define fout as a neural network that approximates the following multiplexer function and its derivatives (as shown
possible by Hornik et al. (1990)):

z~ z~
fout z ; out = 1(z = 0)gpre z ; g + 1(z = 0)hpost(z; h),
z z

(4)

where gpre is a linear function with parameters g such that zL = e(y) satisfies Equation 2 (see Section 6) and hpost(·; h) is a neural network with one or more hidden layers. As shown in Ap-
pendix A.4, the post-update value of fout is

z~ fout z ; out = hpost(z ; h).
z

(5)

Now, combining Equations 3 and 5, we can see that the post-update value is the following:

N
f^(x ;  ) = hpost - Aie(y)~(x; ft, b)T BiT Bi~(x ; ft, b); h
i=1

(6)

In summary, so far, we have chosen a particular form of weight matrices, feature extractor, and
output function to decouple forward and backward information flow and recover the post-update function above. Now, our goal is to show that the above function f^(x ;  ) is a universal learn-
ing algorithm approximator, as a function of (x, y, x ). For notational clarity, we will use ki(x, x ) := ~(x; ft, b)T BiT Bi~(x ; ft, b) to denote the inner product in the above equation, noting that it can be viewed as a type of kernel with the RKHS defined by Bi~(x; ft, b).2 The connection to kernels is not in fact needed for the proof, but provides for convenient notation and an
interesting observation. We then define the following lemma:

2Due to the symmetry of kernels, this requires interpreting b as part of the input, rather than a kernel hyperparameter, so that the left input is (x, b) and the right one is (x , b).

5

Under review as a conference paper at ICLR 2018

Lemma 4.1 Let us assume that e(y) can be chosen to be any linear (but not affine) function of y.

Then, we can choose ft, h, {Ai; i > 1}, {Bi; i < N } such that the function

N
f^(x ;  ) = hpost - Aie(y)ki(x, x ); h

(7)

i=1
can approximate any continuous function of (x, y, x ) on compact subsets of Rdim(y).

Intuitively, Equation 7 can be viewed as a sum of basis vectors Aie(y) weighted by ki(x, x ), which is passed into hpost to produce the output. There are likely a number of ways to prove Lemma 4.1. In Appendix A.1, we provide a simple though inefficient proof, which we will briefly summarize
here. We can define ki to be a indicator function, indicating when (x, x ) takes on a particular value indexed by i. Then, we can define Aie(y) to be a vector containing the information of y and i. Then, the result of the summation will be a vector containing information about the label y and the
value of (x, x ) which is indexed by i. Finally, hpost defines the output for each value of (x, y, x ). The bias transformation variable b plays a vital role in our construction, as it breaks the symmetry within ki(x, x ). Without such asymmetry, it would not be possible for our constructed function to represent any function of x and x after one gradient step.

In conclusion, we have shown that there exists a neural network structure for which f^(x ;  ) is a universal approximator of ftarget(x, y, x ). We chose a particular form of f^(·; ) that decouples forward and backward information flow. With this choice, it is possible to impose any desired post-update function, even in the face of adversarial training datasets and loss functions, e.g. when the gradient points in the wrong direction. If we make the assumption that the inner loss function and training dataset are not chosen adversarially and the error gradient points in the direction of improvement, it is likely that a much simpler architecture will suffice that does not require multiplexing of forward and backward information in separate channels. Informative loss functions and training data allowing for simpler functions is indicative of the inductive bias built into gradient-based meta-learners, which is not present in recurrent meta-learners.

Our result in this section implies that a sufficiently deep representation combined with just a single gradient step can approximate any one-shot learning algorithm. In the next section, we will show the universality of MAML for K-shot learning algorithms.

5 GENERAL UNIVERSALITY OF THE GRADIENT-BASED LEARNER

Now, we consider the more general K-shot setting, aiming to show that MAML can approximate any permutation invariant function of a dataset and test datapoint ({(x, y)i; i  1...K}, x ) for K > 1. Note that K does not need to be small. To reduce redundancy, we will only overview the
differences from the 1-shot setting in this section. We include a full proof in Appendix B.

In the K-shot setting, the parameters of f^(·, ) are updated according to the following rule:



=- 1 K

K

L(yk, f (xk; ))).

k=1

Defining the form of f^ to be the same as in Section 4, the post-update function is the following:

f^(x ;  ) = hpost

1N -
K

K
Aie(yk)ki(xk, x ); h

i=1 k=1

In Appendix C, we show one way in which this function can approximate any function of

({(x, y)k; k  1...K}, x ) that is invariant to the ordering of the training datapoints {(x, y)k; k  1...K}. We do so by showing that we can select a setting of ~ and of each Ai and Bi such that z is a vector containing a discretization of x and frequency counts of the discretized datapoints3.

If z is a vector that completely describes ({(x, y)i}, x ) without loss of information and because hpost is a universal function approximator, f^(x ;  ) can approximate any continuous function of
({(x, y)i}, x ) on compact subsets of Rdim(y). It's also worth noting that the form of the above
equation greatly resembles a kernel-based function approximator around the training points, and a

substantially more efficient universality proof can likely be obtained starting from this premise.

3With continuous labels y and mean-squared error L, we require the mild assumption that no two datapoints may share the same input value x: the input datapoints must be unique.

6

Under review as a conference paper at ICLR 2018

Figure 2: The effect of additional gradient steps at test time when attempting to solve new tasks. The MAML model, trained with 5 inner gradient steps, can further improve with more steps. All methods are provided with the same data ­ 5 examples ­ where each gradient step is computed using the same 5 datapoints.

6 LOSS FUNCTIONS

In the previous sections, we showed that a deep representation combined with gradient descent can approximate any learning algorithm. In this section, we will discuss the requirements that the loss function must satisfy in order for the results in Sections 4 and 5 to hold. As one might expect, the main requirement will be for the label to be recoverable from the gradient of the loss.

As seen in the definition of fout in Equation 4, the pre-update function f^(x, ) is given by gpre(z; g), where gpre is used for back-propagating information about the label(s) to the learner. As stated in Equation 2, we require that the error gradient with respect to z to be:

0 zL(y, f^(x; )) = e(y) , where z =

z~ z

 ~(x; ft, b)  = 0 ,

e(y) b 0

and where e(y) and e(y) must be able to represent [at least] any linear function of the label y.

We define gpre as follows: gpre(z) := W~ g W g w g z = W~ gz~ + W gz + bw g.

To make the top term of the gradient equal to 0, we can set W~ g to be 0, which causes the pre-update

prediction

y^

=

f^(x,

)

to

be

0.

Next,

note

that

e(y)

=

W

T g

y^ L(y,

y^)

and

e(y)

=

w gT

y^ L(y,

y^).

Thus, for e(y) to be any linear function of y, we require a loss function for which y^L(y, 0) is

a linear function Ay, where A is invertible. Essentially, y needs to be recoverable from the loss

function's gradient. In Appendix D and E, we prove the following two theorems, thus showing that

the standard 2 and cross-entropy losses allow for the universality of gradient-based meta-learning.

Theorem 6.1 The gradient of the standard mean-squared error objective evaluated at y^ = 0 is a linear, invertible function of y.

Theorem 6.2 The gradient of the softmax cross entropy loss with respect to the pre-softmax logits is a linear, invertible function of y, when evaluated at 0.

Now consider other popular loss functions whose gradients do not satisfy the label-linearity property. The gradients of the 1 and hinge losses are piecewise constant, and thus do not allow for universality. The Huber loss is also piecewise constant in some areas its domain. These error functions effectively lose information because simply looking at their gradient is insufficient to determine the label. Recurrent meta-learners that take the gradient as input, rather than the label, e.g. Andrychowicz et al. (2016), will also suffer from this loss of information when using these error functions.

7 EXPERIMENTS
Now that we have shown that meta-learners that use standard gradient descent with a sufficiently deep representation can approximate any learning algorithm, we aim to empirically explore the differences between gradient-based and recurrent meta-learners. In particular, we aim to answer the following questions: (1) can a learner trained with MAML further improve from additional gradient steps when learning new tasks at test time, or does it start to overfit? and (2) does the inductive bias of gradient descent enable better few-shot learning performance on tasks outside of the training distribution, compared to learning algorithms represented as recurrent networks?
To study both questions, we will consider two simple few-shot learning domains. The first is 5-shot regression on a family of sine curves with varying amplitude and phase. We trained all models on a uniform distribution of tasks with amplitudes A  [0.1, 5.0], and phases   [0, ]. The second domain is 1-shot character classification using the Omniglot dataset (Lake et al., 2011), following the training protocol introduced by Santoro et al. (2016). In our comparisons to recurrent metalearners, we will use two state-of-the-art meta-learning models: TCML (Mishra et al., 2017) and

7

Under review as a conference paper at ICLR 2018

Figure 3: Learning performance on out-of-distribution tasks as a function of the task variability. Recurrent meta-learners such as TCML and MetaNet acquire learning strategies that are less generalizable than those learned with gradient-based meta-learning.

meta-networks (Munkhdalai & Yu, 2017). In some experiments, we will also compare to a taskconditioned model, which is trained to map from both the input and the task description to the label. Like MAML, the task-conditioned model can be fine-tuned on new data using gradient descent, but is not trained for few-shot adaptation. We include more experimental details in Appendix F.

To answer the first question, we fine-tuned a model trained

using MAML with many more gradient steps than used dur-

ing meta-training. The results on the sinusoid domain, shown

in Figure 2, show that a MAML-learned initialization trained

for fast adaption in 5 steps can further improve beyond 5 gra-

dient steps, especially on out-of-distribution tasks. In con-

trast, a task-conditioned model trained without MAML can

easily overfit to out-of-distribution tasks. We also ran this ex-

periment with the Omniglot dataset. As seen in Figure 4, a

MAML model that was trained with 5 inner gradient steps Figure 4: Comparison of finetuning

can be fine-tuned for 100 gradient steps without leading to from a MAML-initialized network and

any drop in test accuracy. As expected, a model initialized randomly and trained from scratch quickly reaches perfect training accuracy, but overfits massively to the 20 examples.

a network initialized randomly, trained from scratch. Both methods achieve about the same training accuracy. But, MAML also attains good test accu-

Next, we investigate the second question, aiming to compare gradient-based meta-learning with state-of-the-art recurrent meta-learners on a variety of tasks that are related to, but outside of the distribution of training tasks. All three methods achieved similar performance within the distribution of training tasks for 5-way 1-shot Omniglot classification and 5-shot

racy, while the network trained from scratch overfits catastrophically to the 20 examples. Interestingly, the MAMLinitialized model does not begin to overfit, even though meta-training used 5 steps while the graph shows up to 100.

sinusoid regression. In the Omniglot setting, we compare each method's ability to learn to compare

digits that have been sheared or scaled by varying amounts. In the sinusoid regression setting, we

compare on sinusoids with larger amplitudes in the range [5.0, 10.0] and phases in the range [, 2].

The results in Figure 3 and Appendix F show a clear trend that MAML recovers more generalizable

learning strategies. Combined with the theoretical universality results, these experiments indicate

that deep gradient-based meta-learners are not only equivalent in representational power to recurrent

meta-learners, but should also be a considered as a strong contender in settings that contain domain

shift between meta-training and meta-testing tasks, where their strong inductive bias for reasonable

learning strategies provides substantially improved performance.

8 CONCLUSION
In this paper, we show that there exists a form of deep neural network such that the initial weights combined with gradient descent can approximate any learning algorithm. Our findings suggest that, from the standpoint of expressivity, there is no theoretical disadvantage to embedding gradient descent into the meta-learning process. In fact, our experiments show that learning strategies acquired with MAML are more successful when faced with out-of-domain tasks compared to recurrent learners. Furthermore, we show that the representations acquired with MAML are highly resilient to overfitting. These results suggest that gradient-based meta-learning has a number of practical benefits, and no theoretical downsides in terms of expressivity when compared to alternative meta-learning models. Independent of the type of meta-learning algorithm, we formalize what it means for a meta-learner to be able to approximate any learning algorithm in terms of its ability to represent functions of the dataset and test inputs. This formalism provides a new perspective on the learning-to-learn problem, which we hope will lead to further discussion and research on the goals and methodology surrounding meta-learning.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Neural Information Processing Systems (NIPS), 2016.
Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule. In Optimality in Artificial and Biological Neural Networks, 1992.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems (MCSS), 2(4):303­314, 1989.
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. International Conference on Machine Learning (ICML), 2017a.
Chelsea Finn, Tianhe Yu, Tianhao Zhang, Pieter Abbeel, and Sergey Levine. One-shot visual imitation learning via meta-learning. Conference on Robot Learning (CoRL), 2017b.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks. Neural networks, 1989.
David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. International Conference on Learning Representations (ICLR), 2017.
Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International Conference on Artificial Neural Networks. Springer, 2001.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 1989.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural networks, 1990.
Michael Husken and Christian Goerick. Fast learning for problem classes using knowledge based network initialization. In International Joint Conference on Neural Networks (IJCNN), 2000.
Brenden M Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B Tenenbaum. One shot learning of simple visual concepts. In Conference of the Cognitive Science Society (CogSci), 2011.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Learning to generalize: Meta-learning for domain generalization. arXiv preprint arXiv:1710.03463, 2017.
Ke Li and Jitendra Malik. Learning to optimize. International Conference on Learning Representations (ICLR), 2017a.
Ke Li and Jitendra Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017b.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal convolutions. arXiv preprint arXiv:1707.03141, 2017.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. International Conference on Machine Learning (ICML), 2017.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International Conference on Learning Representations (ICLR), 2017.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International Conference on Machine Learning (ICML), 2016.
Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to learn: The meta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approximator. Applied and Computational Harmonic Analysis, 2017.
Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016.
9

Under review as a conference paper at ICLR 2018

A SUPPLEMENTARY PROOFS FOR 1-SHOT SETTING

A.1 PROOF OF LEMMA 4.1

While there are likely a number of ways to prove Lemma 4.1 (copied below for convenience), here we provide a simple, though inefficient, proof of Lemma 4.1.

Lemma 4.1 Let us assume that e(y) can be chosen to be any linear (but not affine) function of y.

Then, we can choose ft, h, {Ai; i > 1}, {Bi; i < N } such that the function

N
f^(x ;  ) = hpost - Aie(y)ki(x, x ); h

(7)

i=1

can approximate any continuous function of (x, y, x ) on compact subsets of Rdim(y).

To prove this lemma, we will proceed by showing that we can choose e, ft, and each Ai and Bi such that the summation contains a complete description of the values of x, x , and y. Then, because hpost is a universal function approximator, f^(x ,  ) will be able to approximate any function of x, x , and y.

Since A1 = I and BN = I, we will essentially ignore the first and last elements of the sum by defining B1 := I and AN := I, where is a small positive constant to ensure positive definiteness. Then, we can rewrite the summation, omitting the first and last terms:

N -1
f^(x ;  )  hpost - Aie(y)ki(x, x ); h

i=2

Next, we will re-index using two indexing variables, j and l, where j will index over the discretization of x and l over the discretization of x .


J-1 L-1



f^(x ;  )  hpost -

Ajle(y)kjl(x, x ); h

j=0 l=0

Next, we will define our chosen form of kjl in Equation 8. We show how to acquire this form in the next section.

Lemma A.1 We can choose ft and each Bjl such that

kjl(x, x ) :=

1 0

if discr(x) = ej and discr(x ) = el otherwise

(8)

where discr(·) denotes a function that produces a one-hot discretization of its input and e denotes the 0-indexed standard basis vector.

Now that we have defined the function kjl, we will next define the other terms in the sum. Our goal is for the summation to contain complete information about (x, x , y). To do so, we will chose e(y) to be the linear function that outputs J  L stacked copies of y. Then, we will define Ajl to be a matrix that selects the copy of y in the position corresponding to (j, l), i.e. in the position j + J  l.
This can be achieved using a diagonal Ajl matrix with diagonal values of 1 + at the positions corresponding to the kth vector, and elsewhere, where k = (j + J  l) and is used to ensure that
Ajl is positive definite.

As a result, the post-update function is as follows:

0

 

...

 

f^(x

;

)  hpost (-v(x, x

, y); h) , where v(x, x


 , y)  


0 y


 , 

0

  

...

  

0

10

Under review as a conference paper at ICLR 2018

where y is at the position j + J  l within the vector v(x, x , y), where j satisfies discr(x) = ej and where l satisfies discr(x ) = el. Note that the vector -v(x, x , y) is a complete description of (x, x , y) in that x, x , and y can be decoded from it. Therefore, since hpost is a universal function approximator and because its input contains all of the information of (x, x , y), the function
f^(x ;  )  hpost (-v(x, x , y); h) is a universal function approximator with respect to its inputs (x, x , y).

A.2 PROOF OF LEMMA A.1

In this section, we show one way of proving Lemma A.1:

Lemma A.1 We can choose ft and each Bjl such that

kjl(x, x ) :=

1 0

if discr(x) = ej and discr(x ) = el otherwise

(8)

where discr(·) denotes a function that produces a one-hot discretization of its input and e denotes the 0-indexed standard basis vector.

Recall that kjl(x, x ) is defined as ~(x; ft, b)T BjTlBjl~(x ; ft, b), where b = 0. Since the gradient with respect to b can be chosen to be any linear function of the label y (see Section 6), we can assume without loss of generality that b = 0.

We will choose ~ and Bjl as follows:


  ~(·; ft, b) := 
  

discr(·) 0 0
discr(·)

if b = 0 otherwise

Bjl =

Ejj Ejl Elj 0

+I

where we use Eik to denote the matrix with a 1 at (i, k) and 0 elsewhere, and I is added to ensure the positive definiteness of Bjl as required in the construction.

Using the above definitions, we can see that:


    ~(x; ft, 0)T BjTl 
   

T

ej 0

if discr(x) = ej

T
0 otherwise
0


   Bjl~(x ; ft, b) 
  

ej 0 0 0

if discr(x ) = el otherwise

Thus, we have proved the lemma, showing that we can choose a ~ and each Bjl such that:



 kjl(x, x ) 

ej

0

0

ej 0

= 1 if discr(x) = ej and discr(x ) = el

otherwise

A.3 FORM OF LINEAR WEIGHT MATRICES

The goal of this section is to show that we can choose a form of W~ , W , and w such that we can simplify the form of z in Equation 1 into the following:

N
z = - Aie(y)~(x; ft, b)T BiT Bi~(x ; ft, b),

i=1

where

A1

=

I,

Ai

=

M

i-1

M

T i-1

for

i

>

1,

Bi

=

M~ i+1

for

i

<

N

and

BN

=

I.

(9)

Recall that we decomposed Wi, , and the error gradient into three parts, as follows:

 W~ i 0 0  Wi :=  0 W i 0 
0 0 wi

 ~(·; ft, b)  (·; ft, b) :=  0 
b

zL(y, f^(x; )) :=

0 e(y) e(y)
(10)

11

Under review as a conference paper at ICLR 2018

where the initial value of b will be 0. The top components, W~ i and ~, have equal dimensions, as do the middle components, W i and 0. The bottom components are scalars. As a result, we can see that z will likewise be made up of three components, which we will denote as z~, z, and z, where,

before the gradient update, z~ =

N i=1

W~ i~(x;

ft),

z

=

0, and z =

0.

Lastly, we construct the

top component of the error gradient to be 0, whereas the middle and bottom components, e(y) and

e(y), can be set to be any linear (but not affine) function of y.

Using the above definitions and noting that ft = ft - ft L = ft, we can simplify the form of z in Equation 1, such that the middle component, z , is the following:

  T

N i-1

i-1


N

T
N



z = -  W j W j e(y)~(x; ft, b)T 

W~ j 

W~ j ~(x ; ft, b)

i=1 j=1

j=1

j=i+1

j=i+1

We aim to independently control the backward information flow from the gradient e and the forward information flow from ~. Thus, choosing all W~ i and W i to be square and full rank, we will set

W~ i = M~ iM~ i-+11

W i = M i--11M i,

so that we have

N
W~ j = M~ i+1
j=i+1

i-1
W j = M i-1,
j=1

for i  {1...N } where M~N+1 = I and M 0 = I. Then we can again simplify the form of z :

N
z = - Aie(y)~(x; ft, b)T BiT Bi~(x ; ft, b),
i=1

where

A1

=

I,

Ai

=

M

i-1

M

T i-1

for

i

>

1,

Bi

=

M~ i+1

for

i

<

N

and

BN

=

I.

(11)

A.4 OUTPUT FUNCTION

In this section, we will derive the post-update version of the output function fout(·; out). Recall that fout is defined as a neural network that approximates the following multiplexer function and its derivatives (as shown possible by Hornik et al. (1990)):

z~ z~
fout z ; out = 1(z = 0)gpre z ; g + 1(z = 0)hpost(z; h).
z z

(12)

The parameters {g, h} are a part of out, in addition to the parameters required to estimate the indicator functions and their corresponding products. Since z = 0 and hpost(z) = 0 when the gradient step is taken, we can see that the error gradients with respect to the parameters in the last term in Equation 12 will be approximately zero. Furthermore, as seen in the definition of gpre in Section 6, the value of gpre(z, g) is also zero, resulting in a gradient of approximately zero for the first indicator function.4
The post-update value of fout is therefore:
z~ z~
fout z ; out  1(z = 0)gpre z ; g + 1(z = 0)hpost(z ; h) = hpost(z ; h)
z z (13)
as long as z = 0. In Appendix A.1, we can see that z is indeed not equal to zero.

4To guarantee that g and h are zero when evaluated at x, we make the assumption that gpre and hpost are neural networks with no biases and nonlinearity functions that output zero when evaluated at zero.

12

Under review as a conference paper at ICLR 2018

B FULL K-SHOT PROOF OF UNIVERSALITY

In this appendix, we provide a full proof of the universality of gradient-based meta-learning in the general case with K > 1 datapoints. This proof will share a lot of content from the proof in the 1-shot setting, but we include it for completeness.

We aim to show that a deep representation combined with one step of gradient descent can approximate any permutation invariant function of a dataset and test datapoint ({(x, y)i; i  1...K}, x ) for K > 1. Note that K does not need to be small.

We will proceed by construction, showing that there exists a neural network function

f^(·; ) such that f^(·;  ) approximates ftarget({(x, y)k}, x ) up to arbitrary precision, where



=



-



1 K

K k=1

L(yk, f (xk; )))

and



is

the

learning

rate.

As we discuss in Section 6,

the loss function L cannot be any loss function, but the standard cross-entropy and mean-squared

error objectives are both suitable. In this proof, we will start by presenting the form of f^ and deriv-

ing its value after one gradient step. Then, to show universality, we will construct a setting of the

weight matrices that enables independent control of the information flow coming forward from the

inputs {xk} and x , and backward from the labels {yk}.

We will start by constructing f^. With the same motivation as in Section 4, we will construct f^(·; )

as the following:

f^(·; ) = fout

N
Wi (·; ft, b); out .

i=1

(·; ft, b) represents an input feature extractor with parameters ft and a scalar bias transformation

variable b,

N i=1

Wi

is

a

product

of

square

linear

weight

matrices,

fout(·,

out)

is

a

readout

function

at the output, and the learned parameters are  := {ft, b, {Wi}, out}. The network structure is

visualized in Figure 1. The input feature extractor and readout function can be represented with

fully connected neural networks with one or more hidden layers, which we know are universal

function approximators, while

N i=1

Wi

corresponds to a set of linear layers.

Since linear layers

are no more expressive than nonlinear layers, this model design is generic, in the sense that such a

network can be represented by a sufficiently deep standard fully connected network.

Next, we will derive the form of the post-update prediction f^(x ;  ). Let zk =

N i=1

Wi

(xk )

and we denote its gradient with respect to the loss as zk L = e(xk, yk). The gradient with respect to any of the weight matrices Wi for a single datapoint (x, y) is given by

 T
i-1


N

T

Wi L(y, f^(xk, )) =  Wj e(x, y)(x; ft, b)T 

Wj .

j=1

j=i+1

Therefore, the post-update value of

N i=1

Wi

=

N i=1

(Wi

-



1 K

k Wi ) is given by

  T

 T 

N K Wi- K

N i-1

i-1

N

 Wj Wj e(xk, yk)(xk; ft, b)T

N
Wj 

Wj-O(2),

i=1

k=1 i=1 j=1

j=1

j=i+1

j=i+1

where we move the summation over k to the left and where we will disregard the last term, assuming that  is comparatively small such that 2 and all higher order terms vanish. In general, these
terms do not necessarily need to vanish, and likely would further improve the expressiveness of the
gradient update, but we disregard them here for the sake of the simplicity of the derivation. Ignoring these terms, we now note that the post-update value of z when x is provided as input into f^(·;  )
is given by

N

z = Wi(x ; ft, b)

(14)

i=1

  T

 T 

- K K

N i-1

i-1

N

 Wj Wj e(xk, yk)(xk; ft, b)T

N
Wj 

Wj(x ; ft, b),

k=1 i=1 j=1

j=1

j=i+1

j=i+1

13

Under review as a conference paper at ICLR 2018

and f^(x ;  ) = fout(z ; out).
Our goal is to show that that there exists a setting of Wi, fout, and  for which the above function, f^(x ,  ), can approximate any function of ({(x, y)k}, x ). To show universality, we will aim independently control information flow from {xk}, from {yk}, and from x by multiplexing forward information from {xk} and x and backward information from {yk}. We will achieve this by decomposing Wi, , and the error gradient into three parts, as follows:

 W~ i 0 0  Wi :=  0 W i 0 
0 0 wi

 ~(·; ft, b)  (·; ft, b) :=  0 
b

0 zk L(yk, f^(xk; )) := e(yk)
e(yk )

(15)

where the initial value of b will be 0. The top components all have equal numbers of rows, as do the

middle components. As a result, we can see that zk will likewise be made up of three components, which we will denote as z~k, zk, and zk. Lastly, we construct the top component of the error gradient

to be 0, whereas the middle and bottom components, e(yk) and e(yk), can be set to be any linear

(but not affine) function of yk. We discuss how to achieve this gradient in the latter part of this

section when we define fout and in Section 6.

In Appendix A.3, we show that we can choose a particular form of W~ i, W i, and wi that will simplify the products of Wj matrices in Equation 14, such that we get the following form for z :

1K z = -
K

N
Aie(yk)~(xk; ft, b)T BiT Bi~(x ; ft, b),

k=1 i=1

(16)

where A1 = I, BN = I, Ai can be chosen to be any symmetric positive-definite matrix, and Bi can be chosen to be any positive definite matrix.

Finally, we need to define the function fout at the output. When a training input xk is passed in, we need fout to propagate information about its corresponding label yk as defined in Equation 15. And, when the test input x is passed in, we need a function defined on z . Thus, we will define fout as a neural network that approximates the following multiplexer function and its derivatives (as shown
possible by Hornik et al. (1990)):

z~ z~
fout z ; out = 1(z = 0)gpre z ; g + 1(z = 0)hpost(z; h),
z z

(17)

where gpre is a linear function with parameters g such that zL = e(y) satisfies Equation 15 (see Section 6) and hpost(·; h) is a neural network with one or more hidden layers. As shown in
Appendix A.4, the post-update value of fout is

z~ fout z ; out = hpost(z ; h).
z

(18)

Now, combining Equations 16 and 18, we can see that the post-update value is the following:

f^(x ;  ) = hpost

- 1 K K

N
Aie(yk)~(xk; ft, b)T BiT Bi~(x ; ft, b); h

k=1 i=1

(19)

In summary, so far, we have chosen a particular form of weight matrices, feature extractor, and
output function to decouple forward and backward information flow and recover the post-update function above. Now, our goal is to show that the above function f^(x ;  ) is a universal learning algorithm approximator, as a function of ({(x, y)k}, x ). For notational clarity, we will use use ki(xk, x ) := ~(xk; ft, b)T BiT Bi~(x ; ft, b) to denote the inner product in the above equation, noting that it can be viewed as a type of kernel with the RKHS defined by Bi~(x; ft, b).5 The

5Due to the symmetry of kernels, this requires interpreting b as part of the input, rather than a kernel hyperparameter, so that the left input is (xk, b) and the right one is (x , b).

14

Under review as a conference paper at ICLR 2018

connection to kernels is not in fact needed for the proof, but provides for convenient notation and an interesting observation. We can now simplify the form of f^(x ,  ) as the following equation:

f^(x ;  ) = hpost

- 1 N K

K
Aie(yk)ki(xk, x ); h

i=1 k=1

(20)

Intuitively, Equation 20 can be viewed as a sum of basis vectors Aie(yk) weighted by ki(xk, x ), which is passed into hpost to produce the output. In Appendix C, we show that we can choose e, ft, h, each Ai, and each Bi such that Equation 20 can approximate any continuous function of ({(x, y)k}, x ) on compact subsets of Rdim(y). As in the one-shot setting, the bias transformation variable b plays a vital role in our construction, as it breaks the symmetry within ki(x, x ). Without such asymmetry, it would not be possible for our constructed function to represent any function of x and x after one gradient step.
In conclusion, we have shown that there exists a neural network structure for which f^(x ;  ) is a universal approximator of ftarget({(x, y)k}, x ).

C SUPPLEMENTARY PROOF FOR K-SHOT SETTING

In Section 5 and Appendix B, we showed that the post-update function f^(x ;  ) takes the following form:

f^(x ;  ) = hpost

- 1 N K

K
Aie(yk)ki(xk, x ); h

i=1 k=1

In this section, we aim to show that the above form of f^(x ;  ) can approximate any function of
{(x, y)k; k  1...K} and x that is invariant to the ordering of the training datapoints {(x, y)k; k  1...K}. The proof will be very similar to the one-shot setting proof in Appendix A.1

Similar to Appendix A.1, we will ignore the first and last elements of the sum by defining B1 to be I and AN to be I, where is a small positive constant to ensure positive definiteness. We will then re-index the first summation over i = 2...N - 1 to instead use two indexing variables j and l
as follows:



f^(x

1 ;  )  hpost - K

J-1 L-1

K

Ajle(yk)kjl(xk, x

); h

j=0 l=0 k=1

As in Appendix A.1, we will define the function kjl to be an indicator function over the values of xk and x . In particular, we will reuse Lemma A.1, which was proved in Appendix A.2 and is copied
below:

Lemma A.1 We can choose ft and each Bjl such that

kjl(x, x ) :=

1 0

if discr(x) = ej and discr(x ) = el otherwise

(8)

where discr(·) denotes a function that produces a one-hot discretization of its input and e denotes the 0-indexed standard basis vector.

Likewise, we will chose e(yk) to be the linear function that outputs J L stacked copies of yk. Then, we will define Ajl to be a matrix that selects the copy of yk in the position corresponding to (j, l), i.e. in the position j + J  l. This can be achieved using a diagonal Ajl matrix with diagonal values of 1 + at the positions corresponding to the nth vector, and elsewhere, where n = (j + J  l) and
is used to ensure that Ajl is positive definite.

15

Under review as a conference paper at ICLR 2018

As a result, the post-update function is as follows:

f^(x ;  )  hpost

- 1 K

K

v(xk, x , yk); h

k=1

0

 

...

 

 

0

 

, where v(x, x

, y)   

yk

, 

0

  

...

  

0

where yk is at the position j + J  l within the vector v(xk, x , yk), where j satisfies discr(xk) = ej and where l satisfies discr(xk) = el.

For discrete, one-shot labels yk, the summation over v amounts to frequency counts of the triplets (xk, x , yk). In the setting with continuous labels, we cannot attain frequency counts, as we do not have access to a discretized version of the label. Thus, we must make the assumption that no two
datapoints share the same input value xk. With this assumption, the summation over v will contain the output values yk at the index corresponding to the value of (xk , x ). For both discrete and continuous labels, this representation is redundant in x , but nonetheless contains sufficient information to decode the test input x and set of datapoints {(x, y)k} (but not the order of datapoints).

Since hpost is a universal function approximator and because its input contains all of the information

of ({(x, y)k}, x ), the function f^(x ;  )  hpost

-

1 K

K k=1

v(xk

,

x

, yk); h

is a universal

function approximator with respect to {(x, y)k} and x .

D PROOF OF THEOREM 6.1

Here we provide a proof of Theorem 6.1:

Theorem 6.1 The gradient of the standard mean-squared error objective evaluated at y^ = 0 is a linear, invertible function of y.

For the standard mean-squared error objective, L(y, y^)

=

1 2

y - y^ 2,

the

gradient

is

y^L(y, 0) = -y, which satisfies the requirement, as A = -I is invertible.

E PROOF OF THEOREM 6.2
Here we provide a proof of Theorem 6.2:
Theorem 6.2 The gradient of the softmax cross entropy loss with respect to the pre-softmax logits is a linear, invertible function of y, when evaluated at 0.
For the standard softmax cross-entropy loss function with discrete, one-hot labels y, the gradient is y^L(y, 0) = c - y where c is a constant vector of value c and where we are denoting y^ as the pre-softmax logits. Since y is a one-hot representation, we can rewrite the gradient as y^L(y, 0) = (C - I)y, where C is a constant matrix with value c. Since A = C - I is invertible, the cross entropy loss also satisfies the above requirement. Thus, we have shown that both of the standard supervised objectives of mean-squared error and cross-entropy allow for the universality of gradientbased meta-learning.

F ADDITIONAL EXPERIMENTAL DETAILS
In this section, we provide two additional comparisons on an out-of-distribution task and using additional gradient steps, shown in Figure 5. We also include additional experimental details.
The recurrent meta-learners were trained using code provided by the authors of the respective papers, using the default model architectures. For MAML with the Omniglot domain, we used the

16

Under review as a conference paper at ICLR 2018
Figure 5: Left: Another comparison with out-of-distribution tasks, varying the phase of the sine curve. There is a clear trend that gradient descent enables better generalization on out-of-distribution tasks compared to the learning strategies acquired using recurrent meta-learners such as TCML. Right: Here is another example that shows the resilience of a MAML-learned initialization to overfitting. In this case, the MAML model was trained using one inner step of gradient descent on 5-way, 1-shot Omniglot classification. Both a MAML and random initialized network achieve perfect training accuracy. As expected, the model trained from scratch catastrophically overfits to the 5 training examples. However, the MAML-initialized model does not begin to overfit, even after 100 gradient steps. convolutional neural network architecture and hyperparameters used by Finn et al. (2017a). For MAML in the sinusoid domain, we used a fully-connected network with two hidden layers of size 100, ReLU nonlinearities, and a bias transformation variable of size 10 concatenated to the input. This model was trained for 70,000 meta-iterations with 5 inner gradient steps of size  = 0.001. We evaluated the MAML and TCML models for 1200 trials, reporting the mean and 95% confidence intervals. For computational reasons, we evaluated the MetaNet model using 600 trials, also reporting the mean and 95% confidence intervals. Following prior work (Santoro et al., 2016), we downsampled the Omniglot images to be 28 × 28. When scaling or shearing the digits to produce out-of-domain data, we transformed the original 105 × 105 Omniglot images, and then downsampled to 28 × 28.
17

