Under review as a conference paper at ICLR 2018
DEEP EPITOME FOR UNRAVELLING GENERALIZED HAMMING NETWORK: A FUZZY LOGIC INTERPRETATION OF DEEP LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
This paper gives a rigorous analysis of trained Generalized Hamming Networks (GHN) proposed by Fan (2017) and discloses an interesting finding about GHNs, i.e., stacked convolution layers in a GHN is equivalent to a single yet wide convolution layer. The revealed equivalence, on the theoretical side, can be regarded as a constructive manifestation of the universal approximation theorem Cybenko (1989); Hornik (1991). In practice, it has profound and multi-fold implications. For network visualization, the constructed deep epitomes at each layer provide a visualization of network internal representation that does not rely on the input data. Moreover, deep epitomes allows the direct extraction of features in just one step, without resorting to regularized optimizations used in existing visualization tools.
1 INTRODUCTION
Despite the great success in recent years, neural networks have long been criticized for their blackbox natures and the lack of comprehensive understanding of underlying mechanisms e.g. in Bentez et al. (1997); Gu¨lc¸ehre & Bengio (2013); Shwartz-Ziv & Tishby (2017); Shrikumar et al. (2017). The earliest effort to interpret neural computing in terms of logic inferencing indeed dated back to the seminal paper of Mcculloch & Pitts (1943), followed by recent attempts to provide explanations from a multitude of perspectives (reviewed in Section 2).
As an alternative approach to deciphering the mysterious neural networks, various network visualization techniques have been actively developed in recent years (e.g. Gru¨n et al. (2016); Seifert et al. (2017) and references therein). Such visualizations not only provide general understanding about the learning process of networks, but also disclose operational instructions on how to adjust network architecture for performance improvements. Majority of visualization approaches probe the relations between input data and neuron activations, by showing either how neurons react to some sample inputs or, reversely, how desired activations are attained or maximized with regularized reconstruction of inputs Erhan et al. (2009); Kavukcuoglu et al. (2010); Zeiler et al. (2010); Simonyan et al. (2013); Mahendran & Vedaldi (2015); Yosinski et al. (2015); Alain & Bengio (2016). Input data are invariably used in visualization to probe how the information flow is transformed through the different layers of neural networks. Although insightful, visualization approaches as such have to face a critical open question: to what extend the conclusions drawn from the analysis of sample inputs can be safely applied to new data?
In order to furnish confirmatory answer to the above-mentioned question, ideally, one would have to employ a visualization tool that is independent of input data. This ambitious mission appears impossible at a first glance -- the final neuron outputs cannot be readily decomposed as the product of inputs and neuron weights because the thresholding in ReLU activations is input data dependent. By following the principle of fuzzy logic, Fan (2017) recently demonstrated that ReLUs are not essential and can be removed from the so called generalized hamming network (GHN). This simplified network architecture, as reviewed in section 3, facilitates the analysis of neuron interplay based on connection weights only. Consequently, stacked convolution layers can be merged into a single hidden layer without taking into account of inputs from previous layers. Equivalent weights of the merged GHN, which is called deep epitome, are computed analytically without resorting to
1

Under review as a conference paper at ICLR 2018
any learning or optimization processes. Moreover, deep epitomes constructed at different layers can be readily applied to new data to extract hierarchical features in just one step (section 4).
2 RELATED WORK
Despite the great success in recent years, neural networks have long been criticized for their blackbox natures e.g. in Bentez et al. (1997): "they capture hidden relations between inputs and outputs with a highly accurate approximation, but no definitive answer is offered for the question of how they work". The spearhead Mcculloch & Pitts (1943) attempted to interpret neural computing in terms of logic inferencing, followed by more "recent" interpretations e.g. in terms of the universal approximation framework Cybenko (1989); Hornik (1991), restricted Boltzmann machine Hinton & Salakhutdinov (2006), information bottleneck theory Shwartz-Ziv & Tishby (2017), Nevertheless the mission is far from complete and the training of neural networks (especially deep ones) is still a trail-and-error based practice.
The early 1990s witnessed the birth of fuzzy neural networks (FNN) Keller et al. (1992); Gupta & Rao (1994) which attempted to furnish neural networks with the interpretability of fuzzy logic Zadeh (1965); Zimmermann (2001); Belohlavek et al. (2017). On the other hand, neural networks have been used as a computational tool to come up with both membership functions and fuzzy inference rules Furukawa & Yamakawa (1995); Takagi (2000). This joint force endeavour remains active in the new millennium e.g. Pedrycz & Succi (2002); Liu & Li (2004); Nauck & Nu¨rnberger (2013); Kar et al. (2014); Hu et al. (2016). Nevertheless, FNNs have been largely overlooked nowadays by scholars and engineers in machine learning (ML) community, partially due to the lack of convincing demonstrations on ML problems with large datasets. The exception case is the recent Fan (2017), which re-interpreted celebrated ReLU and batch normalization with a novel Generalized Hamming Network (GHN) and demonstrated the state-of-the-art performances on a variety of machine learning tasks. While GHNs adopted deep networks with multiple convolution layers, in this paper, we will show how to merge multiple stacked convolution layers into a single yet wide convolution layer.
There are abundant empirical evidences backing the belief that deep network structures is preferred to shallow ones Goodfellow et al. (2016), on the other hand, it was theoretically proved by the universal approximation theorem that, a single hidden layer network with non-linear activation can well approximate any arbitrary decision functions Cybenko (1989); Hornik (1991). Also, empirically, it was shown that one may reduce depth and increase width of network architecture while still attaining or outperforming the accuracies of deep CNN Ba & Caurana (2013) and residual network Zagoruyko & Komodakis (2016). Nevertheless, it was unclear how to convert a trained deep network into a shallow equivalent network. To this end, the equivalence revealed in Section 3 can be treated as a constructive manifestation of the universal approximation theorem.
Various network visualization techniques have been actively developed in recent years, with Erhan et al. (2009) interpreting high level features via maximizing activation and sampling; Kavukcuoglu et al. (2010); Zeiler et al. (2010) learning hierarchical convolutional features via energy or cost minimization; Simonyan et al. (2013) computing class saliency maps for given images; Mahendran & Vedaldi (2015) reconstructing images from CNN features with an natural image prior applied; Yosinski et al. (2015) visualizing live activations as well as deep features via regularized optimization; Alain & Bengio (2016) monitoring prediction errors of individual linear classifiers at multiple iterations. Since all these visualization methods are based on the analysis of examples, the applicability of visualization methods to new data is questionable and no confirmatory answers are provided in a principled manner.
The name "deep epitome" is reminiscent of Jojic et al. (2003); Cheung et al. (2005); Jojic et al. (2010); Chu et al. (2010), in which miniature, condensed "epitomes" consisting of the most essential elements were extracted to model and reconstruct a set of given images. During the learning process, the self-similarity of image(s), either in terms of pixel-to-pixel comparison or spatial configuration, was exploited and a "smooth" mapping between epitome and input image pixels was estimated.
2

Under review as a conference paper at ICLR 2018

3 DEEP EPITOME
We briefly review generalized hamming networks (GHN) introduced in Fan (2017) and present in great detail a method to derive the deep epitome of a trained GHN. Note that we follow notations in Fan (2017) with minor modifications for the sake of clarity and brevity.

3.1 REVIEW OF GHN
According to Fan (2017), the cornerstone notion of generalized hamming distance (GHD) is defined as g(a, b) := a  b = a + b - 2 · a · b for any a, b  R. Then the negative GHD is used to quantify the similarity between neuron inputs x and weights w:

-g(w, x) = 2 w · x - 1 LL

L

wl

-

1 L

L

xl,

l=1 l=1

(1)

in which L denotes the length of neuron weights e.g. in convolution kernels, and g(w, x) is the

arithmetic mean of generalized hamming distance between elements of w and x. By dividing the

constant

2 L

,

(1)

becomes

the

common

representation

of

neuron

computing

(w

· x + b)

provided

that:

1L

L

b=- 2

wl + xl .

l=1 l=1

(2)

It was proposed by Fan (2017) that neuron bias terms should follow the condition (2) analytically without resorting to an optimization approach. Any networks that fulfil this requirement are thus called generalized hamming networks (GHN). In the light of fuzzy logic, the negative of GHD quantifies the degree of equivalence between inputs x and weights w, i.e. the fuzzy truth value of the statement x  w where  denotes a fuzzy equivalence relation. Moreover, g(x, x) leads to a measurement of fuzziness in x, which reaches the maximal fuzziness when x = 0.5 and monotonically decreases when x deviates from 0.5. Also it can be shown that GHD followed by a non-linear activation induces a fuzzy XOR connective Fan (2017).
When viewed in this GHN framework, the ReLU activation function max(0, 0.5-g(x, w)) actually sets a minimal hamming distance threshold of 0.5 on neuron outputs. Fan (2017) then argued that the use of ReLU activation is not essential because bias terms are analytically set in GHNs. Fan (2017) reported only negligible influences when ReLU was completely skipped for the easy MNIST classification problem. For more challenging CIFAR10/100 classifications, removing ReLUs merely prolonged the learning process but the final classification accuracies remained almost the same. To this end, we restrict our investigation in this paper to those GHNs which have no ReLUs. As illustrated below, this simplification allows for strict derivation of deep epitome from individual convolution layers in GHNs.

3.2 GENERALIZED HAMMING DISTANCE AND EPITOME
Fan (2017) postulated that one may analyse the entire GHN in terms of fuzzy logic inference rules, yet no elaboration on the analysis was given. Inspired by the universal approximation framework, we show below how to unravel a deep GHN by merging multiple convolution layers into a single hidden layer.
We first reformulate the convolution operation in terms of generalized hamming distance (GHD) for each layer, then illustrate how to combine multiple convolution operations across different layers. As said, this combination is only made possible with GHNs in which bias terms strictly follow condition (2). Without loss of generality, we illustrate derivations and proofs for 1D neuron inputs and weights (with complete proofs elaborated in appendix A). Nevertheless, it is straightforward to extend the derivation to 2D or high dimensions. And appendices B to D illustrate deep epitomes of GHNs trained for 2D MNIST and CIFAR10/100 image classifications.
Definition 1. For two given tuples xK = {x1, . . . , xK }, yL = {y1, . . . , yL}, the hamming outer product, denoted , is a set of corresponding elements xK yL = xk  yl k = 1 . . . K; l =

3

Under review as a conference paper at ICLR 2018

Figure 1: Left panel: example tuples X3, A2, B2; Middle: Hamming outer products X3 A2, X3 A2 B2; Right: Hamming convolutions X3  A2, X3  A2  B2 and corresponding
epitomes. Indices 1 , 2 ... ,... denote subsets S(1), S(2) . . . S(n) in which element indices satisfying k + (L - l) = n and k + (L - l) + (M - m) = n.

1 . . . L , where  denotes the generalized hamming distance operator. Then the product has following properties,

1. non-commutative: in general xK yL = yL xK but they are permutation equivalent, in the sense that there exist permutation matrices P and Q such that xK yL = P(yL xK )Q.

2. non-linear: in contrast to the standard outer product which is bilinear in each of its entry, the hamming outer product is non-linear since in general xK (yL + zL) = (xK yL) + (xK zL) and (µxK ) yL = xK (µyL) = µ(xK yL) where µ  R is a scalar. Therefore, the hamming
outer product defined as such is a pseudo outer product.

3. associative: xK yL zM = xK yL zM = xK yL zM because of the associativity of GHD. This property holds for arbitrary number of tuples.

4. iterated operation: the definition can be trivially extended to multiple tuples xK yL . . . zM = xk  yl  . . . , zm k = 1 . . . K; l = 1 . . . L; . . . ; m = 1, . . . M .

Definition 2. The convolution of hamming outer product or hamming convolution, denoted , of two tuples is a binary operation that sums up corresponding hamming outer product entries:


xK yL :=

xk  yl for n = 1, . . . , K + L - 1

(3)

(k,l)S(n)

where the subsets S(n) := {(k, l) k + (L - l) = n} for n = 1, . . . , K + L - 1, and the union of all

subsets constitute a partition of all indices

S(n) = (k, l) k = 1 . . . K; l = 1 . . . L .

n=1,...,K+L-1

The hamming convolution has following properties,

1. commutative: xK  yL = yL  xK since the partition subsets S(n) remains the same.

2. non-linear: this property is inherited from the non-linearity of the hamming outer product.
3. non-associative: in general xK  yL  zM = xK  yL  zM since the summation of GHDs is non-associative. Note this is in contrast to the associativity of the hamming outer product.

4. iterated operation: likewise, the definition can be extended to multiple tuples

xK  yL . . . zM =

xk yl . . .zm for n = 1, . . . , K +(L-1)+. . .+(M -1) .

(k,l,...,m)S(n)

Figure 1 illustrates an example in which GHDs are accumulated through two consecutive convolutions. Note that the conversion from the hamming outer products to its convolution is non-invertible,

4

Under review as a conference paper at ICLR 2018

Figure 2: The hamming convolution of two banks of epitomes. Remarks: a) for the inputs A, B the

number of epitomes Ma Md = Mb, Cd = Ca, Ld

must be the same as the number of = (La + Lb - 1). b) the notation

channels  refers

Cb; and for the to the hamming

output bank convolution

between two banks of epitomes (see Definition 5 for details). The convolution of two single-layered

epitomes is treated as a special case with all Ma, Ca, Mb, Cb = 1. c) the notation refers to the

summation of multiple epitomes of the same length, which is defined in Definition 7. d) multiple

(coloured) epitomes in D correspond to different (coloured) epitomes in B; and different (shaded)

channels in D correspond to different (shaded) channels of inputs in A.

in the sense that, it is impossible to recover individual summands xk  yl from the summation (k,l)S(n) xk  yl. As proved in proposition 4, it is possible to compute the convolution of tu-
ples in two (or more) stacked layers without explicitly recovering individual outer product entries of each layer. Due to the non-linearity of the hamming convolutions, computing the composite of two hamming convolutions is non-trivial as elaborated in Section 3.3. In order to illustrate how to carry out this operation, let us first introduce the epitome of a hamming convolution as follows.
Definition 3. An epitome consists of a set of N pairs E = (gn, sn), n = 1, . . . , N where gn denotes the summation of GHD entries from some hamming convolutions, sn the number of summands or the cardinality of the subset S(n) defined above, and N is called the length of the epitome.

A normalized epitome is an epitome with sn = 1 for all n = 1, . . . N . Any epitome can then be normalized by setting (gn/sn, 1) for all elements. A normalized epitome may also refer to input data x or neuron weights w that are not yet involved in any convolution operations. In the latter
case, gn is simply the input data x or neuron weights w.

Remark: the summation of GHD entries gn is defined abstractly, and depending on differ-

ent scenarios, the underlying outer product may operate on arbitrary number of tuples gn =

xK  yL . . . zM (n) =

xk  yl . . .  zm.

(k,l,...,m)S(n)

Fuzzy logic interpretation: in contrast to the traditional signal processing point of view, in which neuron weights w are treated as parameters of linear transformation and bias terms b are appropriate thresholds for non-linear activations, the generalized hamming distance approach treats w as fuzzy templates and sets bias terms analytically according to (2). In this view, the normalization gn/sn is nothing but the mean GHD of entries in the subset S(n), which indicates a grade of fitness (or a fuzzy set) between templates w and inputs x at location n. This kind of arithmetic mean operator has been used for aggregating evidences in fuzzy sets and empirically performed quite well in decision making environments (e.g. see Zimmermann (2001)).

Still in the light of signal processing, the generalized hamming distance naturally induces an information enhancement and suppression mechanism. Since the gradient of g(x, w) with respect to x is 1 - 2w, the information in x is then either enhanced or suppressed according to w : a) the output g(x, w) is always x for w = 0 (conversely 1 - x for w = 1) with no information loss in x; b) for w = 0.5, the output g(x, w) is always 0.5 regardless of x, thus input information in x is completely suppressed; c) for w < 0.0 or w > 1.0 information in x is proportionally enhanced. It was indeed observed, during the learning process in our experiments, a small faction of prominent

5

Under review as a conference paper at ICLR 2018

feature pixels in weights w gradually attain large positive or negative values, so that corresponding input pixels play decisive roles in classification. On the other hand, large majority of obscure pixels remain in the fuzzy regime near 0.5, and correspondingly, input pixels have virtually no influence on the final decision (see experimental results in Section 4). This observation is also in accordance with the information compression interpretation advocated by Shwartz-Ziv & Tishby (2017), and the connection indicates an interesting research direction for future work.

3.3 DEEP EPITOME

This subsection only illustrates main results concerning how to merge multiple hamming convolution operations in stacked layers into a single-layer of epitomes i.e. deep epitome. Detailed proofs are given in appendix A.
Notation: for the sake of brevity, let [Ma ALCaa ] denote a bank of epitomes: mALCaa m = 1, . . . Ma , where ACLaa = AcLa c = 1, . . . Ca are Ca-channels of length-La epitomes, and Ma is the number of epitomes as such in the bank or set [A]. Figure 2 illustrates example banks of epitomes and two operations defined on them (also see Appendix A for detailed definition of ).

Theorem 10. A generalized hamming network consisting of multiple convolution layers, is equivalent to a bank of epitome, called deep epitome [ D ], which can be computed by iteratively applying the composite hamming convolution in equation (8) to individual layer of epitomes:

 
[ D ] := [Ma ALCaa ] [Mb BLCbb ] . . . [Mz ZCLzz ],

(10)

in which = Ca is the number of channels in the first bank A, = Mz is the number of epitomes in the last bank Z, and = La +(Lb -1)+. . .+(Lz -1) is the length of composite deep epitome. Note that for the hamming convolution to be a valid operation, the number of epitomes in the previous
layer and the number channels in the current layer must be the same e.g. Cb = Ma.

Proof. For given inputs represented as a bank of normalized epitomes [Mx XLCxx ] the final network output [Mz YCLxy ] is obtained by recursively applying equation (8) to outputs from the previous layers, and factoring out the input due to the associativity proved in proposition 9:

[Mz YCLxy ] =



[Mx XCLxx ]

[Ma ALCaa ]


[Mb BCLbb ]


. . . [Mz ZCLcz ]


= [Mx XLCxx ]

 

[Ma ALCaa ]

[Mb BLCbb ]

. . . [Mz ZLCcz ] .

(11)

[D ]

Remark: due to the non-linearity of underlying hamming outer products, to prove the associativity of the convolution of epitomes is by no means trivial (see proposition 9). In essence, we have to use proposition 4 to compute the convolution of two epitomes even though individual entries of the underlying hamming outer product are not directly accessible. Consequently, the updating rule outlined in equations (4) and (5) play the crucial role in setting due bias terms analytically for generalized hamming networks (GHN), as opposed to the optimization approach often adopted by many non-GHN deep convolution networks.
Fuzzy logic inferencing with deep epitomes: Eq. (11) can be treated as a fuzzy logic inferencing rule, with which elements of input x are compared with respect to corresponding elements of deep epitomes d. More specifically, the negative of GHD quantifies the degree of equivalence between inputs x and epitome weights d, i.e. the fuzzy truth value of the assertion x  d where  denotes a fuzzy logical biconditional. Therefore, output scores in y indicate the grade of fuzzy equivalences truth values between x and the shifted d at different spatial locations. This inferencing rule, in the same vein of Fan (2017), is applicable to either a single layer neuron weights or the composite deep epitomes as proved by (11).
6

Under review as a conference paper at ICLR 2018

102
102 101 102 101 100

-20

0

-20

0

-20

0

20 20 20

103 102 103

-20

0

20

102
103 102 101 100

-20

0

-20

0

20 20

104

103
104 103 102 101 100
104 103 102 101 100

-50 -50 -50

0 0 0

50 50 50

Figure 3: Histograms of normalized deep epitomes at different layers/iterations for GHN trained with MNIST classification. Left to right: layers 1,2,3. Top, middle and bottom rows: iteration 200, 1000 and 10000 respectively.

Constructive manifestation of the universal approximation theorem: it was proved that a single hidden layer network with non-linear activation can well approximate any arbitrary decision functions Cybenko (1989); Hornik (1991), yet it was also argued by Goodfellow et al. (2016) that such a single layer may be infeasibly large and may fail to learn and generalize correctly. Theorem 10 proves that such a simplified single hidden layer network can actually be constructed from a trained GNH. In this sense Theorem 10 illustrates a concrete solution which materializes the universal approximation theorem.
4 DEEP EPITOME FOR NETWORK VISUALIZATION
We illustrate below deep epitomes extracted from three generalized hamming networks trained with MNIST, CIFAR10/100 classification respectively. Detailed descriptions about the network architectures (number of layers, channels etc.) are included in the appendix.
4.1 DATA INDEPENDENT VISUALIZATION OF DEEP EPITOMES
Deep epitomes derived in the previous section allows one to build up and visualize hierarchical features in an on-line manner during the learning process. This approach is in contrast to many existing approaches, which often apply additional optimization or learning processes with various type of regularizations e.g. in Erhan et al. (2009); Zeiler et al. (2010); Simonyan et al. (2013); Mahendran & Vedaldi (2015); Yosinski et al. (2015). Figures 5, 8 and 11, 12 in appendices illustrate deep epitomes learnt by three generalized hamming networks for the MNIST and CIFAR10/100 image classification tasks. It was observed that geometrical structures of hierarchical features were formed at different layers, rather early during the learning process (e.g. 1000 out of 10000 iterations). Substantial follow up efforts were invested on refining features for improved details. The scrutinization of normalized epitome histograms in Figure 3 showed that a majority of pixel values remain relatively small during the learning process, while a small fraction of epitome weights gradually accumulate large values over thousands of iterations to form prominent features.
The observation of sparse features has been reported and interpreted in terms of sparse coding e.g. Papyan et al. (2016) or the information compression mechanism as advocated by Shwartz-Ziv & Tishby (2017). Following Fan (2017) we adopt the notion of fuzziness (also reviewed in Section 3.1) to provide a fuzzy logic interpretation: prominent features correspond to neuron weights with low fuzziness. It was indeed observed in Figure 4 that fuzziness of deep epitomes in general decrease during the learning process despite of fluctuations at some layers. The inclination towards reduced fuzziness seems in accord with the minimization of classification errors, although the fuzziness is not explicitly minimized.
Finally we re-iterate that the internal representation of deep epitomes is input data independent. For instance in MNIST handwritten images, it is certain constellations of strokes instead of digits that are learnt at layer 3 (see Figure 5). The matching of arbitrary input data with such "fuzzy templates" is then quantified by the generalized hamming distance, and can be treated as generic fuzzy logic
7

Under review as a conference paper at ICLR 2018

Fuzziness Fuzziness
Fuzziness

00

-50

-100
-150 0

Layer 0 Layer 1 Layer 2
2500 5000 7500 10000 Iterations

-1000 -2000 -3000 0

Layer 0 Layer 1 Layer 2 Layer 3
50000 100000 150000 Iterations

0 -100 -200 -300 -400
0

200000 Iterations

Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7

Figure 4: Fuzziness in normalized deep epitomes at different layers and learning iterations. Left: a GHN trained with MNIST classification. Middle: CIFAR10. Right: CIFAR100. See section 3.1 for definition of fuzziness.

inferencing rules learnt by GHNs. The matching score measured by GHDs can also be treated as salient features that are subsequently fed to the next layer (see Section 4.2 with Figures 6 and more results in appendices B and C)1.
4.2 DATA DEPENDENT FEATURE EXTRACTION
Feature extraction of given inputs is straightforward with deep epitomes applied according to eq. (11). Figures 6 (and more results in appendices B and C) show example features extracted at different layers of GHN trained on MNIST, CIFAR10/100 image datasets. Clearly extracted features represent different types of salient features e.g. oriented strokes in hand written images, oriented edgelets, textons with associated colours or even rough segmentations in CIFAR images. These features all become gradually more discriminative during the learning process.
It must be noted that the extraction of these hierarchical salient features is not entirely new and has been reported e.g. in Erhan et al. (2009); Kavukcuoglu et al. (2010). Nevertheless, the equivalence of deep epitomes disclosed in Theorem 10 leads to an unique characteristic of GHNs -- deep layer features do not necessarily rely on features extracted from previous layers, instead, they can be extracted in one step using deep epitomes at desired layers. For extremely deep convolution networks e.g. those with over 100 layers, this simplification may bring about substantial reduction of computational and algorithmic complexities. This potential advantage is worth follow up exploration in future research.
5 DISCUSSION AND CONCLUSION
We have proposed in this paper a novel network representation, called deep epitome, which is proved to be equivalent to stacked convolution layers in generalized hamming networks (GHN). Theoretically this representation provides a constructive manifestation for the universal approximation theorem Cybenko (1989); Hornik (1991), which states that a single layered network, in principle, is able to approximate any arbitrary decision functions up to any desired accuracy. On the other hand, it is a dominant belief Goodfellow et al. (2016), which is supported by abundant empirical evidences, that deep structures play an indispensable role in decomposing the combinatorial optimization problem into layer-wise manageable sub-problems. We concur with the view and supplement with our demonstration that, a trained deep GHN can be converted into a simplified networks for the sake of high interpretability, reduced algorithmic and computational complexities.
The success of our endeavours lies in the rigorous derivation of convolving epitomes across different layers in eq. (4) and (5), which set due bias terms analytically without resorting to optimizationbased approaches. Consequently, deep epitomes at all convolution layers can be computed without using any input data. Moreover, deep epitomes can be used to extract hierarchical features in just one step at any desired layers. In the light of fuzzy logic, the normalized epitome (definition 3) encodes a grade of fitness between the learnt templates and given inputs at certain spatial locations. This fuzzy logic interpretation furnishes a refreshing perspective that, in our view, will open the black box of deep learning eventually.
1This learnt "fuzzy template" is reminiscent of epitomes in Jojic et al. (2003) and gives rise to the name deep epitome.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. CoRR, abs/1610.01644, 2016.
Lei Jimmy Ba and Rich Caurana. Do deep nets really need to be deep? CoRR, abs/1312.6184, 2013.
R. Belohlavek, J.W. Dauben, and G.J. Klir. Fuzzy Logic and Mathematics: A Historical Perspective. Oxford University Press, 2017.
Jos Manuel Bentez, Juan Luis Castro, and Ignacio Requena. Are artificial neural networks black boxes? IEEE Trans. Neural Networks, 8(5):1156­1164, 1997.
Vincent Cheung, Brendan Frey, and Nebojsa Jojic. Video epitomes. In Proceedings of the Computer Vision and Pattern Recognition, CVPR '05, 2005.
Xinqi Chu, Shuicheng Yan, Liyuan Li, Kap Luk Chan, and Thomas S. Huang. Spatialized epitome and its applications. In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010, San Francisco, CA, USA, 13-18 June 2010, pp. 311­318, 2010.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4):303­314, Dec 1989.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of a deep network. Technical Report 1341, University of Montreal, June 2009. Also presented at the ICML 2009 Workshop on Learning Feature Hierarchies, Montre´al, Canada.
Lixin Fan. Revisit fuzzy neural network: Demystifying batch normalization and ReLU with generalized hamming network. In Advances in Neural Information Processing Systems. 2017.
Masuo Furukawa and Takeshi Yamakawa. The design algorithms of membership functions for a fuzzy neuron. Fuzzy Sets and Systems, 71(3):329 ­ 343, 1995. ISSN 0165-0114. Fuzzy Neural Control.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. 2016.
Felix Gru¨n, Christian Rupprecht, Nassir Navab, and Federico Tombari. A Taxonomy and Library for Visualizing Learned Features in Convolutional Neural Networks. Proceedings of the Workshop on Visualization for Deep Learning at International Conference on Machine Learning (ICML), 48, 2016.
C¸ aglar Gu¨lc¸ehre and Yoshua Bengio. Knowledge matters: Importance of prior information for optimization. CoRR, abs/1301.4083, 2013.
M M Gupta and D H Rao. Invited Review on the principles of fuzzy neural networks. Fuzzy Sets and Systems, 61:1­18, 1994.
Geoffrey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504 ­ 507, 2006.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4 (2):251 ­ 257, 1991.
Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard H. Hovy, and Eric P. Xing. Harnessing deep neural networks with logic rules. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers, 2016.
Nebojsa Jojic, Brendan J. Frey, and Anitha Kannan. Epitomic analysis of appearance and shape. In Proceedings of the Ninth IEEE International Conference on Computer Vision - Volume 2, ICCV '03, pp. 34­, 2003.
Nebojsa Jojic, Alessandro Perina, and Vittorio Murino. Structural epitome: a way to summarize one's visual experience. In Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada., pp. 1027­1035, 2010.
9

Under review as a conference paper at ICLR 2018
Samarjit Kar, Sujit Das, and Pijush Kanti Ghosh. Review article: Applications of neuro fuzzy systems: A brief review and future outline. Appl. Soft Comput., 15:243­259, February 2014. ISSN 1568-4946.
Koray Kavukcuoglu, Pierre Sermanet, Y lan Boureau, Karol Gregor, Michael Mathieu, and Yann L. Cun. Learning convolutional feature hierarchies for visual recognition. In J. Lafferty, C. Williams, J. Shawe-taylor, R.s. Zemel, and A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 1090­1098. 2010.
J Keller, R. Yager, and H Tahani. Neural Network Implementation of Fuzzy Logic. Fuzzy Sets and Systems, 45(1), 1992.
Puyin Liu and Hongxing Li. Fuzzy Neural Network Theory and Application. World Scientific Press, 2004. ISBN 9812387862.
Aravindh Mahendran and Andrea Vedaldi. Visualizing deep convolutional neural networks using natural pre-images. CoRR, abs/1512.02017, 2015.
Warren Mcculloch and Walter Pitts. A logical calculus of ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5:127­147, 1943.
Detlef D. Nauck and Andreas Nu¨rnberger. Neuro-fuzzy Systems: A Short Historical Review, pp. 91­109. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013.
Vardan Papyan, Yaniv Romano, and Michael Elad. Convolutional neural networks analyzed via convolutional sparse coding. CoRR, abs/1607.08194, 2016.
Witold Pedrycz and Giancarlo Succi. fXOR fuzzy logic networks. Soft Computing, 7, 2002. Christin Seifert, Aisha Aamir, Aparna Balagopalan, Dhruv Jain, Abhinav Sharma, Sebastian Grottel,
and Stefan Gumhold. Visualizations of Deep Neural Networks in Computer Vision: A Survey, pp. 123­144. Springer International Publishing, Cham, 2017. Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. CoRR, abs/1704.02685, 2017. Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. CoRR, abs/1703.00810, 2017. Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. CoRR, abs/1312.6034, 2013. Hideyuki Takagi. Fusion Technology of Neural Networks and Fuzzy Systems : A Chronicled Progression from the Laboratory to Our Daily. Journal of Applied Mathematics, 10(4):1­20, 2000. Jason Yosinski, Jeff Clune, Anh Mai Nguyen, Thomas J. Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. CoRR, abs/1506.06579, 2015. L.A. Zadeh. Fuzzy sets. Information Control, 8:338­353, 1965. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016. M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus. Deconvolutional networks. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 2528­2535, June 2010. H.-J. Zimmermann. Fuzzy Set Theory -- and Its Applications. Kluwer Academic Publishers, Norwell, MA, USA, 2001. ISBN 0-7923-9624-3.
10

Under review as a conference paper at ICLR 2018

APPENDIX A

Definition 1. For two given tuples xK = {x1, . . . , xK }, yL = {y1, . . . , yL}, the hamming outer product, denoted , is a set of corresponding elements xK yL = xk  yl k = 1 . . . K; l = 1 . . . L , where  denotes the generalized hamming distance operator. Then the product has following properties,
1. non-commutative: in general xK yL = yL xK but they are permutation equivalent, in the sense that there exist permutation matrices P and Q such that xK yL = P(yL xK )Q.
2. non-linear: in contrast to the standard outer product which is bilinear in each of its entry, the hamming outer product is non-linear since in general xK (yL + zL) = (xK yL) + (xK zL) and (µxK ) yL = xK (µyL) = µ(xK yL) where µ  R is a scalar. Therefore, the hamming outer product defined as such is a pseudo outer product.
3. associative: xK yL zM = xK yL zM = xK yL zM because of the associativity of GHD. This property holds for arbitrary number of tuples.
4. iterated operation: the definition can be trivially extended to multiple tuples xK yL . . . zM = xk  yl  . . . , zm k = 1 . . . K; l = 1 . . . L; . . . ; m = 1, . . . M .

Proof. associativity: by definition it suffices to prove element-wise (xk yl)zm = xk (yl zm) because of the associativity of the generalized hamming distance.

non-linearity: by definition xK (yL + zL) has elements xk  (yl + zl), then it suffices to prove non-linearity for each element i.e. xk  (yl + zl) = xk + (yl + zl) - 2xk(yl + zl) = (xk + yl - 2xkyl) + (xk + zl - 2xkzl) = (xk  yl) + (xk  zl). Similarly, (µxk  yl) = µxk + yl - 2µxkyl = xk + µyl - 2µxkyl = µ(xk + yl - 2xkyl) in general.

Definition 2. The convolution of hamming outer product or hamming convolution, denoted , of two tuples is a binary operation that sums up corresponding hamming outer product entries:


xK yL :=

xk  yl for n = 1, . . . , K + L - 1
(k,l)S(n)

(3)

where the subsets S(n) := {(k, l) k + (L - l) = n} for n = 1, . . . , K + L - 1, and the union of all

subsets constitute a partition of all indices

S(n) = (k, l) k = 1 . . . K; l = 1 . . . L .

n=1,...,K+L-1

The hamming convolution has following properties,

1. commutative: xK  yL = yL  xK since the partition subsets S(n) remains the same.

2. non-linear: this property is inherited from the non-linearity of the hamming outer product.
3. non-associative: in general xK  yL  zM = xK  yL  zM since the summation of GHDs is non-associative. Note this is in contrast to the associativity of the hamming outer product.

4. iterated operation: likewise, the definition can be extended to multiple tuples

xK  yL . . . zM =

xk yl . . .zm for n = 1, . . . , K +(L-1)+. . .+(M -1) .

(k,l,...,m)S(n)

Proof. non-associativity: by definition it suffices to prove element-wise in general

xk  yl  zm =

xk 

yl  zm .

(n,m)S (n ) (k,l)S(n)

(k,n)S (n )

(l,m)S(n)

Definition 3. An epitome consists of a set of N pairs E = (gn, sn), n = 1, . . . , N where gn denotes the summation of GHD entries from some hamming convolutions, sn the number of summands or the cardinality of the subset S(n) defined above, and N is called the length of the
epitome.

A normalized epitome is an epitome with sn = 1 for all n = 1, . . . N . Any epitome can then be normalized by setting (gn/sn, 1) for all elements. A normalized epitome may also refer to input data x or neuron weights w that are not yet involved in any convolution operations. In the latter
case, gn is simply the input data x or neuron weights w.

11

Under review as a conference paper at ICLR 2018

Proposition 4. Given two tuples x = {xk|k = 1 . . . K} and y = {yl|l = 1 . . . L}, then

KL

KL

K

L

(xk  yl) =

xk 

yl + (L - 1) xk + (K - 1) yl.

kl

kl

k

l

Proof.

KL

K L KL

LHS =

(xk + yl - 2xkyl) = L xk + K yl - 2 xk yl

kl

k l kl

KL

KL

K

L

= xk + yl - 2 xk yl + (L - 1) xk + (K - 1) yl

kl

kl

k

l

KL K L

= xk  yl + (L - 1) xk + (K - 1) yl = RHS

kl k l

(4)

Remark: eq. (4) allows one to compute summation of all hamming outer product elements on the
right hand side, even though individual elements xk and yl are unable to recover from the given summands k xk and l yl. The definition below immediately follows and illustrates how to merge elements of two epitomes.

Definition 5. Given two epitomes Ea the convolution of two epitomes Ec =

E=a{(gn,Esbni)s|ngi=ven1,b.y.:.

N },

Eb

=

{(gm,

sm)|m

=

1,

.

.

.

M },

Ec = {(gc , sc )|c = 1, . . . , N + M - 1};

(5a)

where gc =

gn  gm + (sm - 1)gn + (sn - 1)gm ,

(n,m)S(c)

(5b)

sc =

snsm,

(n,m)S(c)

S(c) : = {(n, m)|n + (M - m) = c}.

(5c) (5d)

sn

Proof. For each pair of epitome elements (gn, sn) and (gm, sm) since by definition 3 gn = xk

k=1

is a summation of elements and gm in the same vein, then the summation of hamming outer product

sn sm

elements

(xk  yl) follows eq. (4). The number of elements sc is simply the convolution of

k=1 l=1

sn and sm of two given epitomes.

Remark: this operation is applicable to the case when two epitomes are merged via spatial convolution (see Figure 2 for an example). Note that this merging operation is associative due to the following theorem.

Theorem 6. The convolution of multiple epitomes, as defined in 5, is associative:







Ea Eb Ec = Ea Eb

Ec = Ea

Eb Ec .

(6)

Proof. By definition 5, elements of Ea  Eb are the summations of hamming outer product el-

sn sm

ements denoted by

(xk  yl). Then elements of Ea


Eb


Ec

are

the

summation

k=1 l=1

sn sm sq

of hamming outer product elements

(xk  yl  zi), which are equal to elements of

k=1 l=1 i=1

Ea


Eb


Ec

, due to the associativity of hamming outer products by definition 1.

12

Under review as a conference paper at ICLR 2018

Remark: this associative property is of paramount importance for the derivation of deep epitomes, which factor out the inputs x from subsequent convolutions with neuron weights w.
Definition 7. Given two epitomes of the same size Ea = {(gn, sn)|n = 1, . . . N }, Eb = {(gn, sn)|n = 1, . . . N }, the summation of two epitomes Ec = Ea Eb is trivially defined by element-wise summation:

Ec = {(gn, sn)|n = 1, . . . , N }; where gn = gn + gn,
sn = sn + sn.

(7)

Remark: the summation operation is applicable to the case when epitomes are (iteratively) merged

cross different channels (see Figure 2 for an example). Note that the size of two input epitomes must

be the same, and the size of output epitome remain unchanged. Moreover, the operation is trivially

extended to multiple epitomes

:= E1 E2 . . . EM .

{1,2,...,M }

Notation: for the sake of brevity, let [Ma ALCaa ] denote a bank of epitomes: mALCaa m = 1, . . . Ma , where ALCaa = ALc a c = 1, . . . Ca are Ca-channels of length-La epitomes, and Ma is the number of epitomes as such in the bank or set [A]. Figure 2 illustrates example banks of epitomes and
two operations defined on them.

Definition 8. The composite convolution of two banks of epitomes [Ma ACLaa ] and [Mb BCLbb ] with Ma = Cb, is defined as



[Ma ALCaa ]

[Mb BLCbb ] :=

Ma ,Cb



Ama La
ca

Bmb Lb
cb

ma =1,cb =1

ca = 1, . . . Ca; mb = 1, . . . Mb . (8)

The output of this operation, in turn, is a bank with Mb of Ca-channel length-(La +Lb -1) epitomes denoted as [Md DLCdd ] with Md = Mb, Cd = Ca, Ld = La + Lb - 1. See Figure 2 for an example.
Proposition 9. The composite convolutions of multiple epitome banks, as given in definition 8, is associative:



[Ma ALCaa ]

[Mb BCLbb ]


[Mc CLCcc ] = [Ma ACLaa ]



[Mb BCLbb ]

[Mc CLCcc ]

(9)

Proof. The associativity immediately follows the associativity of Theorem 6 and definition 7.

Remark: this associative property, which is inherited from theorem 6, can be trivially extended to multiple banks and lead to the main theorem of the paper as follows.
Theorem 10. A generalized hamming network consisting of multiple convolution layers, is equivalent to a bank of epitome, called deep epitome [ D ], which can be computed by iteratively applying the composite hamming convolution in equation (8) to individual layer of epitomes:

 
[ D ] := [Ma ALCaa ] [Mb BCLbb ] . . . [Mz ZLCzz ],

(10)

in which = Ca is the number of channels in the first bank A, = Mz is the number of epitomes in the last bank Z, and = La +(Lb -1)+. . .+(Lz -1) is the length of composite deep epitome. Note that for the hamming convolution to be a valid operation, the number of epitomes in the previous
layer and the number channels in the current layer must be the same e.g. Cb = Ma.

Proof. For given inputs represented as a bank of normalized epitomes [Mx XLCxx ] the final network output [Mz YCLxy ] is obtained by recursively applying equation (8) to outputs from the previous layers,

13

Under review as a conference paper at ICLR 2018

and factoring out the input due to the associativity proved in proposition 9:

[Mz YCLxy ] =



[Mx XLCxx ]

[Ma ALCaa ]


[Mb BCLbb ]


. . . [Mz ZCLcz ]


= [Mx XLCxx ]

 

[Ma ALCaa ]

[Mb BLCbb ]

. . . [Mz ZLCcz ] .

[D ]

(11)

14

Under review as a conference paper at ICLR 2018

Network architectures used in Appendices B,C,D:

We summarize in Tables below architectures of three generalized hamming networks trained with MNIST, CIFAR10/100 classification respectively. Note that for kernels with stride 2, we resize original kernels to their effective size (×2) when computing deep epitomes. Also we use averagepooling, instead of max-pooling, in all three networks. Subsequent fully connected layers are also reported although they are not involved in the computation of deep epitomes.

Layers conv1 conv2 conv3 fc1 fc2

Kernel 5x5 5x5 5x5 -

GHN for MNIST classification

Str. resized

Ch I/O

1 5x5

3 / 32

2 10x10

32 / 32

2 10x10

32/ 128

--

* / 1024

--

1024 / 10

Epitome size 5x5 14x14 23x23 -

Layers conv1 conv2 conv3 conv4 fc1 fc2 fc3

Kernel 3x3 3x3 5x5 5x5 -

GHN for CIFAR10 classification

Str. resized

Ch I/O

1 3x3

3 / 64

1 3x3

64 / 64

2 10x10

64 / 256

2 10x10

256 / 256

--

*/1024

--

1024/512

--

512/10

Epitome size 3x3 5x5 14x14 23x23 -

Layers conv1 conv2 conv3 conv4 conv5 conv6 conv7 fc1 fc2 fc3

Kernel 3x3 5x5 5x5 5x5 5x5 5x5 5x5 -

GHN for CIFAR100 classification

Str. resized

Ch I/O

1 3x3

3 / 64

2 10x10

64 / 64

1 5x5

64 / 64

1 5x5

64 / 64

1 5x5

64 / 64

1 5x5

64 / 64

2 10x10

64 / 128

--

*/1024

--

1024/512

--

512/10

Epitome size 3x3 12x12 16x16 20x20 24x24 28x28 37x37 -

15

Under review as a conference paper at ICLR 2018
APPENDIX B: DEEP EPITOMES WITH MNIST HANDWRITTEN RECOGNITION
Figure 5: Deep epitomes at layers 1,2 and 3 for a GHN trained with MNIST classification at iterations 100 and 10000 respectively.
16

Under review as a conference paper at ICLR 2018
Figure 6: Example hierarchical features extracted at layers 1,2, and 3 for a GHN trained with MNIST classification.
17

Under review as a conference paper at ICLR 2018
Figure 7: Example hierarchical features extracted at layers 1,2, and 3 for a GHN trained with MNIST classification.
18

Under review as a conference paper at ICLR 2018
APPENDIX C: DEEP EPITOMES WITH CIFAR10 IMAGE CLASSIFICATION
Figure 8: Top to bottom: deep epitomes for first 64 filters at layers 1,2, 3 and 4 of a GHN trained with CIFAR10 classification. Pseudo colour images correspond to three channels of merged epitomes from the first layer filters. Left column: iteration 100; Right column: iteration 180000.
19

Under review as a conference paper at ICLR 2018
Figure 9: Hierarchical features extracted at layers 1,2,3 and 4 for a GHN trained with CIFAR10 at 180000 iterations. The top-left most image in each panel is the input image, and the rest are features extracted with different epitomes (only first 63 features are shown for layer 4). Pseudo colour images correspond to three channels of features outputs for input RGB colour channels. Note that oriented edgelets (layer 1,2), textons with associated colours (layer 2,3) and rough segmentations (layer 4) are extracted from different layers.
20

Under review as a conference paper at ICLR 2018
Figure 10: Hierarchical features extracted at layers 1,2,3 and 4 for a GHN trained with CIFAR10 at 180000 iterations. The top-left most image in each panel is the input image, and the rest are features extracted with different epitomes (only first 63 features are shown for layer 4). Pseudo colour images correspond to three channels of features outputs for input RGB colour channels. Note that oriented edgelets (layer 1,2), textons with associated colours (layer 2,3) and rough segmentations (layer 4) are extracted from different layers.
21

Under review as a conference paper at ICLR 2018
APPENDIX D: DEEP EPITOMES WITH CIFAR100 IMAGE CLASSIFICATION
Figure 11: Top to bottom: deep epitomes for first 64 filters at layers 1,2, 3 and 4 of a GHN trained with CIFAR100 classification. Pseudo colour images correspond to three channels of merged epitomes from the first layer filters. Left column: iteration 10000; Right column: iteration 30000.
22

Under review as a conference paper at ICLR 2018
Figure 12: Top to bottom: deep epitomes for first 64 filters at layers 5, 6 and 7 of a GHN trained with CIFAR100 classification. Pseudo colour images correspond to three channels of merged epitomes from the first layer filters. Left column: iteration 10000; Right column: iteration 30000.
23

Under review as a conference paper at ICLR 2018
Figure 13: Hierarchical features extracted at layers 1,2,3 and 4 for a GHN trained with CIFAR100 at 10000 iterations. The top-left most image in each panel is the input image, and the rest are features extracted with different epitomes (only first 63 features are shown for different layers). Pseudo colour images correspond to three channels of features outputs for input RGB colour channels.
24

Under review as a conference paper at ICLR 2018
Figure 14: Hierarchical features extracted at layers 5,6 and 7 for a GHN trained with CIFAR100 at 10000 iterations. The top-left most image in each panel is the input image, and the rest are features extracted with different epitomes (only first 63 features are shown for different layers). Pseudo colour images correspond to three channels of features outputs for input RGB colour channels.
25

