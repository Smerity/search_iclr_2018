Under review as a conference paper at ICLR 2018
RECURRENT RELATIONAL NETWORKS FOR COMPLEX
RELATIONAL REASONING
Anonymous authors Paper under double-blind review
ABSTRACT
A core component of human intelligence is the ability to reason about objects and their interactions which is something even state-of-the-art deep learning models struggle with. Santoro et al. (2017) introduced the relational network to add such relational reasoning capacity to deep neural networks but the proposed network is severely limited in the complexity of the reasoning it can perform. We introduce the recurrent relational network which can solve tasks requiring an order of magnitude more steps of reasoning. We apply it to solving Sudoku puzzles and achieve state-of-the-art results solving 96.6% of the hardest Sudoku puzzles. For comparison the relational network fails to solve any puzzles. We also apply our model to the BaBi textual QA dataset solving 19/20 tasks which is competitive with stateof-the-art sparse differentiable neural computers. The recurrent relational network is a general purpose module that can be added to any neural network model to add a powerful relational reasoning capacity.
1 INTRODUCTION
A core component of human intelligence is the ability to reason about objects and their interactions (Spelke et al., 1995; Spelke & Kinzler, 2007). Consider the problem of solving a Sudoku. Sudoku is a popular puzzle game in which 81 cells in a 9x9 grid must be filled with digits 1-9 subject to the constraints that the digits 1-9 are present exactly once in each row, column and 3x3 non-overlapping box. The less cells are given from the start the harder the puzzle. 1
As humans we reason about the puzzle in terms of its cells and how they interact with each other, rather than the puzzle as a whole. We refer to this object and interaction centric thinking as relational reasoning. Solving Sudokus using relational reasoning requires many steps. A common strategy is called elimination, in which you iteratively eliminate digits that are not possible. It's a simple and effective strategy, but it requires many steps. Many other real life problems also require such complex multi-step relational reasoning e.g. evaluating game moves, logical deduction, predicting the future of physical systems, and automated planning and resource allocation, e.g. creating exam timetables, scheduling taxis and designing seating plans.
State-of-the-art Deep Learning approaches struggle with problems requiring even simple relational reasoning (Lake et al., 2016; Santoro et al., 2017). Recently Santoro et al. (2017) took a first step towards solving such problems by proposing the relational network. However, the relational network can only naturally perform a single step of relational reasoning, and the datasets evaluated on require a maximum of three steps.
One key insight from (Santoro et al., 2017) is to split the problem into two components: a perceptual front-end and a relational reasoning module. The task of the perceptual front-end is to recognize objects in the raw input and output a representation of them. The task of the relational reasoning module is to reason about the objects and their interactions. Both modules are trained jointly end-toend, such that the relational reasoning module imposes on the perceptual front-end to recognize and represent objects it can reason about. In computer science parlance we have defined an interface. The interface is that the relational reasoning module must operate on a set of objects described with real valued vectors, and be differentiable. The nice thing about this interface is that it allows
1We invite the reader to solve the Sudoku in the appendix to get a feeling for the difficulty of solving a Sudoku with 17 givens.
1

Under review as a conference paper at ICLR 2018

us to consider and improve each side of the interface in isolation. In this paper we only consider the relational reasoning side of that interface since this is what even state-of-the-art deep learning architectures struggle with. As long as we respect the interface we can rest assured that our relational reasoning module will fit together with any perceptual front-end. We focus on the Sudoku problem as it requires an order of magnitude more steps of relational reasoning than has previously been considered.
Solving Sudokus computationally is in itself not a very interesting goal. Using traditional symbolic hand crafted algorithms e.g. constraint propagation and search it is possible to to solve any Sudoku in fractions of a second. For a good explanation and code see Norvig (2006). Many other symbolic algorithms exists that can also solve Sudokus e.g. dancing links (Knuth, 2000), or integer programming. These algorithms are superior in almost every respect, but one; they don't respect the interface, as they don't operate on a set of vectors, and they're not differentiable. As such they cannot be used in a combined model with a deep learning perceptual front-end.
2 RECURRENT RELATIONAL NETWORKS

x3 o3t ht3

m1t 3 m3t 1

m3t 2 mt23

o1t h1t

mt12 m2t 1

ht2 o2t

x1 x2

Figure 1: Recurrent relational network on a fully connected graph with 3 nodes. The node hidden states hit are highlighted. The dashed lines indicate the recurrent connections. Subscript denote node indices and superscript the step. For a figure of the same graph unrolled over 2 steps see the
appendix.

Let's consider what a relational reasoning module will need in order to implement the elimination strategy in a Sudoku. Not that we will directly implement this strategy, or limit the network to it, but it seems a reasonable minimum it should be able to implement. The elimination strategy works by noting that if a certain cell is given as a 7, you can safely remove 7 as an option from other cells in the same row, column and box. If you do this for all cells, you might end up with cells that only have a single possible digit left. Now you can repeat, removing that digit from other cells in the same row, column and box, and so on. To implement this strategy each cell needs to send a message to each other cell in the same row, column and box saying "I'm a 7, so you can't also be a 7". Each cell should then consider all messages coming in, and update its own state. With the updated state each cell should send out new messages, and so forth.
We will formalize this by considering the Sudoku as a graph. The graph has i  {1, 2, ..., 81} nodes, one for each cell in the Sudoku. Each node has an edge to and from all nodes that is in the same row, column and box in the Sudoku. Each node has a feature vector xi. As per the interface this set of feature vectors x = {x1, x2, ..., x81} are the inputs to our relational reasoning module and would in general be the output of a perceptual front-end. For our Sudoku example each xi encodes the initial cell content and the row and column position. At each step t each node has a hidden state vector hit. We initialize this hidden state to the features, such that hi0 = xi. At each step t, each node sends a message to each other node it has an edge to. We define the message from node i to node j at step

2

Under review as a conference paper at ICLR 2018

t, mtij as follows

mtij = f hit-1, htj-1

(1)

where f , the message function, is a multi-layer perceptron (MLP). This allows the network to learn what kind of messages to send. Since a node needs to consider all the incoming messages we sum them

mt.j =

mtij

iN (j)

(2)

where N (j) are all the nodes that have an edge into node j, i.e. the nodes in the same row, column and box. Finally we update the node hidden state.

hjt = g hjt-1, xj , m.tj

(3)

where g, the node function, is another learned neural network. The dependence on the previous node hidden state hjt-1 allows the network to work towards a solution iteratively instead of forgetting everything at every step. Injecting the feature vector xj at each step like this allows the node function to focus on the messages from the other nodes instead of trying to remember the input.

Now that we have the math in place for sending messages and updating node states we can consider
how to train the network to solve a Sudoku. We train the network in a supervised manner. At
every step each node outputs a probability distribution over the digits 1-9 and we minimize the cross
entropy between this output probability distribution and the target digit from the Sudoku solution. The output probability distribution oti for node i at step t is given by

oti = softmax r hti

(4)

where r is a MLP that maps the node hidden state to the output logits. Given the target digit yi (1-9) for cell i, the cross-entropy node loss lit for node i at step t is

lit = - log oit (yi)

(5)

where the parentheses indicates the n'th element of the vector. For a single Sudoku puzzle x = {x1, x2, ..., x81} and its solution y = {y1, y2, ..., y81} the total loss L (x, y) is the sum of losses over all I = 81 nodes and T steps.

TI

L (x, y) =

lit

t=1 i=1

(6)

To train the network we minimize the total loss, with respect to the parameters of the functions f , g and r using stochastic gradient descent. See figure 1 for an example of the recurrent relational network on a fully connected graph with 3 nodes.

At test time we only consider the output probabilities at the last step, but having a loss at every step during training is beneficial. Since the target digits yi are constant over the steps, it encourages the network to learn a convergent algorithm. Secondly, it helps with the vanishing gradient problem. One potential issue with having a loss at every step is that it might force the network to learn a greedy algorithm that gets stuck in a local minima. However, the separate output function r allows the node hidden states and messages to be different from the output probability distributions. As such, the network could use a small part of the hidden state for retaining a current best guess, which might remain constant over several steps, and other parts of the hidden state for running a non-greedy multi-step algorithm.

Sending messages for all nodes in parallel and summing all the incoming messages might seem like an unsophisticated approach that risk resulting in oscillatory behavior and drowning out the important messages. However, since the receiver node hidden state is an input to the message function, the receiver node can in a sense determine which messages it wishes to receive. As such, the sum can be seen as an implicit attention mechanism over the incoming messages. Similarly the network can learn an optimal message passing schedule, by ignoring messages based on the history and current state of the receiving and sending node.

We have described our model from the example of solving Sudokus, but the model is in no way limited to Sudokus. In general, as per the interface, it takes as input a set of objects described by

3

Under review as a conference paper at ICLR 2018
feature vectors and a set of edges detailing how the objects interacts. If the edges are unknown, the graph can be assumed to be fully connected. In this case the network will need to learn which objects interact with each other. If the edges have attributes, eij, the message function in equation 1 can be modified such that mitj = f hit-1, htj-1, eij . If the output of interest is for the whole graph instead of for each node the output in equation 4 can be modified such that there's a single output ot = r ( i hit). The loss can be modified accordingly.
Code to reproduce the experiments will be made available upon publishing.
3 EXPERIMENTS
3.1 SUDOKU
We generate a dataset of 216.000 puzzles with an equal number of 17 to 34 givens from the collection of 49.151 unique 17-givens puzzles gathered by Royle (2014). We use the solver from Norvig (2006) to solve all the puzzles first. Then we split the puzzles into a test, validation and training pool. To generate the training, validation and test set, we sample puzzles from the respective pools, add between 0 to 17 givens from the solution, and swap the digits according to a random map per Sudoku, e.g. 1  5, 2  3, etc.
We consider each of the 81 cells in the 9x9 Sudoku grid a node in a graph, with edges to and from each other cell in the same row, column and box. Denote the digit for cell j dj (0-9, 0 if not given), and the row and column position rowj (1-9) and columnj (1-9) respectively. The node features are then xj = MLP ([embed (dj) ; embed (rowj) ; embed (columnj)]) where each embed is a separate 16 dimensional learnable embedding and [a; b] denotes the concatenation of a and b. We don't use any edge features and we don't treat the cells with given digits in any special way. The message from i to j is mtij = MLP hti-1; htj-1 . The node hidden state is given by htj = LSTM MLP xj; mt.j where LSTM denotes a Long Short Term Memory cell (Hochreiter & Schmidhuber, 1997). The LSTM cell and hidden state is initialized to zero. The output function r is a linear layer with nine outputs to produce the output logits oti. All the MLP's are four layers with 96 nodes. The first 3 layers have ReLU activation functions and the last layer is linear. The LSTM also has 96 nodes. We run the network for 32 steps. We train the model for 300.000 gradient updates with a batch size of 252 using Adam with a learning rate of 2e-4 and L2 regularization of 1e-4 on all weight matrices.
Our network learns to solve 94.1% of even the hardest 17-givens Sudokus after 32 steps. For more givens the accuracy quickly approaches 100%. Since the network outputs a probability distribution for each step, we can visualize how the network arrives at the solution step by step. For an example of this see figure 2. In the first step, the network uses the elimination strategy to reduce the number of possible digits. For subsequent steps it assigns softer probabilities to the digits, and seems to try a number of different configurations. Once the solution is found it locks onto it and doesn't change.
To examine our hypothesis that multiple steps are required we plot the accuracy as a function of the number of steps. See figure 3. We can see that even simple Sudokus with 33 givens require upwards of 10 steps of relational reasoning, whereas the harder 17 givens continue to improve even after 32 steps. Figure 3 also shows that the model has learned a convergent algorithm. The model was trained for 32 steps, but seeing that the accuracy increased with more steps, we ran the model for 64 steps during testing. At 64 steps the accuracy for the 17 givens puzzles increases to 96.6%.
We compare our network to the relational network (Santoro et al., 2017). We train two relational networks: a node and a graph centric. The node centric corresponds exactly to a single step of our network. The graph centric approach is closer to the original relational network. It does one step of relational reasoning as our network, then sums all the node hidden states. The sum is then passed through a 4 layer MLP with 81 · 9 outputs, one for each cell and digit. The graph centric model has larger hidden states of 256 in all layers to compensate somewhat for the sum squashing the entire graph into a fixed size vector. Otherwise both networks are identical to our network. We could not get either of them to solve any Sudokus. Of the two, the node centric trained much faster and got considerably lower loss. The only difference between the node centric relational network and our model is the number of steps, yet the relational network fails to solve any Sudoku. This shows that multiple steps are crucial for complex relational reasoning. The graph centric has over 4 times as
4

Under review as a conference paper at ICLR 2018

(a) Step 0

(b) Step 1

(c) Step 4

(d) Step 8

(e) Step 12

1

2

4 5

3 6

12
45

3 6

12
45

3 6

12
45

3 6

12
4 5

3 6

7

8

9

123

456

789

123

456

789

123

456

789

123

456

789

123

456

789

123

456

789

78

9

12

3

4 56

789

21 3

4 56

789

12

3

4 56

78

9

21 3

64 5

78

9

21 3

4 56

78

9

1 23

4 56

78

9

789

12

3

64 5

97 8

12
54
78

3
6
9

12

3

5 64

78

9

21 3

64 5

78

9

21 3
5 64

78

9

321

654

78

9

87 9

12

3

64 5
97 8

12
54
78

3 6
9

12 3
654

78

9

21 3

64 5

78

9

21 3
5 64

78

9

31 2

4 56

789

87 9

12

3

64 5
97 8

12
5
4
78

3 6 9

12

3

5 64

7 89
21 3
64 5

789
21 3
654

789
31 2

4 56

87 9

1

2

3

123

12

3

12

3

12

3

4

5

6

8

7

9

123

456

789

4 56
87 9
123 54 6 789

45
87 12

6
9
3

4 56
97 8

45
87 12

6 9 3

4 56
97 8

45
8
7
12

6 9 3

456 87 9

Figure 2: Example of how the trained network solves part of a Sudoku. Only the first column of a full 9x9 Sudoku is shown for clarity. See appendix for the full Sudoku. Each cell displays the digits 1-9 with the font size scaled (non-linearly for legibility) to the probability the network assigns to each digit. We only show steps 0, 1, 4, 8 and 12 due to space constraints. Notice how the network eliminates the given digits 4 and 8 from the other cells in the first step. For this particular Sudoku the network converges to the solution after approximately 20 steps. Animations showing how the trained network solves Sodukos, including a failure case can be found at imgur.com/a/ALsfB.

many parameters as our model (944.874 vs. 201.194) but performs even worse than the node centric. 5

Under review as a conference paper at ICLR 2018

1.0

0.8

Accuracy

0.6

17 givens 19 givens

0.4

21 givens 23 givens

25 givens

0.2

27 givens 29 givens

31 givens

0.0 33 givens

0 10 20 30Steps 40 50 60

Figure 3: Accuracy of our trained network on Sudokus as a function of number of steps. Even simple Sudokus with 33 givens require about 10 steps of relational reasoning to be solved. The dashed vertical line indicates the 32 steps the network was trained for. The network appears to have learned a convergent relational reasoning algorithm such that more steps beyond 32 improve on the hardest Sudokus.

We also compare our network to other differentiable methods. See table 1. Our network outperforms loopy belief propagation, with parallel and random messages passing updates (Bauke, 2008). It also outperforms a version of loopy belief propagation modified for solving Sudokus that uses 250 steps, sinkhorn balancing every two steps and iteratively picks the most probable digit (Khan et al., 2014). Finally we outperform Park (2016) which treats the Sudoku as a 9x9 image uses 10 convolutional layers, iteratively picks the most probable digit, and evaluate on easier Sudokus with 24-36 givens. We also tried to train a version of our network that only had a loss at the last step. It was harder to train, performed worse and didn't learn a convergent algorithm.

Method

Givens Accuracy

Ours Loopy BP, modified (Khan et al., 2014) Loopy BP, random (Bauke, 2008) Loopy BP, parallel (Bauke, 2008) Relational Network, node (Santoro et al., 2017) Relational Network, graph (Santoro et al., 2017) Deep Convolutional Network (Park, 2016)

17 17 17 17 17 17 24-36

96.6% 92.5% 61.7% 53.2%
0% 0% 70%

Table 1: Comparison of methods for solving Sudoku puzzles. Only methods that are differentiable are included in the comparison.

3.2 BABI
BaBi is a text based QA dataset from Facebook (Weston et al., 2015) designed as a set of toy prerequisite tasks for reasoning, and is widely used in the deep learning literature. It consists of 20 tasks including deduction, induction, spatial and temporal reasoning, etc. Each question, e.g. "where is john?" is preceded by a number of facts in the form of short sentences, e.g. "john went to the kitchen.". A task is considered solved if a model achieves greater than 95% accuracy. The most difficult tasks require three steps of relational reasoning. As such the relational reasoning required is limited.
6

Under review as a conference paper at ICLR 2018
The relational reasoning module needs to reason about the facts, in context of the questions so we consider each sentence a node in a fully connected graph. The sentences are encoded using a LSTM with 32 hidden units. The question is also encoded using a LSTM with 32 hidden units. We concatenate the last hidden state of the sentence LSTM with the last hidden state of the question LSTM and pass that through a MLP. The output is considered the node features xi. We set all edge features eij to the question encoding following (Santoro et al., 2017). We only consider the preceding 20 sentences to a question. Our message function f is identical to the Sudoku message function, i.e. a MLP which feeds into a LSTM. We run our network for five steps. To get a graph level output, we use a MLP over the sum of the node hidden states, with 3 layers, the final being a linear layer that maps to the output dimensionality logits. Unless otherwise specified we use 128 hidden units for all layers and all MLPs are 3 ReLU layers followed by a linear layer. We train on all the 10.000 training samples, using Adam with a batch size of 640, a learning rate of 2e-4 and L2 regularization with a rate of 1e-4.
Our trained network solves 19 of 20 tasks, which is competitive with state-of-the-art. Most tasks are quickly and perfectly learned. The only task that the network cannot complete is number 16, the induction task. See table 2 for the tasks where the model achieved less than 100% accuracy.
Task 2 3 5 14 16 18 19 Accuracy 99.7% 96.5% 99.6% 99.9% 45.1% 99.7% 99.9%
Table 2: BaBi results. The tasks that are not shown are all 100% accurate.
On the BaBi task the relational network solves 18/20 tasks, notably failing on the 2 and 3 supporting fact tasks (Santoro et al., 2017). Training the relational network on BaBi takes millions of updates, and a couple of days on 10+ K80 GPUs (David Raposo, 2017, personal communication). In comparison our network naturally perform multi-step relational reasoning, and requires around half a million updates which takes approximately 12 hours on 4 Titan X GPUs. We hypothesize that the relational network takes longer to train because it cannot naturally perform multi-step relational reasoning.
End-to-end memory networks (Sukhbaatar et al., 2015) solves 14/20 tasks by using multiple recurrent hops of attention over the encoded sentences. The Sparse Differentiable Neural Computer (SDNC) is a differentiable computer modeled on the Turing Machine (Rae et al., 2016). It has a large external memory bank it updates by using sparse reads and writes. It solves 19/20 tasks which is state-of-the-art. It also fails at the induction task. EntNet reports 20/20 tasks solved, but does so training on each task independently. Trained jointly on all tasks EntNet solves 16/20 tasks (Henaff et al., 2016).
4 DISCUSSION
We have proposed a general relational reasoning model for solving tasks requiring an order of magnitude more complex relational reasoning than the current state-of-the art. It can be added to any deep learning model to provide a powerful relational reasoning capacity. We get state-of-the-art results on Sudokus solving 96.6% of the hardest Sudokus with 17 givens. We also get results competitive with state-of-the-art results on the BaBi dataset solving 19/20 tasks.
Many difficult problems require complex relational reasoning and we see several exciting applications where our model might improve on state-of-the-art. Silver et al. (2017) mastered the game of Go with a deep residual network (He et al., 2016) with 79 convolutional layers in total that evaluate game position values and proposes moves combined with a monte-carlo tree search algorithm. It would be interesting to replace the deep residual network with our proposed model, and see if the capacity for complex relational reasoning could improve on AlphaGo. In a similar manner it might be possible to use it in a deep reinforcement learning setup and improve on the difficult Atari games that require long term planning and reasoning, e.g. Montezuma's revenge or Frostbite (Mnih et al., 2013). Finally we hypothesize it could improve on deep image captioning models (Karpathy & Fei-Fei, 2015) since reasoning about the people and objects involved in an image is essential to describing it.
7

Under review as a conference paper at ICLR 2018
Loopy belief propagation is widely used for performing inference in graphical models with loops (Murphy et al., 1999). For the Sudoku problem our network learned an inference algorithm that outperforms loopy belief propagation, and it would be interesting to see if we could likewise improve on other problems that rely on loopy belief propagation. One prominent example of loopy belief propagation is in error correcting codes which everything from mobile phones to satellites rely on for robust communication (Shannon, 1948; MacKay & Neal, 1996).
5 RELATED WORK
Relational networks (Santoro et al., 2017) and interaction networks (Battaglia et al., 2016) are the most directly comparable to ours. They compare to using a single step of equation 3. Since it only does one step it cannot naturally do complex multi-step relational reasoning. In order to solve the tasks that require more than a single step it must compress all the relevant relations into fixed size vector, then perform the relational reasoning in the last forward layers. Both relational networks, interaction networks and our proposed model can be seen as an instance of Graph Neural Networks (Scarselli et al., 2009), (Gilmer et al., 2017). Our main contribution is showing how these can be used for complex relational reasoning.
Our model can be seen as a completely learned message passing algorithm. Belief propagation is a hand-crafted message passing algorithm for performing exact inference in directed acyclic graphical models. If the graph has cycles, one can use a variant, loopy belief propagation, but it is not guaranteed to be exact, unbiased or even converge. Empirically it works well though and it is widely used (Murphy et al., 1999). Several works have proposed replacing parts of belief propagation with learned modules (Heess et al., 2013; Lin et al., 2015). Ross et al. (2011) proposes Inference Machines which ditch the belief propagation algorithm altogether and instead train a series of regressors to output the correct marginals by passing messages on a graph. Wei et al. (2016) applies this idea to pose estimation using a series of convolutional layers and Deng et al. (2016) introduces a recurrent node update for the same domain.
REFERENCES
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Advances in Neural Information Processing Systems, pp. 4502­4510, 2016.
Heiko Bauke. Passing messages to lonely numbers. Computing in Science & Engineering, 10(2): 32­40, 2008.
Zhiwei Deng, Arash Vahdat, Hexiang Hu, and Greg Mori. Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4772­4781, 2016.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Nicolas Heess, Daniel Tarlow, and John Winn. Learning to pass expectation propagation messages. In Advances in Neural Information Processing Systems, pp. 3219­3227, 2013.
Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3128­3137, 2015.
8

Under review as a conference paper at ICLR 2018
Sheehan Khan, Shahab Jabbari, Shahin Jabbari, and Majid Ghanbarinejad. Solving sudoku using probabilistic graphical models. data last retrieved January, 1, 2014.
Donald E Knuth. Dancing links. arXiv preprint cs/0011047, 2000.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, pp. 1­101, 2016.
Guosheng Lin, Chunhua Shen, Ian Reid, and Anton van den Hengel. Deeply learning the messages in message passing inference. In Advances in Neural Information Processing Systems, pp. 361­ 369, 2015.
David JC MacKay and Radford M Neal. Near shannon limit performance of low density parity check codes. Electronics letters, 32(18):1645, 1996.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Kevin P Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation for approximate inference: An empirical study. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pp. 467­475. Morgan Kaufmann Publishers Inc., 1999.
Peter Norvig. Solving every sudoku puzzle, 2006. URL http://norvig.com/sudoku.html.
Kyubyong Park. Can neural networks crack sudoku?, 2016. URL https://github.com/ Kyubyong/sudoku.
Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex Graves, and Tim Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. In Advances in Neural Information Processing Systems, pp. 3621­3629, 2016.
Stephane Ross, Daniel Munoz, Martial Hebert, and J Andrew Bagnell. Learning message-passing inference machines for structured prediction. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp. 2737­2744. IEEE, 2011.
Gordon Royle. Minimum sudoku, 2014. URL http://staffhome.ecm.uwa.edu.au/ ~00013890/sudokumin.php.
Adam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. arXiv preprint arXiv:1706.01427, 2017.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61­80, 2009.
CE Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27(3): 379­423, 1948.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354­359, 2017.
Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1):89­96, 2007.
Elizabeth S Spelke, Grant Gutheil, and Gretchen Van de Walle. The development of object perception. 1995.
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in neural information processing systems, pp. 2440­2448, 2015.
Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Convolutional pose machines. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4724­ 4732, 2016.
9

Under review as a conference paper at ICLR 2018

Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merrie¨nboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.

6 APPENDIX

o03 o31 o32

h03 h31 h23

x3

m213

x3

m223

x3

m113

m213

o20

m312

o12

m322

o22

h20 h21 h22

x2

m112

x2

m212

x2

m131

m321

o10

m121

o11

m221

o12

h01 h11 h21

x1 x1 x1
Recurrent relational network on a fully connected graph with 3 nodes. The same graph as in figure 1 unrolled over 2 steps. The node hidden states hti are highlighted. Subscript denote node indices and superscript the step. The dashed lines indicate the recurrent connections.

10

Under review as a conference paper at ICLR 2018

An example Sudoku. The full Sudoku from which the column in figure 2 is taken. Each of the 81 cells contain each digit 1-9, which is useful if the reader wishes to try to solve the Sudoku as they can be crossed out or highlighted, etc. The digit font size corresponds to the probability our model assigns to each digit at step 0, i.e. before any steps are taken. Subsequent pages contains the Sudoku as it evolves with more steps of our model.

123123123123123

123

1

2

3

1

2

3

1

2

3

4 64 5 6 4 5 6 4 5 6 4 5 6 4 5 6

456

5

6

4

5

4

5

6

789789789789789

97 8 9

7

8

9

7

8

9

7

8

123123

3 2 123

123123123

1

2

1

3

1

2

3

456456

4 5 64

456456456

4

5

6

4

5

6

5

6

789789

789

789789789

7

8

9

7

8

9

7

8

9

11 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3

123

2

3

456456456456456456456

456

4

5

6

789789789789789789789

789

7

8

9

123

3

123123123123123

2

1

2

1

2

3

1

3

456

456456456456456

4

5

6

4

5

6

4

5

6

789

97 8 9 7 8 9 7 8 9 7 8 9 7 8 9

7

8

9

7

8

7

8

9

123123123

123123

123123

1

2

3

1

2

3

456456456

54 5 6 4 5 6

456456

4

5

6

4

6

87 8 9 7 8 9 7 8 9

789789

789789

7

9

7

8

9

1 2 31

123123123123123123123

2

3

456

456456456456456456456

4

5

6

789

789789789789789789789

7

8

9

123123123123

1 123123123

1

2

3

2

3

456456456456

456456456

4

5

6

4

5

6

789789789789

9

789789789

7

8

7

8

9

123123123123123123123123

1

2

3

456456456456456456456456

4

5

6

8 789789789789789789789789

7

9

123123123123 1
456456456456 4

123123123123

2

3

64 5 6 4 5 6 4 5 6 4 5 6
5

789789789789

789789789789

7

8

9

11

Under review as a conference paper at ICLR 2018

Step 1

2 1 2 1 3 1 3 321 3

1

3

32

2 12

4 5 5 5 5 55 6

4

64

64

64

64

6

7 8 7 8 7 7 8 7 87 8

9

9 9 89 9 9

1 3 2 112 3 1 2 3

2

13

32 1 2 3

5 6 5 6 5 45 6 6 65 6 44 4 4

4

5

1 4
7 1 4

23 1
65 4
78 9

32 3 1 2

55 6

4

6

9 7 88 9

2 3 12 3 12 3

5 55 6 4

64

6

7 9 7 8 9 7 8 7 8 7 8 7 87 7 78

89

89

9 89

9

9

9

2 2 2 3 3 3 2 3 1 31 3 1 3 1 3 1 2 1 2 1 2 1

231 2

5 6 5 6 5 6 5 6 5 5 6 4 4 54 4 4 4 4 6 4

5 645 6

6

7 8 978978 9 7 8 978 9 78978 9 7 8 9 78 9

3 1 1 1 21 2 3 1 2

12 3

23

2 3 12 3

112 3

23

3

5 6 4 5 6 4 5 5 6 4 4 664 4 5 6 4 5

64

56

5

45

6

7 9 7 7 7 7 8 7 87 8 7 878 9

89

89

89

89

9

9

9

2 2 2 1 2 3 2 3 3 1 31

31

31

13 2

3

1

31 2

12

2

6 4 6 4 6 4 6 5 4 6 4 64 6 64 5

555

5 45 4 6

5

5

7 7 7 8 7 7 9 7 7 98 9

89

897 9 89 8

78

9

89

8

2 1 2 3 2 3 2 3 3 3 32 11 3

13

312

1

1

12

12

2

5 6 4 5 4 64 5 6 4 5 6 5 6 4 4 64 4 5 6

64

56

5

5

7 7 8 7 9 778 9

89

98

8

2 3 2 2 31

21

31

3 12

1

9 7 8 978978 9 789
1 2 3 2 3 33 2 3 1 1 1 2

5 6 5 5 4 54 6 4 6 4 4 5 6 4 5 654 66 54 6 4 6 5

7 7 7 7 9 7 8 7 8 7 88 9

89

89

78 9

8

78

9

999

2 1 2 3 2 3 2 3 1 2 3 2 3 1 31 2 3 1

3

31 2

1

1

12

5 54 5 6 4 5 6 4 5 6 4

4 56 6

4

46 5

6 456456

8 7 9 77 9 8

8

79 8

79 8

79 8

7 9 79 8

8

7 99 8

1 2 3 2 1 2 31 3

312

1

4 54 5 4 554 6 6 6

6

4

2 3 1 23123 1 231 2 3

6 5 45 4 6 5

546

4 56 6

7 9 7 9 7 7 7 8 7 87 8 9 7 8 978

8

89

89

89

9

9

12

Under review as a conference paper at ICLR 2018

Step 4

2 1 2 1 11 2 3

1

3

3 33 2

32 1 2

4 5 5 5 545 6

56 4

64

64

64

6

8 7 7 8 7 87 77 8 9

89

9 89

9

9

1 3 2 13 1 12

23

2

13

231 2 3

6 6 44 5

45

6 54 5

64 5

4

6

6
5

1 4 7
1 4

231
6
54

89

7

23 1
5 64
9 78

32
56
89

12 3

2 312 3

5 565 4

64

6

9 97 8

78

7 89

7

21 2 3 1 2 3 1

31

5 5 5 66 64 4 4

4

7 88 9

97

332 1 2

1

6 55 4 6

4

7 88 9

7 89

2 13 32 1

2

65 4 45 6

5

7 89 9

31 2
46 5

3
6

7 7 8 7 9 7 88 9

89 9 7 8

7 98 7 89 87 8 9

7

9

9

3 1 1 1 21 2

31

2

12

3

23

2 3123

12 3 1 2 3

3

5 6 6 4 5 5 6 4 4 64 54 4 5 6 4 5 6

64

56

5

64 5

7 9 7 7 8 7 88 9 87 8 9

7

78

9

89

78 9

9

89 7

9

2 2 1 2 3 3 3 121

31

31

31 2

3

12

1 2

13 2

32

6 4 6 4 6 5 64 6 4 66 44 5

5

55 64 5 5 4 5 4

6

5

7 7 7 8 7 9 7 98 9

89

89 7

79 8 9 8 7 8 9

89 78

2 1 2 3 2 3 3 33 31 113 12 3 1 2 3

2

1

12

12

2

5 6 4 5 4 4 6 66 4 54 54 4 5 6 6

5 6 46 4

56

5

5

7 8 7 7 9 7 9 7 7 98 9 7

89

7

97 89

89

8

8

89

8

2 3 2 2 3 1 3 22 31

1 31 312

1 23

23

1

1

123

4 5 6 5 6 4 5 4 4 6654

4

64 5 6 4

56

56

564 5

7 7 7 9 7 8 8 87 8

9

89

89

897 8

87 9

79

79

9

1 3 2 3 2 3 1 2 3 121 2

31

3

23 1 2

1

1

32 1

32

4 5 6 6 4 5 5 4 5 6 664 5

45

46

56 4

46 5

6

45

8 7 7 7 7 7 9 7 99 787 9

89

89

89

89

8

78 9

8

1 3 2 1 3 1 3 2 12 1

23 3 1 2

2 3 32 31 2 3 1

12

4 5 4 5 4 5 6 4 4 55 46 6 64 5 6

64

5

54 6

56

6

7 9 7 7 8 7 9 797 8

8

8 7 8 98 9

897

78 9

9

89

13

Under review as a conference paper at ICLR 2018

Step 8

2 1 32 1 3 321 3

1

3

1 3 123

12

3

2 12

32 1 2

4 5 5 6555 6 4

64

64

64

56

4

64

5

5 64 4 5 6

7 8 7 987 8 9 7

89 7

9

897

9

89 7 8 9 7 8

789

3 2 11 2 3 1 2 3 1 2

13

1 3 3 32 3 1 2 3

2 12 12

6 4 5 564 5

45

5 6 54 4 6 4 5 6

655 6 4

4

64

6

9 7 8 7 8 7 88 97 8

7

9 97 8 9 7 8

8 87 9 7

999

3 3 2 13 21 2 3 1 2 3 1 2 3 1 2

12

12

13

3 12 3

5 6 6 45 56 64 4 4

64 5

54 6 4 5

45 6 456

56

7 8 9 998 7

7 8 89 9 7

87 9 7 8

8 8 87 9 7 9 7 9

3 1 1 21 2 3 1 2

12 3

112 3

23

23

2 312 31

3

654

4

5

78 9 7 8
21 3 1 2

6 44 5

5

77 8

9

8

2 11 3 2

4 5 5 4666 4 5

45

66 4

56

9 7 7 7 89 7 8

7 89

89

89

9

2 2 33 1

3 11 2

3

31 2

1 2

3

4 6 56 5

464 5

6 65 4 5

4

6

7 8 7 98 99 7

8 79 9 8 7 8 9

4 65 4

87 9

7

3 11 2

64 5

4

78 9 7

5
8
2 5 8

3 2 2 3 3 3123 1

3

2

31 1

12

12

12

6 9 3
6
9
3

5 6 4 5 4 46 4 54 54 4 5 6 6

5 66 4

56

65 4

65

8 9 9 7 9 7 7 97 7 7 88 9 7 8 9

7

9 78

89

8

8

98

3 2 1 21 2

1 23 1

33 1 2

21 3

23

1 231

312 3

5 6 4 4 4 6 644 5 6

645

5 6 5 64 4 5 6

55 6

54

7 7 7 9 7 8 7 8 87 8 9

89

89

89 78

87 9

79 9 9

3 2 2 1 2 3 11 2 3 31 2 3 1 2 3

33 1 2

1

1

12

23

4 4 6 4 5 4 5 6 664 5

56

5

56 4

64

45 6

5

6

54

8 7 9 7 7 7 7 987 9

89

89

7 98 9

78 9

8

78 9

8

1 2 3 3 2 312 3 1 2 3

312

2 121 3 1

32

1

12 3

4 5 4 4 5 6 4 5 54 5 6

6 56

64

5

44 5 6

56

64

6

7 9 7 7 8 9 7 7 997 8

8

89

897

89

7

7 89

89

8

14

Under review as a conference paper at ICLR 2018

Step 12

2 1 31 2 3 1 3 321 2 3

1

3

3

12

3

2 12

12

31

2

4 5 5 65 445 6 4 5 6

6

6 4

56

4

6 4

5

5 56 64 4

7 8 7 98 887 9 7

97

9

897

9

89 7

897

8

789

3 2 1 1 3 1 2 3 31 2 3 1 2 3 1 2

13

22 3 1

3

2

12

6 4 5 564 5

45

6 55 4 564 4 6

65 6 4 5

4

64

6

9 7 8 7 8 7 87 8

8 9 8877 7 8 9

97

89 7

9

9 99

3 2 13 111 2 3 1 2 3

2 312

2

31 2

1

3

23

12

3

5 6 6 45 5 664 4

4

64

5

5 54 6 4

45

645

6

56

7 8 9 98 9 7

7 8 89 9 7

789 7 8

8 887 9

7

97

9

3 1 21 2 3 1 2

12 3

1 3 112 3

23

2

2 31231

3

5 4 6 4 4 66 5 56 4 4 44

56

56

5

64

56

5

6
45

9 7 7 8 877 8 9

89 78

87 9

89

89 7

79

89 7

9

2 3 12 3 21 1 31 3 2

11 2

3

31

32 1

2

31

2

23

6 4 4 54 5

56

6 45 64 5

65 4

564

646 5

654

7 7 8 97 8 9

89

89

7

978 9 7 8

7 997 8

89 7 8

2 1 3 32 3 3 3 11 11

3

22 3 1

3

2

12

12

12

23

4 6 4 5 4 6 66 4 454

56

56

5

456 4

6 56

5 45

8 9 7 7 97 8 9

87 9

7

9 77 8
9

89 7 8 9 78

89

8

3 2 1 21 2

1 231

33 1 2

12 3

32 3

12

1

312 3

5 6 4 5 4 4 644 5 6

64 5

64

56

4

56

56

56 4 5

7 7 9 7 8 8 887 9

78 9

89

897 8

87 9

79

79

9

3 2 1 11 21 2

3 12 3

31

2

1 231

2 33

31
2

23

4 5 4 6 5 4 5 64 5

6

6 4 5 45 6 5 46 4 6 5 6

645

8
8 7 9 7 7 7 97 9

78 9

89

8 99 7 8 9 7 8

8978

1 2 3 3 22 3 1 2 3 1

312

2 1311 2 3

2

31 1 2 3

4 5 4 4 5 6 4 5 54 5 6

6 56

64

5

454 6

56

64

6

7 9 7 7 8 7 987 9

8

89

897 8 9 7

9879

8 978

15

Under review as a conference paper at ICLR 2018

Step 16

1
4
7 1 4 7 1 4
7
1 4 7 1 4 7 1 4 7 1 4 7 1 4 7
1
4 7

2 5 8 2 5 8 2 5 8 2
5
8 2 5 8
2
5 8 2 5 8 2 5
8
2 5 8

3 6 9 3 6
9
3 6 9 3 6 9 3
6
9 3 6 9
3
6 9 3 6 9 3 6 9

1
4
7
1
4
7
1
4
7
1
4
7 1
4
7
1
4
7
1 4
7
1
4
7 1
4
7

2
5 8 2 5 8 2 5
8
2 5 8 2 5
8
2 5 8 2
5
8 2
5
8 2
5
8

3 6
9
3
6
9 3
6
9
3
6
9
3 6 9
3
6
9 3
6 9 3 6
9
3 6
9

1
4 7 1
4
7 1 4 7 1 4 7 1
4
7
1 4 7 1 4
7
1
4
7 1
4
7

2 5 8 2 5 8 2
5
8
2
5 8 2 5
8 2
5
8
2
5 8 2 5 8
2
5 8

3 6 9
3
6
9 3 6 9 3 6
9
3 6 9
3
6
9 3
6
9 3
6
9 3 6 9

1 4
7
1 4 7 1 4 7
1
4 7 1 4 7 1 4 7 1
4
7
1 4 7 1
4
7

2 5 8
2
5 8 2 5 8 2 5 8 2 5
8
2 5 8 2
5
8 2 5 8 2
5
8

3 6 9 3 6 9 3 6
9
3 6 9 3 6 9 3
6
9 3 6 9
3
6 9 3 6 9

1 4 7
1
4 7 1 4 7 1
4
7 1 4 7 1 4 7 1 4 7 1 4
7
1 4 7

2 5
8
2 5 8 2 5 8 2 5 8
2
5 8 2
5
8 2 5 8 2 5 8 2 5 8

3 6 9 3 6 9
3
6 9 3 6 9 3 6 9 3 6 9 3 6
9
3 6 9 3
6
9

1 4 7 1
4
7 1 4 7 1 4
7
1 4 7 1 4 7
1
4 7 1 4 7 1 4 7

2
5
8 2 5 8 2 5 8 2 5 8 2 5 8 2 5 8 2 5 8
2
5 8 2 5
8

3 6 9 3 6 9 3
6
9 3 6 9 3 6
9 3
6 9 3 6 9 3 6 9 3 6 9

1 4 7 1 4
7
1 4 7 1 4 7 1 4 7 1 4 7 1
4
7
1
4 7 1 4 7

2 5 8 2 5 8
2
5 8 2 5
8
2
5
8 2 5 8 2 5 8 2 5 8 2 5 8

3
6
9 3 6 9 3 6 9 3 6 9 3 6 9 3 6
9
3 6 9 3 6 9
3
6 9

1 4 7 1 4
7
1
4 7 1 4 7 1 4 7 1
4
7 1 4
7
1 4
7 1
4
7

2
5
8
2
5
8
2
5
8
2
5 8
2
5
8
2
5
8
2
5
8
2
5
8
2
5
8

3 6
9
3 6 9 3 6 9 3
6
9
3
6 9 3 6 9 3
6 9 3 6 9 3 6 9

1 4 7 1 4 7 1
4
7 1 4 7
1
4 7 1 4
7
1 4 7 1 4 7 1 4 7

2
5 8 2
5 8
2
5 8
2
5 8 2
5 8
2
5 8 2 5
8
2
5 8
2
5
8

3
6 9 3 6 9 3 6 9 3 6 9 3 6 9 3 6 9 3
6
9 3
6
9
3 6
9

16

Under review as a conference paper at ICLR 2018

Step 20

1
4
7 1 4 7 1 4
7
1 4 7 1 4 7 1 4 7 1 4 7 1 4 7
1
4 7

2 5 8 2 5 8 2 5 8 2
5
8 2 5 8
2
5 8 2 5 8 2 5
8
2 5 8

3 6 9 3 6
9
3 6 9 3 6 9 3
6
9 3 6 9
3
6 9 3 6 9 3 6 9

1 4 7 1 4 7 1 4 7 1 4 7 1
4
7
1
4 7 1 4
7
1 4 7 1 4 7

2
5 8 2 5 8 2 5
8
2 5 8 2 5 8 2 5 8 2 5 8 2 5 8 2
5
8

3 6 9 3
6
9 3 6 9
3
6 9 3 6 9 3 6 9 3 6 9 3 6
9
3 6 9

1
4 7 1 4 7 1 4 7 1 4 7 1 4
7
1 4 7 1 4 7 1
4
7 1 4 7

2 5 8 2 5 8 2
5
8 2 5 8 2 5 8 2 5
8
2 5 8 2 5 8
2
5 8

3 6 9
3
6 9 3 6 9 3 6
9
3 6 9 3 6 9 3
6
9 3 6 9 3 6 9

1 4
7
1 4 7 1 4 7
1
4 7 1 4 7 1 4 7 1 4 7 1 4 7 1
4
7

2 5 8
2
5 8 2 5 8 2 5 8 2 5
8
2 5 8 2
5
8 2 5 8 2 5 8

3 6 9 3 6 9 3 6
9
3 6 9 3 6 9 3
6
9 3 6 9
3
6 9 3 6 9

1 4 7
1
4 7 1 4 7 1
4
7 1 4 7 1 4 7 1 4 7 1 4
7
1 4 7

2 5
8
2 5 8 2 5 8 2 5 8
2
5 8 2
5
8 2 5 8 2 5 8 2 5 8

3 6 9 3 6 9
3
6 9 3 6 9 3 6 9 3 6 9 3 6
9
3 6 9 3
6
9

1 4 7 1
4
7 1 4 7 1 4
7
1 4 7 1 4 7
1
4 7 1 4 7 1 4 7

2
5
8 2 5 8 2 5 8 2 5 8 2 5 8 2 5 8 2 5 8
2
5 8 2 5
8

3 6 9 3 6 9 3
6
9 3 6 9 3 6
9 3
6 9 3 6 9 3 6 9 3 6 9

1 4 7 1 4
7
1 4 7 1 4 7 1 4 7 1 4 7 1
4
7
1
4 7 1 4 7

2 5 8 2 5 8
2
5 8 2 5
8
2
5
8 2 5 8 2 5 8 2 5 8 2 5 8

3
6
9 3 6 9 3 6 9 3 6 9 3 6 9 3 6
9
3 6 9 3 6 9
3
6 9

1 4 7 1 4 7
1
4 7 1 4 7 1 4 7 1
4
7 1 4 7 1 4 7 1 4
7

2 5 8 2 5
8
2 5 8 2 5 8 2 5 8 2 5 8
2
5 8 2
5
8 2 5 8

3 6
9
3 6 9 3 6 9 3
6
9
3
6 9 3 6 9 3 6 9 3 6 9 3 6 9

1 4 7 1 4 7 1
4
7 1 4 7
1
4 7 1 4
7
1 4 7 1 4 7 1 4 7

2 5 8 2
5
8 2 5 8
2
5 8 2 5 8 2 5 8 2 5
8
2 5 8 2 5 8

3
6 9 3 6 9 3 6 9 3 6 9 3 6 9 3 6 9 3 6 9 3
6
9 3 6
9

17

