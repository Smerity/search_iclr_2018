Under review as a conference paper at ICLR 2018
LIFELONG WORD EMBEDDING VIA META-LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Learning high-quality word embeddings is of significant importance in achieving better performance in many down-stream learning tasks. On one hand, traditional word embeddings are trained on a large scale corpus for general-purpose tasks, which are often sub-optimal for many domain-specific tasks. On the other hand, many domain-specific tasks do not have a large enough domain corpus to obtain high-quality embeddings. We observe that domains are not isolated and a small domain corpus can leverage the learned knowledge from many past domains to augment that corpus in order to generate high-quality embeddings. In this paper, we formulate the learning of word embeddings as a lifelong learning process. Given knowledge learned from many previous domains and a small new domain corpus, the proposed method can effectively generate new domain embeddings by leveraging a simple but effective algorithm and a meta-learner, where the metalearner is able to provide word context similarity information at the domain-level. Experimental results demonstrate that the proposed method can effectively learn new domain embeddings from a small corpus and past domain knowledges1. We also demonstrate that general-purpose embeddings trained from a large scale corpus are sub-optimal in domain-specific tasks.
1 INTRODUCTION
Learning word embeddings (Mnih & Hinton (2007); Turian et al. (2010); Mikolov et al. (2013a;b;c); Pennington et al. (2014)) has received a significant amount of attention due to its high performance on many down-stream learning tasks. Word embeddings have been shown effective in NLP tasks such as named entity recognition (Siencnik (2015)), sentiment analysis (Maas et al. (2011)) and syntactic parsing (Durrett & Klein (2015)). Such embeddings are shown to effectively capture syntactic and semantic level information associated with a given word (Mikolov et al. (2013a)).
The "secret sauce" of training word embedding is to turn a large scale corpus into billions of training examples. There are two common assumptions for training word embeddings: 1) the training corpus is largely available and bigger than the training data of the potential down-stream learning tasks; and 2) the topic of the training corpus is closely related to the topic of the down-stream learning tasks. However, real-world learning tasks often do not meet one of these assumptions. For example, a domain-specific corpus that is closely related to a down-stream learning task may often be of limited size. If we lump different domain corpora together and train general-purpose embeddings over a large scale corpus (e.g., GloVec embeddings (Pennington et al. (2014)) are trained from the corpus Common Crawl, which covers almost any topic on the web), the performance of such embeddings on many domain-specific tasks is often sub-optimal (we show this in Section 6). A possible explanation is that embeddings often have limited dimensions to encode all possible information. Detailed information with fewer word co-occurrences may be neglected. Further, the context words of a given word may vary a lot in different domains. So the meanings of the same word in different domains may compete to fit in the limited dimensions (e.g., is "java" more likely close to "chocolate" or "python").
To solve the problem of the limited domain corpus, one possible solution is to use transfer learning (Pan & Yang (2010)) for training domain-specific embeddings (Bollegala et al. (2015); Yang et al. (2017)). However, one common assumption of these works is that a pair of similar source domain and target domain is manually identified in advance. In reality, given many domains, manually
1We will release the code upon acceptance of this paper.
1

Under review as a conference paper at ICLR 2018
catching useful information in so many domains are very hard. Also, the corpus size of a single source domain may not be enough to help. In contrast, we humans learn the meaning of a word more smartly. We accumulate different domain contexts for the same word. When a new learning task comes, we may quickly identify the new domain contexts and borrow the word meanings from existing domain contexts.
This is where lifelong learning comes to the rescue. Lifelong machine learning (LML) is a continual learning paradigm that retains the knowledge learned in past tasks 1, . . . , n, and uses it to help learning the new task n + 1 (Thrun (1996); Silver et al. (2013); Chen & Liu (2016)). In the setting of word embedding: we assume that the learning system has seen n domain corpora: (D1, . . . , Dn), when a new domain corpus Dn+1 comes by demands from that domain's potential down-stream learning tasks, the learning system can automatically generate word embeddings for the n + 1-th domain by effectively leveraging useful past domain knowledge.
The main challenges of this task are 2 fold. 1) How to identify useful past domain knowledge to train the embeddings for the new domain. 2) How to automatically identify such kind of information, without help from human beings. To tackle these challenges, the system has to learn to learn how to identify similar words in other domains for a given word in a new domain. This, in general, belongs to meta-learning (Vilalta & Drissi (2002); Peng et al. (2002)). Here we do not focus on specific embedding learning but focus on learning how to characterize corpora of different domains for embedding learning.
The main contributions of this paper can be summarized as follows: 1) we propose the problem of lifelong word embedding, which may benefit many down-stream learning tasks. We are not aware of any existing work on word embedding using lifelong learning 2) we propose a lifelong embedding learning method, which leverages meta-learning to aggregate useful knowledge from past domain corpora to generate embeddings for the new domain.
2 RELATED WORKS
Learning word embeddings has been studied for a long time (Mnih & Hinton (2007)). Many earlier methods employ complex neural network architectures (Collobert & Weston (2008); Mikolov et al. (2013c)). Recently, a simple and effective unsupervised model called skip-gram (Mikolov et al. (2013b;c)) was proposed to turn plain text corpus into large-scale training examples without any human annotation. It uses the current word to predict the surrounding words in a context window by maximizing the likelihood of the predictions. The learned parameters for each word are then the embeddings of that word. Although such embeddings can be trained in large scale and easily obtained online (Pennington et al. (2014); Bojanowski et al. (2016)), they are sub-optimal for many domain specific tasks (Bollegala et al. (2015); Yang et al. (2017)). Domain corpus also suffers from limited size to train high-quality embeddings.
Our work is most related to Lifelong Machine Learning (LML)) (or lifelong learning). Much of the work on LML focused on supervised learning (Thrun (1996); Silver et al. (2013); Ruvolo & Eaton (2013); Chen & Liu (2016)) Recent years, several works have also been done in the unsupervised setting, mainly on topic modeling (Chen & Liu (2014)), information extraction (Mitchell et al. (2015)) and graph labeling (Shu et al. (2016)). However, we are not aware of any existing research that has been done on using lifelong learning for word embedding. LML is related to transfer learning and multi-task learning (Pan & Yang (2010)), which have been leveraged in word embeddings (Bollegala et al. (2015); Yang et al. (2017)). However, LML is different from transfer learning (see the survey book from Chen & Liu (2016)). Given many domains with uncertain noise for the new domain, the lack of guidance on which kind of information is worth learning from the past domains is a problem. And there's no good measure of similarity of two words in different domains.
The proposed method leverages meta-learning (Vilalta & Drissi (2002)), which is to perform machine learning on learning tasks. Recently, meta-learning (or learning to learn) has been used to learn parameters of an optimizer (Andrychowicz et al. (2016)), to learn neural architectures (Fernando et al. (2017)). We leverage a meta-learner to accumulate knowledge during lifelong learning.
2

Under review as a conference paper at ICLR 2018
Figure 1: Meta-learning on domain-level word context similarity.
Figure 2: Lifelong word embedding learning process.
3 MODEL OVERVIEW
The overall lifelong learning process is depicted in Figure 1 and Figure 3. Given a series of domain corpora Dn = d1, d2, . . . , dn, the lifelong learning system first learns a meta model (learner) on domain-level word context similarity from the first m domain corpus. As more domains arrive, the system accumulates the knowledge of domain corpora, as in Figure 3. When a new domain Dn+1 comes, the system uses the meta-learner to catch past domain knowledge that is useful and related to the new domain Dn+1 as augmented knowledge. With the augmented knowledge, the word embedding learning process is performed and the resulting embeddings are used for further down-stream learning tasks. The meta-learner here plays a central role in automatically identifying useful knowledge from past domains to help the new domain.
4 META LEARNER
In this subsection, we describe how a meta-learner can help to identify similar words from many past domain corpora. When it comes to borrowing knowledge from past domains, the first problem is what to borrow. Although binary cross-domain embeddings are studied in Bollegala et al. (2015); Yang et al. (2017), they mostly assume that a relevant domain is already identified and shared words between two domains have similar meanings. In reality, given a wide spectrum of domains, borrowing knowledge from a non-relevant domain may not helpful or even harmful for word embeddings
3

Under review as a conference paper at ICLR 2018

(we show this Section 6). The meaning of one word in one domain may be quite different from the same word in another. For example, the word "java" in the programming context is different from the restaurant context. Borrowing the knowledge from a restaurant corpus can be harmful to the representations of "java" in a programming context.

4.1 TRAINING EXAMPLES

On top of learning embeddings for specific domains, we build a meta-learner to learn a general word
context similarity from the first m domains. As shown in Figure 1, we first characterize words in a
domain corpus as training examples. Given a specific word in a domain, we choose its co-occurrence
with words in the vocabulary as features. We choose co-occurrence because it's close to the training
examples of the skip-gram model we use later. Due to the large size a vocabulary, we only select the
top f frequent words throughout all m domains as word features Vwf. Then given a domain corpus Di, we sample l sub-corpora Di,j  P (Di) by selecting a fixed amount of chunks in Di. A chunk can be a sentence or a document in Di. We randomly select a fixed amount of chunks because the word features built from the sub-corpus are expected to be on the same scale. Then we randomly
select a subset of words from top f words as training example words Vmeta. These training example words are the same through all domains D1:m. We use these words in Vmeta as co-occurrence features and build features uwi,j,k for the word wk  Vmeta on the j-th sub-corpus of the i-th domain. We build word features for all m domain sub-corpora D1:m,1:l.

Finally, a pairwise meta-learner is trained on pairs of word features drawn from different domain

sub-corpora

for

the

same

word.

Given

a

word

wk



Vmeta,

a

pair

of

word

features

(uwi,j,k ,

uwi,j

),
,k

where j

=

j

,

forms

a

postive

example;

whereas

(uwi,j,k , uwi

,j

)
,k

with

i

=

i

(j and j

can be

equal or not) forms a negative example.

The m domains are split into disjoint mt training domains, validation domains, and testing domains. So both the validation and testing examples are unseen examples during training. We enforce such isolation and wish the trained meta-learner can be more generally applied to the rest n - m domains.

4.2 PAIRWISE NETWORK
We train a simple but efficient neural network to learn pairwise domain-level word context similarity. The idea of making such a network small but high-throughput is crucial in lifelong settings. This is because the meta-learner is heavily used in the later lifelong learning process. Given so many domains with so many words asking for detecting similarity, a small pairwise network with fewer parameters is desirable to save more memory being used for high-throughput inference.
The proposed pairwise network contains only one aggregation layer and two fully-connected layers followed by a sigmoid activation. The hidden units in the first fully-connected layer is activated by ReLu units. The network is parameterized as follows:

(W2

·

ReLu(W1

·

Aggreg(uwi,j,k uwi

,j

)
,k

+

b1)

+

b2),

(1)

where W s and bs are weights and (·) is the sigmoid function. The aggregation layer is defined as:

Aggreg(·, ·) = [func1(·, ·)  func2(·, ·)  . . . ],

(2)

which is a concatenation of multiple aggregating functions. We leverage the following functions to

obtain

multi-perspective

features:

Cosine(x, y)

=

x·y ||x||2 ·||y ||2

,

Dot(x, y)

=

x · y,

Multiply(x, y)

=

xy and Diff(x, y) = x - y. In practice, more complicated functions can be added.

5 LIFELONG LEARNING PROCESS
The previous section ends up with a meta-learner M . In this section, we describe how the lifelong learning system leverages M on n - m domains and the new domain corpus Dn+1.
5.1 WORD CONTEXT RETRIEVAL
Assume the lifelong learning system has seen n domain corpora. The system stores knowledge into a knowledge base K. The knowledge base K contains a meta-learner M trained over the first m

4

Under review as a conference paper at ICLR 2018

domains and knowledge over past (n - m) domains. The knowledge includes the vocabulary of word features Vwf, (n - m) domain corpora Dm:n, vocabularie on (n - m) domains Vm:n, and word features on those vocabularies Em:n. The word features Em:n are computed from one sample from each domain corpus.
Given a new domain corpus Dn+1, the lifelong learning system first retrieval word context information as augmented knowledge for later embedding training. The retrieval process is described in Algorithm 1. Line 1-2 build word features on the new domain corpus. Line 3 retrieves past domain knowledge. Line 5-15 retrieves relevant words from past domains using the meta-learner at Line 9 and select words with the highest probability above a threshold at Line 11. Finally, Line 16-20 retrieves co-occurrence information as augmented knowledge. Line 21 simply stores the knowledge for the new domain. Further, the computed co-occurrence can also be cached into knowledge base K to speed up the further retrieving process.
Algorithm 1: Lifelong domain-level word context retrieval
Input : a knowledge base K containing knowledge over past (n - m) domains and a meta learner M, a new domain corpus Dn+1
Output: a word co-occurrence set A, where each element is a 2-tuple (wt, wc), representing useful knowledge from past domains.
1 Vn+1  BuildVocab(Dn+1) 2 En+1  BuildWordFeature(Dn+1, K.Vwf) 3 (Dm:n, Vm:n, Em:n)  Km:n 4 W  5 for (Vi, Ei)  Vm:n, Em:n do 6 O  Vi  Vn+1 7 Fi  RetrieveWordFeature(Ei, O) 8 Fn+1  RetrieveWordFeature(En+1, O) 9 S  M.inference(Fi, Fn+1) 10 for o  O do 11 if S[o]   and S[o] > W [o].first then 12 W [o]  (S[o], i) 13 end
14 end
15 end 16 W  InvertbyDomainIndex(W ) 17 A   18 for (Di, Oi)  (Dm:n, Wm:n) do 19 A  A  ScanCo-Occurrence(Di, Oi) 20 end 21 Kn+1  (Dn+1, Vn+1, En+1) 22 return A

5.2 LIFELONG WORD EMBEDDING

In this subsection, we first describe the skip-gram model introduced by Mikolov et al. (2013b)
in the context of a new domain in the lifelong setting. Given a new domain corpus Dn+1 with a vocabulary Vn+1, the goal of the skip-gram model is to learn a vector representation for each word w  Vn+1 in that domain. Assume the domain corpus is represented as a sequence of words Dn+1 = (w1, . . . , wT ), the objective of the skip-gram model is to maximize the following loglikelihood:

T

LDn+1 = ( (log (uTwt · vwc ) +

log (-uwT t · vwc ))),

t=1 cCt

c Nt

(3)

where Ct is the set of indices of words surrounding word wt in a fixed context window; Nt is a set of indices of words (negative samples) drawn from the vocabulary Vn+1 for the t-th word; u

5

Under review as a conference paper at ICLR 2018

and v represent word vectors (or embeddings) we are trying to learn. The goal of skip-gram is to independently predict the presence (or absence) of context words wc given the word wt. When the size T of the corpus is extremely large, the skip-gram model, in fact, can be fed with billions of training examples. So the vector of a word can be trained to have a good representation for the similarity with the word's context words.
However, depending on the specific down-stream tasks, many domain corpora may not have a large scale corpus. And a random sequence of words drawn from other domains may not truly reflect the distribution P (wc|wt) in domain Dn+1. This is where the previously computed augmented word co-occurrence A come to rescue. Assume our lifelong learning system has seen m domains to build the meta-learner M and n - m domains to build the knowledge into K. Given a new domain corpus Dn+1, we first perform Algorithm 1 to obtain the augmented word co-occurrence from past domains A. Then this co-occurence information A is integrated into the objective function of skip-gram as following:

LDn+1 = LDn+1 +

(log (uTwt · vwc ) + log (-uwT t · vwc )),

(wt ,wc )A

(4)

where wc is a random word drawn from the vocabulary. We use the default hyperparameters of skip-gram model (Mikolov et al. (2013b)). Note that in the skip-gram model as we scan through the
corpus wt can also be wc's context word. But in the augmented information here, we do not allow such bi-directional co-occurrence happen since wt may not be a useful context word for the word wc in the (n + 1)-th domain.

6 EXPERIMENTAL RESULTS
We present extensive evaluations to assess the effectiveness of our approach. Following the advice of Nayak et al. (2016); Faruqui et al. (2016), we leverage the learned word embeddings as continuous features in several down-stream tasks, including document classification, aspect extraction, and question similarity detection. We do not evaluate the learned embeddings directly on dictionaries of similar and dissimilar words as in traditional word embedding papers (Mikolov et al. (2013b); Pennington et al. (2014)) because such dictionaries are in general not available on various domains.

6.1 DATASETS
We use the Amazon Review datasets He & McAuley (2016) to train our meta-learner and new task embeddings. We consider each product category as a domain and aggregate all reviews under each category as one domain corpus. We ignore domain corpora that are less than 20 MB as they are too small to obtain domain corpus sample and to further train word embeddings. We pick the domain "Laptops" out to make it as the new domain for aspect extraction task (its annotation is on Laptop corpus). We randomly select 56 (m) domains to train and evaluate the meta-learner. Then a random collection of 80 (n - m) domains is used as past domains.

6.2 EVALUATION OF META-LEARNER
We split the 56 domains as 39 (mt) domains for training, 5 domains for validation and 12 domains for testing. We sample 2 (l) sub-corpora over the set of reviews from each domain and limit the size of the sub-corpora as 10 MB. The word features have 5000 words (f ). We randomly select 500 words (|Vmeta| = 500) to obtain pairwise examples. So we end up with 66723 training examples, 8782 validation examples, and 20634 testing examples. The accuracy is 83.1%. Based on this result, we empirically set  in Algorithm 1 as 0.8.

6.3 DOWN-STREAM TASKS
We use 3 down-stream tasks to evaluate the effectiveness of our approach. For each task, we leverage an embedding layer to store the pre-trained embeddings. We keep the number of dimensions of embeddings as 300, which is the same size as general-purpose embeddings (GloVec.800B (Pennington et al. (2014)) or fastText Wiki English (Bojanowski et al. (2016))). We freeze the embedding

6

Under review as a conference paper at ICLR 2018

Table 1: Accuracy of different embeddings on product type classification tasks

No Pre-trained Embedding Wiki.en (Bojanowski et al. (2016)) GloVe.840b (Pennington et al. (2014)) New Domain Corpus 80 Domains+New Domain Corpus Lifelong Word Embedding

Comp. Compo.
63.2 68.1 77.9 78.4 77.0 79.8

Kitch. Stor.& Org.
65.2 69.2 73.3 74.0 74.6 75.8

Cats Supples
78.3 77.5 83.4 84.6 85.7 86.59

layers during training, so the result is less affected by the rest of the model and the training data. To make all tasks consistent and less likely affected by the performance of a model for a specific task, we leverage the same Bi-LSTM model (Hochreiter & Schmidhuber (1997)) on top of an embedding layer for all down-stream tasks. The input size of Bi-LSTM is the same as the embedding layer (300) and the output size is 100. We only use many-to-many Bi-LSTM for aspect extraction since its a sequence labeling task. And all other tasks leverage the many-to-one Bi-LSTMs for classification purpose. In the end, a fully-connected layer is applied after Bi-LSTM, with the size specific to each task.
Product Type Classification: This task is to classify a review into a product type. There are many product types under each product category (domain). We use the randomly selected 3 domains as the new domains to form 3 multi-class classification sub-tasks. These domains are: Computer Components, Kitchen Storage and Organization and Cats Supplies. For each sub-task, we randomly draw 1200 reviews for each product type. We drop classes with less than 1200 reviews. This ends up with 13, 17 and 11 classes for Computer Components, Kitchen Storage and Organization and Cats Supplies, respectively. For each sub-task, we keep 10000 reviews as the testing data (to make the result more consistent) and split the rest as 7:1 for training and validation data, respectively. All sub-tasks are evaluated on accuracy.
Sentiment Classification: We select 6000 4-rating reviews as positive reviews and 6000 2-rating reviews as negative reviews from 3 domains mentioned above to form a binary sentiment classification task.
Aspect Extraction: Aspect extraction is an important task in sentiment analysis (Liu (2012; 2015)). We leverage the dataset from SemEval-2014 Task 4: Aspect-based sentiment analysis (Pontiki et al. (2014)). This dataset contains human annotated Laptop aspects and their polarities. It has 3045 training examples and 800 testing examples. We use the Laptop domain corpus from the Amazon Review Dataset as the new domain corpus to train the lifelong embedding. We leverage the original evaluation script to report precision, recall, and F1-score.
6.4 BASELINES
We keep all baseline embeddings to be consistently 300 dimensions.
No Embedding: We randomly initialize the word vectors and train the word embedding layer during the training process of each down-stream task. Note that only in this baseline do we allow trainable weights for embeddings.
Wiki.en: This is the lower-cased embeddings pre-trained from English Wikipedia using fastText (Bojanowski et al. (2016)). We lower the cases of all corpora of down-stream tasks to match the words in this embedding. Note that although the corpus of Wikipedia contains a wide spectrum of domains covering almost everything of human knowledge, the amount of corpus for a specific domain (e.g, a few articles) may not be large enough. The total amount of Wikipedia is just several billion tokens, which is on the same scale as Amazon Review dataset (8 billion tokens).
GloVe.840b: This is the cased embeddings pre-trained from Common Crawl, which has 840 billions of tokens. It contains almost all web pages available before 2015. We show that although GloVe.840b may perform well on almost any task, its performance is sub-optimal on many domain tasks.
7

Under review as a conference paper at ICLR 2018

Table 2: Accuracy of different embeddings on sentiment classification task

No Pre-trained Embedding Wiki.en (Bojanowski et al. (2016)) GloVe.840b (Pennington et al. (2014)) New Domain Corpus 80 Domains+New Domain Corpus Lifelong Word Embedding

Accuracy
82.7 79.3 80.55 82.7 82.83 82.9

Table 3: Performance of different embeddings on aspect extraction task

No Pre-trained Embedding Wiki.en (Bojanowski et al. (2016)) GloVe.840b (Pennington et al. (2014)) New Domain Corpus 80 Domains+New Domain Corpus Lifelong Word Embedding

Precision
55.8 67.1 71.1 71.0 69.6 70.2

Recall
29.5 32.8 38.0 40.5 37.1 41.2

F1-Score
38.6 44.1 50.0 51.6 48.4 52.0

New Domain Corpus: This is a baseline embedding pre-trained only from the new domain corpus. We show that although we allow the new domain corpus to be much larger than a typical supervised learning task, the embeddings trained from such a corpus only are still not good enough.
80 Domains+New Domain Corpus: Another straightforward embedding is to concatenate the corpora from all past domains and the new domain together to train relatively general embeddings. We use this baseline to show that "brute-forced" embeddings that roughly cover the new domain may contain noisy information. How to smartly adapt the embeddings into a closer context is crucial for down-stream tasks.
6.5 EVALUATION OF DOWN-STREAM TASKS
Product Type Classification: From Table 1, we can see that the difficulty of different classification tasks depends on the number of classes in each task. The lifelong embeddings perform best. Surprisingly, the GloVe trained on 840 billions of tokens does not perform well enough compared to the limited new domain corpora. Putting all 80 domain corpora together with the new domain corpus makes the result worse than lifelong embedding. This is because those 80 domains contain noises for the new domain. The huge differences between Wiki.en and GloVe.840b demonstrate that the Wikipedia corpus is not large enough to cover many domains.
Sentiment Classification: From Table 2 we can see the performance of most baselines is very close. This is close to our previous experience. A possible explanation is that sentiment classification relies on sentiment words like "good" or "bad". However, those words have similar context words, e.g., "This phone is good." and "This phone is bad.". So the co-occurrence-based preprocessing of the corpus is not good for learning the embeddings of sentiment words.
Aspect Extraction: From Table 3, we can see that the performance of different baselines on aspect extraction follows a similar trend as in product type classification. Since the testing data of this task is small. The result may contain random noise.
7 CONCLUSIONS
In this paper, we formulate a lifelong word embedding learning process. Given many previous domains and a small new domain corpus, the proposed method can effectively generate new domain embeddings by leveraging a simple but effective algorithm and a meta-learner. The meta-learner is able to provide word context similarity information on domain-level. Experimental results show that
8

Under review as a conference paper at ICLR 2018
the proposed method is effective in learning new domain embeddings from a small corpus and past domain knowledge.
REFERENCES
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pp. 3981­3989, 2016.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016.
Danushka Bollegala, Takanori Maehara, and Ken-ichi Kawarabayashi. Unsupervised cross-domain word representation learning. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 730­740, Beijing, China, July 2015. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P15-1071.
Zhiyuan Chen and Bing Liu. Topic modeling using topics from many domains, lifelong learning and big data. In International Conference on Machine Learning, pp. 703­711, 2014.
Zhiyuan Chen and Bing Liu. Lifelong Machine Learning. Morgan & Claypool Publishers, 2016.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pp. 160­167. ACM, 2008.
Greg Durrett and Dan Klein. Neural crf parsing. arXiv preprint arXiv:1507.03641, 2015.
Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer. Problems with evaluation of word embeddings using word similarity tasks. arXiv preprint arXiv:1605.02276, 2016.
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super neural networks. arXiv preprint arXiv:1701.08734, 2017.
R. He and J. McAuley. Ups and downs: Modeling the visual evolution of fashion trends with oneclass collaborative filtering. In World Wide Web, 2016.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Bing Liu. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1­167, 2012.
Bing Liu. Sentiment Analysis: Mining Opinions, Sentiments, and Emotions. Cambridge University Press, 2015.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pp. 142­150. Association for Computational Linguistics, 2011.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013a.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111­3119, 2013b.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In hlt-Naacl, volume 13, pp. 746­751, 2013c.
9

Under review as a conference paper at ICLR 2018
T Mitchell, W Cohen, E Hruschka, P Talukdar, J Betteridge, A Carlson, B Dalvi, M Gardner, B Kisiel, J Krishnamurthy, N Lao, K Mazaitis, T Mohamed, N Nakashole, E Platanios, A Ritter, M Samadi, B Settles, R Wang, D Wijaya, A Gupta, X Chen, A Saparov, M Greaves, and J Welling. Never-Ending Learning. In AAAI, 2015.
Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pp. 641­648. ACM, 2007.
Neha Nayak, Gabor Angeli, and Christopher D Manning. Evaluating word embeddings using a representative suite of practical tasks. ACL 2016, pp. 19, 2016.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345­1359, 2010.
Yonghong Peng, Peter A Flach, Carlos Soares, and Pavel Brazdil. Improved dataset characterisation for meta-learning. In International Conference on Discovery Science, pp. 141­152. Springer, 2002.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pp. 1532­43, 2014.
Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pp. 27­35, Dublin, Ireland, August 2014. Association for Computational Linguistics and Dublin City University. URL http://www.aclweb.org/anthology/S14-2004.
Paul Ruvolo and Eric Eaton. Ella: An efficient lifelong learning algorithm. ICML (1), 28:507­515, 2013.
Lei Shu, Bing Liu, Hu Xu, and Annice Kim. Lifelong-rl: Lifelong relaxation labeling for separating entities and aspects in opinion targets. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.
Scharolta Katharina Siencnik. Adapting word2vec to named entity recognition. In Proceedings of the 20th Nordic Conference of Computational Linguistics, NODALIDA 2015, May 11-13, 2015, Vilnius, Lithuania, number 109, pp. 239­243. Linko¨ping University Electronic Press, 2015.
Daniel L Silver, Qiang Yang, and Lianghao Li. Lifelong Machine Learning Systems: Beyond Learning Algorithms. In AAAI Spring Symposium: Lifelong Machine Learning, pp. 49­55, 2013.
Sebastian Thrun. Is learning the n-th thing any easier than learning the first? In NIPS, pp. 640­646, 1996.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pp. 384­394. Association for Computational Linguistics, 2010.
Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial Intelligence Review, 18(2):77­95, 2002.
Wei Yang, Wei Lu, and Vincent Zheng. A simple regularization-based algorithm for learning crossdomain word embeddings. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2888­2894, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ D17-1311.
10

