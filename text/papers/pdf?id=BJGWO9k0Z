Under review as a conference paper at ICLR 2018
CRITICAL PERCOLATION AS A FRAMEWORK TO ANALYZE THE TRAINING OF DEEP NETWORKS
ABSTRACT
In this paper we approach two relevant deep learning topics: i) tackling of graph structured input data and ii) a better understanding and analysis of deep networks and related learning algorithms. With this in mind we focus on the topological classification of reachability in a particular subset of planar graphs (Mazes). Doing so, we are able to model the topology of data while staying in Euclidean space, thus allowing its processing with standard CNN architectures. We suggest a suitable architecture for this problem and show that it can express a perfect solution to the classification task. The shape of the cost function around this solution is also derived and, remarkably, does not depend on the size of the maze in the large maze limit. Responsible for this behavior are rare events in the dataset which strongly regulate the shape of the cost function near this global minimum. We further identify an obstacle to learning in the form of poorly performing local minima in which the network chooses to ignore some of the inputs. We further support our claims with training experiments and numerical analysis of the cost function on networks with up to 128 layers.
1 INTRODUCTION
Deep convolutional networks have achieved great success in the last years by presenting human and super-human performance on many machine learning problems such as image classification, speech recognition and natural language processing (LeCun et al. (2015)). Importantly, the data in these common tasks presents particular statistical properties and it normally rests on regular lattices (e.g. images) in Euclidean space (Bronstein et al. (2016)). However, recently, more attention has been given to other highly relevant problems in which the input data belongs to non-Euclidean spaces and consequently cannot be direct approached with standard CNNs. Such kind of data may present a graph structure when it represents, for instance, social networks, knowledge bases, brain activity, protein-interaction, 3D shapes and human body poses. Although some works found in the literature propose methods and network architectures specifically tailored to tackle graph-like input data (Bronstein et al. (2016); Bruna et al. (2013); Henaff et al. (2015); Li et al. (2015); Masci et al. (2015a;b)), in comparison with other topics in the field this one is still not vastly investigated.
Another recent focus of interest of the machine learning community is in the detailed analysis of the functioning of deep networks and related algorithms (Daniely et al. (2016); Ghahramani (2015)). The minimization of high dimensional non-convex loss function by means of stochastic gradient descent techniques is theoretically unlikely, however the successful practical achievements suggest the contrary. The hypothesis that very deep neural nets do not suffer from local minima (Dauphin et al. (2014)) is not completely proven (Swirszcz et al. (2016)). The already classical adversarial examples (Nguyen et al. (2015)), as well as new doubts about supposedly well understood questions, such as generalization (Zhang et al. (2016)), bring even more relevance to a better understanding of the methods.
In the present work we aim to advance simultaneously in the two directions described above. To accomplish this goal we focus on the topological classification of graphs (Perozzi et al. (2014); Scarselli et al. (2009)). However, we restrict our attention to a particular subset of planar graphs constrained by a regular lattice. The reason for that is threefold: i) doing so we still touch upon the issue of real world graph structured data, such as the 2D pose of a human body (Andriluka et al. (2014); Jain et al. (2016)) or road networks (Masucci et al. (2009); Viana et al. (2013)); ii) we maintain the data in Euclidean space, allowing its processing with standard CNN architectures; iii)
1

Under review as a conference paper at ICLR 2018
this particular class of graphs has various non-trivial statistical properties derived from percolation theory and conformal field theories (Cardy (2001); Langlands et al. (1994); Smirnov & Werner (2001)), allowing us to analytically compute various properties of a deep CNN proposed by the authors to tackle the problem. Specifically we introduce Maze-testing, a specialized version of the reachability problem in graphs (Yu & Cheng (2010)). In Maze-testing, random mazes, defined as binary images, are classified as solvable or unsolvable according to the existence of a path between given starting and ending points in the maze (vertices in the planar graph). The Maze-testing problem enjoys a high degree of analytical tractability thereby allowing us to gain important theoretical insights regarding the learning process. We propose a deep network to tackle the problem that consists of O(L2) layers, alternating convolutional, sigmoid, and skip operations, followed at the end by a logistic regression function. We prove that such a network can express an exact solution to this problem which we call the optimal-BFS minima. We derive the shape of the cost function around this minimum. Quite surprisingly we find that gradients around the minimum do not scale with L. This peculiar effect is attributed to rare events in the data. In addition we shed light on a type of sub-optimal local minima in the cost function which we dub "neglect minimua". Such minima occur when the network discards some important features of the data samples and instead develops a sub-optimal strategy based on the remaining features. Minima similar in nature to the above optimal-BFS and neglect minima are shown to occur in numerical training and dominate the training dynamics. Despite the fact the Maze-testing is a toy problem, we believe that its fundamental properties can be observed in real problems, as is frequently the case in natural phenomena (Schmidt & Lipson (2009)), making the presented analytical analysis of broader relevance. The paper is organized as follows: Sec. 2 describes in detail the Maze-testing problem. In Sec. 3 we suggest an appropriate architecture for the problem. In Sec. 4 we describe an optimal set of weights for the proposed architecture and proves that it solves the problem exactly. In Sec. 6 we report on training experiments and describe the observed training phenomena. In Sec. 6.1 we provide an analytical understanding of the observed training phenomena. Finally, we conclude with a discussion and an outlook.
2 MAZE-TESTING
Let us introduce the Maze-testing classification problem. Mazes are constructed as a random two dimensional, L by L, black and white array of cells (I) where the probability () of having a black cell is given by c = 0.59274(6) and other cells are white. An additional image (H0), called the hot-spot image, is provided. It defines the starting point by being zero (Off) everywhere except on a 2 by 2 square of cells having the value 1 (On) chosen at a random position. A sample from the Maze-testing dataset (i.e. a maze and a hot-spot image) is labelled Solvable if the ending point, defined as a 2 by 2 square at the center of the maze, is reachable from the starting point (defined by the hot-spot image) by moving horizontally or vertically along black cells. The sample is labelled Unsolvable otherwise.
Figure 1: Maze-testing dataset and proposed architecture.
A maze in the Maze-testing dataset has various non-trivial statistical properties which can be derived analytically based on results from percolation theory and conformal field theory (Cardy (2001); Langlands et al. (1994); Smirnov & Werner (2001)). Throughout this work we directly employ such
2

Under review as a conference paper at ICLR 2018
statistical properties, however we refer the reader to Refs. Cardy (2001); Langlands et al. (1994); Smirnov & Werner (2001) for further details and mathematical derivations from percolation theory.
At the particular value chosen for  the problem is at the percolation-threshold which marks the phase transition between the two different connectivity properties of the maze: below c the chance of having a solvable maze decays exponentially with r (the geometrical distance between the ending and starting points). Above c it tends to a constant at large r. Exactly at c the chance of having a solvable maze decays as a power law (1/r,  = 5/24). We note in passing that although Mazetesting can be defined for any , only the choice  = c leads to a computational problem whose typical complexity increases with L.
Maze-testing datasets can be produced very easily by generating random arrays and then analyzing their connectivity properties using breath-first-search (BFS) whose worse case complexity is O(L2). Notably as the system size grows larger the chance of producing solvable mazes decay as 1/L and so for very large L the labels will be biased towards unsolvable. There are several ways to de-bias the dataset. One is to select an unbiased subset of the dataset. Alternatively one can gradually increase the size of the starting-point a starting-square whose length scales as L to counter the power law decay. Unless stated otherwise, below we simply leave the dataset biased but define a normalized test error (err) which is proportional to the average mislabeling rate of the dataset divided by the average probability of being solvable.
3 THE ARCHITECTURE
Here we introduce an image classification architecture to tackle the problem. One may have doubts concerning the use of image classification architectures in a task which is, arguably, in the realm of graph theory. In this context, other approaches such as gated graph neural networks have been proposed (Henaff et al. (2015); Li et al. (2015)). To address such objections, we firstly recall that our attention is restricted to a subclass of planar graphs defined as regular lattices in the Euclidean space, in a manner that such data present the required statistical properties to be handle by a CNN; secondly, note that this restriction allow us to achieve our motivation of understanding how image classification performs on graph structured inputs. Furthermore, as shown below, our architecture can express an exact solution to the problem and, at least for small Mazes (L  16), can discover quite good solutions during training. Lastly, also the graph oriented architectures find it difficult to handle large sparse graphs due to regularization issues (Li et al. (2015)) whereas we can show that our architecture can perform reasonably well.
Our network, shown in Fig. (1), is a deep feedforward network with skip-layers followed by a logistic regression module. The deep part of the network consists of alternative convolutional and sigmoid layers. Each such layer (n) receives two L by L images, one corresponding to the original maze (I) and the other is the output of the previous layer (Hn-1). It performs the operation Hn = (Khot  Hn-1 + K  I + b), where  denotes a 2D convolution, the K convolutional kernel is 1×1, the Khot kernel is 3×3, b is a bias, and (x) = (1+e-x)-1 is the sigmoid function. The logistic regression layer consists of two perceptrons (j = 0, 1) acting on Hn as [p0, p1]T = WjHn + breg where H~ 0 is the rasterized hot-spot image and other H~ n are of the same size, Wj is a 2 by L2 matrix, and breg is a vector of dimension 2. The logistic regression module outputs the label Solvable if p1  p0 and Unsolvable otherwise. The cost function we used during training was the the standard negative log-likelihood.
4 AN OPTIMAL SOLUTION: THE BREATH-FIRST-SEARCH MINIMUM
As we next show, the architecture above can provide an exact solution to the problem by effectively forming a cellular automaton executing a breath-first-search (BFS). A choice of parameters which achieves this is   c = 9.727 ± 0.001,Khot = [[0, , 0], [, , ], [0, , 0]], K = 5c, and b = -5.5c and [W ]jq = (-1)j qcenterq, breg = [0.5, -0.5]T where qcenter is the index of the rasterized image (Hn) which corresponds to the center of the maze.
Let us explain pictorially how the above neural network processes the image. Initially H0 is On only at the starting-point. Passing through the first sigmoid-conv layer it outputs H1 which will be On (i.e.
3

Under review as a conference paper at ICLR 2018
have values close to one) on all black cells which neighbor the On cells and well as on the original starting point. Thus On regions spread on the black cluster which contains the original starting-point, while white clusters and black clusters which do not contain the starting-point remain Off (close to zero in Hn). The final logistic regression layer simply checks whether one of the four cells at the center of the maze are On and outputs the labels accordingly. To formalize the above we start by defining two activation thresholds, vl and vh, and refer to activations which are below vl as being Off and to those above vh as being On. The quantity vl is defined as the smallest of the three real solutions of the equation vl = (5vl - 0.5). Notably we previously chose  > c as this is the critical value above which three real solutions to vl (rather than one) exist. For vh we choose 0.9. Next we go case by case and show that the action of the sigmoid-convolutional layer switches activations between On and Off just as a BFS would. Since the kernel size is 3 × 3 this is a finite task which involves bounding the expression (Khot  Hn-1 + K  I0 + b) for all possibly 3 × 3 sub-arrays of H and 1 × 1 sub-arrays of I.
Figure 2: Case by case inspection of the action of the sigmoid-conv layer.
Consider the following cases shown in Fig. 2: Case 1. Consider a white cell at position r. For such cells I(r) = 0 and therefore the argument of the above sigmoid is smaller than -0.5c implying activation is below vl and so it is Off. Case 2. consider a black cell at r with Hn-1 in its vicinity all being Off (vicinity here refers to the cell and its 4 neighbors). Here the argument is smaller or equal to 5vl - 0.5c, and so activation remains Off as desired. Case 3. consider a black cell at r with one or more On activation of Hn-1 in its vicinity. Here the argument is larger than vhc - 0.5c = 0.4c. The sigmoid is then larger than 0.97 implying it is On. This exhausts all possibilities and shows that the above choice of weights performs one BFS iteration per layer. Lastly it can be easily verified that given an On (Off) activation at the middle of the maze the logistic regression layer will output the label Solvable (Unsolvable). Let us now determine the required depth for this specific architecture. The previous analysis tells us that at depth d unsolvable mazes would always be labelled correctly however solvable mazes would be label correctly only if the shortest-path between the starting-point and the center is d or less. The worse case scenario thus occurs when the center of the maze and the starting-point are connected by an essentially one dimensional curve twisting its way along O(L2) sites. Therefore for perfect performance the network depth would have to scale as the number of sites namely O(L2). A tighter bound on the minimal depth can be established by borrowing various results from percolation theory. It is known Zhou et al. (2012) that the typical length of the shortest path (l) for critical percolation scales as rdmin , where r is the geometrical distance and dmin = 1.1(3). Moreover it is known that the probability distribution P (l|r) has a tail which falls as l-2 for l > rdmin Dokholyan et al. (1999). Consequently the chance that at distance r the shortest path is longer than rdmin ra where a is some small positive number decays to zero and so d should scale as L with a power slightly larger than dmin (say 1.2).
5 TRAINING EXPERIMENTS
We have performed several training experiments with our architecture on L = 16 and L = 32 mazes with depth 16 and 32 respectively and datasets of sizes N = 1000, N = 10000, and N = 50000. For
4

Under review as a conference paper at ICLR 2018
L = 16, batch size of 20, learning rate of 0.02, N >= 10000, Khot and K initialized with random numbers. In the following, we split into two different groups corresponding to the related phenomena observed during training, which will the analyzed in detail in the next section.
Optimal-BFS like minima. For L = 16 mazes and a positive random initialization for Khot and K in [0, 6/8] the network found a solution with  9% normalized test error performance in 3 out of 7 attempts (baseline test error was 50%). In all three successful cases the minima was a variant of the Optimal-BFS minima which we refer to as the checkerboard-BFS minima. It is distinct from the optimal-BFS only as it spreads the activations from the starting-point using a checkerboard pattern rather than a uniform one. The fact that it reaches  9% test error rather than zero is attributed to this checkerboard behavior which can occasionally misses out the exit point from the maze.
Neglect minima. Again for L = 16 however allowing negative entries in K and Khot test error following 14 attempts and 500 epochs did not improve below 44%. Analyzing the weights of the network, the 6% improvement over the baseline error (50%) came solely from identifying the inverse correlation between many white cells near the center of the maze and the chance of being solvable. Notably, this heuristic approach completely neglects information regarding the starting-point of the maze. For L = 32 mazes, despite trying several random initialization strategies, dataset sizes, and learning rates, the network always settled into such a partial neglect minimum.
In an unsuccessful attempt to push the weights away from such partial neglect behavior we performed further training experiments with a biased data-set in which the maze itself was directly correlated with the label or more accurately marginalizing over the starting-point there is an equal chance for both labels. To achieve this a maze shape was chosen randomly and then many random locations were tried-out for the starting-point using that same maze. From these we picked 5 that resulted in a Solvable label and 5 that resulted in an Unsolvable label. Maze shapes which were always Unsolvable were discarded. Both the L = 16 and L = 32 mazes trained on this biased dataset performed poorly and yielded 50% test error. Interestingly they improved their cost function by settling into weights in which b  -10 is large compared to [Khot]ij < 1 while W and b were close to zero (order of 0.01). We have verified that such weights imply that that activations in the last layer have a negligible dependence on the starting-point and a weak dependence on the maze shape. We thus refer to this minimum as a "total neglect minimum".
6 COST FUNCTION LANDSCAPE AND THE OBSERVED TRAINING PHENOMENA
Here we seek an analytical understanding of the aforementioned training phenomena through the analysis of the cost function around the solutions similar or equal to those the network settled into during training. Specifically we shall first study the cost function landscape around the optimal-BFS minimum. As would become clearer at the end of that analysis, the optimal BFS shares many similarities with the checkerboard-BFS minimum from during training and one thus expects a similar cost function landscape around both of these. The second phenomena analyzed below is the total neglect minimum which is an extreme version of the partial neglect minima.
6.1 THE SHAPE OF THE COST FUNCTION NEAR THE OPTIMAL-BFS MINIMUM
Our analysis of the cost function near the optimal-BFS minimum will be based on two separate models capturing the short and long scale behavior of the network near this miminum. In the first model we approximate the network by linearizing its action around weak activations. This model would enable us to identify the density of what we call "bugs" in the network. In the second model we discretize the activation levels of the neural network into binary variables and study how the resulting cellular automaton behaves when the these such bugs are introduced.
6.1.1 LINEARIZATION AROUND THE OPTIMAL-BFS MINIMUM AND THE EMERGENCE OF BUGS
Unlike an algorithm, a neural network is an analog entity and so a-priori there are no sharp distinctions between a functioning and a dis-functioning neural network. An algorithm can be debugged and the bug can be identified as happening at a particular time step. However it is unclear if one can generally pin-point a layer and a region within where a deep neural network clearly malfunctioned. Interestingly we show that in our toy problem such pin-pointing of errors can be done in a sharp
5

Under review as a conference paper at ICLR 2018

fashion by identifying fast and local processes which cause an unwanted switching been Off and On activations in Hn. We call these events bugs, as they are local, harmful, and have a sharp meaning in the algorithmic context. Below we obtain asymptotic expressions for the chance of generating such bugs as the network weights are perturbed away from the optimal-BFS minimum.

For concreteness let us perturb the parameter , controlling the internal weights and consider the shape of the cost function. Recalling the analysis in Sec. 4, initially as it is decreased  has no effect but to shift vl (the Off activation threshold) to a higher value. However at the critical value ( = c, vl = vl,c) the solution corresponding to vl vanishes (becomes complex) and the correspondence with the BFS algorithm no longer holds in general. This must not mean that all Off activations are no longer stable. Indeed recall that in Sec. 4 the argument that a black Off cell in the vicinity of Off cells remains Off (Fig. 2 panel (2)) assumed a worse case scenario in which all the cells in its vicinity where both Off and black and had the maximal activation allowed (vl). However if some cells in its vicinity are white, their Off activations levels are mainly determined by the absence of the large K term in the sigmoid argument and are orders of magnitude smaller than vl. We come to the conclusion that black Off cells in the vicinity of many white cells are less likely to be spontaneously turned On than black Off cells which are part of a large cluster of black cells. In fact one can show that infinitesimally below c only uniform black mazes will cause the network to malfunction.

To establish this analytically consider a maze of size l × l where the hot-spot image is initially all zero and thus Off. Intuitively this hot-spot image should be thought of as a sub-area of a larger maze located far away from the starting-point of this larger maze. In this case a functioning network must leave all activation levels below vl. As a first step in quantifying the chance of bugs we thus study the probability that the output of the final sigmoid-convolutional layer will have one or more On cells.

To this end, we find it useful to linearize the system around low activation yielding (see Appendix for complete derivation)



n(rb) = ~ n-1(rb) +

n-1(rb) + O(~n2 )

rb ,rb

(1)

where rb denotes black cells (I(rb) = 1), the sum is over the nearest neighboring black cells to rb,

n(r)

=

Hn(r)

-

vl,c,

and

~

=



d dx

(-1(vl,c)).

For a given maze (I), Eq. (11), defines a linear Hermitian operator (LI) with random off-diagonal matrix elements dictated by I via the restriction of the off-diagonal terms to black cells. Stability is ensured if this linear operator is contracting or equivalently if all its eigenvalues are smaller than 1 in magnitude.

Hermitian operators with random off-diagonal elements have been studied extensively in physics,
in the context of disordered systems and Anderson localization (Kramer & MacKinnon (1993)). Let us describe the main relevant results. For almost all I's the spectrum of L consists of localized eigenfunctions. Any such function is centered around a random site (rm) and decays exponentially away from that site with a decay length of  which in our case would be a several cells long. Thus given a localized eigenfunction (m) with an eigenvalue |Em| > 1, t repeated actions of the sigmoidconvolutional layer will make n(r) in a  vicinity of rm grow in size as eEmt. Thus (|Em| - 1)-1 gives the characteristic time it takes these localized eigenvalue to grow into an unwanted localized
region with an On activation which we define as a bug.

Our original question of determining the chance of bugs now translates into a linear algebra task:

Finding, N~, The number of eigenvalues in LI which are larger than 1 in magnitude, averaged over I, for a given . Since ~ simply scales all eigenvalues one finds that N~ is the number of eigenvalues larger than ~-1 in LI with ~ = 1. Analyzing this latter operator, it is easy to show that

the maximal eigenvalues occurs when n(r) has a uniform pattern on a large uniform region where

the I is black. Indeed if I contains a black uniform true box of dimension lu by lu, the maximal

eigenvalue is easily shown to be Elu = 5 - 22/(lu)2. However the chance that such a uniform

region

exists

goes as

(l/lu)2elog(c)lu2

and

so

P (E)



l e ,2

log(c )22 (E)

where

E

=

5 - E.

This

reasoning is rigorous as far as lower bounds on N~ are concerned, however it turns out to capture the functional behavior of P (E) near E = 0 accurately (Johri & Bhatt (2012)) which is given

6

Under review as a conference paper at ICLR 2018

by

P (E



0+)

=

l e2

-

C (E)

,

where

the

unknown

constant

C

captures

the

dependence

on

various

microscopic details. In the Appendix we find numerically that C  0.7. Following this we find

N~  l2

E 0

dxP (x)

where

E

=

5

-

~-1



0.

The

range

of

integration

is

chosen

to

includes

all eigenvalues which, following a multiplication by ~, would be larger than 1.

To conclude we found the number of isolated unwanted On activations which develop on l × l Off

regions. Dividing this number by l2 we obtain the density of bugs (bug) near   c. The last

technical step is thus to express bug in terms of . Focusing on the small bug region or E  0+,

we

find

that

E

=

0

occurs

when

d dx

(-1(

()))

=

1/(5),

~

=

1/5,

and



=

c

=

9.72(7).

Expanding

around



=

c

we

find

E

=

49-c 10c

(c

-

)

+

O((c

- )2)

and

so

bug 

49-c 10c

(c -)

dxP (x)



C
e -c

0

(2)

where C

=

C

10c 49-c

.

6.1.2 EFFECTS OF BUGS ON BREATH FIRST SEARCH

In this section we wish to understand the large scale effect of bug namely, its effect on the overall performance of the network and therefore on the cost function. To this end, we note that as a bug is created in a large maze, it quickly switches On the cells within the black "room" in which it was created. From this region it spreads according to BFS and turns On the entire cluster connected to the buggy room (See the Appendix for a visualization). To asses the effect this bug has on performance first note that solvable mazes would be labelled Solvable regardless of bugs however unsolvable mazes might appear solvable if a bug occurs on a cell which is connected to the center of the maze. Assuming we have an unsolvable maze, we thus ask what is the chance of it being classified as solvable.

Given a particular unsolvable maze instance (I), the chance of classifying it as solvable is given by perr(I) = 1 - (1 - bug)s = 1 - e-bugs + O(b2ug) where s counts the number of sites in the cluster connected to the central site (central cluster). The probability distribution of s for percolation is known an given by p(s) = Bs1- ,  = 187/91 (Cardy & Ziff (2003)) with B an order of one

constant which depends on the underlying lattice. Since clusters have a fractional dimension, the

maximal cluster size is Ldf , df = 91/48. Consequently perr(I) averaged over all I instances is given

by perr =

Ldf 0

dsp(s) [1

-

e-bug s ]

which

can

be

easily

expressed

in

terms

of

gamma

functions

((x), (a, x)). In the limit of bug < L-df where its derivatives with respect to bug are maximal

it simplifies to

perr = ( - 2)-1BbugLdf (3- )  bugL2-5/24

(3)

whereas for  > L-df its behavior changes to perr = (-B(2 -  ))(-2)  5bu/9g1. Notably once bug becomes of order one, several of the approximation we took break down.
Let us relate perr to the test error (err). In Sec. (2) the cost function was defined as the mislabeling chance over the average chance of being solvable (psolvable). Following the above discussion the mislabelling chance is perrpsolvable and consequently err = perr.
One should appreciate the potential severity of the above results: Considering L  , if bug was simply proportional to c - , cost will have a sharp singularity near zero. For instance as one reduces err by a factor of 1/e, the gradients increase by e86/5  3E + 7. For finite L this behavior would saturate when the gradients reach a magnitude of L2-5/24. These effects are in accordance with ones intuition that a few bugs in a long algorithm will typically have a devastating effect on performance.

Interestingly however the essential singularity in bug(), derived in the previous section, completely flattens the gradients near c. Indeed by combining Eqs. 2 and 3 we obtain our key result

err  L2-5/24

49-c 10c

(c -)

dxP (x)



L2-5/24

e

C -c

0

(4)

7

Under review as a conference paper at ICLR 2018

in the range (c - ) < C d-f 1/ log(L). Note that the seemingly divergence factor of L2-5/24 is misleading here as applicability range shrinks as L grows large. Outside this range it becomes

err



5bu/9g1



5C /91
e -c

(5)

for C df-1/ log(L) + O(log-2(L)) < (c - ) < C .
Thus the essentially singularity which comes directly from rare events in the dataset strongly regulates the test error and in a related way the cost function. However it also have a negative side-effect concerning the robustness of generalization. Given a finite dataset the rarity of events is bounded and so having  < c may still provide perfect performance. However when encountering larger dataset some samples with rarer events would appear and the network will fail sharply on these (i.e. the wrong prediction would get a high probability). Further implications of this dependence on rare events on training and generalization error will be studied in future work. In particular we believe that the instabilities found for checker-board BFS minima during training are related to rare mini-batches containing several mazes with room whose size is just at the threshold for bugs.

Lastly we comment that although only a single type of perturbation (a change of ) was considered, the above considerations should hold for generic small perturbations as the distribution of extreme eigenvalues of bounded disordered linear operators typically show an essential singularity near the band edge (Johri & Bhatt (2012)). The constants C and c will require re-evaluations though. We have tested this conjecture on several other types of perturbations (including ones that violate the hermitian nature of the linear operator) and found that it holds. Similar analysis can thus be repeated for the checkerboard-BFS minima.

6.2 COST FUNCTION NEAR A TOTAL NEGLECT MINIMA

We turn to the discuss a second phenomena encountered during training which is the tendency of the network weights to converge to a total neglect minima which the final layers of the network are independent of the data.
To provide an explanation for this phenomena let us divide the activations of the upper layer to its starting-point dependent and independent parts. Let HN denote the activations at the top layer. We expand them as a sum of two functions

HN = A(H0, I) + B(I)

(6)

where the function A and B are normalized such that their variance on the data (,  resp.) is 1. Notably near the reported total neglect minima    e-10. Also note that within the bias dataset the maze itself is uncorrelated with the labels and thus  can be thought of as noise. Clearly then any solution to the Maze testing problem requires the starting-point dependent part () to become larger than the independent part (). We argue however that in the process of increasing  the activations will have to go through an intermediate "noisy" region. In this noisy region  grows in magnitude however much less than  and in particular obeys  < 2. As shown in the Appendix the negative log-likelihood, a commonly used cost function, is proportional to 2 -  for ,  1. Thus it penalizes random false predictions and, within a region obeying  < 2 it has a minimum (global with respect to that region) when  =  = 0. The later being the definition of a total neglect minima.
Establishing the above  2 conjecture analytically requires several pathological cases to be examined and is left for future work. In this work we provide an argument for its typical correctness along with supporting numerics.
A deep convolution network with a finite kernel has a notion of distance and locality. For many parameters ranges, it exhibits a typical correlation length (). That is: a scale beyond which two activations are statistically independent. Clearly to solve the current problem  has to grow to an order of L such that information from the input reaches the output. However as  gradually grows, relevant and irrelevant information is being mixed and propagated onto the final layer. While  depends on information which is locally accessible at each layer (i.e. the maze shape),  requires information to travel from the first layer to the last. Consequently  and  are expected to scale differently, as e-L/ and e-1/ respectively (for  << L). Given this one finds that  2 as claimed.

8

Under review as a conference paper at ICLR 2018
Further numerical support of this conjecture is shown in the Appendix where an upper bound on the ratio /2 is studied on 100 different paths leading from the total neglect miminum found during training to the checkerboard-BFS minimum. In all cases there is a large region around the total neglect minimum in which  2.
7 CONCLUSIONS
Despite their black-box reputation, in this work we were able to shed some light on how a particular deep CNN architecture learns to classify topological properties of graph structured data. Instead of focusing our attention on general graphs, which would correspond to data in non-Euclidean spaces, we restricted ourselves to planar graphs over regular lattices, which are still capable of modelling real world problems while being suitable to CNN architectures.
We described a toy problem of this type (Maze-testing) and showed that a simple CNN architecture can express an exact solution to this problem. Our main contribution was an asymptotic analysis of the cost function landscape near two types of minima which the network typically settles into: BFS type minima which effectively executes a breath-first-search algorithm and poorly performing minima in which important features of the input are neglected.
Quite surprisingly, we found that near the BFS type minima gradients do not scale with L, the maze size. This implies that global optimization approaches can find such minima in an average time that does not increase with L. Such very moderate gradients are the result of an essential singularity in the cost function around the exact solution. This singularity in turn arises from rare statistical events in the data which act as early precursors to failure of the neural network thereby preventing a sharp and abrupt increase in the cost function.
In addition we identified an obstacle to learning whose severity scales with L which we called neglect minima. These are poorly performing minima in which the network neglects some important features relevant for predicting the label. We conjectured that these occur since the gradual incorporation of these important features in the prediction requires some period in the training process in which predictions become more noisy. A "wall of noise" then keeps the network in a poorly performing states.
It would be interesting to study how well the results and lessons learned here generalize to other tasks which require very deep architectures. These include the importance of rare-events, the essential singularities in the cost function, the localized nature of malfunctions (bugs), and neglect minima stabilized by walls of noise.
These conjectures could be tested analytically using other toy models as well as on real world problems, say ones involving realistic street maps. More specifically the importance of rare-events can be analyzed by studying the statistics of errors on the dataset as it is perturbed away from a numerically obtained minimum. Technically one should testing whether the perturbation induces an typical small deviation of the prediction on most samples in the dataset or rather a strong deviation on just a few samples. Bugs can be similarly identified by comparing the activations of the network on the numerically obtained minimum and on some small perturbation to that minimum while again looking at typical versus extreme deviations. Such an analysis can potentially lead to safer and more robust designs were the network fails typically and mildly rather than rarely and strongly.
Turning to partial neglect minima these can be identified provided one has some prior knowledge on the relevant features in the dataset. The correlations or mutual information between these features and the activations at the final layer can then be studied to detect any sign of neglect. If problems involving neglect are discovered it may be beneficial to add extra terms to the cost function which encourage more mutual information between these neglected features and the labels thereby overcoming the noise barrier and pushing the training dynamics away from such neglect minimum.
REFERENCES
Andriluka, Mykhaylo, Pishchulin, Leonid, Gehler, Peter, and Schiele, Bernt. 2d human pose estimation: New benchmark and state of the art analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3686­3693, 2014.
9

Under review as a conference paper at ICLR 2018
Bronstein, Michael M, Bruna, Joan, LeCun, Yann, Szlam, Arthur, and Vandergheynst, Pierre. Geometric deep learning: going beyond euclidean data. arXiv preprint arXiv:1611.08097, 2016.
Bruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and LeCun, Yann. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Cardy, John. Conformal invariance and percolation. arXiv preprint math-ph/0103018, 2001.
Cardy, John and Ziff, Robert M. Exact results for the universal area distribution of clusters in percolation, ising, and potts models. Journal of statistical physics, 110(1):1­33, 2003.
Daniely, Amit, Frostig, Roy, and Singer, Yoram. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In Advances In Neural Information Processing Systems, pp. 2253­2261, 2016.
Dauphin, Yann N, Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, Ganguli, Surya, and Bengio, Yoshua. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in neural information processing systems, pp. 2933­2941, 2014.
Dokholyan, Nikolay V, Buldyrev, Sergey V, Havlin, Shlomo, King, Peter R, Lee, Youngki, and Stanley, H.Eugene. Distribution of shortest paths in percolation. Physica A: Statistical Mechanics and its Applications, 266(1 - 4):55 ­ 61, 1999. ISSN 0378-4371.
Ghahramani, Zoubin. Probabilistic machine learning and artificial intelligence. Nature, 521(7553): 452­459, 2015.
Henaff, Mikael, Bruna, Joan, and LeCun, Yann. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015.
Jain, Ashesh, Zamir, Amir R, Savarese, Silvio, and Saxena, Ashutosh. Structural-rnn: Deep learning on spatio-temporal graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5308­5317, 2016.
Johri, S and Bhatt, RN. Singular behavior of eigenstates in anderson's model of localization. Physical review letters, 109(7):076402, 2012.
Kramer, Bernhard and MacKinnon, Angus. Localization: theory and experiment. Reports on Progress in Physics, 56(12):1469, 1993.
Langlands, Robert, Pouliot, Philippe, and Saint-Aubin, Yvan. Conformal invariance in twodimensional percolation. arXiv preprint math/9401222, 1994.
LeCun, Yann, Bengio, Yoshua, and Hinton, Geoffrey. Deep learning. Nature, 521(7553):436­444, 2015.
Li, Yujia, Tarlow, Daniel, Brockschmidt, Marc, and Zemel, Richard. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.
Masci, Jonathan, Boscaini, Davide, Bronstein, Michael, and Vandergheynst, Pierre. Geodesic convolutional neural networks on riemannian manifolds. In Proceedings of the IEEE international conference on computer vision workshops, pp. 37­45, 2015a.
Masci, Jonathan, Boscaini, Davide, Bronstein, Michael, and Vandergheynst, Pierre. Shapenet: Convolutional neural networks on non-euclidean manifolds. Technical report, 2015b.
Masucci, Adolfo Paolo, Smith, D, Crooks, A, and Batty, Michael. Random planar graphs and the london street network. The European Physical Journal B-Condensed Matter and Complex Systems, 71(2):259­271, 2009.
Nguyen, Anh, Yosinski, Jason, and Clune, Jeff. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427­436, 2015.
10

Under review as a conference paper at ICLR 2018
Perozzi, Bryan, Al-Rfou, Rami, and Skiena, Steven. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701­710. ACM, 2014.
Scarselli, Franco, Gori, Marco, Tsoi, Ah Chung, Hagenbuchner, Markus, and Monfardini, Gabriele. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61­80, 2009.
Schmidt, Michael and Lipson, Hod. Distilling free-form natural laws from experimental data. science, 324(5923):81­85, 2009.
Smirnov, Stanislav and Werner, Wendelin. Critical exponents for two-dimensional percolation. arXiv preprint math/0109120, 2001.
Swirszcz, Grzegorz, Czarnecki, Wojciech Marian, and Pascanu, Razvan. Local minima in training of deep networks. arXiv preprint arXiv:1611.06310, 2016.
Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May 2016. URL http://arxiv.org/abs/ 1605.02688.
Viana, Matheus P, Bordin, Patricia, Barthelemy, Marc, and Strano, Emanuele. The simplicity of planar networks. Nature Scientific Reports, 3(arXiv: 1312.3788):3495, 2013.
Yu, Jeffrey Xu and Cheng, Jiefeng. Graph reachability queries: A survey. In Managing and Mining Graph Data, pp. 181­215. Springer, 2010.
Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Benjamin, and Vinyals, Oriol. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Zhou, Zongzheng, Yang, Ji, Deng, Youjin, and Ziff, Robert M. Shortest-path fractal dimension for percolation in two and three dimensions. Physical Review E, 86(6):061101, 2012.
A VISUALIZATION OF THE OPTIMAL-BFS MINIMUM
Fig. (3) gives an example of how black clusters are gradually turned On in Hn as a function of n when the network is tuned to the optimal-BFS minimum.
B TEST ERROR AWAY FROM THE OPTIMAL-BFS SOLUTION
We have implemented the architecture described in the main text using Theano (Theano Development Team (2016)) and tested how cost changes as a function of  = c -  (c = 9.727..) for mazes of sizes L = 24, 36 and depth (number of layers) 128. These depths are enough to keep the error rate negligible at  = 0. A slight change made compared to Maze-testing as described in the main text, is that the hot-spot was fixed at a distance L/2 for all mazes. The size of the datasets was between 1E + 5 and 1E + 6. We numerically obtained the normalized performance (costL()) as a function of L and . As it follows from Eq. (4) in the main text the curve, log(L-2+5/24costL()), for the L = 24 and L = 36 results should collapse on each other for bug < L-df . Fig. (4) depicts three such curves, two for L = 36, to give an impression of statistical error, and one for L = 24 curve (green), along with the fit to the theory (dashed line). The fit, which involves two parameters (the proportionally constant in Eq. (4) of the main text and C) captures well the behavior over three orders of magnitude. As our results are only asymptotic, both in the sense of large L and   c, minor discrepancies are expected.
11

Under review as a conference paper at ICLR 2018

Figure 3: A numerical experiment showing how our maze classification architecture processes a
particular sample consisting a maze (black and white image) and a hot-spot image marking the
starting-point (panel (0)) when its weights are tuned to the optimal BFS solution. The first layer receives a hot-spot image which is On only near the starting-point of the maze H0) (panel (0)). This On activation then spreads on the black cluster containing the start-point in (Hn with n = 1, 2, 4, 8, panels 1,2,4,8 resp.). Notably other region are Off (i.e. smaller than vl) but they are not zero as shown by the faint imprint of the maze on Hn.

C LINEARIZATION OF THE SIGMOID-CONVOLUTIONAL NETWORK AROUND OFF ACTIVATION

To prepare the action of sigmoid-convolutional for linearization we find it useful to introduce the following variables on locations (rb, rw) with black (b) and white (w) cells

n(r) = Hn(r) - a(r) a(rb) = vl,c a(rw) = e-5.5.

(7) (8) (9)

Rewriting the action of the sigmoid-convolutional layer in terms of these we obtain





n(r) + a(r) =   n-1(r) + n-1(r ) + 5I(r) - 5.5 + d(r) , (10)
r ,r

d(r) = a(r) +

a(r )

r ,r

where r ,r means summing over the 4 sites neighboring r. Next we treating n(r) as small and Taylor expand





n(rw) =  dx |-5.5 n-1(r) +

n-1(r ) + d(rw)

r ,rw

(11)



d

n(rb) =  dx |-1(vl,c) n-1(r) +

n-1(r ) + (d(rb) - 5vl,c)

r ,rb

12

Under review as a conference paper at ICLR 2018

Figure 4: Logarithm of the scaled (L-2+5/24) normalized test error (err) as a function of a deviation () from the optimal-BFS weights for two maze sizes along with a fit to the theory. The collapse of the two graphs supports our scaling predictions and the fit with the theory verifies the existence of an essential singularity.

where vl,c  0.02(0) is the low (and marginally stable) solution of the equation vl,c = (-0.5c + 5vl,c).

Next in the consistency with our assumption that |n-1(r)| is small, we can assume |n-1(r)| < 1, and obtain that n(rw) < e-5.5(5 + e-0.5) and therefore, since we are working near  = 9.727.. it is negligible. The equation of n(rb) now appears as



n(rb) = ~ n-1(r) +

n-1(r

)

+

d(rb)

-

5vl,c)

+

d2 O( dx2

2)

rb ,rb

(12)

where

the

summation

of

neighbor

now

includes

only

black

cells

and

~

=



d dx

| .-1(vl,c)

Due

to

form

the

sigmoid

function,



d2  dx2

|-1

[

]

is

of

the

same

magnitude

as

~,

and

consequently

the

relative

smallness of this terms is guaranteed as long as n 1.

We thus obtained a linearized version for the sigmoid-convolutional network which is suitable for
stability analysis. Packing n(rb) and d(rb) - 5vl,c into vectors (n,d(rb)) the equation we obtained can be written as

n = Sn-1 + d

(13)

with S being a symmetric matrix. Denoting by nT and sn the left eigenvectors and eigenvalues of S, we multiply the above equation from the left with Tn and obtain

cn = sncn-1 + Tn d

(14)

n = cnn.
n

13

Under review as a conference paper at ICLR 2018

Stability analysis on this last equation is straightforward: For |sn| < 1, a stable solution exists

given

by

cn

=

Tn d (1-sn

)

.

Furthermore

as

the

matrix

S

has

strong

disorder,

n

are

localized

in

space.

Consequently nT d is of the same order of magnitude as d  e-0.5  0.01 and as long as sn < 0.9,

these stable solutions are well within the linear approximation we have carried. For |sn| > 1, there

are no stable solutions.

There is an important qualitative lesson to be learned from applying these results on an important test case: A maze with only black cells. In this case it is easy to verify directly on the non-linear sigmoid-convolutional map that a uniform solution becomes unstable exactly at  = c. Would we find the same result within our linear approximation?

To answer the above, first note that the maximal eigenvalue of S will be uniform with smax = 5~. Furthermore for an all black maze d would be exactly zero and the linear equation becomes

homogeneous. Consequently destabilization occurs exactly at ~ = 1/5 and is not blurred by the

inhomogeneous terms. Recall that c is defined as the value at which the two lower solutions of

x = [-0.5c + 5cx] and it also satisfies the equation vl,c = [-0.5c + 5cvl,c]. Taking a

derivative

of

the

former

and

putting

x

=

vl,c

one

finds

that

1

=

5c

d [-0.5c +5vl,c ] dx

.

It

is

now

easy

to verify that even within the linear approximation destabilization occurs exactly at c. The source of

this agreement is the fact that d vanishes for a uniform black maze.

The qualitative lesson here is thus the following: The eigenvectors of S with large s, are associated
with large black regions in the maze. It is only on the boundaries of such regions that d is non-zero. Consequently near   c the d term projected on the largest eigenvalues can, to a good accuracy, be ignored and stability analysis can be carried on the homogeneous equation  = S where sn < 1 means stability and sn > 1 implies a bug.

D LOG-LIKELIHOOD AND NOISY PREDICTIONS

Consider an abstract classification tasks where data point x  X are classified into two categories l  {0, 1} using a deterministic function f : X  {0, 1} and further assume for simplicity that the chance of f (x) = a is equal to f (x) = b. Phrased as a conditional probability distribution Pf (l|x) is given by Pf (f (a)|x) = 1 while Pf (!f (a)|x) = 0. Next we wish to compare the following family of approximations to Pf

P , (l|x) = 1/2 +  (2l - 1)(2f (x) - 1) +  (2l - 1)(2g(x) - 1)

(15)

where g|X  {0, 1} is a random function, uncorrelated with f (x), outputting the labels {0, 1} with equal probability. Notably at  = 1/2,  = 0 it yields Pf while at  ,  = 0 it is simply the maximum entropy distribution.

Let us measure the log-likelihood of P , under Pf for  ,  1

L( ,  ) = Pf (x, l) log (1/2 +  (2l - 1)(2f (x) - 1) +  (2l - 1)(2g(x) - 1)) (16)
(x,l)
 Pf (x, l) log(1/2) + 2 [ (2l - 1)(2f (x) - 1) +  (2l - 1)(2g(x) - 1)]
(x,l)
- 2 [ (2l - 1)(2f (x) - 1) +  (2l - 1)(2g(x) - 1)]2 = log(1/2) + 2 - 2 2 - 2 2

We thus find that  reduces the log-likelihood in what can be viewed as a penalty to false confidence or noise. Assuming, as argued in the main text, that  is constrained to be smaller than  2 near   0, it is preferable to take both  and  to zero and reach the maximal entropy distribution. We note by passing that the same arguments could be easily generalized to f (x), g(x) taking real values leading again to an O() - O(2) dependence in the cost function.
Let us relate the above notations to the ones in the main text. Clearly x = ({I}, H0) and {0, 1} = {U nsolvable, Solvable}. Next we recall that in the main text  and  multiplied the vectors function representing the H0-depended and H0-independent parts of Hn. The probability estimated by the

14

Under review as a conference paper at ICLR 2018

logistic regression module was given by

eKS olvable ·Hn

P (Solvable|x) =

e + e-KSolvable·Hn

-KU nsolvable ·Hn

eKU nsolvable ·Hn

P (U nsolvable|x) =

e + e-KSolvable·Hn

-KU nsolvable ·Hn

which yields, to leading order in  and 

(17)

P,(l|x) = 1/2 + (2l + 1)Kl- · A + (2l + 1)Kl- · B

(18)

where K- = (KSolvable - KUnsolvable)/2 and (2l + 1) understood as the taking the values ±1.
Consequently (2f - 1) and (2g - 1) are naturally identified with KSolvable · A/NA and KSolvable · B/NB respectively with NA and NB being normalization constants ensuring a variance of 1. While ( ,  ) = (NA, NB). Recall also that by construction of the dataset, the g we thus obtain is uncorrelated with f .

E NUMERICAL SUPPORT FOR THE  2 CONJECTURE
Here we provide numerical evidence showing that  2 in a large region around the total neglect minima found during the training of our architecture on the biased dataset (i.e. the one where marginalizing over the starting-point yields a 50/50 chance of being solvable regardless of the maze shape).
For a given set of Khot, K and b parameters we fix the maze shape and study the variance of the top layer activations given O(100) different starting points. We pick the maximal of these and then average this maximal variance over O(100) different mazes. This yields our estimate of . In fact it is an upper bound on  as this averaged-max-variance may reflect wrong prediction provided that they depend on H0.
We then obtain an estimate of  by again calculating the average-max-variance of the top layer however now with H0 = 0 for all maze shapes.
Next we chose a 100 random paths parametrized by  leading from the total neglect minima ( = 0) for the total neglect through a random point at  = 15, and then to the checkerboard-BFS minima at  = 30. The random point was placed within a hyper-cube of length 4 having the total neglect minima at its center. The path was a simple quadratic interpolation between the three point. Fig. 5 shows the statistics of /2 on these 100 different paths. Notably no path even had  > e-302 within the hyper-cube. We have tried three different other lengths for the hyper cube (12 and 1) and arrived at the same conclusions.

15

Under review as a conference paper at ICLR 2018
Figure 5: The natural logarithm of an upper bound to /2 as a function of a paramterization () of a path leading from the numerically obtained total neglect minima to the checkerboard BFS minima through a random point. The three different curves show the max,mean, and median based on a 100 different paths. Notably no path violated the  2 constrain in the vicinity of the total neglect minima.
16

