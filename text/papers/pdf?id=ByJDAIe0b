Under review as a conference paper at ICLR 2018
INTEGRATING EPISODIC MEMORY INTO A REINFORCEMENT LEARNING AGENT USING RESERVOIR SAMPLING
Anonymous authors Paper under double-blind review
ABSTRACT
Episodic memory is a psychology term which refers to the ability to recall specific events from the past. We suggest one advantage of this particular type of memory is the ability to easily assign credit to a specific state when remembered information is found to be useful. Inspired by this idea we propose a novel algorithm which uses a reservoir sampling procedure to maintain a memory consisting of a fixed number of past states. The algorithm allows a deep reinforcement learning agent to learn online to preferentially remember those states which are found to be useful to recall later on.
Much of reinforcement learning (RL) theory is based on the assumption that the environment has the Markov property, meaning that future states are independent of past states given the present state. This implies the agent has all the information it needs to make an optimal decision at each time and therefore has no need to remember the past. This is however not realistic in general, realistic problems often require significant information from the past to make an informed decision in the present, and there is often no obvious way to incorporate the relevant information into an expanded present state. It is thus desirable to establish techniques for learning a representation of the relevant details of the past (e.g. a memory, or learned state) to facilitate decision making in the present.
A popular approach to integrate information from the past into present decision making is to use some variant of a recurrent neural network, possibly coupled to some form of external memory, trained with backpropagation through time. This can work well for many tasks, but generally requires backpropagating many steps into the past which is not practical in an online RL setting. In this work we investigate an alternative approach, which we liken to the idea of episodic memory from psychology. In this approach the information stored in memory is constrained to consist of a finite set of past states experienced by the agent. In this work, by states we mean observations explicitly provided by the environment. In general, states could be more abstract, such as the internal state of an RNN or predictions generated by something like the Horde architecture of Sutton et al. (2011). By storing states explicitly we enforce that the information recorded also provides the context in which it was recorded. We can therefore assign credit to the recorded state without explicitly backpropagating through time between when the information proves useful and when it was recorded. If a recorded state is found to be useful we train the agent to preferentially remember similar states in the future.
In our approach the set of states in memory at a given time is drawn from a distribution over all n-subsets (subsets of size n) of visited states, parameterized by a weight value assigned to each state by a trained model. To allow us to draw from such a distribution without maintaining all visited states in memory we introduce a reservoir sampling technique. Reservoir sampling refers to a class of algorithms for sampling from a distribution over n-subsets of items from a larger set streamed one item at a time. The goal is to ensure, through specific add and drop probabilities, that the n items in the reservoir at each time-step correspond to a sample from the desired distribution over n-subsets of all observed items. Two important examples which sample from different distributions are found in Chao (1982) and Efraimidis & Spirakis (2006). In this work we will define our own distribution and sampling procedure to suit our needs.
1

Under review as a conference paper at ICLR 2018

1 RELATED WORK
Deep learning systems which make use of an external memory have received a lot of interest lately. Two prototypical examples are found in Graves et al. (2014) and the follow-up Graves et al. (2016). These systems use an LSTM controller attached to read and write heads of a fully differentiable external memory and train the combined system to perform algorithmic tasks. Contrary to our approach, training is done entirely by backpropagation through time. See also Zaremba & Sutskever (2015), Joulin & Mikolov (2015), Sukhbaatar et al. (2015), Gulcehre et al. (2017) and Kaiser et al. (2017) for more examples of deep learning systems with integrated external memory.
More directly related to the present work is the application of deep RL to non-markov tasks, in particular Oh et al. (2016). They experiment with architectures using a combination of key-value memory and a recurrent neural networks. The memory saves keys and values corresponding to the last N observations for some integer N , thus it is inherently limited in temporal extent but does not require any mechanism for information triage. They test on problems in the Minecraft domain which could provide compelling testbeds for a potential follow-up to the present work. See also Bakker et al. (2003), Wierstra et al. (2010), Zhang et al. (2016) and Hausknecht & Stone (2015) for more examples of applying deep RL to non-markov tasks.

2 ARCHITECTURE

Our model is based around an advantage actor critic architecture (Mnih et al., 2016) consisting of separate value and policy networks. In addition we include an external memory M consisting of a
set of n past visited states (St0 , .., Stn-1 ) with associated importance weights (wt0 , ..., wtn-1 ). The query network q(St) outputs a vector of size equal to the state size with tanh activation. At each time step a single item Sti is drawn from the memory to condition the policy according to:

Q(Sti |Mt) = exp ( q(St)|Sti / )

n-1
exp
j=0

q(St)|Stj /

(1)

where  is a positive learnable temperature parameter. The state, mt, selected from memory is given as input to the policy network along with the current state, both of which condition the resulting policy. Finally the write network takes the current state as input and outputs a single value with sigmoid activation. This value is used to determine how likely the present state is to be written to and subsequently retained in the memory according to the distribution in equation 7 which will be throughly explained in section 3. An illustration of this architecture is shown in figure 1.

V V (S)

(a1|S,mx)

S

mx M

(a2|S,mx) (a3|S,mx)

q wwww321s

mmm321 S

w

Figure 1: Episodic memory architecture, each grey circle represents a neural network module. Input state (S) is given separately to the query (q), write (w), value (V) and policy () networks at each time step. The query network outputs a vector of size equal to the input state size which is used (via equation 1) to choose a past state from the memory (m1 ,m2 or m3 in the above diagram) to condition the policy. The write network assigns a weight to each new state determining how likely it is to stay in memory. The policy network assigns probabilities to each action conditioned on current state and recalled state. The value network estimates expected return (value) from the current state.

2

Under review as a conference paper at ICLR 2018

3 ALGORITHM
For the most part our model is trained using standard stochastic gradient descent on common RL loss functions. The value network is trained by gradient descent on the squared one step temporal different error t2 where t = rt+1 + V (St+1) - V (St), and the gradient is passed only through V (St). The policy is trained using the advantage loss -t log((at|St, mt)) with gradients passed only through (at|St, mt). The query network is trained similarly on the loss -t log(Q(mt|St)) with gradients passed only through Q(mt|St). We train online, performing one update per timestep with no experience replay. The main innovation of this work is in the training method for the write network which is described in sections 3.1 and 3.2. There are two main desiderata we wish to satisfy with the write network. First we want to use the weights w(St) generated by the network in a reservoir sampling algorithm such that the probability of a particular state St^ being present in memory at any given future time t > t^ is proportional to the associated weight w(St^). Second we want to obtain estimates of the gradient of the return with respect to the weight of each item in memory such that we can perform approximate gradient descent on the generated weights.

3.1 GRADIENT ESTIMATE
For brevity, in this section we will use the notation Et[x] to denote E[x|S0, ..., St] i.e. the expectation conditioned on the entire history of state visitation up to time t. Similarly Pt(x) will represent probability conditioned on the entire history of state visitation. All expectations and probabilities are assumed to be with respect to the current policy, query and write network. Let A represent the set of available actions and At the action selected at time t.

3.1.1 ONE-STATE MEMORY CASE

To introduce the idea we first present our gradient estimation procedure for the case when our memory can store just one state, and thus there is no need to query. Here mt represents the state in memory at time t and thus, in the one-state memory case, the state read from memory by the agent at time t. Assume the stored memory is drawn from a distribution parameterized as follows by a set of weights {wi|i  {0, ..., t - 1}} associated with each state Si when the state is first visited:

Pt(mt = Si) = wi

t-1
wj
j=0

(2)

We can then write the expected return Rt = rt+1 + rt+2 + ... as follows:

t-1

Et[Rt] = Pt(mt = Sk) (a|St, Sk)Et[Rt|At = a]

k=0

aA

(3)

In order to perform gradient descent on the weights wi we wish to estimate the gradient of this expectation value with respect to each weight. In particular we will derive an estimate of this gradient
using an actor critic method which is unbiased if the critic's evaluation is correct. Additionally our estimate will be non-zero only for the wi associated with the index i such that mt = Si. This means if our weights wi are generated by a neural network, we will only have to propagate gradients through the single stored state. This is crucial to allow our algorithm to run online, as otherwise we
would require storing every visited state to compute the gradient estimate.

 t-1 wi Et[Rt] = k=0

Pt(mt = wi

Sk )

aA

(a|St,

Sk )Et [Rt |At

=

a]

+

Pt(mt

=

Sk )

aA

(a|St,

Sk

)



Et

[Rt|At wi

=

a]

(4)

3

Under review as a conference paper at ICLR 2018

We can rewrite this as:

1 wi Et[Rt] = wi Pt(mt = Si)

(a|St, Si)Et[Rt|At = a] - Et[Rt]
aA
 + Et wi Et+1[Rt+1]

(5)

See appendix A for a detailed derivation of this expression. We will use a policy gradient approach,

similar to REINFORCE (Williams, 1992), to estimate the first term in this gradient using an esti-

mator

Gi

such

that

Et[Gi]





Et [Rt wi

]

.

The

second

term

is

then

estimated

recursively

on

subsequent

time-steps. In the present work we will focus on the undiscounted episodic case with the start-state

value objective, for which it suffices to follow the first term in the above gradient expression for each

visited state. This is also true in the continuing case with an average-reward objective. See Sutton

et al. (2000) for further discussion of this distinction. Consider the gradient estimator:

Gi =

t/wi 0

if mt = Si otherwise

(6)

which has expectation:

Et[Gi]

=

1 wi Pt(mt

=

Si) (a|St, Si)Et[rt+1
aA

+ V (St+1) - V

(St)|At

=

a]

1



wi Pt(mt

=

Si) (a|St, Si)(Et[rt+1
aA

+

Et+1[Rt+1]|At

=

a] - Et[Rt])

1

= wi Pt(mt = Si)

(a|St, Si)Et[Rt|At = a] - Et[Rt]
aA

Where the approximation is limited by the accuracy of our value function. In conventional policy gradient subtracting the state value (e.g. using t = rt+1 + V (St+1) - V (St) instead of rt+1 + V (St+1)) is a means of variance reduction. Here it is critical to avoid computing gradients with respect to the denominator of equation 2, which allows our algorithm to run online while computing
the gradient with respect to only the weight stored in memory.

Given

these

estimated

gradients

with

respect

to

wi

we

apply

the

chain

rule

to

compute

Rt w

=

Rt wi

wi w



Gi

 w(Si ) w

,

for

each

parameter

w

of the write network.

This gradient estimate is used in

a gradient descent procedure to emphasize retention of states which improve the return.

Gradient estimates are generated based on the stored values of wi in memory but applied to the parameters of the network at the present time. With online updating, this introduces a multiple timescale issue which perhaps warrants further investigation. We assume this should be acceptable as long as the learning rate is sufficiently small, but leave further investigation to future work.

There are a number of ways to extend the distribution defined in equation 2 to the case where multiple elements of a set must be selected (see for example Efraimidis & Spirakis (2006)). We will focus on a generalization which is less explored but which we will see in the following section results in gradient estimates which are an elegant generalization of the single-state memory case.

3.1.2 MULTIPLE-STATE MEMORY CASE

In this section and those that follow we will routinely use the notation

Z n

where Z is a set and n an

integer to indicate the set of all n-subsets of Z. Note that

Z 0

= {} and we adopt the convention

x = 1 thus

x = 1 which will be important in a few places in what follows.

x Z^(Z0 ) xZ^

We will introduce some notation to facilitate reasoning about sets of states. Let Tt = {t : 0  t 

t - 1} be the set of all time indices from 0 to t - 1. Let T^ 

Tt n

be a set of n indices chosen from

Tt where n is the memory size. Let ST^ be the set of states {St^ : t^  T^}. Let Mt be the set of

4

Under review as a conference paper at ICLR 2018

states in memory at time t. Let Q(St^|ST^) be the probability of querying St^ given Mt = ST^. The probability for a particular set of states being contained in memory is defined to be the following:

Pt(Mt = ST^) = wi
iT^

wi

(T~

Tt n

)

iT~

(7)

A straightforward extension of the derivation of the equation 5 shows that this choice results in a

gradient estimate which is an elegant extension of the one-state memory case. The derivation is

given

in

appendix

B,

the

result

for

 wi

Et

[Rt

]

is:

1

wi

Et[Rt]

=

(T^(

Tt n

)

wi Pt(Mt = ST^)
i)

Q(Sj|ST^) (a|St, Sj)Et[Rt|At = a]

jT^

aA

- Et[Rt]

+ Et

 wi Et+1[Rt+1]

(8)

As in the single-state memory case we recursively handle the second term. To estimate the first term we could choose the following estimator Gi:

Gi =

t/wi 0

if Si  Mt otherwise

(9)

This estimator is unbiased under the assumption the critic is perfect, however it scales poorly in terms of both variance and computation time as the memory size increases. This is because it requires updating every state in memory regardless of whether it was queried, spreading credit assignment and requiring compute time proportional to the product of the number of states in memory with the number of parameters in the write network. Instead we will further approximate the second term and perform an update only for the queried item. We rewrite the first term of equation 8 as follows:

1

wi Pt(Mt

Si) Pt(mt = Si|Mt

Si) (a|St, Sj)Et[Rt|At = a] - Et[Rt]
aA

+ Pt(mt = Si|Mt Si)

Pt(At = a|Mt Si, mt = Si)Et[Rt|At = a] - Et[Rt]

aA

1

 wi Pt(mt = Si)

(a|St, Sj)Et[Rt|At = a] - Et[Rt]
aA

This approximation is accurate to the extent that our query network is able to accurately select useful states. To see this, note that if querying a state when it's in memory helps to generate a better expected return a well trained query network should do it with high probability and hence P (mt = Si|Mt Si) will be low. On the other hand if querying a state in memory is unhelpful

Pt(At = a|Mt Si, mt = Si)Et[Rt|At = a] - Et[Rt] will generally be small. With this
aA
approximation the gradient estimate becomes identical to the one-state memory case:

Gi =

t/wi 0

if mt = Si otherwise

(10)

While this justification is not rigorous, this approximation should significantly improve computational and sample efficiency, and is used in our experiments in section 4.

3.2 RESERVOIR SAMPLING PROCEDURE
In the previous section we derived a gradient estimator for our desired memory distribution. in this section we introduce a method for sampling from this distribution online. Specifically we will

5

Under review as a conference paper at ICLR 2018

Algorithm 1 A reservoir sampling algorithm for drawing samples from equation 11

1:   Zeros(n+1) 2: ~  Zeros(n) 3: W  Zeros(n) 4: T^  Zeros(n) 5: for time 0  t  n - 1 do
6: Receive wt 7: W [t]  wt 8: T^[t]  t
9: end for
10: Apply equivalent random permutation to W and T^
11: [n]  1 12: ~ [n - 1]  1 13: for n - 1  i  0 do 14: [i] = W [i] · [i + 1]
15: end for 16: for n - 2  i  0 do 17: ~ [i] = [i + 1] + W [i] · ~ [i + 1]
18: end for 19: for all time t  n do
20: Receive wt 21: UPDATE(wt,t) 22: end for

1: function UPDATE(w,t)

2:   w

3:   t

4: for 0  i  n - 1 do 5:   [i] +  · ~ [i]

6: if i=n-1 then

7:   [i + 1]

8: else 9:   [i + 1] +  · ~ [i + 1]

10: end if

11:

P



1-

 [i]  [i+1]

12: Swap  with W [i] and  with T^[i]

with probability P

13: [i]  

14: end for

15: for n - 2  i  0 do 16: ~ [i] = [i + 1] + W [i] · ~ [i + 1]

17: end for

18: end function

formulate a reservoir sampling algorithm for drawing a subset T^ of size n from a set of indices
T = {0, ..., t - 1} according to a distribution parameterized by a weight wi for each index i  T . Following equation 7 the probability for a given subset T^ is defined as follows:

P~(T^; T, n) = wi
iT^

wi T~(Tn) iT~

(11)

This distribution can be sampled from by selecting members sequentially for i  {0, .., n - 1} with the following conditional probabilities:



P^(T^[i]|T^[0 : i - 1]; T, n) = wT^[i]

wj

( )T~

T \T^[0:i] n-i-1

jT~

(n - i) 

wj

 

( )T~

T \T^[0:i-1] n-i

jT~

(12)

We abuse notation slightly and use T^ to refer to both an ordered vector and the set of its elements.
Lemma 1. Selecting elements sequentially according to equation 12 will result in a vector T^ whose elements correspond to a sample drawn from equation 11.

Proof. See appendix C.

We use lemma 1 to derive a reservoir sampling procedure which works online to update the reservoir T^ at each time-step when a new index is added to T along with an associated weight. The result is algorithm 1. At each time-step UPDATE moves through T^ starting from index 0 and chooses
whether to swap the item and weight at each index with the ones currently contained in a buffer ( and , initially set to contain the newly added item and associated weight). The probability
of swapping is chosen such that it corrects the conditional probability of the item at each index
(conditioned on the items before it) to compensate for the item in the buffer being added to the set of possible items for that index. After doing this sequentially at each index the overall probability of T^
will be correct with the newly added item. Computing the necessary swap probabilities is nontrivial

6

Under review as a conference paper at ICLR 2018

in itself, however we show that it is possible to do this in O(n) time per time-step (where here n is the memory size) by iteratively updating two vectors  and ~ .
Theorem 1. In algorithm 1 let t refer to the parameter of the call to UPDATE, T^t[i] refer to the value of T^[i] when that call is made, and Tt = {t : 0  t  t - 1} refer to the set of all time indices from 0 to t - 1. t  n, 0  i  n - 1:
P (T^t[i] = ti|T^t[0 : i - 1] = [t0, ..., ti-1]) = P^(ti|[t0, ..., ti-1]; Tt, n)
where [t0, ..., ti] is any arbitrary vector of unique elements of Tt.

Proof. See appendix D.

Corollary 1.1. At the call to UPDATE with parameter t, t  n, T^ 

Tt n

:

P ({T^t[0], ..., T^t[n - 1]} = T^) = P~(T^; Tt, n)

Proof. The proof follows from theorem 1 and lemma 1.

Thus algorithm 1 produces reservoirs which are indeed valid samples from the desired distribution at each time-step. Note that it runs in O(n) time per time-step where n is the size of the memory. We use this algorithm along with the weights generated by our write network to manage updating the memory on each new state visitation.

4 EXPERIMENTS AND RESULTS
We test our algorithm on a toy problem we call "the secret informant problem". The problem is intended to highlight the kinds of sharp, long-term dependencies that are often difficult for recurrent models but which our algorithm is intended to deal with. The problem is such that in order to behave optimally an agent must remember specific, initially unknown past states. An instance of the problem is shown in figure 2 and a detailed explanation of the problem structure is available in the associated caption. In each training episode a new random instance of the problem is created (with the chain length, number of actions and number of decisions held fixed for a particular training run). This consists of randomly choosing the final action sequence, the location of the informative state for each decision, and the implied action and decision state for each of the uninformative states.
All experiments are run for 3 repetitions with error bars indicating standard error in the mean over these 3 runs. The architecture and hyper-parameters used in each experiment are identical. We use the architecture from section 2 with 1 hidden layer for the value, query and write networks and 2 hidden layers for policy. The value network and query network outputs each use tanh activation, the policy uses softmax, and the write network uses sigmoid. Each hidden layer has 10 units. We train using ordinary stochastic gradient descent with learning rate 0.005 and gradient estimates generated as specified in section 3.
We also ran a recurrent baseline with full backpropagation through time which we found required more fine-tuning to train with online updates. To make comparison as meaningful as possible the recurrent baseline used essentially the same architecture but with the entire memory module replaced by a basic GRU (Cho et al., 2014) network of 10 units. Stochastic gradient descent alone was found to give very poor results with the recurrent learner so RMSProp was used instead with learning rate 0.0005. Additionally to obtain reasonable results with the recurrent learner, and avoid catastrophic looping behavior, it was necessary to add a discount factor ( = 0.9, applied only for learning purposes and not used in computing the plotted return) as well as entropy regularization on the policy with a relatively low weight of 0.0005. Perhaps surprisingly neither of these were necessary with the episodic memory based system as it tended to proceed quickly through the chain without significant looping even without discounting or entropy regularization. This baseline is not intended to be representative of the performance of all possible architectures based on RNN variants trained with backpropagation through time, but merely to provide context for the main experimental results of this work.
Results and descriptions of the experiments are shown in figures 3, 4 and 5, in each plot the x-axis shows number of training episodes. One additional experiment with twice the environment length

7

Under review as a conference paper at ICLR 2018

000000110

000000000

010010100

100011000

100100100

001011000

100011000

100010100

001101000

000001011

000000110

000000111

r=0 r=0 r=0 r=0 r=0 r=0 r=1 r=0 r=0

(a) An instance of the secret informant problem

100100100
(b) A state of the secret informant problem

Figure 2: (a) shows an instance of the secret informant problem with 3 actions and 2 decision states. The start state uniquely contains all zeros. In the final 2 states of an episode (which we call decision states), the agent must select the right sequence of actions to receive a +1 reward, other action sequences give reward 0. At all other states the middle action leads forward along the chain while other actions keep the agent in the same state. The action at each decision state is indicated by bits 1-3 of a certain informative state containing matching values of bits 6 and 7. Informative states are distinguished from uninformative states by bits 4 and 5. The agent must learn to remember states with the pattern 10 in bits 4 and 5 and subsequently query them at the associated decision state. (b) shows a particular state of the problem. The first 3 bits are action indicators, a one-hot encoding of the action the state is suggesting should be taken at the associated decision state. The next two bits are informative and uninformative indicators. If these bits are 01 the state is uninformative means the decision state and associated action it suggests are uniformly random and give no indications of a correct action. If these bits are 10 then the state is informative and the associated action it suggests is on the path toward the reward at the decision state it indicates. The next two bits are decision state identifiers, if present in an informative state they indicate which decision state it provides information about, if present in a decision state they serve as an identifier for that decision state. Thus, the correct thing for an agent to do in each decision state is to take the action suggested by the informative state with the same values of bits 6 and 7. The next bit is a decision state indicator and will be 1 if and only if the state is a decision state. The final bit is a correct path indicator and indicates for a decision state whether all the decisions made so far have been correct. This is necessary for our current system because without it the final decision states all look the same and it is not possible to learn via one step updates which decision is correct at the first decision state, in future work we would like to investigate eliminating the need for information like this by using multi-step updates or eligibility traces. The particular state shown above is informative, it indicates that the correct action for the second decision state will be the top action. Versions of this problem can be created with variable length (which we use to refer to the number of informative states plus the number of uninformative states), number of actions, and number of decision states by modifying the above description in the obvious way.

(a) Write weights

(b) Learning curves

Figure 3: Experiment with environment length 10, 1 decision and a 1 state memory. In the 1 state
memory case the query module is unnecessary. (a) shows average write weight assigned to informative states (·) and uninformative states ( ). (b) shows average return with episodic memory learner (·) and recurrent baseline ( ).

is shown in appendix E. Notice that in each case the episodic memory learner was is able to learn a good query policy, drive the write weight of the uninformative states to near 0 while keeping the value for informative states much larger, and obtain close to perfect average return. Comparing
8

Under review as a conference paper at ICLR 2018

(a) Write weights

(b) Learning curves

(c) Queried values

Figure 4: Experiment with environment length 10, 1 decision and a 3 state memory. In this case the query module is necessarily. (a) shows average write weight assigned to informative states (·) and uninformative states ( ). (b) shows average return with episodic memory learner (·) and recurrent
baseline ( ). (c) shows the value of several relevant query vector elements in the decision state: the uninformative indicator ( ), the informative indicator ( ) and the first decision state identifier (·)
(unnecessary here since there is only one decision state, but included for uniformity).

(a) Write weights

(b) Learning curves (c) First decision state (d) Second decision state

queried values

queried values

Figure 5: Experiment with environment length 10, 2 decisions and a 3 state memory. (a) shows average write weight assigned to informative states (·) and uninformative states ( ). (b) shows average return with episodic memory learner (·) and recurrent baseline ( ). (c) shows the value of
several relevant query vector elements in the first decision state: the uninformative indicator ( ), the informative indicator ( ), the first decision state identifier (·), and the second decision state
identifier ( ). (d) shows the same thing but for the query generated in the second decision state.

figures 3 and 4 it appears that the addition of redundant memory size may accelerate initial learning though it has little effect on the overall convergence time. Comparing figures 4 and 5 the number of episodes to converge appears to roughly double from approximately 25, 000 to 50, 000 with the addition of the extra decision state but the training remains quite stable.
5 CONCLUSION
We present a novel algorithm for integrating a form of episodic memory with trainable reading and writing into a RL agent. The method depends on the observation that if we restrict the information stored in memory to be a set of past visited states, the information recorded also provides the context in which it was recorded. This means it is possible to assign credit to useful information without needing to backpropagate through time to when it was recorded. To achieve this we devise a reservoir sampling technique which uses a sampling procedure we introduce to generate a distribution over memory configurations for which we can derive gradient estimates. The whole algorithm is O(n) in both the number of trainable parameters and the size of the memory. In particular neither memory required nor computation time increase with history length, making it feasible to run in an online RL setting. We show that the resulting algorithm is able to achieve good performance on a toy problem we introduce designed to have sharp long-term dependencies which can be problematic for recurrent models.
ACKNOWLEDGEMENTS
We acknowledge the support of the Natural Sciences and Engineering Council of Canada (NSERC).
9

Under review as a conference paper at ICLR 2018
REFERENCES
Bram Bakker, Viktor Zhumatiy, Gabriel Gruener, and Ju¨rgen Schmidhuber. A robot that reinforcement-learns to identify and memorize important previous observations. In International Conference on Intelligent Robots and Systems, 2003.
Min-Te Chao. A general purpose unequal probability sampling plan. Biometrika, 69(3):653­656, 1982.
Kyunghyun Cho, Bart Van Merrie¨nboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. In Workshop on Syntax, Semantics and Structure in Statistical Translation, 2014.
Pavlos S Efraimidis and Paul G Spirakis. Weighted random sampling with a reservoir. Information Processing Letters, 97(5):181­185, 2006.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwin´ska, Sergio Go´mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538 (7626):471­476, 2016.
Caglar Gulcehre, Sarath Chandar, and Yoshua Bengio. Memory augmented neural networks with wormhole connections. arXiv preprint arXiv:1701.08718, 2017.
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps. In AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents, 2015.
Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Neural Information Processing Systems, 2015.
Lukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events. In International Conference on Representation Learning, 2017.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016.
Junhyuk Oh, Valliappa Chockalingam, Satinder P. Singh, and Honglak Lee. Control of memory, active perception, and action in minecraft. In International Conference on Machine Learning, 2016.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In Neural Information Processing Systems, 2015.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Neural Information Processing Systems, 2000.
Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In International Conference on Autonomous Agents and Multiagent Systems, 2011.
Daan Wierstra, Alexander Fo¨rster, Jan Peters, and Ju¨rgen Schmidhuber. Recurrent policy gradients. Logic Journal of IGPL, 18(5):620­634, 2010.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines. arXiv preprint arXiv:1505.00521, 419, 2015.
10

Under review as a conference paper at ICLR 2018

Marvin Zhang, Sergey Levine, Zoe McCarthy, Chelsea Finn, and Pieter Abbeel. Policy learning with continuous memory states for partially observed robotic control. In International Conference on Robotics and Automation, 2016.

A DERIVATION OF EXPRESSION FOR GRADIENT FOR ONE-STATE MEMORY

Pt(mt = Si) = wi

t-1
wj
j=0

t-1

Et[Rt] = Pt(mt = Sk) (a|St, Sk)Et[Rt|At = a]

k=0

aA

 t-1 wi Et[Rt] = k=0

Pt(mt = wi

Sk )

aA

(a|St,

Sk )Et [Rt |At

=

a]

+

Pt(mt

=

Sk) (a|St,
aA

Sk

)



Et[Rt 

|At wi

=

a]

Working out the first term:

t-1 k=0

Pt(mt = wi

Sk )

(a|St,
aA

Sk )Et [Rt |At

=

a]

=

t-1 k=0



log(Pt(mt wi

=

Sk)) Pt(mt

=

Sk )

aA

(a|St,

Sk )Et [Rt |At

=

a]

=

t-1 k=0

 wi (log(wk)

-

t-1
log(
j=0

wj ))Pt(mt

=

Sk )

aA

(a|St,

Sk )Et [Rt |At

=

a]

=

1 wi Pt(mt

=

Si) (a|St, Si)Et[Rt|At
aA

=

a]

t-1
-
k=0

1 j wj

Pt(mt

=

Sk )

aA

(a|St,

Sk )Et [Rt |At

=

a]

=

1 wi Pt(mt

=

Si) (a|St, Si)Et[Rt|At
aA

=

a]

-

1 j wj Et[Rt]

1 = wi Pt(mt = Si)

(a|St, Si)Et[Rt|At = a] - Et[Rt]
aA

Working out the second term:

t-1 k=0

Pt(mt

=

Sk )

aA

(a|St

,

Sk

)

Et[Rt|At wi

=

a]

t-1

= Pt(mt = Sk) (a|St, Sk)

k=0

aA

Et[rt+1|At = a] +  Et[Rt+1|At = a] wi wi

 = Et wi Et+1[Rt+1]

11

Under review as a conference paper at ICLR 2018

Where

we

are

able

to

drop

 Et [rt+1 |At =a] wi

because

the

immediate

reward

is

independent

of

the

state

in memory once conditioned on the action. Thus we finally arrive at:

1 wi Et[Rt] = wi Pt(mt = Si)

(a|St, Si)Et[Rt|At = a] - Et[Rt]
aA
+ Et

 wi Et+1[Rt+1]

B DERIVATION OF EXPRESSION FOR GRADIENT FOR MULTIPLE-STATE MEMORY

Pt(Mt = ST^) = wi
iT^

wi

(T~

Tt n

)

iT~

Et[Rt] =

Pt(Mt = ST^) Q(Sj|ST^) (a|St, Sj)Et[Rt|At = a]

(T^

Tt n

)

jT^

aA

 wi Et[Rt] = T^(Tn)

Pt(Mt = wi

ST^ )

jT^

Q(Sj |Mt

=

ST^ )

aA

(a|St,

Sj )Et[Rt|At

=

a]

+

Pt(Mt

=

ST^ )

jT^

Q(Sj |ST^)

aA

(a|St

,

Sj

)

Et[Rt|At wi

=

a]

Working out the first term:

(T^

Tt n

)

Pt(Mt = wi

ST^ )

jT^

Q(Sj |

=

ST^ )

(a|St,
aA

Sj )Et[Rt|At

=

a]



=

Pt(Mt

(T^

Tt n

)

=

ST^) wi (log(Pt(Mt

=

ST^ ))
jT^

Q(Sj |ST^)

(a|St, Sj)Et[Rt|At = a]

aA

1

=

(T^(

Tt n

)

i)

wi Pt(Mt

=

ST^ )
jT^

Q(Sj|ST^) (a|St, Sj)Et[Rt|At
aA

=

a]

 wj 

-

(T^

Tt n

)

Pt(Mt

=

ST^ )



(T~(

Tt n

)

i) jT~\{i}

  wj

(T~

Tt n

)

j

T~

 

Q(Sj |ST^)



jT^

(a|St, Sj)Et[Rt|At = a]

aA

 wj 

=

 jT^\{i} 

 

(T^(

Tt n

)

 wj 

i)

(T~

Tt n

)

jT~

Q(Sj|ST^) (a|St, Sj)Et[Rt|At = a]

jT^

aA

- Et[Rt]

=

(T^(

Tt n

)

1 wi

Pt(Mt

=

ST^ )

i)

Q(Sj|ST^) (a|St, Sj)Et[Rt|At = a]

jT^

aA

- Et[Rt]

12

Under review as a conference paper at ICLR 2018

Working out the second term:

T^(Tn)

Pt(Mt

=

ST^ )

jT^

Q(Sj |ST^)

aA

(a|St

,

Sj

)

Et[Rt|At wi

=

a]

= Pt(Mt = ST^) Q(Sj|ST^) (a|St, Sj)

T^(Tn)

jT^

aA

Et[rt+1|At = a] wi

+  Et[Rt+1|At = a] wi

 = Et wi Et+1[Rt+1] So all together we get:

1

wi

Et[Rt]

=

(T^(

Tt n

)

wi Pt(Mt = ST^)
i)

Q(Sj|ST^) (a|St, Sj)Et[Rt|At = a]

jT^

aA

- Et[Rt]

+ Et

 wi Et+1[Rt+1]

C PROOF OF LEMMA 1
Lemma 1. Selecting elements sequentially according to equation 12 will result in a vector T^ whose elements correspond to a sample drawn from equation 11.

Proof. Selecting elements sequentially according to equation 12 gives the following probability for a given vector T^.
P (T^) = P (T^[0])P (T^[1]|T^[0])...P (T^[n - 1]|T^[0 : n - 2])

wT^[0]

wj wT^[1]

wj

= ( ) ·T~

T \T^[0] n-1

jT~

( )T~

T \T^[0:1] n-2

jT~

· ... ·

n wj (n - 1)

wj 1

wT^[n-1]

wj

T~(Tn) jT~

( )T~

T \T^[0] n-1

jT~

( )T~

T \T^[0:n-2] 1

jT~



= wi
iT^

n! 

wi

 

T~(Tn) iT~

To complete the proof note that this is one of n! vectors with the same set of elements in different or-

der, each of which will have equal probability, hence to obtain the probability for the corresponding set we simply have to multiply by n! which gives us equation 11.

D PROOF OF THEOREM 1

We divide the proof into a series of lemmas. First we will define some notation to facilitate referencing algorithm 1 in the proofs below. Let Tt be the set {0, ..., t - 1} and T^t[0 : i - 1] be the value of T^[0 : i - 1] at the call to UPDATE with parameter t. Note that T^t[0 : -1] = . Let t[i] and ~ t[i] be the values of [i] and ~ [i] respectively, at the call to UPDATE with parameter t. Let Pt,i be the
value of P when it is set in loop index i of the loop starting at line 4 within the call to UPDATE with
parameter t. Let t,i and t,i be the values of  and  respectively after they are set within index i of the loop starting at line 4 within the UPDATE call with parameter t. Let t,i and t,i be the values of  and  respectively at the beginning of loop index i of the loop starting at line 4 within
the call to UPDATE with parameter t. Note that t,i = wt,i and that:

Tt+1 \ T^t+1[0 : i - 1] = Tt \ T^t[0 : i - 1]  {t,i}

(13)

13

Under review as a conference paper at ICLR 2018

Lemma 2.

wt~ =

wt~ + wt^ ·

wt~

( )T~

T {t^} m

t~T~

T~(mT ) t~T~

T~(mT-1) t~T~

Proof. The proof follows from noting that the first sum on the right side includes every term of the left sum where T~ does not contain t^, while the second term on the right side is equivalent to summing over those T~ in the left sum that do contain t^.

Lemma 3. For t  n:

t[i] =

wt~, i  {0, ..., n}

( )T~

Tt \T^t [0:i-1] n-i

t~T~

and also

~ t[i] =

wt~, i  {0, ..., n - 1}

( )T~

Tt \T^t [0:i-1] n-i-1

t~T~

Proof. Towards a proof by induction, assume the lemma holds at time t, a quick trace through algorithm 1 will show that the update leading to t+1[i] for i  {0, ..., n - 1} is always:

t+1[i] = t[i] + t,i · ~ t[i]

From here we can apply lemma 2, along with equation 13 as follows to get the desired result:

t+1[i] = t[i] + t,i · ~ t[i]

= wt~ + t,i · wt~

( )T~

Tt \T^t [0:i-1] n-i

t~T~

( )T~

Tt \T^t [0:i-1] n-i-1

t~T~

= wt~

( )T~

Tt+1 \T^t+1 [0:i-1] n-i

t~T~

Now note that ~ t[n] = 1 for all t which is also the correct value thus we have completed the induction step for the first half of the lemma.

On the other hand the update leading to ~ t+1[i] is as follows:

~ t+1[i] = t+1[i + 1] + wT^t+1[i] · ~ t+1[i + 1]

As a second level of induction assume the lemma holds for ~ t+1[i + 1] then applying lemma 2 we get:

~ t+1[i] = t+1[i + 1] + wT^t+1[i] · ~ t+1[i + 1]

= wt~ + wT^t+1[i] · wt~

( )T~

Tt+1 \T^t+1 [0:i] n-i-1

t~T~

( )T~

Tt+1 \T^t+1 [0:i] n-i-2

t~T~

= wt~

( )T~

Tt+1 \T^t+1 [0:i-1] n-i-1

t~T~

As the base case for this second level of induction, note that ~ t+1[n - 1] = 1 for all t which is also the correct value, thus assuming t+1 is correct we will also get the correct values for ~ t+1. This
completes the induction step for the lemma, it remains to prove the base case.

Note that [n] and ~ [n - 1] each start at 1. The beginning of the algorithm before line 19 is then intended to initialize all  and ~ values to have the correct value at time t = n, to see that this is the

14

Under review as a conference paper at ICLR 2018

case, first note i  {0, ..., n}:

n-1

n[i] =

wT^n [j ]

j=i

= wt~

( )T~

Tn \T^n[0:i-1] n-i

t~T~

And also, by induction on the trivial i = n - 1 case, i  {0, ..., n - 2}:

~ n[i] = n[i + 1] + wT^n[i] · ~ n[i + 1]

= wt~ + wT^n[i] · wt~

( )T~

Tn \T^n [0:i] n-i-1

t~T~

( )T~

Tn \T^n[0:i] n-i-2

t~T~

= wt~

( )T~

Tn \T^n [0:i-1] n-i-1

t~T~

Thus indeed the lemma holds at time n which serves as a base case for the rest of the proof.

Lemma 4.

Pt,i

=

1

-

P^ (T^t [i]|T^t+1 [0:i-1];Tt+1 ,n) P^ (T^t [i]|T^t [0:i-1];Tt ,n)

Proof. Substituting the definition from equation 12, along with the value assigned to Pt,i and simplifying slightly what we wish to show is:

wj wj

t,it[i]

= ( )T~

Tt+1 \(T^t+1 [0:i-1]{T^t [i]}) n-i-1

jT~

( )T~

Tt \T^t [0:i-1] n-i

jT~

t,it[i + 1]

wj wj

( )T~

Tt+1 \T^t+1 [0:i-1] n-i

jT~

( )T~

Tt \T^t [0:i] n-i-1

jT~

Substituting in the values of t[i] and t+1[i] from lemma 3 into the above formula, it will suffice to apply lemma 2 and lemma 3 to additionally show:

t,i = t[i] + t,i · ~ t[i]

= wt~ + t,i · wt~

( )T~

Tt \T^t [0:i-1] n-i

t~T~

( )T~

Tt \T^t [0:i-1] n-i-1

t~T~

= wj

( )T~

Tt+1 \T^t+1 [0:i-1] n-i

jT~

and for 0  i  n - 2

t,i = t[i + 1] + t,i · ~ t[i + 1]

= wt~ + t,i · wt~

( )T~

Tt \T^t [0:i] n-i-1

t~T~

( )T~

Tt \T^t [0:i] n-i-2

t~T~

= wj

( )T~

Tt+1 \(T^t+1 [0:i-1]{T^t [i]}) n-i-1

jT~

The first is a simple application of equation 13. The second follows from a similar observation in
addition to noting that T^t[i] has been removed from each term. For i = n - 1, t,i = 1 which trivially obeys the same formula.

15

Under review as a conference paper at ICLR 2018

Lemma 5. Pt,i  0.

Proof.

wj wj

( )T~

Tt+1 \(T^t+1 [0:i-1]{T^t [i]}) n-i-1

jT~

P = 1 -t,i

wj

( )T~

Tt \T^t [0:i-1] n-i

jT~

wj

( )T~

Tt+1 \T^t+1 [0:i-1] n-i

jT~

( )T~

Tt \T^t [0:i] n-i-1

jT~

hence

Pt,i  0



wj wj

( )T~

Tt+1 \T^t+1 [0:i-1] n-i

jT~

( )T~

Tt \T^t [0:i] n-i-1

jT~

 wj wj

( )T~

Tt+1 \(T^t+1 [0:i-1]{T^t [i]}) n-i-1

jT~

( )T~

Tt \T^t [0:i-1] n-i

jT~





lem. 2  

wj + wT^t[i]

wj

 

( )T~

Tt+1 \(T^t+1 [0:i-1]{T^t [i]}) n-i

jT~

( )T~

Tt+1 \(T^t+1 [0:i-1]{T^t [i]}) n-i-1

jT~

wj

( )T~

Tt \T^t [0:i] n-i-1

jT~



 

wj + wT^t[i]

wj

 

wj

( )T~

Tt \T^t [0:i] n-i

jT~

( )T~

Tt \T^t [0:i] n-i-1

jT~

( )T~

Tt+1 \(T^t+1 [0:i-1]{T^t [i]}) n-i-1

jT~



wj wj

( )T~

Tt+1 \(T^t+1 [0:i-1]{T^t [i]}) n-i

jT~

( )T~

Tt \T^t [0:i] n-i-1

jT~

 wj

wj

( )T~

Tt \T^t [0:i] n-i

jT~

( )T~

Tt+1 \(T^t+1 [0:i-1]{T^t [i]}) n-i-1

jT~



lem. 2  

wt~ + t,i ·

wt~

wj

( )T~

Tt \T^t [0:i] n-i

t~T~

( )T~

Tt \T^t [0:i] n-i-1

t~T~

( )T~

Tt \T^t [0:i] n-i-1

jT~



 

wt~ + t,i ·

wt~

wj

( )T~

Tt \T^t [0:i] n-i-1

t~T~

( )T~

Tt \T^t [0:i] n-i-2

t~T~

( )T~

Tt \T^t [0:i] n-i

jT~

 2

  

wj

 

( )T~

Tt \T^t [0:i] n-i-1

jT~

 wj wj

( )T~

Tt \T^t [0:i] n-i

jT~

( )T~

Tt \T^t [0:i] n-i-2

jT~



wj wj

( ) ( )T~

Tt \T^t [0:i-1] n-i-1

T~



Tt \T^t [0:i-1] n-i-1

jT~

jT~

 wj wj

( ) ( )T~

Tt \T^t [0:i-1] n-i-2

T~



Tt \T^t [0:i-1] n-i

jT~

jT~

It is relatively straightforward to show that the lemma holds in this last form. To do so note that except for terms for which T~ and T~ are identical on the left (which are not possible on the right),

16

Under review as a conference paper at ICLR 2018

each term on the left of the inequality is also present on the right, however the number of repetitions

of each term varies between the left and right. In the left sum if a term includes m values shared

between T~ and T~ , this term will appear

2(n-i-1-m) n-i-1-m

times. This is because we can choose n -

i - m non-duplicate values to be in T~ and place the rest in T~ , each of these permutations will

correspond to a term in the sum. On the other hand in the left sum if a term includes m values shared

between T~ and T~ , this term will appear

2(n-i-1-m) n-i-2-m

times. Similarly this is because in this case

we can choose n - i - 1 - m non-duplicate values to be in T~ and place the rest in T~ , each of these

permutations will correspond to a different term in the sum.

Since

2N N

>

2N N -1

,

N

every

term

which

is

present

on

the

right

side

is

present

on

the

left

with

more repetitions and thus the left side must be greater than the right and the lemma holds.

This lemma shows that P is indeed a valid probability, which means that swapping according to it in algorithm 1 is admissible. Theorem 1. t  n, 0  i  n - 1:
P (T^t[i] = ti|T^t[0 : i - 1] = [t0, ..., ti-1]) = P^(ti|[t0, ..., ti-1]; Tt, n)
where [t0, ..., ti] is an arbitrary vector of unique elements of Tt.

Proof. Let T^i,t be the value of T^ right before making the swap decision on line 12 in loop index i of the loop starting at line 4 within the call to UPDATE with parameter t. Towards a proof by induction assume that at time index t directly prior to making the swap decision for T^[i] at line 12 of UPDATE
we have for any arbitrary vector [t0, ..., ti-1] of unique elements of Tt:

P (T^i,t[j] = tj|T^t[0 : j - 1] = [t0, ..., tj-1]) = P^(tj|[t0, ..., tj-1]; Tt, n), j s.t. i  j  n - 1

and for any arbitrary vector [t0, ..., ti-1] of unique elements of Tt+1:

P (T^i,t[j] = tj|T^t+1[0 : j - 1] = [t0, ..., tj-1]) = P^(tj|[t0, ..., tj-1]; Tt+1, n), j s.t. 0  j  i - 1

That is all elements of T^ from 0 to i - 1 have the desired probability conditioned on proceeding elements of T^t+1 while all elements from i to n - 1 still have the desired probability when conditioned on proceeding elements of T^t. This is a natural inductive assumption given we have already made our swap decisions up to but not including i and are just about to make our decision for i.
Consider two mutually possible selections of T^t+1[0 : i - 1] = [t0, ..., ti-1] and T^t[0 : i - 1] = [t0, ..., ti-1] and note that together these uniquely determine the value t,i. Fixing these values, the only possible way to end up with T^t+1[i] = t^ for t^  Tt \ {t0, ..., tj-1} is to have T^t[i] = t^ and then choose not to swap T^[i] at time t. Thus, substituting in 1 - Pt,i using the expression for Pt,i obtained in lemma 4 we get:

P (T^t+1[i] = t^|T^t+1[0 : i - 1] = [t0, ..., tj-1], T^t[0 : i - 1] = [t0, ..., tj-1])

=

P^(t^|[t0,

...,

ti-1];

Tt,

n)

P^(t^|[t0, ..., ti-1]; Tt+1, n) P^(t^|[t0, ..., ti-1]; Tt, n)

= P^(t^|[t0, ..., ti-1]; Tt+1, n)

17

Under review as a conference paper at ICLR 2018

On the other hand to end up with T^t+1[i] = t,i we may start with any t^  Tt \ ({t0, ..., tj-1}) and then choose to swap T^[i] at time t, in this case we get:

P (T^t+1[i] = t,i|T^t+1[0 : i - 1] = [t0, ..., tj-1], T^t[0 : i - 1] = [t0, ..., tj-1])

= P^(t^|[t0, ..., tj-1]; Tt, n)
t^Tt \{t0 ,...,tj -1 }

1

-

P^(t^|[t0, ..., tj-1]; Tt+1, n) P^(t^|[t0, ..., tj-1]; Tt, n)

= P^(t^|[t0, ..., tj-1]; Tt, n) - P^(t^|[t0, ..., tj-1]; Tt+1, n)
t^Tt \{t0 ,...,tj -1 }

=1-

P^(t^|[t0, ..., tj-1]; Tt+1, n)

t^Tt \{t0 ,...,tj -1 }

=1-

P^(t^|[t0, ..., tj-1]; Tt+1, n)

t^Tt+1 \({t0 ,...,tj -1 }{t,i })

= P^(t,i|[t0, ..., tj-1]; Tt+1, n)

Thus for any fixed mutually possible selections of T^t+1[0 : i - 1] = [t0, ..., tj-1] and T^t[0 : i - 1] = [t0, ..., tj-1] for both t,i and all other t^  Tt+1 \ {t0, ..., tj-1} we end up with correct conditional probabilities for T^t+1[i] assuming they are correct for T^t[i]. Now note that the resulting probabilities are ultimately independent of [t0, ..., tj-1], hence we would get the same values by conditioning on [t0, ..., tj-1] alone. Thus if our inductive assumption holds we have for any arbitrary vector
[t0, ..., ti-1] of unique elements of Tt+1:

P (T^i+1,t[j] = tj|T^t+1[0 : j - 1] = [t0, ..., tj-1]) = P^(tj|[t0, ..., tj-1]; Tt+1, n), j s.t. 0  j  i

Which completes the inductive step. To prove the base case note that we initialize T^ such that
at time n it is filled with all available items in random order. It is easy to show that equation 12 implies that the probability of any ordering of a given set T^ is equal thus if the items in T exactly fill T^ then ordering them at random will give the desired probabilities P^(T^n[i]|T^t+1[0 : i - 1]; Tt+1, n), i s.t. 0  i  n - 1, which gives us the base case to complete the inductive proof.

18

Under review as a conference paper at ICLR 2018
E LENGTH 20 ENVIRONMENT EXPERIMENT
Figure 6 shows the results of an experiment on the secret informant problem with environment length 20. Comparing figures 5 and 6 the number of episodes to convergence increases from 50, 000 to around 80, 000 with a doubling of the environment length while training still remains stable.

(a) Write weights

(b) Learning curve (c) First decision queried (d) Second decision

values

queried values

Figure 6: Experiment with environment length 20, 2 decisions and a 3 state memory. (a) shows average write weight assigned to informative states (·) and uninformative states ( ). (b) shows average return with episodic memory learner (·), we did not train a recurrent baseline for this problem. (c)
shows the value of several relevant query vector elements in the decision state. corresponds to the uninformative indicator, to the informative indicator, · to the first decision state identifier,
to the second decision state identifier. (d) shows the same thing but for the query generated in the
second decision state.

19

