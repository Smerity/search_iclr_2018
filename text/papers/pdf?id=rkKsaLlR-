Under review as a conference paper at ICLR 2018
DETECTING ANOMALIES IN COMMUNICATION PACKET STREAMS BASED ON GENERATIVE ADVERSARIAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
The fault diagnosis in modern communication system is traditionally supposed to be difficult, or even impractical for a purely data-driven machine learning approach, for it is a totally manmade system of intensive knowledge. A few labeled raw packet streams extracted from fault archive, can hardly be sufficient to deduce the intricate logic of underlying protocols. In this paper, we supplement these limited samples with two inexhaustible data sources: the unlabeled records probed from a system in service, and the labeled data simulated in an emulation environment. In order to transfer their inherent knowledge to the target domain, we construct a directed information flow graph, whose nodes are neural network components consisting of 2 generators, 3 discriminators and 1 classifier, and whose every forward path represents a pair of adversarial optimization goals, in accord with the semi-supervised and transfer learning demands. The multiheaded network can be trained in an alternative approach, where every iteration we select one target to update the weights along the path upstream, and refresh the residual layer-wisely to all outputs downstream. The actual results show that, it can achieve comparable accuracy on classifying TCP streams without deliberate expert features. The solution has relieved operation engineers from heavy works of understanding and maintaining rules, and provide a quick solution independent of specific protocols.
1 INTRODUCTION
A telecommunications network is a large collection of distributed devices, fully designed and manufactured by humans for a variety of transmission, control and management tasks, striving to a provide transparent channel between external terminals, via an actual internal relay process node by node. Acting as an usual conversation in the style of client and server, the two linked nodes send their messages in the form of packets, encapsulated the load with miscellaneous attributes in headers to ensure the correctness, consistency, and smoothness of the entire process. A typical header includes packet sequence number, source and destination addresses, control bits, error detection codes, etc.
The large-scale network cannot always work perfectly, due to its inherent complexity inside massive devices and their interactions. When there is a malfunction of a device, either caused by the traffic overload, or software bugs, or hardware misconfiguration, or malicious attacks, it will be reflected on the packet streams that pass through, such as packet loss, timeout, out of order, etc. The suspicious streams can be captured by the system administrators and send back to the service center for cautious offline analysis, which is time-consuming and domain-specific.
The main challenge of automatic diagnosis is that, it is almost impossible to formalize all the logic inside the system and make them available to artificial intelligence. A typical modern communication system consists of tens of thousands devices end to end, and runs based on a list of hundreds of protocols layer by layer (?). If we could figure out the latent states of protocols by constructing exact features from raw bytes, the subsequent classification tasks would be quite straightforward and easy to implement. For instance, the TCP relies on sequence numbers to judge the receiving order of packets, which may be just big integers roughly linearly growing from the view of machine learning models. Another example is a few important control bits may reside among much more useless bits,
1

Under review as a conference paper at ICLR 2018
such as checksum codes, which is actual harmful noises for models. Even we have the patience to dive into all the industrial protocols and build up an exhausted feature library, essentially we will fail again to achieve the target of automation, one of the main advantage of modern data-driven approach.
Another difficulty is the scarce of labeled samples. In spite of there are seemingly numerous packet flows running through Internet all the time, the real valid faults occur at random and occupy only a tiny portion of whole traffic volume. The real labeled data are usually collected from the archive of fault cases, which is hard to have enough samples for all possible categories, or cannot at least cover them completely.
The previous works on this issue mainly follows two technical routes: 1) a traditional two-phase framework, using expert features and some general-propose classifiers (?); 2) an end-to-end approach based on deep learning for automatic feature extraction (?). All these prior arts seldom use the generative models, which is usually more promising for expressing complex relationship among random variables. And they may fuse 1-2 data sources in semi-supervised setting(?), but not scale to even more data sources.
In this paper, we resort to a generative model to mimic the messages in a terminals conversation, and enrich the target data domain from two abundant but different information sources: labeled but from simulation, and genuine but unlabeled. The transfer and semi-supervised demands are integrated into an intuitive framework, composed of a connected graph of multiple simple GANs components, trained in an alternative optimization approach. The contribution of this paper includes: 1) combine three kinds of data sources in a generative approach, to solve the small-sample problem in a simulatable environment; 2) extend the two players in usual GANs to a system of multiple ones, still keeping its merit of end-to-end training; 3) verify its effectiveness on our practice problem of packet sequence classification.
The left of paper is organized as below: first, we introduce the previous work selectively in network anomaly detection and the research frontier in generative neural network. Next, we present the model and algorithm in detail with feature design at different levels. The results of experiments are followed in Section ??. Finally, we conclude the whole article.
2 RELATED WORK
The anomaly detection in communication packets has been long-term studied, either for the QoS management or instruction detection. The artical ? organizes the past works based on their applied technologies, which almost cover all popular machine learning methods. The work ? firstly applied neural network at abnormal packet detection, and work ? defined a modular system to accomplish the job at flow-level. The paper ? applied the deep learning on this problem to extract feature automatically, and observed accuracy improvements. To our best knowledge, it is the first time we apply the generative neural networks for semi-supervised and transfer learning simultaneously on this problem.
The Generative Adversarial Networks is implemented by two neural components contesting with each other in a zero-sum game (?). This unsupervised learning framework can be extended to semisupervised learning, by modifying the binary discriminator to a multi-class one in ?, separating N classes for real data and 1 class for generated data. The triple GAN (?) goes further by uncoupling the duty of classifying samples into an extra single building block. The paper ? addresses the mode collapse problem in GANs by training many generators and a classifier. A notable improvement is WGAN in ?, introducing a new symmetric loss metrics called as Earth-Mover. GANs have also be quickly applied to more scenarios, such as transfer learning in ?, and more data types, such as discrete-valued sequences in ?.
3 SOLUTION
3.1 PROBLEM DEFINITION AND LEVELS OF FEATURES
The packet stream in reality is a sequence of attributed events e = {(ti, {ai,j})|i  N, j  N }, where ti is the timestamp sealed by the receiver, and ai,j is a tuple of key-value parsed from packet
2

Under review as a conference paper at ICLR 2018

headers. The labels c of e can be K classes of anomalies containing 1 special class for normality. In order to focus the main aspect of our problem, we prefer to simply it by two assumptions: 1) anomalies can only happen at one side of the communication, such as server side, to prevent the number of possible situations from blowing up to K2. It seldom happens that, both terminals have problems simultaneously and produce complicated interaction that can fully not be diagnosed by a single-side model. In fact we can train two individual models for both sides, and thus the records from client side can be removed from train set in experiments. 2) the continuous valued (or finegrained in 10-6s) timestamps are ignored, while only their ascending index is kept, from 1 to T . We insert dummy packets to denote the timeout between two timestamps. This is justified, because most protocols are indifferent to the exact time intervals only if they do not exceed the predefined timeout threshold.
The available content of attributes depends on how much effort we want to pay for accurate inspection on packet headers. The clearer we want to know about the states of system running, the much we should know about the details of a given protocol. There are 3 levels of feature engineering: 1) raw bytes of headers, which needs little effort; 2) the numerical values (sequence index, integer or boolean) parsed by a data scheme indicating their positions and necessity; 3) the latent states defined in protocols, based on complete domain knowledge and automata-driven analysis. Neural networks can digest all above levels, though an extra effort are needed for discrete values at level 3, discussed later in Sec. ??.
3.2 NN DESIGN
3.2.1 OPTIMIZATION GOAL
Assume that the N -dimensional sequence Y = {yt : t  T } is generated from repetitive applying of a function G on a latent M -dimensional vector z:

yt = G(t)(z, c; )

(1)

Where c is a categorical variable to switch between the types of anomalies, and  is the parameters of G. We assume that, if z  N (0, 2) and c  Cat(), the resulted Y will conform to a distribution where our observations Y^ come from. In the adversary approach, we guide the training of G by
minimizing the distance (cross entropy, etc.) between the p(Y, c) and observed p^(Y, c), via the
transformation of an extra binary discriminator Dr:

(G) = argmin max Lr
(G) (Dr )

(2)

Lr = H(p(Dr(G(z)), c), p^(Dr(Y), c))

(3)

. Similarly, we define a function Gs that transforms Y to its correspondence Ys in the simulation world, and a function Ds to discriminate them from real simulation data:

(G,Gs) = argmin max Ls
(G,Gs) (Ds)

(4)

Ls = H(p(Ds(Gs(G(z))), c), p^(Ds(Ys), c))

(5)

. For the unlabeled data, we define a Dc to distinguish them from the generated samples without labels c:

(G) = argmin max Lu
(G) (Du)
Lu = H(p(Du(G(z)), ·), p^(Du(Y), ·))

(6) (7)

3

Under review as a conference paper at ICLR 2018

Figure 1: The layout of building blocks in our solution. The arrows denote for the information flow
among blocks, and they are connected as three forward paths in (a), representing real, simulated
and unlabeled information sources colored as orange, green, and blue. The noises driven the whole process is assumed to be sampled from two independent sources, z  N (0, 2) and c  Cat(). The  are estimated directly based on real labeled data. In (b), the white block C is trained with a fixed G, colored with shading.

. The overall optimization goal is a convex combination of Eq. ??, ?? and ??:

L = Lr + sLs + cLc

(8)

where s, c is the coefficients to adjust the importance of targets. Once we obtain the p(Y, c) in the implicit form of G, the classifier p(c|Y) can be derived with the marginal distribution p(c)
according to the Bayes rule.

3.2.2 LAYOUT OF GANS
The neural components and their connections are shown in Fig. ??. There are 3 information sinks in Fig. ??a), each of which connects to exact 2 data sources and corresponds to one part of loss function in Eq. ??; minimizing them concurrently, is equivalent to make the data come from different paths look the same with best effort. Note that the graph G is connected, directed and acyclic, which makes any topological order (must exist based on graph theory) of nodes is viable to an gradient descent approach. It provides a convenience for the design of optimization heuristics in Sec. ??. The trained G will be frozen in a secondary training process in Fig. ??b), to consistently offer the classifier C its pseudo-samples as well as labels until convergence.
The neural blocks in Fig. ?? can be built from even more fine-grained modules: 1 an input layer concatenating a vector of z (or y) and an optional one-hot vector for c; 2 a LSTM layer mapping a sequence to a vector; 3 a LSTM layer reversely mapping a vector to a sequence; 4 a fully connected layer; 5 a sigmoid (or softmax) layer classifying real/fake samples (or outputting labels). The generators G is built by 1 + 3 , and Gs is 2 + 3 , and C is 2 + 5 ; all D, Ds, Du share a same structure, which is 2 + 1 + 5 . The modules 2 , 3 and 4 are kept as one single hidden layer here, after the multi-layer structures have been tried in practice and their improvement was found to be negligible. For the discrete state sequence at feature level 3, it is feasible to map its discrete values into continuous vector globally by Word2vec during preprocessing, since the C is what we finally interested for diagnosis and the intermediate results are not visible to end users indeed.

3.2.3 ALGORITHM
We need a heuristics to guide the whole optimization process of G, depicted in Algo. ??. During every mini-batch iterations of training, every time we select a forward path whose loss function contributes most in the Eq. ??, weighted by s, and update the  of blocks on the way in the form of gradient descent. The all three sub-loss functions will be updated after G has been modified. Once the process has found a selected target that makes no contribution to overall goal, the update will be rolled back and the algorithm will switch to others. The failure record for any target will not bring into next batch. For individual blocks, RMSprop are preferred for components containing a recurrent layer, including G, Gs and C, while all discriminators use Adam instead.

4

Under review as a conference paper at ICLR 2018
Input: forward path set {P } of G 1 initialize masking m = (0, 0, 0); 2 while #n  #batch do 3 // select one path 4 {P }  filter {P } by m; 5 {P }  sort {P } by L descendingly;
6 // back propagation 7 for v  {P0} do 8 (v)  (v) - J ((v)); 9 end
10 // forward update 11 for Pi  {P } do 12 for v  Pi do 13 update J (v); 14 end 15 end
16 // check process 17 update L, L; 18 if L > threshold then 19 m = 0; 20 roll back {(v)|v  G}; 21 else 22 mP0 = 1 23 end 24 end
Algorithm 1: The main procedure of optimization of G guided by heuristic.
4 EXPERIMENTS
4.1 DATA
The real labeled data are collected from the archive of fault cases, probed at the core section of a wireless telecom site in service. The problematic TCP streams are classified into 4 categories: 1) uplink packet loss at random, 2) downlink packet loss at random, 3) packet loss periodically when load exceeds capacity, 4) checksum error at random, 5) normal but false alarmed. The faults in the connection establishment and release phase are not considered here. In the historical records, all we have are 18 samples, unevenly distributed from category 1-5, which are 5, 7, 3, 1, and 2.
The unlabeled data are captured from the same site, having 2000 real samples sufficient for training propose. Though its size is much larger than the labeled, it can hardly contain valid anomalies by an arbitrary, short-term collection. Yet it can provide a reference for the normal level of service quality for a specific site. The simulation are conducted from a mirror environment in lab, with a similar, or maybe simplified configuration. The 5 types of anomalies are generated in equal ratio, and each of them has 400 records. The phenomenon of occurring errors in flows are much more obvious than reality: 1) for uplink and 2) downlink packets, the portability of loss is 50%, 3) the stream have 50% of time transmitting over the limit of throughput; 4) the probability of checksum error is 50%. The sequence lengths of all synthetic data are all fixed to 500.
The data preprocessing on the TCP header is based on different levels discussed in Sec. ??, where the latent states of TCP include normal, resend, and timeout, distilled from 7 useful attributes, and also from 24 bytes of raw binary records. A simple automata is defined to compress the attributes to several states, according to TCP's standard logic. All sequence are split into the size of 500, and shorter sequences are padded by 0.
5

Under review as a conference paper at ICLR 2018

Table 1: The comparison of performance on 3×4 combinations of data and features. The meaningful improvements are colored as green, and referential models are colored as blue.

Data

Feature

Accuracy

Group Lab. Unl. Sim. 0 1 2 average weighted

X 1

\ 0.388 0.200 X X 0.766 0.876

X X 0.279 0.208

2X X

X 0.300 X 0.367

0.182 0.230

X X X 0.307 0.169

3 XX XX

X 0.386 X 0.425

0.235 0.271

X X X 0.374 0.214

4X X

X X 0.538 0.381 X X 0.584 0.419

XXX X

0.363 0.199

5 XXX XXX

X 0.558 X 0.599

0.432 0.490

4.2 SETUP

The performance of our multi-class problem is evaluated by the accuracy Acc = Ncorrect/N , which

is only metrics cared by operators. We measure two variations of accuracy: averaging on samples as

above, and averaging on class, i.e.,

K c=1

N Nc

Accc,

to

emphasize

the

performance

on

minor-classes.

3-fold cross validation are used for the set of real labeled data, with all other 2 datasets always

assisting the train partition for every fold. The program is based on Keras1 and a GAN library2.

The one-to-many recurrent layer of Keras is simply implemented in the form of many-to-many, by

repeating the input as a vector of equal length as output. The dimension of noise X is set to 20, and

hidden dimension of LSTM is 10 with L2 regularization. The learning rates of all components are set to 10-3. All other parameters are kept as default.

4.3 COMPARISON RESULT
We have two kinds of factors to combine for a workable model: feature levels and data sources, shown in Tab. ??. The solution in Fig. ??a) can be trimmed based on available data sources. At the group 1 of Tab.??, we give two referential models for comparison, one (Line 1) is the dummy model which always give the most frequent class as prediction, and the other (Line 2) is the result if we only use the simulation data both for train and test with one standalone classifier. With the deliberately amplified anomalies and evenly distributed classes, the performance can be quite ideal, and to some extent be an empirical upper limit of diagnosis. It can be observed in group 45 that, the simulation data are crucial for substantial enhancement by adding more typical anomalies, while the unlabeled contributes sightly via only supplying normal samples. The improvements on weighted accuracy are more obvious, which is more than twice that of dummy model.
On the other hand, the features still acts an important role in our problem. The level 3 feature can always performs better than the rest two levels, while the level 1 can especially not produce any
1https://keras.io/ 2https://github.com/bstriner/keras-adversarial

6

Under review as a conference paper at ICLR 2018
(a) (b) Figure 2: the convergence of loss during the training of G (left) and C (right). The dashed lines denote the simulation data.
meaningful results. The level 2 can approximate the best performance with the help of massive data, offering an always worse but still acceptable accuracy, without the effort of understanding protocols. 4.4 TRAINING The evolution of losses of 3 discriminators is demonstrated in Fig. ??a), and the classifier is shown in Fig. ??b). Generally, all loss curves of GANs' components converge to their roughly horizontal limits, with more or less mutually influenced fluctuations, caused by the continious attempts to seek for a better equilibrium in every batch. However, these efforts seem to merely make the overall loss tremble, instead of moving to lower places. The variances of simulated data are obviously much smaller than real data, which may be ascribed to the sufficient size of data and evenly distribution among classes, whereas the real data are imbalance, and have few samples to validate the improvements during training. We terminated the whole process at iteration 104, and use the trained G to gain a corresponding C, which is much easier to train as the loss goes steadily to its convergence level, shown in Fig. ??b).
5 CONCLUSION
In this paper, the widely used semi-supervised and transfer learning requirements have been implemented in an integrated way, via a system of cooperative or adversarial neural blocks. Its effectiveness has been verified in our application of packet flow classification, and it is hopefully to be a widely adopted method in this specific domain. The work also prompts us that, complex machine learning tasks and their compound loss functions can be directly mapped into connected networks, and their optimization process can be designed over an entire graph, rather than each individual's hierarchical layers. In future work, we may study how to apply this approach to even larger scale tasks, and make theoretical analysis on the existing of equilibrium and why we can always reach it.
7

Under review as a conference paper at ICLR 2018
REFERENCES
Martin Arjovsky, Soumith Chintala, and Le´on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
Monowar H Bhuyan, Dhruba Kumar Bhattacharyya, and Jugal K Kalita. Network anomaly detection: methods, systems and tools. Ieee communications surveys & tutorials, 16(1):303­336, 2014.
M Dondo and J Treurniet. Investigation of a neural network implementation of a tcp packet anomaly detection system. Technical report, DEFENCE RESEARCH AND DEVELOPMENT CANADAOTTAWA (ONTARIO), 2004.
Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. arXiv preprint arXiv:1611.01673, 2016.
Kevin R Fall and W Richard Stevens. TCP/IP illustrated, volume 1: The protocols. addison-Wesley, 2011.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Ahmad Javaid, Quamar Niyaz, Weiqing Sun, and Mansoor Alam. A deep learning approach for network intrusion detection system. In Proceedings of the 9th EAI International Conference on Bio-inspired Information and Communications Technologies (formerly BIONETICS), pp. 21­26. ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering), 2016.
Yu Jin, Nick Duffield, Jeffrey Erman, Patrick Haffner, Subhabrata Sen, and Zhi-Li Zhang. A modular machine learning system for flow-level traffic classification in large networks. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(1):4, 2012.
Matt J Kusner and Jose´ Miguel Herna´ndez-Lobato. Gans for sequences of discrete elements with the gumbel-softmax distribution. arXiv preprint arXiv:1611.04051, 2016.
Chongxuan Li, Kun Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. arXiv preprint arXiv:1703.02291, 2017.
Nicolas Papernot, Mart´in Abadi, U´ lfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semisupervised knowledge transfer for deep learning from private training data. arXiv preprint arXiv:1610.05755, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2234­2242, 2016.
8

