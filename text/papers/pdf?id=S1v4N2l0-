Under review as a conference paper at ICLR 2018
UNSUPERVISED REPRESENTATION LEARNING BY PREDICTING IMAGE ROTATIONS
Anonymous authors Paper under double-blind review
ABSTRACT
Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitative and quantitative that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them remarkably good performance. Even more, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art, thus they significantly close the gap between unsupervised and supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-ofthe-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. Similar striking results we get when transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, and CIFAR-10 classification.
1 INTRODUCTION
In recent years, there is a tremendous progress on the field of computer vision that is thanks to the use of deep convolutional neural networks (LeCun et al., 1998) (ConvNets) for learning powerful image representations for semantic image understanding. Typically, those ConvNets learn high level image features by being trained on the object recognition (Russakovsky et al., 2015) or the scene classification (Zhou et al., 2014) tasks using a massive amount of manually labeled data. The image features learned by the ConvNets in this supervised manner have been proven extremely helpful when they are transferred to other vision tasks, such as object detection (Girshick, 2015), semantic segmentation (Long et al., 2015), or image captioning (Karpathy & Fei-Fei, 2015). However, supervised feature learning has the main limitation of requiring intensive manual labeling effort, which is both expensive and infeasible to scale on the vast amount of visual data that are available today.
Due to that, there is lately an increased interest to learn high level ConvNet based representations in an unsupervised manner that avoids manual annotation of visual data. Some examples of unsupervised feature learning methods, are the clustering based methods (Dosovitskiy et al., 2014; Liao et al., 2016; Yang et al., 2016), the reconstruction based methods (Bengio et al., 2007; Huang et al., 2007; Masci et al., 2011), and genative probabilistic models Goodfellow et al. (2014); Donahue et al. (2016); Radford et al. (2015). Among them, a prominent paradigm is the so-called self-supervised learning that defines an annotation free pretext task, using only the visual information present on the images or videos, in order to provide a surrogate supervision signal for feature learning. For example, in order to learn features, Zhang et al. (2016a) and Larsson et al. (2016) train ConvNets to colorize gray scale images, Doersch et al. (2015) and Noroozi & Favaro (2016) predict the relative position of image patches, and Agrawal et al. (2015) predict the egomotion (i.e., self-motion) of a moving vehicle between two consecutive frames. The rationale behind such self-supervised tasks
1

Under review as a conference paper at ICLR 2018
is that solving them will force the ConvNet to learn semantic image features that can be useful for other vision task. In fact, image representations learned with the above self-supervised tasks, although they have not managed to match the performance of supervised-learned representation, they have proved to be good alternatives for transferring on other vision tasks, such as object recognition, object detection, and semantic segmentation (Zhang et al., 2016a; Larsson et al., 2016; Zhang et al., 2016b; Larsson et al., 2017; Doersch et al., 2015; Noroozi & Favaro, 2016; Noroozi et al., 2017; Pathak et al., 2016a; Doersch & Zisserman, 2017).
Our work follows this self-supervised paradigm and proposes to learn image representations by training ConvNets to recognize the geometric transformation that is applied to the image that it gets as input. More specifically, we first define a small set of discrete geometric transformations, then each of those geometric transformations are applied to each image on the dataset and the produced transformed images are fed to the ConvNet model that is trained to recognize the transformation of each image. In this formulation, it is the set of geometric transformations that actually defines the classification pretext task that the ConvNet model has to learn. Therefore, in order to achieve unsupervised semantic feature learning, it is of crucial importance to properly choose those geometric transformations (we further discuss this aspect of our methodology in section 2.2). What we propose is to define the geometric transformations as the image rotations by 0, 90, 180, and 270 degrees. Thus, the ConvNet model is trained on the 4-way image classification task of recognizing one of the four image rotations (see Figure 2). We argue that in order a ConvNet model to be able recognize the rotation transformation that was applied to an image it will require to understand the concept of the objects depicted in the image (see Figure 1), such as their location in the image, their type, and their pose. Throughout the paper we support that argument both qualitatively and quantitatively. Furthermore we demonstrate on the experimental section of the paper that despite the simplicity of our self-supervised approach, the task of predicting rotation transformations provides a powerful surrogate supervision signal for feature learning and leads to dramatic improvements on the relevant benchmarks.
Note also that our self-supervised task is different from the work of Dosovitskiy et al. (2014) that also employs geometric (as well as chromatic) transformations. Basically, they train a ConvNet model to yield representations that are discriminative between images and at the same time invariant on geometric and chromatic transformations. In contrast, we train a ConvNet model to recognize the geometric transformation applied to an image.
Our contributions are:
· We propose a new self-supervised task that is very simple and at the same time, as we demonstrate throughout the paper, offers a powerful supervisory signal for semantic feature learning.
· We exhaustively evaluate our self-supervised method under various settings (e.g. semisupervised or transfer learning settings) and in various vision tasks (i.e., CIFAR-10, ImageNet, and PASCAL classification or detection).
· In all of them, our novel self-supervised formulation demonstrates extremely good performance and achieves state-of-the-art results with dramatic improvements w.r.t. prior unsupervised approaches.
· As a consequence we show that for several important vision tasks, our self-supervised learning approach significantly narrows the gap between unsupervised and supervised feature learning.
In the following sections, we describe our self-supervised methodology in §2, we provide experimental results in §3, and finally we conclude in §4.
2 METHODOLOGY
2.1 OVERVIEW
The goal of our work is to learn ConvNet based semantic features in an unsupervised manner. To achieve that goal we propose to train a ConvNet model F (.) to estimate the geometric transformation applied to an image that is given to it as input. Specifically, we define a set of K discrete geometric
2

Under review as a conference paper at ICLR 2018

90 rotation

270 rotation

180 rotation

0 rotation

270 rotation

Figure 1: Images rotated by random multiples of 90 degrees (e.g., 0, 90, 180, or 270 degrees). The core intuition of our self-supervised feature learning approach is that if someone is not aware of the concepts of the objects depicted in the images, he cannot recognize the rotation that was applied to them.

transformations G = {g(.|y)}yK=1, where g(.|y) is the operator that applies to image X the geometric transformation with label y that yields the transformed image Xy = g(X|y). The ConvNet model F (.) gets as input an image Xy (where the label y is unknown to model F (.)) and yields as output
a probability distribution over all possible geometric transformations:

F (Xy|) = {F y(Xy|)}yK=1,

(1)

where F y(Xy|) is the predicted probability for the geometric transformation with label y and  are the learnable parameters of model F (.).

Therefore, given a set of N training images D = {Xi}Ni=0, the self-supervised training objective that the ConvNet model must learn to solve is:

1N

min N

loss(Xi, ),

i=1

(2)

where the loss function loss(.) is defined as:

1 loss(Xi, ) = K

N

log(F y(g(Xi|y)|)).

i=1

(3)

In the following subsection we describe the type of geometric transformations that we propose in our work.

2.2 CHOOSING GEOMETRIC TRANSFORMATIONS: IMAGE ROTATIONS
The type of geometric transformation G in the above formulation is of crucial importance since they define a classification task that solving it must require learning semantic features useful for vision perception tasks (e.g., object detection or image classification). In our work we propose to define the set of geometric transformations G as all the image rotations by multiples of 90 degrees, i.e., 2d image rotations by 0, 90, 180, and 270 degrees (see Figure 2). More formally, if Rot(X, ) is an operator that rotates image X by  degrees, then our set of geometric transformations consists of the K = 4 image rotations G = {g(X|y)}4y=1, where g(X|y) = Rot(X, (y - 1)90).
Forcing the learning of semantic features: The core intuition behind using these image rotations as the set of geometric transformations relates to the simple fact that it is essentially impossible for a ConvNet model to effectively perform the above rotation recognition task unless it has first learnt to recognize and detect classes of objects as well as their semantic parts in images. More specifically, to successfully predict the rotation of an image the ConvNet model must necessarily learn to localize salient objects in the image, recognize their orientation and object type, and then relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images. In Figure 3 we visualize some attention maps generated by a model trained on the rotation recognition task. These attention maps are computed based on the magnitude of activations at each spatial cell of a convolutional layer and essentially reflect where the network puts most of its focus in order to classify an input image. We observe, indeed, that in order for the model to

3

Under review as a conference paper at ICLR 2018

g ( X , y=0) Rotate 0 degrees

Rotated image: X 0

ConvNet model F(.)

Objectives:
Maximize prob. F 0( X 0)
Predict 0 degrees rotation (y=0)

g( X , y=1) Rotate 90 degrees

Rotated image: X 1

ConvNet model F(.)

Maximize prob. F1( X 1)
Predict 90 degrees rotation (y=1)

Image X

g ( X , y=2) Rotate 180 degrees

Rotated image: X 2

ConvNet model F(.)

Maximize prob. F 2( X 2)
Predict 180 degrees rotation (y=2)

g( X , y=3) Rotate 270 degrees

Rotated image: X 3

ConvNet model F(.)

Maximize prob. F 3( X 3)
Predict 270 degrees rotation (y=3)

Figure 2: Illustration of the self-supervised task that we propose for semantic feature learning. Given four possible geometric transformations, the 0, 90, 180, and 270 degrees rotations, we train a ConvNet model F (.) to recognize the rotation that is applied to the image that it gets as input. F y(Xy ) is the probability of rotation transformation y predicted by model F (.) when it gets as input an image that has been transformed by the rotation transformation y.

accomplish the rotation prediction task it learns to focus on high level object parts in the image, such as eyes, nose, tails, and heads. Furthermore, in Figure 4 we visualize the first layer filters that were learnt by an AlexNet model trained on the proposed rotation recognition task. As can be seen, they appear to have a big variety of edge filters on multiple orientations and multiple frequencies. Remarkably, these filters seem to have a greater amount of variety even than the filters learnt by the supervised object recognition task.
Absence of low-level visual artifacts: An additional important advantage of using image rotations by multiples of 90 degrees over other geometric transformations, is that they can be implemented by flip and transpose operations (as we will see below) that do not leave any easily detectable low-level visual artifacts that will lead the ConvNet to learn trivial features with no practical value for the vision perception tasks. In contrast, had we decided to use as geometric transformations, e.g., scale and aspect ratio image transformations, in order to implement them we would need to use image resizing routines that leave easily detectable image artifacts.
Well-posedness: Furthermore, human captured images tend to depict objects in an "up-standing" position, thus making the rotation recognition task well defined, i.e., given an image rotated by 0, 90, 180, or 270 degrees, there is usually no ambiguity of what is the rotation transformation (with the exception of images that only depict round objects). In contrast, that is not the case for the object scale that varies significantly on human captured images.
Implementing image rotations: In order to implement the image rotations by 90, 180, and 270 degrees (the 0 degrees case is the image itself), we use flip and transpose operations. Specifically, for 90 degrees rotation we first transpose the image and then flip it vertically (upside-down flip), for 180 degrees rotation we flip the image first vertically and then horizontally (left-right flip), and finally for 270 degrees rotation we first flip vertically the image and then we transpose it.
2.3 DISCUSSION
The simple formulation of our self-supervised task has several advantages. It has the same computational cost as supervised learning, similar training convergence speed (that is significantly faster than
4

Under review as a conference paper at ICLR 2018

Image

Conv1 27 × 27

Conv3 13 × 13

Conv5 6 × 6

Figure 3: Attention maps generated by a AlexNet model trained on the self-supervised task of recognizing image rotations. In order to generate the attention map of a conv. layer we first compute the feature maps of this layer, then we raise each feature activation on the power p, and finally we sum the feature activations at each location of the feature map. For the conv. layers 1, 2, and 3 we used the powers p = 1, p = 2, and p = 4 respectively.

Supervised

Self-supervised to recognize rotations

Figure 4: Visualization of the first layer filters learned by a AlexNet model trained on left at the supervised object recognition task and on right on the self-supervised task of recognizing rotated images. We observe that the filters learned by the self-supervised task are mostly oriented edge filters on various frequencies. Remarkably, the filter learned with the self-supervised task seem to have more variety than those learned on the supervised task of object recognition.

image reconstruction based approaches; our AlexNet model trains in around 2 days using a single Titan X GPU), and can trivially adopt the efficient parallelization schemes devised for supervised learning (Goyal et al., 2017), making it an ideal candidate for unsupervised learning on internetscale data (i.e., billions of images). Furthermore, our approach does not require any special image pre-processing routine in order to avoid learning trivial features, as many other unsupervised or self-supervised approaches do. Despite the simplicity of our self-supervised formulation, as we will
5

Under review as a conference paper at ICLR 2018

Table 1: Evaluation of the unsupervised learned features by measuring the classification accuracy
that they achieve when we train an object classifier on top of them. The reported results are from CIFAR-10. The size of the ConvB1 feature maps is 96 × 16 × 16 and the sizes of the rest features is 192 × 8 × 8.

Model
RotNet with 3 conv. blocks RotNet with 4 conv. blocks RotNet with 5 conv. blocks

ConvB1
85.45 85.07 85.04

ConvB2
88.26 89.06 89.76

ConvB3
62.09 86.21 86.82

ConvB4
61.73 74.50

ConvB5
50.37

see in the experimental section of the paper, the features learned by our approach achieve dramatic improvements on the unsupervised feature learning benchmarks.
3 EXPERIMENTAL RESULTS
In this section we conduct an extensive evaluation of our approach, on the most commonly used image datasets, such as ImageNet, PASCAL, and CIFAR-10, and on various vision tasks, such as object detection and image classification. We also consider several learning scenarios, including transfer learning and semi-supervised learning. In all cases, we compare our approach with corresponding state-of-the-art methods.
3.1 CIFAR EXPERIMENTS
We start by evaluating the performance of our self-supervised ConvNet based learned features (using the rotation prediction task) when transferred on the object recognition task of CIFAR10 (Krizhevsky & Hinton, 2009). We will here after call a ConvNet model that is trained on the self-supervised task of rotation prediction RotNet model.
Implementation details: In our CIFAR-10 experiments we implement the RotNet models with Network-In-Network (NIN) architectures (Lin et al., 2013). In order to train them on the rotation prediction task, we use SGD with batch size 128, momentum 0.9, weight decay 5e - 4 and lr of 0.1. We drop the learning rates by a factor of 5 after epochs 30, 60, and 80 epochs. We train in total for 100 epochs. In our preliminary experiments we found that we get significant improvement when during training we train the network by feeding it all the four rotated copies of an image simultaneously instead of each time randomly sampling a single rotation transformation. Therefore, at each training batch the network sees 4 times more images than the batch size.
Evaluation of the learned feature hierarchies: First, we explore how the quality of the learned features depends from their depth (i.e., the depth of the layer that they come from) as well as from the total depth of the RotNet model. For that purpose, we first train 3 RotNet models which have 3, 4, and 5 convolutional blocks respectively on the rotation prediction task using the CIFAR-10 training images (note that each conv. block in the NIN architectures that implement the RotNet models have 3 conv. layers; therefore, the total number of conv. layers of the examined RotNet models is 9, 12, and 15 for 3, 4, and 5 conv. blocks respectively).
Afterwards, we learn classifiers on top of the feature maps generated by each conv. block of each RotNet model. Those classifiers are trained in a supervised way on the object recognition task of CIFAR-10. They consist of 3 fully connected layers (plus batch-norm and relu after the first two layers) and the 2 hidden layers have 200 feature channels each. We report the accuracy results of CIFAR-10 test set in Table 1. We observe that in all cases the best performance comes from the feature maps generated by the 2nd conv. block (that actually has depth 6 in terms of the total number of conv. layer till that point) and they rich very high accuracy, i.e., between 88.26% and 89.06%). After the 2nd conv. block the self-supervised learned features start degrade the object recognition accuracy, which we assume is because they start becoming more and more specific on the self-supervised task of rotation prediction. Also, we observe that increasing the total depth of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers (and after the 1st conv. block). We assume that this is because increasing the depth
6

Under review as a conference paper at ICLR 2018

Table 2: Evaluation of unsupervised feature learning methods on CIFAR-10. Note that the Supervised NIN and the (Ours) RotNet + conv entries have exactly the same architecture but the first was trained fully supervised while on the second the first 2 conv. blocks were trained unsupervised with our rotation prediction task and the 3rd block only was trained in a supervised manner. In the Random Init. + conv entry a conv. classifier (similar to that of (Ours) RotNet + conv) is trained on top of two NIN conv. blocks that are randomly initialized and stay frozen.

Method
Supervised NIN
Random Init. + conv
(Ours) RotNet + non-linear (Ours) RotNet + conv
Roto-Scat + SVM Oyallon & Mallat (2015) ExemplarCNN Dosovitskiy et al. (2014) DCGAN Radford et al. (2015) Scattering Oyallon et al. (2017)

Accuracy
92.80
72.50
89.06 91.16
82.3 84.3 82.8 84.7

of the model and thus the complexity of its head (i.e., top ConvNet layers) allows the features of earlier layers to be less specific to the rotation prediction task.
Comparison against supervised and other unsupervised methods: In Table 2 we compare our unsupervised learned features against other unsupervised (or hand-crafted) features on CIFAR-10. For our entry we use the feature maps generated by the 2nd conv. block of a RotNet model with 4 conv. blocks in total. On top of those RotNet features we train 2 different classifiers: (a) a non-linear classifier with 3 fully connected layers as before (entry (Ours) RotNet + non-linear), (b) three conv. layers plus a linear prediction layer (entry (Ours) RotNet +conv.; note that this entry is basically a 3 blocks NIN model with the first 2 blocks coming from a RotNet model and the 3rd being randomly initialized and trained on the recognition task). We observe that we significantly improve over the prior unsupervised approaches and we achieve state-of-the-art results in CIFAR-10. More notably, the accuracy gap between the RotNet based model and the fully supervised NIN model is very small, only 1.64 percentage points (92.80% vs 91.16%).
Correlation between object classification task and rotation prediction task: In Figure 5a, with the yellow curve we plot the rotation prediction accuracy and the object classification accuracy (of a classifier that is applied on self-supervised features) as a function of the training epochs used for solving the self-supervised task of recognizing rotations. We observe that the rotation prediction accuracy is correlated with the object recognition accuracy and also that the object recognition accuracy converges fast w.r.t. the number of training epochs used for solving the pretext task of rotation prediction.
Semi-supervised setting: Motivated by the very high performance of our unsupervised feature learning method, we also evaluate it on a semi-supervised setting. More specifically, we first train a 4 block RotNet model on the rotation prediction task using the entire image dataset of CIFAR-10 and then we train on top of its feature maps object classifiers using only a subset of the available images and their corresponding labels. As feature maps we use those generated by the 2nd conv. block of the RotNet model. As a classifier we use a set convolutional layers that actually has the same architecture as the 3rd conv. block of a NIN model plus a linear classifier, all randomly initialized. For training the object classifier we use for each category 20, 100, 400, 1000, or 5000 image examples. Note that 5000 image examples is the extreme case of using the entire CIFAR-10 training dataset. Also, we compare our method with a supervised model that is trained only on the available examples each time. In Figure 5b we plot the accuracy of the examined models as a function of the available training examples. We observe that our unsupervised trained model exceeds in this semi-supervised setting the supervised model when the number of examples per category drops below 1000. Furthermore, as the number of examples decrease the performance gap in favor of our method becomes more striking. This empirical evidence demonstrates the usefulness of our method on semi-supervised settings.
7

Under review as a conference paper at ICLR 2018

(a) (b)
Figure 5: (a) Plot with the rotation prediction accuracy and object recognition accuracy as a function of the training epochs used for solving the rotation prediction task. The red curve is the object recognition accuracy of a fully supervised model (a NIN model), which is independent from the training epochs on the rotation prediction task. The yellow curve is the object recognition accuracy of an object classifier trained on top of feature maps learned by a RotNet model at different snapshots of the training procedure. (b) Accuracy as a function of the number of training examples per category in CIFAR-10. Ours semi-supervised is a NIN model that the first 2 conv. blocks are RotNet model that was trained in a self-supervised way on the entire training set of CIFAR-10 and the 3rd conv. block along with a prediction linear layer that was trained with the object recognition task only on the available set of labeled images.

Table 3: ImageNet Top-1 classification results with nonlinear layers. We compare our selfsupervised feature learning approach with other unsupervised approaches by training non-linear object classifiers (for the ImageNet classification task) on top of the feature maps generated by our unsupervised RotNet model. For instance, for the conv5 feature map we train the layers that follow the conv5 layer in the AlexNet architecture (i.e., fc6, fc7, and fc8). Similarly for the conv4 feature maps. We implemented those non-linear classifiers with batch normalization units after each linear layer (fully connected or convolutional) and we do not employ drop out. During testing we use a single crop and do not perform flipping augmentation. We report top-1 classification accuracy.

Method
ImageNet labels from (Bojanowski & Joulin, 2017)
Random from (Noroozi & Favaro, 2016)
Tracking Wang & Gupta (2015) Context (Doersch et al., 2015) Colorization (Zhang et al., 2016a) Jigsaw Puzzles (Noroozi & Favaro, 2016) BiGAN (Donahue et al., 2016) NAT (Bojanowski & Joulin, 2017)
(Ours) RotNet

Conv4
59.7
27.1
38.8 45.6 40.7 45.3 41.9
-
50.0

Conv5
59.7
12.0
29.8 30.4 35.2 34.6 32.2 36.0
43.8

3.2 IMAGENET AND PASCAL EXPERIMENTS
Here we evaluate the performance of our self-supervised ConvNet models on the ImageNet and PASCAL VOC datasets. Specifically, we first train a rotation prediction model RotNet on the training images of the ImageNet dataset and then we evaluate the performance of the self-supervised features on the image classification tasks of ImageNet and PASCAL datasets and on the object detection task of PASCAL VOC 2007.
Implementation details: For the ImageNet and PASCAL VOC experiments we implement our RotNet model with an AlexNet architecture. Our implementation of the AlexNet model does not
8

Under review as a conference paper at ICLR 2018

Table 4: Transfer learning evaluation on PASCAL VOC 2007 classification and detection tasks. To obtain those results we use the publicly available testing frameworks of Kra¨henbu¨hl et al. (2015) and Girshick (2015). For classification, we either fix the features before conv5 (column fc6-8) or we fine-tune the whole model (column all). For detection we use multi-scale training and single scale testing. After training the RotNet model on ImageNet, we absorb the batch normalization units on the linear layers and we use the weight rescaling technique proposed by Kra¨henbu¨hl et al. (2015) (which is common among the unsupervised methods). We report mean average prevision as customary on PASCAL VOC.

Trained layers
ImageNet labels
Egomotion (Agrawal et al., 2015) Context Encoders (Pathak et al., 2016b) Tracking (Wang & Gupta, 2015) Context (Doersch et al., 2015) Colorization (Zhang et al., 2016a) BiGAN (Donahue et al., 2016) Jigsaw Puzzles (Noroozi & Favaro, 2016) NAT (Bojanowski & Joulin, 2017) Split-Brain (Zhang et al., 2016b) Counting (Noroozi et al., 2017)
(Ours) RotNet

Classification
fc6-8 all
78.9 79.9
31.0 54.2 34.6 56.5 55.6 63.1 55.1 65.3 61.5 65.6 52.3 60.1
- 67.6 56.7 65.3 63.0 67.1
- 67.7
70.87 72.97

Detection
all
56.8
43.9 44.5 47.4 51.1 46.9 46.9 53.2 49.4 46.7 51.4
54.4

have local response normalization units or groups and it includes batch normalization units after each linear layer (either convolutional or fully connected). In order to train the AlexNet based RotNet model, we use SGD with batch size 192, momentum 0.9, weight decay 5e - 4 and lr of 0.01. We drop the learning rates by a factor of 10 after epochs 10, and 20 epochs. We train in total for 30 epochs. As in the CIFAR experiments, during training we feed the RotNet model all the four rotated copies of an image simultanouesly (in the same mini-batch).
ImageNet classification task: We evaluate the task generalization of our self-supervised learned features by training on top of them object classifiers for the ImageNet classification task (following the evaluation scheme of (Noroozi & Favaro, 2016)). In Table 3 we report the classification performance of our self-supervised features and we compare it with the other unsupervised approaches. We observe that our approaches surpass all the other methods by a significant margin. For the feature maps generated by the Conv4 layer, our improvement is more than 4 percentage points and for the feature maps generated by the Conv5 layer, our improvement is even bigger, around 8 percentage points. Furthermore, our approach narrows significantly the gap between unsupervised features and supervised features.
Transfer learning evaluation on PASCAL VOC: We also explore the ability of our self-supervised features to transfer on the PASCAL VOC classification and detection datasets. The performance of our models on those tasks is reported in Table 4 along with the performance of other unsupervised methods. As with the ImageNet classification task, we outperform by significant margin all the competing methods both in the classification and the detection tasks thus significantly narrowing the gap with the supervised features. Notably, the PASCAL VOC 2007 object detection performance that our self-supervised features achieve is 54.4% mAP, which is only 2.4 points lower than the supervised case.
4 CONCLUSIONS
In our work we propose a novel formulation for self-supervised feature learning that trains a ConvNet model to be able to recognize the image rotation that is applied to the images that it gets as input. Despite the simplicity of our self-supervised task, we demonstrate that it successfully forces the ConvNet model that tries to solve it, to learn semantic features useful for perception vision
9

Under review as a conference paper at ICLR 2018
tasks, such as object recognition and object detection. We exhaustively evaluate our method in various unsupervised and semi-supervised benchmarks and we achieve in all of them extremely good performance. Even more, our self-supervised approach achieves state-of-the-art results on unsupervised feature learning for ImageNet classification, PASCAL classification, PASCAL detection, and CIFAR-10 classification, surpassing the prior approaches by significant margin and thus drastically reducing the gap between unsupervised and supervised feature learning.
REFERENCES
Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In Proceedings of the IEEE International Conference on Computer Vision, pp. 37­45, 2015.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Advances in neural information processing systems, pp. 153­160, 2007.
Piotr Bojanowski and Armand Joulin. Unsupervised learning by predicting noise. arXiv preprint arXiv:1704.05310, 2017.
Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. CoRR, abs/1708.07860, 2017.
Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422­1430, 2015.
Jeff Donahue, Philipp Kra¨henbu¨hl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.
Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 766­774, 2014.
Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 1440­1448, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Priya Goyal, Piotr Dolla´r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Fu Jie Huang, Y-Lan Boureau, Yann LeCun, et al. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on, pp. 1­8. IEEE, 2007.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3128­3137, 2015.
Philipp Kra¨henbu¨hl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations of convolutional neural networks. arXiv preprint arXiv:1511.06856, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic colorization. In European Conference on Computer Vision, pp. 577­593. Springer, 2016.
Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Colorization as a proxy task for visual understanding. arXiv preprint arXiv:1703.04044, 2017.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
10

Under review as a conference paper at ICLR 2018
Renjie Liao, Alex Schwing, Richard Zemel, and Raquel Urtasun. Learning deep parsimonious representations. In Advances in Neural Information Processing Systems, pp. 5076­5084, 2016.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.
Jonathan Masci, Ueli Meier, Dan Cires¸an, and Ju¨rgen Schmidhuber. Stacked convolutional autoencoders for hierarchical feature extraction. Artificial Neural Networks and Machine Learning­ ICANN 2011, pp. 52­59, 2011.
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, pp. 69­84. Springer, 2016.
Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to count. arXiv preprint arXiv:1708.06734, 2017.
Edouard Oyallon and Ste´phane Mallat. Deep roto-translation scattering for object classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2865­ 2873, 2015.
Edouard Oyallon, Eugene Belilovsky, and Sergey Zagoruyko. Scaling the scattering transform: Deep hybrid networks. arXiv preprint arXiv:1703.08961, 2017.
Deepak Pathak, Ross Girshick, Piotr Dolla´r, Trevor Darrell, and Bharath Hariharan. Learning features by watching objects move. arXiv preprint arXiv:1612.06370, 2016a.
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2536­2544, 2016b.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211­252, 2015.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2794­2802, 2015.
Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations and image clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5147­5156, 2016.
Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In European Conference on Computer Vision, pp. 649­666. Springer, 2016a.
Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. arXiv preprint arXiv:1611.09842, 2016b.
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27, pp. 487­495. Curran Associates, Inc., 2014.
11

