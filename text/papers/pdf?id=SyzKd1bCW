Under review as a conference paper at ICLR 2018
BACKPROPAGATION THROUGH THE VOID: OPTIMIZING CONTROL VARIATES FOR BLACK-BOX GRADIENT ESTIMATION
Anonymous authors Paper under double-blind review
ABSTRACT
Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables, based on gradients of a learned function. These estimators can be jointly trained with model parameters or policies, and are applicable in both discrete and continuous settings. We give unbiased, adaptive analogs of state-of-the-art reinforcement learning methods such as advantage actor-critic. We also demonstrate this framework for training discrete latent-variable models.
1 INTRODUCTION
Gradient-based optimization has been key to most recent advances in machine learning and reinforcement learning. The back-propagation algorithm (Rumelhart & Hinton, 1986), also known as reverse-mode automatic differentiation (Speelpenning, 1980; Rall, 1981) computes exact gradients of deterministic, differentiable objective functions. The reparameterization trick (Williams, 1992; Kingma & Welling, 2014; Rezende et al., 2014) allows backpropagation to give unbiased, lowvariance estimates of gradients of expectations of continuous random variables. This has allowed effective stochastic optimization of large probabilistic latent-variable models.
Unfortunately, there are many objective functions relevant to the machine learning community for which backpropagation cannot be applied. In reinforcement learning, for example, the function being optimized is unknown to the agent and is treated as a black box (Schulman et al., 2015). Similarly, when fitting probabilistic models with discrete latent variables, discrete sampling operations create discontinuities giving the objective function zero gradient with respect to its parameters. Much recent work has been devoted to constructing gradient estimators for these situations. In reinforcement learning, advantage actor-critic methods (Sutton et al., 2000) give unbiased gradient estimates with reduced variance obtained by jointly optimizing the policy parameters with an estimate of the value function. In discrete latent-variable models, low-variance but biased gradient estimates can be given by continuous relaxations of discrete variables (Maddison et al., 2016; Jang et al., 2016).
A recent advance by Tucker et al. (2017) used a continuous relaxation to construct a control variate for functions of discrete random variables. Low-variance estimates of the expectation of the control variate can be computed using the reparameterization trick to produce an unbiased estimator with lower variance than previous methods. Furthermore, Tucker et al. (2017) showed how to tune the free parameters of these relaxations to minimize the estimator's variance during training.
In this work we generalize the method of Tucker et al. (2017) to learn a free-form control variate parameterized by a neural network, giving a lower-variance, unbiased gradient estimator which can be applied to a wider variety of problems with greater flexibility. Most notably, our method is applicable even when no continuous relaxation is available, as in reinforcement learning or black box function optimization. Furthermore, we derive improved variants of popular reinforcement learning methods with unbiased, action-dependent gradient estimates and lower variance.
1

Under review as a conference paper at ICLR 2018

Loss Log Variance of Gradient Estimates

0.2506 0.2504 0.2502 0.2500 0.2498 0.2496 0.2494 0.2492 0.2490
0

REINFORCE REBAR RELAX (ours) Exact gradient
2000 4000 6000 8000 10000 Steps

5.0
7.5
10.0
12.5
15.0
17.5 REINFORCE REBAR
20.0 RELAX (ours) 2000 4000 6000 8000 10000 Steps

Figure 1: Left: Training curves comparing different gradient estimators on a toy problem: L() = Ep(b|)[(b - 0.499)2] Right: Variance of each estimator's gradient.

2 BACKGROUND: GRADIENT ESTIMATORS

How can we choose the parameters of a distribution to maximize an expectation? This problem
comes up in reinforcement learning, where we must choose the parameters  of a policy distribution (a|s, ) to maximize the expected reward E [R] over state-action trajectories  . It also comes up in fitting latent-variable models, when we wish to maximize the marginal probability p(x|) = z p(x|z)p(z|) = Ep(z|) [p(x|z)]. In this paper, we'll consider the general problem of optimizing

L() = Ep(b|)[ f (b)] .

(1)

When the parameters  are high-dimensional, gradient-based optimization is appealing because it

provides information about how to adjust each parameter individually. Stochastic optimization is

essential for scalablility. However, it is only guaranteed to converge to a fixed point of the objective

when

the

stochastic

gradients

g^

are

unbiased,

i.e.

E [g^]

=

 

Ep(b|)

[f (b)]

(Robbins

&

Monro,

1951).

How

can

we

build

unbiased,

stochastic

estimators

of

 

L()?

There

are

several

standard

methods:

The score-function gradient estimator One of the most generally-applicable gradient estimators is known as the score-function estimator, or REINFORCE (Williams, 1992):

g^REINFORCE[f ]

=

f

(b)

 

log

p(b|),

b  p(b|)

(2)

This estimator is unbiased, but in general has high variance. Intuitively, this estimator is limited by the fact that it doesn't use any information about how f depends on b, only on the final outcome f (b).

The reparameterization trick When f is continuous and differentiable, and the latent variables b can be written as a deterministic, differentiable function of a random draw from a fixed distribution, the reparameterization trick (Williams, 1992; Kingma & Welling, 2014; Rezende et al., 2014) creates a low-variance, unbiased gradient estimator by making the dependence of b on  explicit through a reparameterization function b = T (, ):

 f T

g^reparam[f ] =

f (b) = 

T

, 

 p( )

(3)

This gradient estimator is often used when training high-dimensional, continuous latent-variable

models, such as variational autoencoders or GANs (Goodfellow et al., 2014). One intuition for why

this

gradient

estimator

is

preferable

to

REINFORCE

is

that

it

depends

on

f b

,

which

exposes

the

dependence of f on b.

Control variates Control variates are a general method for reducing the variance of a Monte Carlo
estimator. Given an estimator g^(b), a control variate is a function c(b) with a known mean Ep(b)[c(b)]. Subtracting the control variate from our estimator and adding its mean gives us a new estimator:

g^new(b) = g^(b) - c(b) + Ep(b)[c(b)]

(4)

2

Under review as a conference paper at ICLR 2018

This new estimator has the same expectation as the old one:

Ep(b) [g^new(b)] = Ep(b) g^(b) - c(b) + Ep(b) [c(b)] = Ep(b) [g^(b)]

(5)

Importantly, the new estimator has lower variance than g^(b) if c(b) is positively correlated with f (b).

3 CONSTRUCTING AND OPTIMIZING A DIFFERENTIABLE SURROGATE

In

this

section,

we

introduce

a

gradient

estimator

for

the

expectation

of

a

function

 

Ep(b|)[f (b)]

that

can be applied even when f is unknown, or not differentiable, or when b is discrete. Our estimator

combines the score function estimator, the reparameterization trick, and control variates. We obtain an

unbiased estimator whose variance can potentially be as low as the reparameterization-trick estimator,

even when f is not differentiable or not computable.

First, we consider the case where b is continuous, but that f cannot be differentiated. Instead of
differentiating through f , we build a surrogate of f using a neural network c, and differentiate c instead. Since the score-function estimator and reparameterization estimator have the same expectation, we can simply subtract the score-function estimator for c and add the reparameterization estimator for c, to produce a gradient estimator which we call LAX:

g^LAX = gREINFORCE[f ] - gREINFORCE[c] + greparam[c]

=

[f (b)

-

c(b)]

 

log

p(b|)

+

  c(b)

b = T (, ),  p( ).

(6)

This estimator is unbiased for any choice of c and when c = f , our estimator becomes the reparameterization estimator for c. Thus our estimator can have variance at least as low as the reparameterization estimator.

3.1 OPTIMIZING THE GRADIENT CONTROL VARIATE WITH GRADIENTS

Since g^LAX is unbiased for any choice of the surrogate c, the only remaining problem is to choose a c that gives low variance to g^LAX. How can we find a  which gives our estimator low variance? We simply optimize c using stochastic gradient descent, at the same time as we optimize the parameters of our model or policy.
To optimize c, we require the gradient of the variance of our gradient estimator. To estimate these gradients, we could simply differentiate through the empirical variance over each mini-batch. Or, following Ruiz et al. (2016) and Tucker et al. (2017), we can construct an unbiased, single-sample estimator using the fact that our gradient estimator is unbiased. For any unbiased gradient estimator g^ with parameters :

 Variance(g^)


=

 

E[g^2

]

-

 

E[g^]2

=

 

E[g^2

]

=

E

 g^2 

=E

g^ 2g^


.

(7)

Thus,

an

unbiased

single-sample

estimate

of

the

gradient

of

the

variance

of

g^

is

given

by

2g^

g^ 

.

This method of directly minimizing the variance of the gradient estimator stands in contrast to other
methods such as Q-Prop (Gu et al., 2016) and advantage actor-critic (Sutton et al., 2000), which train the control variate to minimize the squared error (f (b) - c(b))2. Our algorithm, which jointly optimizes the parameters  and the surrogate c is given in Algorithm 1.

3.1.1 OPTIMAL SURROGATE

What is the form of the variance-minimizing c? Inspecting the square of (6), we can see that this

loss

encourages

c(b)

to

approximate

f (b),

but

with

a

weighting

based

on

 

log p(b).

Moreover,

as

c



f

then

g^LAX



 

c.

Thus,

this

objective

encourages

a

balance

between

the

variance

of

the reparameterization estimator and the variance of the REINFORCE estimator. Figure 2 shows the

learned surrogate on a toy problem.

3

Under review as a conference paper at ICLR 2018

Algorithm 1 LAX: Optimizing parameters and a gradient control variate simultaneously.

Require: f (·), log p(b|), reparameterized sampler b = T (, ), neural network c(·)

while not converged do

i  p( ) bi  T ( i, ) g  [f (bi) - c(bi)]  log p + c(bi)

Sample noise Compute input Estimate gradient

g



2g

g 

   + 1g

   + 2g

Estimate gradient of variance of gradient
Update parameters Update control variate

end while

return 

3.2 DISCRETE RANDOM VARIABLES AND CONDITIONAL REPARAMETERIZATION

We can adapt the LAX estimator to the case where b is a discrete random variable by introducing a "relaxed" continuous variable z. We require a continuous, reparameterizable distribution p(z|) and a deterministic mapping H(z) such that H(z) = b  p(b|) when z  p(z|). In our implementation,
we use the Gumbel-softmax trick, the details of which can be found in appendix B.

The discrete version of the LAX estimator is given by:

g^DLAX

=

 f (b)


log

p(b|)

-

 c(z) 

log

p(z|)

+

  c(z),

b = H(z), z  p(z|). (8)

This estimator is simple to implement and general. However, when f = c we do not recover

the reparameterization estimator as we do with LAX. To achieve this, we must be able to replace

the

 

log p(z|)

in

the

control

variate

with

 

log p(b|).

This

is

the

motivation

behind

our

next

estimator which we call RELAX.

To construct a more powerful gradient estimator, we incorporate a further refinement due to Tucker et al. (2017). Specifically, we evaluate our control variate both at a relaxed input z  p(z|), and also at a relaxed input conditioned on the discrete variable b, denoted z~  p(z|b, ). Thus we define our
estimator as

g^RELAX

=

[f (b)

-

c(z~)]

 

log

p(b|)

+

  c(z)

-

  c(z~)

b = H(z), z  p(z|), z~  p(z|b, )

(9)

This estimator is unbiased for any c. A proof and a detailed algorithm can be found in appendix A. We note that the distribution p(z|b, ) must also be reparameterizable. We demonstrate how to perform
this conditional reparameterization for Bernoulli and categorical random variables in appendix B.

3.3 CHOOSING THE CONTROL VARIATE ARCHITECTURE
The variance-reduction objective introduced above allows us to use any differentiable, parametric function as our control variate c. How should we choose the architecture of c? Ideally, we will take advantage of any known structure in f .
If f is a known, differentiable function of discrete random variables, we can use the concrete relaxation (Jang et al., 2016; Maddison et al., 2016) and let c(z) = f ((z)). In this special case, our estimator is exactly the REBAR estimator. We are also free to add a learned component to the concrete relaxation and let c(z) = f ((z)) + r(z) where r is a neural network with parameters . We took this approach in our experiments training discrete variational auto-encoders. If f is unknown, we can simply let c be a generic function approximator such as a neural network. We took this simpler approach in our reinforcement learning experiments.

3.4 REINFORCEMENT LEARNING
We now describe how we apply the LAX estimator in the reinforcement learning (RL) setting. By reinforcement learning, we refer to the problem of optimizing the parameters  of a policy distribution

4

Under review as a conference paper at ICLR 2018

(a|s, ) to maximize the sum of rewards. In this setting, the random variable being integrated over

is  , which denotes a series of actions and states [(s1, a1), (s2, a2), ..., (sT , aT )]. The function whose

expectation is being optimized, R, maps  to the sum of rewards R( ) =

T t=1

rt(st

,

at

).

Again,

we

want

to

estimate

the

gradient

of

an

expectation

of

a

black-box

function:

 

Ep(

|)[R(

)].

The de facto standard approach is the advantage actor-critic estimator (A2C) (Sutton et al., 2000):

g^A2C =



 log (at|st, ) 


rt - c(st) ,

t=1 t =t

at  (at|st, )

(10)

Where c(st) is an estimate of the state-value function, c(s)  V (s) = E [R|s1 = s]. This estimator is unbiased when c does not depend on at. The main limitations of A2C are that c does not depend on at, and that it's not obvious how to optimize c. Using the LAX estimator addresses both of these problems.

First, we assume (at|st) is reparameterizable, meaning that we can write at = a( t, st, ), where t does not depend on . We again introduce a differentiable surrogate c(a, s). Crucially, this surrogate is a function of the action as well as the state.

Our estimator is defined as:

g^LRALX =



 log (at|st, ) 

t=1


rt - c(at, st)

 +  c(at, st),

t =t

at = a( t, st, )

t  p( t).

(11)

This estimator is unbiased if the true dynamics of the system are Markovian w.r.t. the state st. When T = 1, we recover the special case g^LRALX = g^LAX. Comparing g^LRALX to the standard advantage actor-critic estimator in (10), the main difference is that our baseline c(at, st) is action-dependent while still remaining unbiased.

To optimize the parameters  of our control variate c(at, st), we can again use the single-sample estimator of the gradient of our estimator's variance given in (7). This approach avoids unstable
training dynamics, and doesn't require storage and replay of previous rollouts.

Details of this derivation, as well as the discrete and conditionally reparameterized version of this estimator can be found in appendix C.

4 SCOPE AND LIMITATIONS
The work most related to ours is the recently-developed REBAR method (Tucker et al., 2017), which inspired our work. The REBAR estimator is a special case of the RELAX estimator, when the surrogate is set to c(z) =  · f (softmax(z)). The only free parameters of the REBAR estimator are the scaling factor , and the temperature , which gives limited scope to optimize the surrogate. REBAR can only be applied when f is known and differentiable. Furthermore, it depends on essentially undefined behavior of the function being optimized, since it evaluates the discrete loss function at continuous inputs.
Because LAX and RELAX can construct a surrogate from scratch, they can be used for optimizing black-box functions, as in reinforcement learning settings where the reward is an unknown function of the environment. LAX and RELAX only require that we can query the function being optimized, and can sample from and differentiate p(b|).
Can RELAX be used to optimize deterministic black-box functions? The answer is yes, with the caveat that one must introduce stochasticity to the inputs. Thus, RELAX is most suitable for problems where one is already optimizing a distribution over inputs, such as in inference or reinforcement learning.
Direct dependence on parameters Above, we assumed that the function f being optimized does not depend directly on , which is usually the case in black-box optimization settings. However, a dependence on  can occur when training probabilistic models, or when we add a regularizer to

5

Under review as a conference paper at ICLR 2018

a black-box optimization problem. In both these settings, if the dependence on  is known and differentiable, we can use the fact that

  Ep(b|)[f (b, )] = Ep(b|)



 f (b, ) + f (b, )

log p(b|)

 

(12)

and

simply

add

the

term

 

f

(b,

)

to

our

gradient

estimate.

5 RELATED WORK

Miller et al. (2017) reduce the variance of reparameterization gradients in an orthogonal way to ours by approximating the gradient-generating procedure with a simple model and using that model as a control variate. NVIL (Mnih & Gregor, 2014) and VIMCO (Mnih & Rezende, 2016) provide reduced variance gradient estimation in the special case of discrete latent variable models and discrete latent variable models with Monte-Carlo objectives. Salimans et al. (2017) estimate gradients using a form of finite differences, evaluating hundreds of different parameter values in parallel to construct a gradient estimate. In contrast, our method is a single-sample estimator.
Staines & Barber (2012) address the general problem of developing gradient estimators for deterministic black-box functions or discrete optimization. They introduce a sampling distribution, and optimize an objective similar to ours. Wierstra et al. (2014) also introduce a sampling distribution to build a gradient estimator, and consider optimizing the sampling distribution.
In the reinforcement learning setting, the work most similar to ours is Q-prop (Haarnoja et al., 2017). Like our method, Q-prop reduces the variance of the policy gradient with an learned, action-dependent control variate whose expectation is approximated via a monte-carlo sample from a taylor series expansion of the control variate. Unlike our method, their control variate is trained off-policy. While our method is applicable in both the continuous and discrete action domain, Q-prop is only applicable to continuous actions.

6 APPLICATIONS

REINFORCE

We demonstrate the effectiveness of our estimator on a number of challenging optimization problems. Following Tucker et al. (2017) we begin with a simple toy example to illuminate the potential of our method and then continue to the more relevant problems of optimizing binary VAE's and reinforcement learning.

0.251
00..225501 00..2254901 0.2054.902 0.249
00..02

f(b = H(z(u)))
f(b = H(z(u))) f(b = H(z(u))) REBfA( R(z(u)))
f( (z(u)))

6.1 TOY EXPERIMENT
As a simple example, we follow Tucker et al. (2017) in minimizing Ep(b|)[(b - t)2] as a function of the parameter  where p(b|) = Bernoulli(b|). Tucker et al. (2017) set the target t = .45. We focus on the more challenging case where t = .499. Figures 1a and

0.002..002 0.0 0.2105 00..2150 0.0
0.15 0.0
0.0

f( (z(u))) c (z(u))

c (z(u))RELAX

c0(.z2(u)) 0.4 u 0.6

0.2 0.2

0.4 0.4

u u

0.6 0.6

0.8
0.8 0.8

1.0
1.0 1.0

1b show the relative performance and gradient Figure 2: The optimal relaxation for a toy loss

log-variance of REINFORCE, REBAR, and RE- function, using different gradient estimators. Be-

LAX.

cause REBAR uses the concrete relaxation of f ,

Figure 2 plots the learned surrogate c for a fixed value of . We can see that c is near f for all z, keeping the variance of the REINFORCE

which happens to be implemented as a quadratic function, the optimal relaxation is constrained to be a warped quadratic. In contrast, RELAX can

part of the estimator small. Moreover the deriva- choose a free-form relaxation.

tive of c is positive for all z meaning that the reparameterization part of the estimator will produce gradients pointing in the correct direction to optimize the expectation. Conversely, the concrete

relaxation of REBAR is close to f only near 0 and 1 and its gradient points in the correct direction

6

Under review as a conference paper at ICLR 2018

-ELBO

MNIST

120 REBAR train

118

REBAR valid RELAX train

RELAX valid

116

114

112

110 0 200 4e0p0ochs600 800

135.0 132.5 130.0 127.5 125.0 122.5 120.0 117.5 115.0 0

Omniglot 500 e1p0o0c0hs 1500 2000

Figure 3: Training curves for the one-layer VAE Experiments with the 1 layer linear model. The horizontal dashed line indicates the lowest validation error obtained by REBAR.

only

for

values

of

z

>

log(

1-t t

).

These

factors

together

result

in

the

RELAX

estimator

achieving

the

best performance.

6.2 DISCRETE VARIATIONAL AUTOENCODER
Next, we evaluate the RELAX estimator on the task of training a variational autoencoder (Kingma & Welling, 2014; Rezende et al., 2014) with Bernoulli latent variables. We reproduced a subset of the experiments from Tucker et al. (2017), training models with 1 and 2 layers of 200 Bernoulli random variables with linear mappings between them, on both the MNIST and Omniglot (Lake et al., 2015) datasets. Details of these models and our experimental procedure can be found in appendix E.1.
To take advantage of the available structure in the loss function, we choose the form of our control variate to be c(z) = f ((z)) + r^(z) where r^ is a neural network with parameters  and f ((z)) is the discrete loss function (the evidence lower-bound) evaluated at continuously relaxed inputs as in REBAR. In all experiments, the learned control variate improved the training and validation performance, over the state-of-the-art baseline of REBAR.

Dataset MNIST
Omniglot

Model
Nonlinear linear 1 layer linear 2 Layer
Nonlinear linear 1 layer linear 2 Layer

Concrete
-102.2 -111.3 -99.62
-110.4 -117.23 -109.95

NVIL
-101.5 -112.5 -99.6
-109.58 -117.44 -109.98

MuProp
-101.1 -111.7 -99.07
-108.72 -117.09 -109.55

REBAR
-81.01 -111.6 -98.22
-62.28 -116.63 -108.71

RELAX
-78.13 -111.20 -98.00
-58.55 -116.57 -108.54

Table 1: Best obtained training objective.

To obtain training curves we created our own implementation of REBAR, which gave identical or slightly improved performance compared to the implementation of Tucker et al. (2017).
While we obtained a modest improvement in training and validation scores (tables 1 and 3), the most notable improvement provided by RELAX is in its rate of convergence. Training curves for the linear models can be seen in figure 3 and in appendix D. In table 4 we compare the number of training epochs that are required to match the best validation score of REBAR. In all experiments, RELAX provides an increase in rate of convergence.

6.3 REINFORCEMENT LEARNING
We apply our gradient estimator to a few simple reinforcement learning environments with discrete and continuous actions. We use the RELAX and LAX estimators for discrete and continuous actions,

7

Under review as a conference paper at ICLR 2018

Reward

Log-Variance

200 150 100 50
0 0
5
0 0

Cart-pole
A2C RELAX
100S0t0e0ps 200000

Lunar lander
200
0

Inverted pendulum
1000 750 500

Inverted double pendulum
7500 5000

200 250

2500

0

0 2000S0t0e0ps 4000000

0

5S0t0e0p0s0

0 1000000 0

2000S0t0e0ps 4000000

10.0 0

7.5 10 10

5.0 2.5 20

5

E2p0i0sodes 400

0.0 0

2E0p0is0odes 4000

30 0

Episo2d0e0s0

0 0 Episodes1000

Figure 4: Top row: Reward curves. Bottom row: Variance of policy gradients (log scale). In each curve, the center line indicates the mean reward over 5 random seeds. The opaque bars in the top row indicate the 25th and 75th percentiles. The opaque bars in the bottom row indicate 1 standard deviation. After every 10th training episode 100 episodes were run and the sample log-variance is reported averaged over all policy parameters.

respectively. We compare with the advantage actor-critic algorithm (A2C) (Sutton et al., 2000) as a baseline. Full details of our experiments can be found in Appendix E.

6.3.1 EXPERIMENTS
In the discrete action setting, we test our approach on the Cart Pole and Lunar Lander environments as provided by the OpenAI gym (Brockman et al., 2016). In the continuous action setting, we test on the MuJoCo-simulated (Todorov et al., 2012) environments Inverted Pendulum and Inverted Double Pendulum also found in the OpenAI gym. In all tested environments we observe improved performance and sample efficiency using our method. The results of our experiments can be seen in figure 4, and table 2.
We found that our estimator produced policy gradients with drastically reduced variance (see figure ??) allowing for larger learning rates to be used while maintaining stable training. In both discrete environments our estimator achieved great than a 2-times speedup in convergence over the baseline.

Model

Cart-pole

Lunar lander Inverted pendulum Inverted double pendulum

A2C 1152 ± 90 162374 ± 17241 LAX/RELAX 472 ± 114 68712 ± 20668

9916 ± 235 6237 ± 45

78260 ± 1877 60967 ± 1669

Table 2: Mean episodes to solve each task. Definition of solving each task can be found in appendix E.

7 CONCLUSIONS AND FUTURE WORK
In this work we synthesized and generalized many of the standard approaches for constructing gradient estimators. We proposed a simple and generic gradient estimator that can be applied to expectations of known or black-box functions of discrete or continuous random variables. We also derive a simple extension to apply our method to reinforcement learning in both discrete- and continuous-action domains. This approach is relatively simple to implement and adds little computational overhead.
The scope and generality of our estimator opens up many new possibilities for models which can now be trained via gradient decent. For example, we could apply our estimator to train a VAE with
8

Under review as a conference paper at ICLR 2018
continuous latent variables whose generative model is non-differentiable (a rendering engine perhaps). We also feel that there is much room to explore model design choices for the control variate and to better understand the properties of the optimal control variate.
We believe our results in reinforcement learning are promising and should motivate further research into using action-dependent control-variates for policy-gradient methods. We are interested in combining our approach with other popular variance reduction techniques such as generalized advantage estimation (Kimura et al., 2000). We are also interested in ways to train our control variate off-policy as in Q-prop (Gu et al., 2016). We also feel that the relationship between our learned control variate and the action-value function (commonly denoted as Q) is worth exploring and understanding in greater detail.
REFERENCES
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
Thomas Unterthiner Djork-Arne´ Clevert and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). International Conference on Learning Representations, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-prop: Sample-efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247, 2016.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.
Christopher Hesse, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Openai baselines. https://github.com/openai/baselines, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.
Hajime Kimura, Shigenobu Kobayashi, et al. An analysis of actor-critic algorithms using eligibility traces: reinforcement learning with imperfect value functions. Journal of Japanese Society for Artificial Intelligence, 15(2):267­275, 2000.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. International Conference on Learning Representations, 2014.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332­1338, 2015.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.
Andrew C Miller, Nicholas J Foti, Alexander D'Amour, and Ryan P Adams. Reducing reparameterization gradient variance. arXiv preprint arXiv:1705.07880, 2017.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 1791­1799, 2014.
Andriy Mnih and Danilo Rezende. Variational inference for monte carlo objectives. In International Conference on Machine Learning, pp. 2188­2196, 2016.
Louis B Rall. Automatic differentiation: Techniques and applications. 1981.
9

Under review as a conference paper at ICLR 2018
Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on Machine Learning, pp. 1278­1286, 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pp. 400­407, 1951.
Francisco J.R. Ruiz, Michalis K Titsias, and David M Blei. Overdispersed black-box variational inference. In Uuncertainty in Artificial Intelligence, 2016.
David E Rumelhart and Geoffrey E Hinton. Learning representations by back-propagating errors. Nature, 323:9, 1986.
Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel. Gradient estimation using stochastic computation graphs. In Advances in Neural Information Processing Systems, pp. 3528­ 3536, 2015.
Bert Speelpenning. Compiling Fast Partial Derivatives of Functions Given by Algorithms. PhD thesis, University of Illinois at Urbana-Champaign, 1980.
Joe Staines and David Barber. Variational optimization. arXiv preprint arXiv:1212.4507, 2012. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in neural information processing systems, pp. 1057­1063, 2000. T. Tieleman and G. Hinton. Lecture 6.5--RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026­ 5033. IEEE, 2012. George Tucker, Andriy Mnih, Chris J Maddison, and Jascha Sohl-Dickstein. Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models. arXiv preprint arXiv:1703.07370, 2017. Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, and Ju¨rgen Schmidhuber. Natural evolution strategies. Journal of Machine Learning Research, 15(1):949­980, 2014. Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229­256, 1992.
10

Under review as a conference paper at ICLR 2018

APPENDICES

A THE RELAX ALGORITHM

We prove that g^RELAX is unbiased. Following Tucker et al. (2017):

E [g^RELAX] =

(13)

Ep(b|)

f (b) - Ep(z|b,)[c(z)]

 

log

p(b|)

-

  Ep(z|b,)[c(z)]

 +  Ep(z|)[c(z)]

=

  Ep(b|)

f (b) - Ep(z|b,) [c(z)]

 +  Ep(z|)[c(z)]

=

 

Ep(b|)[f

(b)]

-

  Ep(z|)[c(z)] +

  Ep(z|)[c(z)]

=

  Ep(b|)[ f (b)]

(14)

Algorithm 2 RELAX: Low-variance control variate optimization for black-box gradient estimation.

Require: f (·), log p(b|), reparameterized samplers b = H(z), z = S( , ) and z~ = S( , |b),

neural network c(·) while not converged do

i, i  p( )

Sample noise

zi  S( i, )

Compute unconditional relaxed input

bi  H(zi)

Compute input

zi  S( i, |bi)

Compute conditional relaxed input

g  [f (bi) - c(zi)]  log p + c(zi) - c(zi)

Estimate gradient

g



2g

g 

Estimate gradient of variance of gradient

   + 1g

Update parameters

   + 2g

Update control variate

end while

return 

B CONDITIONAL RE-SAMPLING FOR DISCRETE RANDOM VARIABLES

When applying the RELAX estimator to a function of discrete random variables b  p(b|), we require that there exists a distribution p(z|) and a deterministic mapping H(z) such that if z  p(z|) then H(z) = b  p(b|). Treating both b and z as random, this procedure defines a probabalistic model p(b, z|) = p(b|z)p(z|). The RELAX estimator requires reparameterized samples from p(z|) and p(z|b, ). We describe how to sample from these distributions in the common cases of p(b|) = Bernoulli() and p(b|) = Categorical().

Bernoulli When p(b|) is Bernoulli distribution we let H(z) = I(z > 0) and we sample from p(z|) with

u

z

=

log

1

-



+

log

1

-

, u

u  uniform[0, 1].

We can sample from p(z|b, ) with

z~ =

v· b=0 v(1 - ) +  b = 1

where v  uniform[0, 1].

Categorical When p(b|) is a Categorical distribution where i = p(b = i|), we let H(z) = argmax(z) and we sample from p(z|) with
z = log  - log(- log u), u  uniform[0, 1]k
where k is the number of possible outcomes.

11

Under review as a conference paper at ICLR 2018

Intuitively, to sample from p(z|b, ) we should first sample v  uniform[0, 1]k, then compute

gb = log b - log(- log(vb)). Then we must determine how to scale each vi=b such that gi=b < gb.

We can define v such that

vi =

vi i = b
i
vi · (vb) b i = b

and then z~ = log  - log(- log v ) which is our sample from p(z|b, ).

C DERIVATIONS OF ESTIMATORS USED IN REINFORCEMENT LEARNING

We give the derivation of the LAX estimator used for continuous RL tasks.

Theorem C.1. The LAX estimator,

g^LRALX =



 log (at|st, ) 

t=1


rt - c(at, st)

 +  c(at, st),

t =t

at = at( t, st, ), t  p( t),

is unbiased.

(15)

Proof. Note that by using the score-function estimator, for all t, we have

Ep( )



log

(at 

|st,

)

c(at,

st

)

= Ep(a1:t-1,s1:t)

  E(at|st,) c(at, st)

Then, by adding and subtracting the same term, we have

.

  Ep()[f ( )] = Ep()

f ( ) ·  log p( ; ) 

-

Ep( )



log

(at 

|st,

)

c(at,

st)

+

t

 Ep(a1:t-1,s1:t)  E(at|st,) c(at, st)
t

= Ep( )

  log (at|st, ) 


rt - c(at, st)

t=1 t =t

 + Ep(a1:t-1,s1:t) Ep( t)  c(at( t, st, ), st)
t

= Ep( )

  log (at|st, ) 


rt - c(at, st)

 +  c(at( t, st, ), st)

t=1 t =t

In the discrete control setting, our policy parameterizes a soft-max distribution which we use to sample actions. We define zt  p(zt|st), which is equal to (log  - log(- log(u))) where u  Unif[0, 1], at = argmax(zt),  is the soft-max function. We also define z~t  p(zt|at, st) and uses the same reparametrization trick for sampling z~t as explicated in Appendix B.
Theorem C.2. The RELAX estimator,

g^RRELLAX =



 log (at|st, ) 

t=1


rt - c(z~t, st)

-

 

c(z~t,

st)

+

 

c(zt,

st),

t =t

z~t  p(zt|at, st), zt  p(zt|st),

(16)

is unbiased.

Proof. Note that by using the score-function estimator, for all t, we have

Ep(a1:t ,s1:t )



log

(at|st, 

)

Ep(zt|at,st)[c(zt,

st)]

= Ep(a1:t-1,s1:t) = Ep(a1:t-1,s1:t)

  E(at|st,) Ep(zt|at,st)[c(zt, st)]   Ep(zt|st)[c(zt, st)]

12

Under review as a conference paper at ICLR 2018

-ELBO

106 105 104 103 102 101 100 99 98
0

MNIST REBAR train REBAR valid RELAX train RELAX valid
200 4e0p0ochs600 800

120 118 116 114 112 110 108 0

Omniglot 500 e1p0o0c0hs 1500 2000

Figure 5: Training curves for the VAE Experiments with the 2 layer linear model. The horizontal dashed line indicates the lowest validation error obtained by REBAR.

Then, by adding and subtracting the same term, we have

  Ep()[f ( )] = Ep()

f ( ) ·  log p( ; ) 

-

Ep(a1:t ,s1:t )



log

(at|st, 

)

Ep(zt|at,st)[c(zt,

st)]

+

t

 Ep(a1:t-1,s1:t)  Ep(zt|st)[c(zt, st)]
t

= Ep( )

  log (at|st, ) 

t=1


rt - Ep(zt|at,st)[c(zt, st)]
t =t

 + Ep(a1:t-1,s1:t)  Ep(zt|st)[c(zt, st)]
t

= Ep( )

  log (at|st, ) 

t=1


rt - Ep(zt|at,st)[c(zt, st)
t =t

 -  Ep(zt|at,st)[c(zt, st)] +  Ep(zt|st)[c(zt, st)]

Since p(zt|st) is reparametrizable, we obtain the estimator in Eq.(16).

D FURTHER RESULTS ON DISCRETE VARIATIONAL AUTOENCODERS

Dataset MNIST
Omniglot

Model
1 layer 2 Layer
1 layer 2 Layer

REBAR
-114.32 -101.20
-122.44 -115.83

RELAX
-113.62 -100.85
-122.11 -115.42

Table 3: Best obtained validation objective.

Dataset MNIST
Omniglot

Model
1 layer 2 Layer
1 layer 2 Layer

REBAR
857 900
2086 1027

RELAX
531 620
566 673

Table 4: Epochs needed to achieve REBAR's best validation score.

13

Under review as a conference paper at ICLR 2018
E EXPERIMENTAL DETAILS
E.1 DISCRETE VAE
In the one layer models we optimize the evidence lower bound (ELBO):
log p(x)  L() = Eq(b|x)[log p(x|b) + log p(b) - log q(b|x)]
where q(b1|x) = (x · Wq + q) and p(x|b1) = (b1 · Wp + p) with weight matrices Wq, Wp and bias vectors q, p. The parameters of the prior p(b) are also learned. We run all models for 2, 000, 000 iterations with a batch size of 24. For the REBAR models, we tested learning rates in {.005, .001, .0005, .0001, .00005}.
RELAX adds more hyperparameters. These are the depth of the neural network component of our control variate r, the weight decay placed on the network, and the scaling on the learning rate for the control variate. We tested neural network models with l layers of 200 units using the ReLU nonlinearity with l  {2, 4}. We trained the control variate with weight decay in {.001, .0001}. We trained the control variate with learning rate scaling in {1, 10}.
To limit the size of hyperparameter search for the RELAX models, we only test the best performing learning rate for the REBAR baseline and the next largest learning rate in our search set. In many cases, we found that RELAX allowed our model to converge at learning rates which made the REBAR estimators diverge. We believe further improvement could be achieved by tuning this parameter.
All presented results are from the models which achieve the highest ELBO on the validation data.
E.1.1 TWO LAYER MODEL
In the two layer models we optimize the ELBO
L() = Eq(b2|b1)q(b1|x)[log p(x|b1) + log p(b1|b2) + log p(b2) - log q(b1|x) - log q(b2|b1)]
where q(b1|x) = (x · Wq1 + q1 ), q(b2|b1) = (b1 · Wq2 + q2 ), p(x|b1) = (b1 · Wp1 + p1 ), and p(b1|b2) = (b2 ·Wp2 +p2 ) with weight matrices Wq1 , Wq2 , Wp1 , Wp2 and biases q1 , q2 , p1 , p2 . As in the one layer model, the prior p(b2) is also learned. We run an identical hyperpameter search in the 2 layer model as we do in the 1 layer model.
E.2 DISCRETE RL
In both the baseline A2C and RELAX models, the policy and control variate (value function in the baseline model) were 2 layer neural networks with 10 units per layer. The ReLU non linearity was used on all layers except for the output layer.
For these tasks we estimate the policy gradient with a single Monte Carlo sample. We run one episode of the environment to completion, compute the discounted rewards, and run one iteration of gradient decent. We believe using larger batches will improve performance but would less clearly demonstrate the potential of our method.
As our control variate does not have the same interpretation as the value function of A2C, it was not directly clear how to add reward bootstrapping and other variance reduction techniques common in RL into our model. We leave the task of incorporating these and other variance reduction techniques to future work.
Both models were trained with the RMSProp (Tieleman & Hinton, 2012) optimizer and a reward discount factor of .99 was used.
Both models have 2 hyperparameters to tune; the global learning rate and the scaling factor on the learning rate for the control variate (or value function). We complete a grid search for both parameters in {0.01, 0.003, 0.001} and present the model which "solves" the task in the fewest number of episodes averaged over 5 random seeds. "Solving" the tasks was defined by the creators of the OpenAI gym (Brockman et al., 2016). The Cart Pole task is considered solved if the agent receives an average reward greater than 195 over 100 consecutive episodes. The Lunar Lander task
14

Under review as a conference paper at ICLR 2018
is considered solved if the agent receives an average reward greater than 200 over 100 consecutive episodes. The Cart Pole experiments were run for 250,000 frames. The Lunar Lander experiments were run for 5,000,000 frames. E.3 CONTINUOUS RL The continuous tasks uses both the value function and the control variate to enable bootstrapping, which is needed due to the increased complexity of the problem. The three models- policy, value, and control variate, are 2 layer neural networks with 64 hidden units per layer. The value and control variate networks are identical, with the ELU(Djork-Arne´ Clevert & Hochreiter, 2016) nonlinearity in each hidden layer. The policy network has tanh nonlinearity. The policy network, which parameterizes the Gaussian policy comprises of a network (with the architecture mentioned above) that outputs the mean, and a separate, trainable log standard deviation value that is not input dependent. All three networks have a linear output layer. We selected the batch size to be 2500, meaning for a fixed timestep (2500) we collect multiple rollouts of a task and update the networks' parameters with the batch of episodes. Per one policy update, we optimize both the value and control variate network multiple times. The number of times we train the value network is fixed to 25, while for the control variate, it was chosen to be a hyperparameter. All models were trained using ADAM (Kingma & Ba, 2015), with 1 = 0.9, 2 = 0.999, and = 1e - 08. The baseline A2C case has 2 hyperparameters to tune: the learning rate for the optimizer for the policy and value network. A grid search was done over the set: {0.03, 0.003, 0.003}. RELAX has 4 hyperparameters to tune: 3 learning rates for the optimizer per network, and the number of training iterations of the control variate per policy gradient update. Due to the large number of hyperparameters, we restricted the size of the grid search set to {0.003, 0.0003} for the learning rates, and {10, 25, 50} for the control variate training iteration number. We chose the hyperparameter setting that yielded the shortest episode-to-completion time averaged over 5 random seeds. As with the discrete case, we used the definition of completion defined by OpenAI gym (Brockman et al., 2016) for each task. The Inverted Pendulum experiments were run for 1,000,000 frames. The Inverted Double Pendulum experiments were run for 50,000,000 frames.
15

