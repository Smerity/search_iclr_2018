Under review as a conference paper at ICLR 2018

Sparse Regularized Deep Neural Networks For Efficient Embedded Learning
Anonymous authors Paper under double-blind review

Abstract
Deep learning is becoming more widespread in its application due to its power in solving complex classification problems. However, deep learning models often require large memory and energy consumption, which may prevent them from being deployed effectively on embedded platforms, limiting their applications. This work addresses the problem by proposing methods Weight Reduction Quantisation for compressing the memory footprint of the models, including reducing the number of weights and the number of bits to store each weight. Beside, applying with sparsity-inducing regularization, our work focuses on speeding up stochastic variance reduced gradients (SVRG) optimization on non-convex problem. Our method that minibatch SVRG with 1 regularization on non-convex problem has faster and smoother convergence rates than SGD by using adaptive learning rates. Experimental evaluation of our approach uses MNIST and CIFAR-10 datasets on LeNet-300-100 and LeNet-5 models, showing our approach can reduce the memory requirements both in the convolutional and fully connected layers by up to 60× without affecting their test accuracy.

1 Introduction

Artificial intelligence is finding wider application across a number of domains where computational resources can vary from large data centres to mobile devices. However, state-ofthe-art techniques such as deep learning (LeCun et al., 2015) require significant resources, including large memory requirements and energy consumption. Reducing the size of the deep learning model to a compact model that has small memory footprint without compromising its performance is a desirable research aim to address the challenges for deploying these leading approaches on mobile devices. 1 regularization can be used as a penalty to train models to prevent the model from over-fitting the training data. As well as providing, 1 regularization is a powerful compression techniques to penalize some weights to be zero. As the results, our research focus on improving the method based on 1 regularization to reduce memory requirements. Moreover, as deep neural network optimization is a non-convex problem, the optimization can be stuck in local-minimal, which can reduce the performance. To address the problem, we improve SGD optimization for non-convex function to enhancing sparse representations obtained with 1 regularization. In this paper, we propose our compression method Weight Reduction Quantisation which reduces both the number of weights and bits-depth of model without sacrificing accuracy. To reduces the number of weights, our method employs sparsity-inducing 1 regularization to encourage many connections in both convolutional and fully connected layers to be zero during the training process. Formally, in this paper we consider the following unconstrained minimization problem, Given training labels y1, y2, ..., yN as correct outputs for input data x1, x2, ..., xN, the optimization problem to estimate the weights in all layers, W, is defined by

1 min WN

N

L(yi, f(xi; W)) + r(W),

i=1

(1)

where  is a hyper-parameter controlling the degree of regularization and the weights in all layers is given by W. The problem 1 can be strongly convex or possibly non-convex

1

Under review as a conference paper at ICLR 2018

(Allen-Zhu & Yuan, 2016). Following update rule, the mini-batch SGD method with 1 regularization is a popular approach for performing the optimization, and the weight update rule is given by



wjk+1

=

wjk

­

k

 wj

1 
B

B i=1

L(yi,

f

(xi;

W))

+

 M

M j=1

|wj| ,

(2)

where each weight of network can be represented by wj,the total number of weights is M. k is the iteration counter and k is the learning rate and B is mini-batch size (1 < B < N) used to approximate the full gradient. However, SGD optimization with 1 regularization has two
challenges: firstly, it inefficiently encourages weight to be zero due to fluctuations generated
by SGD (Tsuruoka et al., 2009). Secondly, SGD optimization slowing down convergence
rate due to the high variance of gradients. The two methods of cumulative 1 regularization
and SVRG can solve the two challenges respectively:

Cumulative 1 regularization Tsuruoka et al. (2009) proposed a method cumulating

the 1 penalties to resolve the problem. The method clips regularization at zero, which

avoids

the

derivative

 wj

Mj=1(

 M

|wj

|)

being

non-differentiable

when

wj

=

0

and

provides

a

more stable convergence for the weights. Moreover, the cumulative penalty can reduce the

weight to zero more quickly.

Mini-batch SVRG As SGD optimization has slow convergence asymptotically due to noise, Johnson & Zhang (2013) proposed SVRG that can efficiently decrease the noise of SGD by reducing the variance of gradients by:

wkj +1 = wjk ­ k

1 B

B
(i(wjk) ­ i(w~ j)) + µ~j

,

i=1

(3)

where µ~j is the average gradient of sub-optimal weights w~j which is the weight after every

m SGD iterations

µ~j

1 =
N

N i=1

L(yi, f(xi; W~ )) wj

(4)

1 =
N

N

i(w~j),

i=1

where W~ is the sub-optimal weights after m SGD iterations in all layers. For succinctness

we

also

write

i(wkj )

=

L(yi,f (xi wj

;W))

.

They determined that reduction of variance helps

initial weights w0 close to global minima at the beginning in order to boost the convergence

rate of SGD in strongly convex problems. Johnson & Zhang (2013) further prove that the

performance of SGD degrades with mini-batching by the theoretical result of complexity. Specifically, for batch size of B, SGD has a 1/ B dependence on the batch size. In contrast,

SVRG in a parallel setting has 1/B dependence on the batch size which is much better than

SGD. Hence, SVRG allows more efficient mini-batching. However, for non-strongly convex

problems, global minimization of non-convex function is NP-hard(Allen Zhu & Hazan, 2016).

Johnson & Zhang (2013) have a assumption that SVRG can also be applied in neural

networks to accelerate the local convergence rate of SGD. Further, Allen Zhu & Hazan

(2016) prove non-asymptotic rates of convergence of SVRG for non-convex optimization and

proposed improved SVRG that is provably faster than SGD. Hence, a promising approach

is to use mini-batch SVRG instead of SGD with cumulative 1 regularization.

Main Contributions We summarize our main contributions below:
1. Reducing memory requirements: 1.1 We analyse a method that combines SVRG with cumulative 1 regular-
ization to reduce the number of weights, and propose our method Delicate-SVRGcumulative- 1 which can significantly reduce the number of weights by up to 25×

2

Under review as a conference paper at ICLR 2018

without affecting their test accuracy. To our knowledge, ours is the first work that to combine mini-batch SVRG with cumulative 1 regularization for non-convex optimization.
1.2 To further reduce the memory requirements of models, we aim to reduces the number of bits to store each weight. Compression method Weight Reduction Quantisation, including both reducing number of weights and bit-depth, can reduce the memory footprints up to 60× without affecting accuracy.
2. Accelerating convergence rates:
2.1 We analyse non-convex stochastic variance reduced gradient (SVRG). Based on the results from (Reddi et al., 2016), we provide the condition when SVRG has faster rates of convergence than SGD.
2.2 We empirically show that modified SVRG in our method have faster rates of convergence than ordinary SVRG and SGD.

2 Related Works
Different methods have been proposed to remove redundancy in deep learning models. Sparse representation is a good approach to reduce the number of parameters. Han et al. mainly explored pruning which is a direct approach to remove small values of connection and focuses on the important connections with large weight values in all layers of the network. However, a disadvantage is that after pruning the needs networks to be retrained. One idea from matrix factorization can be applied to compressed parameters in models by finding a low rank approximation of the weight matrix Denton et al. (2014). However, in practice whilst it improves computation performance, it dose not significantly reduce memory requirements.
Weight sharing aims to approximate weights by a single weight. Chen et al. proposed HashedNets binning network connections into hash buckets uniformly at random by a hash function. As part of a three stage compression pipeline, Han et al. use k-means clustering to identify the shared weights for each layer of a trained network.
Weight quantization for reducing the bit-width to store each weight is an other approach to reduce memory requirements of models. Gysel et al. can successfully condense CaffeNet and SqueezeNet to 8 bits with only slight accuracy loss. Han et al. quantizes the sparse weights matrix to be an index which encodes in 8-bit for convolutional layers and 5-bit for fully connected layers. Rastegari et al. used binary operations to find the best approximations of the convolutions, in which the bit-size can be reduced to 1-bit.
Another type of approach uses regularization to induce sparsity. Hinton et al. proposed "dropout" that refers to dropping out neurons that are from visible and hidden layers in neural network during training, which can be shown to be a kind of regularization. Collins & Kohli applied 1 regularization and shrinkage operators in the training process. However, it only reduced the weights by only 4× with inferior accuracy. Tsuruoka et al. improved on this with 1 regularization with superior compression, but the methods use SGD and has slow asymptotical convergence due to the inherent variance Johnson & Zhang (2013).

3 Mini-batch Non-convex SVRG

For Problem 1, a stochastic iterative learning algorithm estimate a stationary point x and achieve -accuracy in finite iterations satisfying || f (x)||2  , which is termed of the -
accurate solution. For a non-convex problem, the goal is to find a reasonable local minimum.

However, the challenge is that gradients are easy to be stuck into saddle-point or a local

minimum. As a result, such an algorithm aims to help gradients escape from saddle-point or

local-minimal, e.g.(Ge et al., 2015) demonstrated that adding additional noise can help the

algorithm escape from saddle points. To our best knowledge, there is no theoretically proof

that can guarantee SVRG has faster rates of convergence than SGD. (Reddi et al., 2016)

compared the Incremental First-order Oracle (IFO) complexity Agarwal & Bottou (2015)

of SGD and SVRG on non-convex problem, O 1/2

and O

n

+

(n

2 3

/)

respectively. For

3

Under review as a conference paper at ICLR 2018

our analysis, whether non-convex SVRG can be efficiently close to reasonable optimal local

minimum depends on the number of training samples. Suppose fi is non-convex for i  [n]

and f has -bounded gradients, the IFO complexity of mini-batch SGD with a adaptive

learning rate is O

1/2

and for mini-batch SVRG with a fixed learning rate O

n

+

(n

2 3

/)

.

If the value of  is constant, the speed of convergence rates of SVRG depends on the number of training samples: when n is small, SVRG is faster than SGD for non-convex optimization and vice versa. Our experiment results showed in Figure1 and Figure5can support our view.

3.1 Mini-batch Non-convex SVRG on Sparse Representation

In our case, SVRG is applied on sparse representation. However, if directly combining minibatch non-convex SVRG with cumulative 1 regularization (called SVRG-cumulative- 1): let uk be the average value of the total 1 penalty given by

k

uk

=

 M

t.

t=1

(5)

At each training sample, weights that are used in current sample can be updated as

wjk+

1 2

=

wjk

­ k

1 B

B
(i(wkj ) ­ i(w~ j)) + µ~j

i=1

(6)

if

wjk+

1 2

>

0

then

wjk+1

=

max(0,

wjk+

1 2

­

(uk

+

qjk­1)),

else

if

wjk+

1 2

<

0

then

(7)

wjk+1

=

min(0,

wjk+

1 2

+ (uk

­ qkj ­1)),

where, qjk is the total difference of two weights between the SGD update and the 1 regularization update,

k

qjk =

(wjt+1

­

wjt+

1 2

),

(8)

t=1

where t is an index to calculate cumulative value of q, the algorithm has two problems: (1)

As we mentioned, SVRG on sparse representation cannot guarantee to be faster than SGD.

Figure 1 shows that for small dataset (e.g. MNIST) the convergence of SVRG is faster

than SGD but slower than SGD using a larger dataset (e.g. CIFAR-10), (2) The trade-off

Loss Loss

Loss per Epoch 1.2

1.2

Train(SGD-C-L1)

Train(SVRG-C-L1)

1

Test(SGD-C-L1)

1

Test(SVRG-C-L1)

0.8 0.8

0.6 0.6

0.4 0.4

0.2 0.2

0 5

0 10 15 20

Compression Rate=10%

Loss per Epoch
Train(SGD-C-L1) Train(SVRG-C-L1) Test(SGD-C-L1) Test(SVRG-C-L1)

1.6 1.4 1.2

1

0.8

0.6

0.4

0.2 5 10 15 20
Compression Rate=50%

Loss per Epoch Train(SGD-C-L1) Train(SVRG-C-L1) Test(SGD-C-L1) Test(SVRG-C-L1)
5 10 15 20 Compression Rate=90%

(a) MNIST dataset on LeNet-300-100 model

Loss per Epoch 2.5 2.5

Train(SGD-C-L1)

Train(SVRG-C-L1)

2

Test(SGD-C-L1) Test(SVRG-C-L1)

2

1.5 1.5

11

0.5 0.5

00 0 10 20 30 40 50 0 Compression Rate=10%

Loss per Epoch
Train(SGD-C-L1) Train(SVRG-C-L1) Test(SGD-C-L1) Test(SVRG-C-L1)

2.4 2.2
2

1.8

1.6

1.4

1.2

1

0.8 10 20 30 40 50 0

Compression Rate=50%

Loss per Epoch Train(SGD-C-L1) Train(SVRG-C-L1) Test(SGD-C-L1) Test(SVRG-C-L1)
10 20 30 40 50 Compression Rate=90%

(b) CIFAR-10 dataset on LeNet-5 model

Figure 1: With cumulative 1 regularization, we compare the convergence rates of SGD and SVRG. SVRG-cumulative- 1 has faster convergence rate in Figure 1(a). However, in Figure 1(b), SGD-cumulative- 1 can significantly converge into lower loss than SVRG-cumulative- 1 when compression rate equal 50% and 90%.

of SVRG-cumulative- 1 in the variance reduction versus the sparsity of the cumulative 1

regularization. After the variance of the gradient is reduced by SVRG in Equation 6, the ab-

solute value

of

the

updated

weight

wjk+

1 2

is higher

than

that

using SGD, which causes

SVRG

4

Under review as a conference paper at ICLR 2018

to have an adverse effect on the sparsity of 1-regularization. Compared to ordinary SVRG, (Reddi et al., 2016) proposed an extension of SVRG: MSVRG that introduces adapts the learning rate, which guarantee that their method has equal or better than SGD. Therefore, similar to the method MSVRG, our method provides separate adaptive learning rates for SVRG-cumulative- 1, which empirically demonstrates that it has faster convergence rate than SGD.

3.2 Delicate-SVRG-cumulative- 1
To reduce the number of weights, we introduce our compression method Delicate-SVRGcumulative- 1 that have two main improvements :

(1) Separate Adaptive Learning Rate Learning rates play an important rule in effecting the convergence rate of optimization during the training process which must be chosen carefully to ensure that the convergence rate is fast, but not too aggressive in which case the algorithms may become unstable. Reddi et al. believe that adaptive learning rates can be applied with reduced variance to provide faster convergence rates on nonconvex optimization. As a result, the convergence rate of the algorithms can be improved if the learning rate is adaptively updated. Our algorithm includes three parameters to provide greater fidelity in controlling the convergence of gradients for implementation of the 1 regularization.

Firstly, the learning rate k is chosen based on the learning rate from Collins et al. shown as,

k

=

1

+

0 (k/N)

(9)

where 0 is an initial learning rate with large value. Our experiments determined the parameters in three learning rates are over range of values, and a value of =0.6 as determined to be efficient. The learning rate schedule can emphasis the large distance between the gradient in the current optimization iteration and the sub-optimal solutions after every m iteration in the beginning, which avoids the current gradient being stuck in a local minimum at the start. It has a fast convergence rate to start with which decreases over time to minima local station.

The second learning rate, k, that reduces the variance of the SVRG-cumulative- 1 and better balances the trade-off in both of SVRG and cumulative 1 regularization. k is chosen such that k > k with slower convergence as

k

=

1

+

0 (k/N)q

,

(10)

here k=0.75, and the results of experiment is the best when q = 3 that can keep relatively large penalty of average gradients. During updating weight, it is efficient to prevent the absolute value of weight from being increased by SVRG, which can reduce the bad effect of 1 regularization, and sparsity.

We retain the same learning rate k for cumulative 1 regularization Tsuruoka et al. (2009) shown as,

k = 0k/N

(11)

The exponential decay ensures that the learning rates dose not drop too fast at the beginning and too slowly at the end.

(2) Bias-based Pruning To further reduce the number of weights, we add a bias-based pruning ~b after the 1 regularization in each iteration. The pruning rule is based on following heuristic Fonseca & Fleming (1995): connections (weights) in each layer will be removed if their value is smaller than the network's minimal bias. If the absolute value of weight connections are smaller than the absolute value of the smallest bias of the entire network in each batch, these connections have least contribution to the node, which can be removed. In practice, bias-based pruning has no effect on train and test loss.

5

Under review as a conference paper at ICLR 2018

Consequently, Delicate-SVRG-cumulative- 1 that incorporates the adaptive learning rate schedules and bias-based pruning as,

wjk+

1 2

=wkj ­

N

k N

(i(wjk) ­ i(w~ j)) + kµ~j

i=1

if

wjk+

1 2

>0

then

wkj +1

=

max(0,

wjk+

1 2

­ (uk

+ qjk­1

+ ~b)),

else

if

wkj +

1 2

<

0

then

wjk+1

=

min(0,

wjk+

1 2

+ (uk

­ qkj ­1

­ ~b)).

(12)

The pseudo code of our method is illustrated as Algorithm 1 in the Appendix.

4 Weight Quantization for Bit-depth Reduction
To further compress the model, weight quantization can significantly reduce memory requirement by reducing bit precision. We quantize to 3-bit after reducing by DelicateSVRG-cumulative- 1 for convolutional layers and encode 5 bits for fully connected layers. Consequently, we propose our final compression method Weight Reduction Quantisation.

Table 1: Comparison of the compression results of the pruning method from Han et al. (2016) and our method in each layer. Using MNIST dataset train and test on LeNet-300-100 1(a) and LeNet-5 model1(b). D is Delicate-SVRG-cumulative- 1 and Q is weight quantization.
(a) MNIST dataset with LeNet-300-100 model.

Layer Original network #Weights Memory Compress rate Compress rate Deep compression

(D) (D+Q)

(D)

(D+Q) Han et al. (2016)

Compress rate

ip1 235K(940KB) 8.0K 14.36KB 3%

1.63%

2.32%

ip2 30K(120KB) 2.5K 3.392KB 8.3%

2.82%

3.04%

ip3

1K(4KB)

0.3K 0.308KB 30%

7.7%

12.70%

Total 266K(1070KB) 10.8K 18.06KB 4%(25×) 1.68%(60×) 2.49%(40×)

Top-1 Error

1.64%

-

-

1.58%

1.57%

1.58%

(b) MNIST dataset with LeNet-5 model.

Layer Original network D-SVRG-C-L1 Memory Compress rate Compress rate Deep compression

(D) (D+Q) (D)

(D+Q) Han et al. (2016)

Compress rate

conv1

0.5K(2KB)

0.33K 1.16KB

78%

58%

67.85%

conv2 25K(100KB)

3K

2.42KB

12%

2.42%

5.28%

ip1 400K(1600KB)

32K

24KB

3.7%

1.5%

2.45%

ip2 5K(40KB) 0.95K 2.112KB 17%

5.28%

6.13%

Total 431K(1720KB)

35K

30KB 4.5%(22×) 1.8%(57×) 2.55%(39×)

Top-1 Error

0.80%

-

-

0.74%

0.737%

0.74%

5 Experiments
In order to estimate and compare the effect of our compression method on different topologies, e.g. fully connected networks and convolutional networks, we select deep neural networks (DNNs) and convolutional neural networks (CNNs). The DNN chosen is LeNet-300100 which has two fully connected layers as hidden layers with 300 and 100 neurons respectively. The CNN chosen is LeNet-5 which has two convolutional layers and two fully connected layers. We evaluate the performance of our new compression method using MNIST,
6

Under review as a conference paper at ICLR 2018

and CIFAR as benchmarks. MNIST (LeCun et al., 2001) is a set of handwritten digits which is a commonly used dataset in machine learning. It has 60,000 training examples and 10,000 test samples. Each image is grey-scale with 28×28 pixels. CIFAR-10 is a dataset that has 10 classes with 5,000 training images and 1,000 test images in each class. In total, it contains 50,000 training images and 10,000 test images with 32×32 pixels. CIFAR-10 images are RGB. Two types of error rate are used to measure the performance of models, which are top-1 and top-5 error rate. Here, we consider top-1 error on MNIST, while top-5 error on CIFAR-10 because many images in CIFAR are small and ambiguous. Our compression method was implemented using Caffe1.

5.1 Comparison with leading results
Applying Weight Reduction Quantisation to the MNIST dataset, we choose the results with the best combination of compression and error rate for comparison. Our method can reduce 98% of the memory requirements with a 1.57% test error rate on the LeNet-300100 model and 98% of the parameters with a 0.74% test error on the LeNet-5 model. In Table 1, the compression pipeline is summarised with weight statistics in comparison to the method from Han et al. (2016). In our first stage Delicate-SVRG-cumulative- 1 that focus on reducing the number of weights, we compare the results of pruning method from Han et al. (2016) that is the first stage of their compression method. The two tables show that both Delicate-SVRG-cumulative- 1 and Han et al. pruning method can significantly remove many weights in the fully connected layers. For LeNet-300-100 models, the number of weights in the first fully connected layers (ip1) contains about 88% of the total number of weights and this can be compressed by 97% by Delicate-SVRG-cumulative- 1. Furthermore, both Delicate-SVRG-cumulative- 1 and pruning method have very similar compression rate in convolutional layers (conv1 and conv2) in reducing the number of weights in LeNet-5 model, but Delicate-SVRG-cumulative- 1 is more effective to reduce the number of weights of the two fully connected layers (ip1 and ip2) sparse. Both Delicate-SVRG-cumulative- 1 and Han et al. pruning method can achieve lower test error than that of uncompressed models, whilst delivering overall compression rates up to 25× and 12× respectively. The

Test Error Test Error

0.03 0.028 0.026 0.024 0.022
0.02
0.018
0.016

10-3

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(without BiasPruning) SGD-C-L1

9.5 9

SVRG-Cumulative-L1 D-SVRG-C-L1 D-SVRG-C-L1(without BiasPruning) SGD-Cumulative-L1

8.5

Deep compression (92%, 0.0158)

104 # Weights

105

8 Deep compression
(92%, 0.0074)

7.5

0.5 1 1.5 2 2.5 3 3.5 4

# Weights

105

0.14

SVRG-C-L1

0.14

0.13 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) 0.12

SGD-C-L1 0.12

0.1

0.11
0.08 0.1

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1

0.09 0.06 0.08

0.07

0.04 0 2 4 6 8 10 0 1 2 3 4 5

# Weights

105

# Weights

105

(a) MNIST dataset on LeNet-300-100 model (b) CIFAR-10 dataset on LeNet-300-100 model (left) and LeNet-5 (right): error rate is top-1 er- (left) and LeNet-5 (right): error rate is top-5 error ror

Figure 2: Four 1 regularization compression methods experiment on two deep learning models, including LeNet-300-100 and LeNet-5 using MNIST datasets and CIFAR-10.

second stage is to further compress the model by bit-depth reduction, and Table1 shows our method Weight Reduction Quantisation that combines Delicate-SVRG-cumulative- 1 with bit-depth reduction can achieve 1.56% error rate on MNIST, and 0.737% error rate on CIFAR-10, where the two errors are all lower than that of original model. The compression rates are up to 60× in LeNet-300-100 and up to 57× in LeNet-5 model.

1Caffe is a deep learning framework. http://caffe.berkeleyvision.org
7

Source code can be download:

Under review as a conference paper at ICLR 2018

5.2 Evaluation the Trade-off Between Memory Requirements and Performance
Focusing on Delicate-SVRG-cumulative- 1 to examine the performance of method at different compression rates controlled by threshold , we compare the performance of different model-compression based on 1 regularization over the range of memory requirements. Figure 2 shows how the test error rate and weight sparsity vary as the regularization parameter  is adjusted. Where pareto fronts are not available for comparison, we compare with a single trade-off and determine the related performance by the side of the pareto front that the point lies.
LeNet on MNIST Figure 2(a) shows LeNet on MNIST. Compared with SVRGcumulative- 1, SGD-cumulative- 1 has the better ability of compression, but the error rate is higher due to the variance generated by SGD optimiser. Replacing SGD with SVRG, SVRG-cumulative- 1 reduces the test error but the compression ability is also reduced. The Delicate-SVRG-cumulative- 1 method has the least number of weights and the best performance having the lowest test error for almost every compression value. Its performance is similar with the method without bias-based pruning, which means that adding bias-based pruning can further reduce the number of weights without side-effect on the performance. The pink box on 2(a) showed that the results within the box is better than pink point.
LeNet on CIFAR-10 Figure 2(b) shows LeNet on CIFAR-10 dataset that is a larger and more complicated dataset than MNIST. SVRG-cumulative- 1 has chances to achieve lower test error than SGD-cumulative 1 but can not guarantee that the performance is always better than SGD-cumulative- 1. Delicate-SVRG-cumulative- 1 method has better performance than the other methods. Its performance is further enhanced by adding biasbased pruning. Consequently,Delicate-SVRG-cumulative- 1 can be effectively applied in LeNet-300-100 and LeNet-5 models without accuracy loss when applied to MNIST and CIFAR-10.
5.3 Combining Delicate-SVRG-cumulative- 1 and Weight Quantization
Figure3 shows the test error at different compression rates for Delicate-SVRG-cumulative- 1 and weight Quantization. Individually, weight quantization can reduce more memory before the test error increases significantly in Delicate-SVRG-cumulative- 1 using MNIST dataset, but the reverse results applied on CIFAR-10 dataset. However, if combining together, the approach consistently outperforms.

Test Error Test Error

0.0175 0.017
0.0165 0.016
0.0155 1%

MNIST ON LeNet-300-100

D Q D+Q

11 10-3 10.5

10

9.5

9

8.5

8

7.5

3% 5% 9%

20% 35% 60% 95%

Compression Rate

7

1%

MINIST ON LeNet-5

D Q D+Q

3% 5%

20% 40% 80%

Compression Rate

0.065 0.064 0.063 0.062 0.061
0.06 0.059 0.058 0.057 0.056

CIFAR-10 ON LeNet-300-100

D Q D+Q

0.03

0.028

0.026

0.024

0.022

CIFAR-10 ON LeNet-5

D Q D+Q

5% 7% 10%

30%

Compression Rate

50% 70%

0.02

0.6% 1%

3% 10% 20% 35% 65% Compression Rate

(a) MNIST dataset on LeNet-300-100 model (b) CIFAR-10 dataset on LeNet-300-100 model

(left) and LeNet-5 (right).

(left) and LeNet-5 (right).

Figure 3: The test error with compression rate under different compression methods. D is Delicate-SVRG-cumulative- 1, Q is weight quantization. Combining Delicate-SVRGcumulative- 1 with weight quantization can achieve the best performance.

8

Under review as a conference paper at ICLR 2018
5.4 Comparison of Convergence Rates
To confirm the theoretical insights that our method has no bad effect on convergence rate to achieve similar fast convergence with SGD-cumulative- 1 or SVRG-cumulative- 1, we calculate the training loss of two LeNet models on MNIST and CIFAR datasets during increasing iterations. In Figure 4(a), all methods have similar convergence rates in LeNet300-100. In all of our experiments, Delicate-SVRG-cumulative- 1 has same or lower training loss and faster convergence rate than other methods, meaning that adaptive learning rate can help SVRG with cumulative 1 regularization to escape the local minimum in the beginning and quickly converge to a good local minimum within finite training iterations. Moreover, Delicate-SVRG-cumulative- 1 without bias-based pruning has a similar train loss, which illustrates that adding bias-based pruning in 1 regularization has no obvious bad effect on the convergence of weights. Consequently, applying adaptive learning rates, DelicateSVRG-cumulative- 1 is a efficient compression method for neural network problems.
6 Discussion
In this paper, we proposed Weight Reduction Quantisation that efficiently compressed neural networks without scarifying accuracy. Our method has two stages that reduce the number of weights and reduce the number of bits to store each weight. We show that SVRG and cumulative 1 regularization can improve over SGD and 1-regularization. By combining them, we have presented a new compression method Delicate-SVRG-cumulative- 1 that can efficiently reduce the number of parameters by the separate adaptive learning rates. The three adaptive learning rates are applied on SVRG and cumulative 1 penalty, which provides a high accuracy and reduced number of weights. Besides, our method improved SVRG that can be used on non-convex problem with fast convergence rate. In our experiments on LeNet-300-100 and LeNet-5, our method can significantly reduce the memory requirements up to 60× without accuracy loss. After compression by our method, a compact deep neural network can be efficiently deployed on an embedded device with performance of the original model.
References
Alekh Agarwal and Leon Bottou. A lower bound for the optimization of finite sums. In David Blei and Francis Bach (eds.), Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 78­86. JMLR Workshop and Conference Proceedings, 2015. URL http://jmlr.org/proceedings/papers/v37/agarwal15.pdf.
Zeyuan Allen Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 699­707, 2016. URL http: //jmlr.org/proceedings/papers/v48/allen-zhua16.html.
Zeyuan Allen-Zhu and Yang Yuan. Improved svrg for non-strongly-convex or sum-of-nonconvex objectives. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML'16, pp. 1080­1089. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id=3045390.3045505.
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. CoRR, abs/1504.04788, 2015. URL http://arxiv.org/abs/1504.04788.
Maxwell D. Collins and Pushmeet Kohli. Memory bounded deep convolutional networks. CoRR, abs/1412.1442, 2014. URL http://arxiv.org/abs/1412.1442.
Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, and Peter L. Bartlett. Exponentiated gradient algorithms for conditional random fields and max-margin markov networks. Journal of Machine Learning Research, 9:1775­1822, 2008. URL http: //jmlr.csail.mit.edu/papers/v9/collins08a.html.
9

Under review as a conference paper at ICLR 2018
Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. CoRR, abs/1404.0736, 2014. URL http://arxiv.org/abs/1404.0736.
Carlos M. Fonseca and Peter J. Fleming. An overview of evolutionary algorithms in multiobjective optimization. Evol. Comput., 3(1):1­16, March 1995. ISSN 1063-6560. doi: 10.1162/evco.1995.3.1.1. URL http://dx.doi.org/10.1162/evco.1995.3.1.1.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic gradient for tensor decomposition. CoRR, abs/1503.02101, 2015. URL http: //arxiv.org/abs/1503.02101.
Philipp Gysel, Mohammad Motamedi, and Soheil Ghiasi. Hardware-oriented approximation of convolutional neural networks. CoRR, abs/1604.03168, 2016. URL http: //arxiv.org/abs/1604.03168.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. International Conference on Learning Representations (ICLR), 2016.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012. URL http://arxiv.org/abs/1207.0580.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 26, pp. 315­323. Curran Associates, Inc., 2013.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In Intelligent Signal Processing, pp. 306­351. IEEE Press, 2001.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553): 436­444, 05 2015. URL http://dx.doi.org/10.1038/nature14539.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. CoRR, abs/1603.05279, 2016a. URL http://arxiv.org/abs/1603.05279.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. CoRR, abs/1603.05279, 2016b. URL http://arxiv.org/abs/1603.05279.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabás Póczós, and Alex Smola. Stochastic variance reduction for nonconvex optimization. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML'16, pp. 314­323. JMLR.org, 2016. URL http://dl.acm.org/citation.cfm?id= 3045390.3045425.
Yoshimasa Tsuruoka, Jun'ichi Tsujii, and Sophia Ananiadou. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL '09, pp. 477­485, Stroudsburg, PA, USA, 2009. Association for Computational Linguistics. ISBN 978-1-932432-45-9. URL http://dl.acm.org/citation.cfm? id=1687878.1687946.
10

Under review as a conference paper at ICLR 2018

Algorithm 1 Delicate-SVRG-cumulative- 1: Stochastic descent training with cumulative 1 penalty

procedure Train( )

u0

µ~  0

Initial wj and qj with zero for all number of weights M

for k=0 to Maximal Iterations do





0 1 + (k/N)





0 1 + (k/N)3

for t=0 to k do

  0t/N end for

u  u + /M

end for

for j  features used in sample i do

randomly select m features from train samples

wj  wj ­

k N

N i=1

(i

(wj

)

­

i

(w~ j

))

+

k

µ~

i(w~ j) = i(wj)

if wj and w~ j converge to the same weights then

µ~ = 0

end if

µ~



µ~

+

1 N

i(w~ )

end for

end procedure

procedure Apply Penalty(i)

z  wj ~b is minimal bias in all layers.

if wj > 0 then wj  max(0, wj ­ (u + qj + ~b)),
else

if wj < 0 then wj  min(0, wj + (u ­ qj ­ ~b)),
end if

end if

qj  qj + (wj ­ z) end procedure

7 Appendix
7.1 The algorithm of Delicate-SVRG-cumulative- 1
7.2 Comparison of the convergence rates of between our method and SVRG and SGD when combining with 1 regularization.
The results showed in Figure4.
7.3 Using multiple initializations to compare the performance of our method and other three methods.
The experiments were run with multiple initializations and there was some small variability in the results. However, the relative performance of the our method is always better than SVRG and SGD combining with cumulative 1 regularization. The results showed in Figure5

11

Under review as a conference paper at ICLR 2018

Train Loss (Log)

101 Loss per Epoch
SGD-C-L1

1.7

SVRG-C-L1

D-SVRG-C-L1(wo BiasPruning) 1.6
D-SVRG-C-L1

100 1.5

1.4

10-1 1.3 1.2

Loss per Epoch

SGD-C-L1 SVRG-C-L1

1.7

D-SVRG-C-L1(wo BiasPruning)

D-SVRG-C-L1

0.9

Loss per Epoch
SGD-C-L1 SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) D-SVRG-C-L1

10-2

1.1 10 20 30 40 50
# Epoch

10 20 30 40 # Epoch

0.1 50

10 20 30 40 # Epoch

50

(a) The train loss: MNIST dataset on LeNet-5 (left) and CIFAR-10 dataset on LeNet-300-100 (middle) and LeNet-5 (right)

Test Loss (Log)

0.1 0.08 0.06

Loss per Epoch
SGD-C-L1 SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) D-SVRG-C-L1

1.7 1.6

1.5

Loss per Epoch

SGD-C-L1 SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) D-SVRG-C-L1

2

1.5

0.04

1.4 1.3

1

Loss per Epoch
SGD-C-L1 SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) D-SVRG-C-L1

0.02 0

10 20 30 40 50 0 # Epoch

20 40 50 0 # Epoch

20 # Epoch 40 50

(b) The test loss: MNIST dataset on LeNet-5 (left) and CIFAR-10 dataset on LeNet-300-100 (middle) and LeNet-5 (right)

Figure 4: Estimate the convergence rate when using four compression methods, including our method Delicate-SVRG-cumulative- 1, Delicate-SVRG-cumulative- 1 (without BiasPruning) that without bias-based pruning in 1 regularization,SVRG-cumulative- 1 and SGD-cumulative- 1, on LeNet-300-100 and LeNet-5 models with MNIST and CIFAR-10 datasets. Here we choose the compression rate that equal 90% to observe training and test loss. For MNIST dataset, we did not notice subtle difference train and test loss on LeNet-300-100 model generated by four methods.

12

Under review as a conference paper at ICLR 2018

Test Error

Test Error

0.05 0.045
0.04 0.035
0.03
0.025

Initial weights 1
SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1

0.04 0.035

0.03

0.025

0.02 0.02

104 105 #Weights

104

Initial weights 2
SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1

0.03 0.028

0.026

0.024

0.022

0.02

105 #Weights

0.018 104

Initial weights 3
SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1
105 #Weights

0.019 0.017 0.016 0.015 0.014
0.13 0.012 0.011
0.01
0.009
0.008 102

(a) MNIST dataset on LeNet-300-100

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1

0.017 0.016 0.015 0.014 0.013 0.012
0.011

0.01

104 #Weights

0.009 0.008

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1
5 #Weights

0.018 0.017 0.016 0.015 0.014 0.013 0.012
0.011
0.01
0.009
10 15 104

(b) MNIST dataset on LeNet-5

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1

#Weights

105

13

Under review as a conference paper at ICLR 2018

Test Error

0.078 0.076 0.074 0.072
0.07 0.068 0.066 0.064 0.062
0.06
0.5 0.4 0.3 0.25 0.19 0.17 0.15 0.13 0.1 0.09 0.07 0.05
0.03

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1

0.082 0.08
0.078 0.076

0.074

0.072

0.07

0.068

0.066

0.064

105 #Weights

0.062 106

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1

0.08 0.078 0.076

0.074

0.072

0.07

0.068

0.066

0.064

105 #Weights

0.062 106

(c) CIFAR-10 dataset on LeNet-300-100

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1

105#Weights

106

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1

0.3 0.25
0.19 0.17 0.15 0.13

0.1 0.09

0.07

0.05

104 #Weights

0.03

SVRG-C-L1

0.4

D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) 0.3

SGD-C-L1

0.25

0.19 0.17 0.15 0.13

0.1

0.07

0.05

104 #Weights

0.03

SVRG-C-L1 D-SVRG-C-L1 D-SVRG-C-L1(wo BiasPruning) SGD-C-L1
102 104 #Weights

Test Error

(d) CIFAR-10 dataset on LeNet-5
Figure 5: Using three types of initial weights, we compare our method with other three methods. D-SVRG-C-L1 and D-SVRG-C-L1(wo BiasPruning) are always better than other two methods. This experiment also can verify the our view that the performance of SVRG is better or worse than SGD that depends on the number of training samples. In our experiment, if choosing small dataset (e.g. MNIST), SVRG is better than SGD. Otherwise, if choosing relatively large dataset (e.g. CIFAR-10), SVRG is worse than SGD.

14

