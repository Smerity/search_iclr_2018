Under review as a conference paper at ICLR 2018

3C-GAN: AN CONDITION-CONTEXT-COMPOSITE GENERATIVE ADVERSARIAL NETWORKS FOR GEN-
ERATING IMAGES SEPARATELY

Yeu-Chern Harn Department of Computer Science University of North Carolina at Chapel Hill Chapel Hill, NC 27516, USA ycharn@cs.unc.edu

Vladimir Jojic Department of Computer Science University of North Carolina at Chapel Hill now at Calico South San Francisco, CA 94080, USA vjojic@gmail.com

ABSTRACT
We present 3C-GAN: a novel multiple generators structures, that contains one conditional generator that generates a semantic part of an image conditional on its input label, and one context generator generates the rest of an image. Compared to original GAN model, this model has multiple generators and gives control over what its generators should generate. Unlike previous multi-generator models use a subsequent generation process, that one layer is generated given the previous layer, our model uses a process of generating different part of the images together. This way the model contains fewer parameters and the generation speed is faster. Specifically, the model leverages the label information to separate the object from the image correctly. Since the model conditional on the label information does not restrict to generate other parts of an image, we proposed a cost function that encourages the model to generate only the succinct part of an image in terms of label discrimination. We also found an exclusive prior on the mask of the model help separate the object. The experiments on MNIST, SVHN, and CelebA datasets show 3C-GAN can generate different objects with different generators simultaneously, according to the labels given to each generator.

1 INTRODUCTION
Recently Generative adversarial network (GAN) is gaining much attention because it has shown promising results for generating natural images (Goodfellow et al. (2014); Goodfellow (2016)). Still, it is limited in generating globally coherent, or high-resolution images. Therefore, to have GAN generate better quality images is the main research topic. There have been many works tackling this topic, including using novel network structures, (Radford et al. (2015)), Denton et al. (2015)), using novel objective functions for training ( Zhao et al. (2016), Arjovsky et al. (2017), Gulrajani et al. (2017)), using multi-stage network generation (Im et al. (2016), Kwak & Zhang (2016), Yang et al. (2017)), applying 3D structure information for image generation( Wang & Gupta (2016)).
Another main research avenue of GAN is to make a generator more controllable, that is, the generator can generate the image given the criterion we want. Conditional GAN serves this purpose by ensuring the generation is conditional on the given criterion (Odena et al. (2016), Mansimov et al. (2015), Reed et al. (2016a), Kaneko et al. (2017), Zhu et al. (2016)). Specifically, in these conditional models, the generator not only accepts the noise code, but also a conditional code, either from a one-hot representation of class labels, or a sentence embedding from a text, or an image, such that this given side information encodes the criterion, and the generated images are conditional on that. Aside from the conditional model, that is a supervised approach, there are models employed an unsupervised approach. InfoGAN (Chen et al. (2016)) does not need any labels: it discovered the high-level meaning of the image purely from data, and assign the meanings to its noise code. For example, for MNIST dataset, InfoGAN model learn to have one code control the width of the generated digits, and one code controls the rotation of the digits. As an unsupervised approach, InfoGAN discovered the criteria that we might think is useful for the dataset.
1

Under review as a conference paper at ICLR 2018

Both the supervised and unsupervised models are one-generator approach, there is another multigenerators approach that also aims to make the model more controllable (Kwak & Zhang (2016) Yang et al. (2017)). They generate images part-by-part, such that each part has its semantic meaning, and combing all parts results in a realistic image. Since a part of an image has some persistent characteristics, these model learn which parts of the data should be generated together in an unsupervised way. In addition, the model of Yang et al. (2017) includes a transformer network that learns an affine transformation for the foreground in an image. Since each part of an image is generated by different generators, these model provide a high-level meaning of the generation for each generator and possess a finer control over original GAN.
We develop a model that also aims to improve the controllability over original GAN. Specifically, our model is a multi-generators approach, where one generator is a "context" generator that does not take any conditional labels as input, and another is a conditional generator that takes its label information. The conditional generator generates the part of the image that is related to its labels, whereas the context generator generates the other part of the image. Both generators learn where to generate (a mask), and what to generate (an appearance). The context generator shared its input codes with the conditional generator so it "knows" what context it is on when generating its own part. However, there is no restriction for the conditional generator to not generate the context. We propose a cost function on the label discrimination to penalize the conditional generator to do so. In addition, we proposed an exclusive prior on the mask such that any pixels in an image should be generated from one generator only. All in all, our contributions can be listed as follows.
1. We are the first one showing that by applying label information, the model and the cost function we proposed, we can have a generator to learn to generate the part of the image only related to the label.
2. We show that a simpler multi-generators structure, without using Recurrent Neural Net to generate different part of the image subsequently, works to generate images part-by-part simultaneously.

2 METHOD

The algorithm we proposed is based on Wasserstein GAN, conditional GAN, and model an image with a layered structure, we introduce them in the background section.

2.1 BACKGROUND

2.1.1 WASSERSTEIN GAN

A generative adversarial network (GAN) consists of two neural networks trained simultaneously and in opposition to one another. Assume a real sample x follows an unknown distribution pdata. The generator network G takes as input a random code vector z and output the fake data xf , such that xf = G(z), whereas the discriminator network D takes as input either a training sample or a synthesized sample from G and outputs a probability that the input is real or fake. The discrimina-
tor is trained to maximize the probability of assigning the correct source to both training samples
and samples from G, where the generator is trained to minimize the probability that D assigns its
generated sample as fake. The objective function is

min max
G D

Exrpdata(xr)[log D(x; D)] + Ezpz(z)[log(1 - D(G(z; G); D))]

, where D and G are parameterized by D and G respectively.

(1)

Compared to original GAN, Wasserstein GAN is more stable and resilient to hyper-parameters changes (Arjovsky et al. (2017)). Another advantage over original GAN is the cost function of it decreases steadily among training. Wasserstein GAN also has two networks D and G that are trained simultaneously and adversarially. The main difference is instead outputting a probability, D outputs a real value. The objective function becomes:

min max
G D

Exrpdata(xr)[D(xr; D)] - Ezpz(z)[D(G(z; G); D)]

(2)

The difference between the first and the second term is the estimated Wasserstein distance (W ). While D maximizes the distance, G minimizes it. Importantly, D needs to be a 1-Lipschitz function

2

Under review as a conference paper at ICLR 2018

so that the estimation is accurate. A penalty term involves the first-order derivative of D is added to the objective function to encourage this requirement. Readers can find the details of it in Gulrajani et al. (2017).

2.1.2 CONDITIONAL GAN

Conditional GAN is an extension of GAN that makes it conditional on the prior information of
the data, and here we focus on a type of the conditional GAN that is conditional on image labels
or attributes. More specifically, we applied the conditional GAN model proposed in Odena et al. (2016) (AC-GAN). Here we briefly introduce this model. Assume a real sample x and its label lx follows an unknown distribution Pdata. The label is represented as a one-hot representation. The generator G takes as input a random code vector zu concatenated with a one-hot representation of a random label zl to generate a fake sample, such that zc = [zu, zl], xf = G(zc). To encourage G to generate a sample that is conditional on zl, we have an auxiliary classifier C, such that D and C share all layers except the last layer. While D outputs the Wasserstein distance mentioned above, C takes the input sample and its label, and outputs a cross-entropy loss between the label and the
probability distribution over the class labels. The objective function becomes:

min max min
G D C

Expdata(x)[D(x; D)] - Ezcpzc (zc)[D(G(zc; G); D)]

+

Ex,lxpdata(x,lx)[- log C(lx|x; C )] + Ezu,zlpzc (zu,zl)[- log C(zl|G(zc; G); C )] (3)

Where the first two terms correspond to the Wasserstein distance in equation 2, the third term corresponds to the cross-entropy loss for a real image (Lrc), and the fourth term corresponds to the cross-entropy loss for the fake image (Lfc ).

2.1.3 LAYERED-STRUCTURE MODELING FOR IMAGES

Since an image is taken from our 3D world, it usually contains a layered structure. Modeling an image as with a mask is common. An example of that is a two-layered foreground/background modeling,
x = f m + b (1 - m)
, where f is the foreground image, b is the background, m is the mask for the foreground, and is element-wise multiplication. This modeling has already been applied in synthesize natural images (Isola & Liu (2013), Reed et al. (2016b), Yan et al. (2016)). This modeling has also been applied to videos (Darrell & Pentland (1991), Wang & Adelson (1994), Jojic & Frey (2001), Kannan et al. (2005), Vondrick et al. (2016).
Recently there are two GAN models also applied the foreground/background layer setting Kwak & Zhang (2016), Yang et al. (2017), the generation process for them is the second layer (foreground) is generated subsequently after the first layer (background). In contrast, we model an image as a context layer and a conditional layer and generate the two layers together.

2.2 CONDTION-CONTEXT-COMPOSITE GAN (3C-GAN)
2.2.1 MODEL DEFINITION
Our model is based on the conditional GAN and Wasserstein GAN. It has one Discriminator D, one auxiliary classifier C, but two generators G1 and G2. G1 is an ordinary generator, but G2 is a conditional generator. While G1 generates the context of an image, G2 generates the part of an image that is conditional on its input label. The final output, (Xf ), is the composite image from G1 and G2. The model definition is shown below.
m1, f1 = G1(zu, zl) m2, f2 = G2(zu, zv, zl) mn1 , mn2 = softmax(m1, m2)
o1 = f1 m1n o2 = f2 m2n xf = Gc(zu, zv, zl) = o1 + o2

3

Under review as a conference paper at ICLR 2018

Figure 1: The architecture of 3C-GAN. The  stands for softmax normalization, and the stands for element-wise multiplication. Note that classifier network C is reused three times for different input and label pairs.

Figure 1 shows the architecture of 3C-GAN. A part of input noise codes (zu) that is for context generation. zu is shared for both G1 and G2 because G2 also needs to know the context to generate. The same zu for G1 and G2 inputs enable them to "communicate" with each other. At the same time, G2 takes additional input codes (zv) and label codes (zl). zv let G2 has additional power to control over the conditional part.

We denote Gc as the composite model of G1 and G2. Each generator generates a mask (m1, m2) and an appearance (f1, f2). The masks generated from G1 and G2 are further normalized by softmax. Each appearance element-wisely multiplies by each mask and summed together. This way the
mask controls to generate which part of an image. The composite fake image xf is then given to discriminator and the optimization is as same as equation 3.

There is an auxiliary classifier C helps the model conditional on its label. As AC-GAN, Both xr
and its label lx pair, and xf , zl pair are given to C to compute the classification loss. The losses
for two pairs are the (Lrc) and (Lcf ) in equation 3, respectively. The loess encourage Gc as a whole to be conditional on the label; However, they do not encourage G2 to be a conditional generator.
Intuitively, if G2 generates the conditional part, G2 should have to be discriminated as the same label as Gc; therefore, we proposed an additional classification loss (Lcp) for the input pair o2, zl, given the same classifier C. That is:

Lpc = Ezu,zv,zl [- log C(zl|G2(zu, zv, zl))]

(4)

In practice, there is no restriction on the structure of D and G. To have good generating quality and fast convergence, we applied the structure of DCGAN (Radford et al. (2015)) . To save computational power, G1 and G2 can share all the layers except the first(bottom) layer, that is a fully-connected layer computing the 4*4 feature map from the input code.
Besides the basic GAN cost and conditional cost (classifier loss), there are two additional costs that are essential for our purpose.

2.2.2 LABEL DIFFERENCE COST
In the model defined above, G2 is not restricted to generate the unconditional part of an image, that is, G2 could take the job G1 does. In the extreme case, G2 could generate all images, that is, mn1 is zero everywhere. Here we proposed a cost to penalize G2 such that it generates only the succinct

4

Under review as a conference paper at ICLR 2018

part when changing the condition of an image.

Lld = Ezu,zv,zl

|Gc(zu, zv, zl) - Gc(zu, zv, zlf )|

(5)

, where zfl (label flip) is any label code that is different from zl, and the summation is over all pixels in an image. Since only G2 is accessible to the condition information, G2 is forced to generate the succinct part of condition changes when we apply this cost.

2.2.3 EXCLUSIVE PRIOR

In the above model, except that m1 + m2 = 1, we do not have any prior on these masks. In practice, if one pixel is generated by a generator, we want it to take a full responsibility for generating it, that

is, the value of its mask for that pixel should be close to 1. Here we proposed an exclusive prior on

the two masks.

Lex = Ezu,zv,zl

|m1 m2|

(6)

, where the summation is over all pixels in an image. We found adding this prior makes G1 and G2 generate a part of an image separately.

2.2.4 OBJECTIVE FUNCTION

The overall loss function for 3C-GAN is:

Lall = W + Lcr + Lfc + Lcp + Lld + Lex

(7)

The definition of W, Lcr, Lcf is the same as that in conditional GAN. Lcp is defined in the model definition for 3C-GAN.  and  are additional hyper-parameters that need to be tuned. In practice,
we first tune  with  fixed to 0, find the best alpha, then tune  with the best . We can tune both
,  qualitatively or quantitatively. If we visualize xf , we can see if it is generated according to labels. If not, alpha for that setting is too high. Also, if we visualize m1 or m2, we can see if they are near zero or one. On the other hand, if the loss Lfc and Lcp becomes too high, we know the alpha is set too high.

3 EXPERIMENT
We conduct experiments on three datasets 1)MNIST LeCun et al. (1998) 2)SVHN Netzer et al. (2011) 3)CelebA Liu et al. (2015). To display our method can separately generate the part of an image with its label, we modify MNIST data making the image background a uniform grayscale value between [0, 200], and resize it to 32*32. We name this modified version "MNISTBACKGROUND". For SVHN, we use the extra set of the cropped digits. For CelebA, we focus on the label of smile/not smile, and use the set of aligning face. We implement our method based on open source code1. The details of parameters of each experiment will be discussed in each section.
3.1 RESULTS FOR MNIST-BACKGROUND DATASET
MNIST-BACKGROUND is similar to MNIST, with 10 different digits and 50000 images for training. We set the channel number of the model to be 32, and the input noise code number to be 30. There are 15 context codes (zu), 15 object codes (zv), and 10 label codes (zl). Therefore, the input dimensionality for G1 is 15 and for G2 is 40. The weight for label difference cost (Lld) is set to be 1 ( = 1), and the weight for exclusive cost (Lex) is set to be 0 ( = 0). The two generators shared all structure except the bottom layer, and model is trained for 100000 iterations.
The results are shown in figure 2 to figure 4. The three figures are generated with the same input noise code so we can compare them. The samples are generated conditional on digit class 0, 1, ... to 9. There are totally 128 samples, each digit sample repeats 12 times, and the rest is fed with digit class 0. Since this dataset is simple, the training is converged, and the model shows good generation result (2). In addition, we show the results generated by G1 and G2 in figure 3 and figure 4. For visualization purpose, we let a checkerboard pattern c composite with the output from G1, such that
1https://github.com/igul222/improved_wgan_training

5

Under review as a conference paper at ICLR 2018
figure 3 shows f1 mn1 + c (1 - mn1 ). Similarly, figure 4 shows f2 m2n + c (1 - m2n). The obvious checkerboard pattern in both figures means the mask is zero in those pixels. We can see G1 only generate the context while G2 generate the part of the image related to the label.
Figure 2: composite generated samples xf for MNIST-BACKGROUND dataset
Figure 3: generated samples from G1 for MNIST-BACKGROUND dataset 3.2 RESULTS FOR SVHN DATASET SVHN is also a dataset with 10 different digits classes. In an image, a digit with the class label is in the center; however, there could be digits on the side of an image that does not related to the image's label. When training on this dataset, we set the channel number of the model to be 64, and the input noise code number to be 64. There are 32 context codes (zu), 32 object codes (zv), and 10 label codes (zl). Therefore, the input dimensionality for G1 is 32 and for G2 is 74.  is set to 10, and  is set to 0.5. The two generators also shared all structure except the bottom layer, and model is trained for 100000 iterations. The results are shown in figure 5 to figure 7. The setting is the same as before: the three figures are generated with the same input noise code, and the samples are generated conditional on digit class 0, 1, ... to 9. Figure 5 shows the model generates fairly reasonable images. In figure 6 and figure 7, we see G2 only generates the center digit in an image that is labeled, other digits are left to
6

Under review as a conference paper at ICLR 2018
Figure 4: generated sample from G2 for MNIST-BACKGROUND dataset G1. Compared to the results for MNIST-BACKGROUND, one major difference is the digits in G2 include the context other than the digit itself. We suspect this happens because sometimes the digits are dark while the background is light, and sometimes vice versa. And the digits generated by G2 need to include the context around to have the classifier C give it a high probability.
Figure 5: composite generated samples xf for SVHN dataset 3.3 RESULTS FOR CELEBA DATASET There are 20599 images on this dataset. We pre-process the dataset by resizing the images to 64by-64. We choose the attribute of smiling as image label for the conditional generator in our model because the number of images of smiling and not-smiling is similar. When training on this dataset, we set the channel number of the model to be 128, and the input noise code number to be 128 ( 64 context codes (zu), 64 object codes (zv), and 2 label codes (zl).  is set to 100, and  is set to 0.5. The results are shown in figure 8 to figure 10. There are 100 samples, with the first fifty samples belong to the class "not smiling", and the second fifty belongs to "smiling". The results show G2 focus on face generation while G1 focus on background generation. Since the condition of smiling/not smiling is determined by the face, it makes sense that G2 take the responsibility to generate the faces. However, the separation is not as clear as the results on SVHN and MNISTB.
7

Under review as a conference paper at ICLR 2018
Figure 6: generated samples from G1 for SVHN dataset
Figure 7: generated sample from G2 for SVHN dataset We suspect this happens because we use the attribute smiling/not smiling as a condition, which only affects a small fraction of pixels in an image, and that is not strong enough to provide G2 a solution to generate the whole face.
4 CONCLUSION
In this paper, we present a novel GAN structure that has one generator generates the context while the other conditional generator generate the part of the image based on the label it has. Compared to previous multi-generators model, our model has fewer parameters and generate the different part simultaneously. In addition, we proposed a new cost that makes the conditional generator learns to generate only the essential part of the condition changing. Also, we proposed an exclusive prior so that the two generators do not generate the same pixel. Experiments show our model separated the data as mentioned, and therefore; provide more controllability over original GAN.
8

Under review as a conference paper at ICLR 2018
Figure 8: composite generated samples xf for CelebA dataset
Figure 9: generated samples from G1 for CelebA dataset
REFERENCES
Arjovsky, Martin, Chintala, Soumith, and Bottou, Lon. Wasserstein GAN. arXiv:1701.07875 [cs, stat], 2017. URL http://arxiv.org/abs/1701.07875. arXiv: 1701.07875.
Chen, Xi, Duan, Yan, Houthooft, Rein, Schulman, John, Sutskever, Ilya, and Abbeel, Pieter. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2172­2180, 2016. 9

Under review as a conference paper at ICLR 2018
Figure 10: generated sample from G2 for CelebA dataset
Darrell, Trevor and Pentland, Alexander. Robust estimation of a multi-layered motion representation. In Visual Motion, 1991., Proceedings of the IEEE Workshop on, pp. 173­178. IEEE, 1991.
Denton, Emily L, Chintala, Soumith, Fergus, Rob, et al. Deep generative image models using alaplacian pyramid of adversarial networks. In Advances in neural information processing systems, pp. 1486­1494, 2015.
Goodfellow, Ian. Nips 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Gulrajani, Ishaan, Ahmed, Faruk, Arjovsky, Martin, Dumoulin, Vincent, and Courville, Aaron. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017.
Im, Daniel Jiwoong, Kim, Chris Dongjoo, Jiang, Hui, and Memisevic, Roland. Generating images with recurrent adversarial networks. arXiv preprint arXiv:1602.05110, 2016.
Isola, Phillip and Liu, Ce. Scene collaging: Analysis and synthesis of natural images with semantic layers. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3048­3055, 2013.
Jojic, Nebojsa and Frey, Brendan J. Learning flexible sprites in video layers. In Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, volume 1, pp. I­I. IEEE, 2001.
Kaneko, Takuhiro, Hiramatsu, Kaoru, and Kashino, Kunio. Generative attribute controller with conditional filtered generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6089­6098, 2017.
Kannan, Anitha, Jojic, Nebojsa, and Frey, Brendan J. Generative model for layers of appearance and deformation. In Aistats, volume 2005, pp. 1, 2005. 10

Under review as a conference paper at ICLR 2018
Kwak, Hanock and Zhang, Byoung-Tak. Generating images part by part with composite generative adversarial networks. arXiv preprint arXiv:1607.05387, 2016.
LeCun, Yann, Bottou, Le´on, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278­2324, 1998.
Liu, Ziwei, Luo, Ping, Wang, Xiaogang, and Tang, Xiaoou. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.
Mansimov, Elman, Parisotto, Emilio, Ba, Jimmy Lei, and Salakhutdinov, Ruslan. Generating images from captions with attention. arXiv preprint arXiv:1511.02793, 2015.
Netzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessandro, Wu, Bo, and Ng, Andrew Y. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 5, 2011.
Odena, Augustus, Olah, Christopher, and Shlens, Jonathon. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016.
Radford, Alec, Metz, Luke, and Chintala, Soumith. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Reed, Scott, Akata, Zeynep, Yan, Xinchen, Logeswaran, Lajanugen, Schiele, Bernt, and Lee, Honglak. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016a.
Reed, Scott E, Akata, Zeynep, Mohan, Santosh, Tenka, Samuel, Schiele, Bernt, and Lee, Honglak. Learning what and where to draw. In Advances in Neural Information Processing Systems, pp. 217­225, 2016b.
Vondrick, Carl, Pirsiavash, Hamed, and Torralba, Antonio. Generating videos with scene dynamics. In Advances In Neural Information Processing Systems, pp. 613­621, 2016.
Wang, John YA and Adelson, Edward H. Representing moving images with layers. IEEE Transactions on Image Processing, 3(5):625­638, 1994.
Wang, Xiaolong and Gupta, Abhinav. Generative image modeling using style and structure adversarial networks. In European Conference on Computer Vision, pp. 318­335. Springer, 2016.
Yan, Xinchen, Yang, Jimei, Sohn, Kihyuk, and Lee, Honglak. Attribute2image: Conditional image generation from visual attributes. In European Conference on Computer Vision, pp. 776­791. Springer, 2016.
Yang, Jianwei, Kannan, Anitha, Batra, Dhruv, and Parikh, Devi. Lr-gan: Layered recursive generative adversarial networks for image generation. arXiv preprint arXiv:1703.01560, 2017.
Zhao, Junbo, Mathieu, Michael, and LeCun, Yann. Energy-based generative adversarial network. arXiv preprint arXiv:1609.03126, 2016.
Zhu, Jun-Yan, Kra¨henbu¨hl, Philipp, Shechtman, Eli, and Efros, Alexei A. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision, pp. 597­613. Springer, 2016.
11

