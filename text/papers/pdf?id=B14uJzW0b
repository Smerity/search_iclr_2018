Under review as a conference paper at ICLR 2018

NO SPURIOUS LOCAL MINIMA IN A TWO HIDDEN UNIT RELU NETWORK
Anonymous authors Paper under double-blind review

ABSTRACT
Deep learning models can be efficiently optimized via stochastic gradient descent, but there is little theoretical evidence to support this. A key question in optimization is to understand when the optimization landscape of a neural network is amenable to gradient-based optimization. We focus on a simple neural network two-layer ReLU network with two hidden units, and show that all local minimizers are global. This combined with recent work of Lee et al. (2017); Lee et al. (2016) show that gradient descent converges to the global minimizer.

1 INTRODUCTION

Deep learning has been used to achieve state-of-art performance on a wide variety of problems in machine learning, artificial intelligence, computer vision, and natural language processing. In all these applications, deep models often use hundreds of millions of parameters and are trained with stochastic gradient descent (or other gradient-based methods such as Adagrad (Duchi et al., 2011), Adam (Kingma and Ba, 2014)), a surprisingly simple method, and yet finds solutions with both low train and test error.

Despite the empirical success, the mathematical justification for gradient-based methods is not well-understood. Zhang et al. (2016a) empirically demonstrated that sufficiently over-parametrized networks can be efficiently optimized to near global optimality with stochastic gradient. For a two-layer network with leaky ReLU activation, Soudry and Carmon (2016) showed that gradient descent on a modified loss function can obtain a global minimum of the modified loss function; however, this does not imply reaching a global minimum of the original loss function. Under the same setting, Xie et al. (2016) showed that critical points with large "diversity" are nearly globally optimal. Choromanska et al. (2015) used several assumptions to simplify the loss function to a polynomial with i.i.d. Gaussian coefficients. They then showed that every local minima of the simplified loss has objective value comparable to the global minima. Kawaguchi (2016) used similar assumptions to show that all local minimum are global minimum in a nonlinear network. However the assumptions of Choromanska et al. (2015); Kawaguchi (2016) require independent activations, meaning that the activations of the hidden units are independent of the input and/or mutually independent, which is violated in practice.

Multiple works have been proposed to circumvent this assumption when dealing with the two-layer

ReLU network F (x; W ) =

K j=1

(wjT

x),

where



=

max(0,

x)

is

the

ReLU

activation

function.

Under the realizable setting (i.e. the labels are generated from a network with "teaching" parameters

w) and isotropic Gaussian input, Tian (2017) shows that when there is only a single ReLU node

gradient descent converges to the global optimum. For K = 2, he conjectured that there are no

spurious local minima, and provided a partial characterization of the critical point structure. With the

same assumptions, Brutzkus and Globerson (2017) proved, for a two-layer ReLU network with a

single non-overlapping convolutional filter, all local minimizers are global. Zhang et al. (2017a) show

that for two-layer networks with non-standard activation functions that gradient descent converges to

global minimizers.

In this paper, we focus on the case when K = 2 and prove that every local minimum is global. As
in previous works (Brutzkus and Globerson, 2017; Tian, 2017; Hardt and Ma, 2016), we focus on
the population loss. The ReLU function is positive homogeneous, so we can rewrite the function as F (x; W ) = v1(w1T x) + v2(w2T x) where w1 and w2 are unit vectors; for simplicity, we will

1

Under review as a conference paper at ICLR 2018
assume that v1 = v2 = 1. Using these assumptions and an additional orthogonality assumption, we prove that all local minima of the loss surface are global. Although the setting is a simplification of practical neural networks, this is a meaningful step towards understanding the success of gradientbased methods in deep learning and other non-convex optimization problems. For the non-orthogonal case, we provide a partial characterization of the critical point structure.
The paper is organized as follows: Section 2 discusses related works, and Section 3 introduces the notation and definitions. Section 4 shows our main result that all local minima are global and gives a proof sketch and the formal proofs are in Section 5. Section 6 provides some extensions to the non-orthogonal case. Section 7 presents the result of the experiments, and finally, Section 8 concludes the paper.
2 RELATED WORK
Single Hidden Node Networks: For a neural network with a single hidden unit and monotone activation function , numerous authors (Mei et al., 2016; Hazan et al., 2015; Kakade et al., 2011; Kalai and Sastry, 2009; Soltanolkotabi, 2017; Tian, 2017) have shown that gradient-based methods converge to the true parameter w. In the case of a single hidden unit, the loss function is weakly quasi-convex, meaning that the gradient points in the direction of w, which explains the success of gradient-based methods. For K > 1 hidden units, the loss function is no longer quasi-convex, so this analysis does not easily generalize. In fact, our analysis for K = 2 is considerably more involved, and requires analyzing the gradient and hessian simultaneously.
Improper Learning: On the improper learning side, Shalev-Shwartz et al. (2011) pioneered a kernel-based approach that can be used for learning a single halfspace or smoothed ReLU. This was generalized to fully-connected deep neural networks in Zhang et al. (2016b) using the recursive kernel method. Goel et al. (2016) designed a new smoothed ReLU function that is a better approximation to the ReLU. Instead of learning a neural network, these methods learn a function in a RKHS, hence improper learning. Zhang et al. (2017b) improved upon this by learning a neural network, instead of a kernel machine, via a boosting approach, and with much lower sample complexity. The disadvantages of improper learning are two-fold: 1) the sample complexity for these methods is exponentially larger than the Rademacher complexity of the network, and 2) the practical success of deep learning is intricately tied to using gradient-based training procedures, and the learnability of these networks using improper learning does not explain the success of gradient-based methods. On a related line of work, Janzamin et al. (2015) propose a method of moments estimator using tensor decomposition.
Over-Parametrization There have been several works on studying the effect of over-parametrization on the training of neural networks (Poston et al., 1991; Haeffele and Vidal, 2015). These results require the width of a hidden layer to be greater than the number of training samples, which is not the case for commonly used networks. Finally, Zhang et al. (2016a) empirically demonstrated that commonly used over-parametrized networks can be efficiently optimized to near global optimality with stochastic gradient descent.
Non-Convex Optimization: Since the loss function of neural networks is non-convex, the theory of training neural networks is closely related to the theory of non-convex optimization. Recently, there is considerable progress on convergence guarantees of first-order and second-order methods, including some applications in machine learning problems. Lee et al. (2016) and Lee et al. (2017) show gradient descent and other first-order methods converge only to local minima, and not saddle points. Jin et al. (2017) and Ge et al. (2015) show that variants of stochastic gradient method converge to local minimizers in polynomial time. Ge et al. (2016) and Ge et al. (2017) show there is no spurious local minima in matrix completion problem and non-convex low rank problems. For the phase retrieval problem, Sun et al. (2016) show that there is no spurious local minimum.
3 PRELIMINARIES
We study a simple two RELU hidden node network with output function
F (x; w) = (w1T x) + (w2T x).
2

Under review as a conference paper at ICLR 2018

For the duration of this paper, we will assume that x is standard normal in Rn and all expectations are with respect to the standard normal. The population loss function is:

L(x, W )

=

1 E[(F (x, W )

-

F (x, W ))2].

2

Define

g(v1, v2) = E[(v1T x)(v2T x)],

so the loss can be rewritten as (ignoring additive constants, then multiplied by 4):

(1) (2)

f (W ) =

g(wi, wj) - 2g(wi, wj) .

i,j{1,2}

(3)

From Brutzkus and Globerson (2017) we get

1 g(u, v) = u
2

v (sin u,v - ( - u,v) cos u,v) .

and

g 1 =v
u 2

u u

sin u,v

+

1 (
2

-

u,v )v.

(4) (5)

In this paper, we study the landscape of f over the manifold R = { w1 = w2 = 1}. The manifold gradient descent algorithm is:
xk+1 = PR(xk - Rf (xk)),
where PR is the orthogonal projector onto the manifold R, and R is the manifold gradient of f .

4 MAIN RESULT AND PROOF SKETCH
First we state the main result of this paper: Theorem 4.1. Assume w1 = w2 = 1 and w1T w2 = 0, then there is no spurious local minimizer of the objective function (3) on the manifold R = { w1 = w2 = 1}. Furthermore, every saddle point or local maximizer has a direction of negative curvature.
The next theorem shows that manifold gradient descent with random initialization converges to the global minimizer Theorem 4.2. With probability one, manifold gradient descent will converge to the global minimizers.
Proof. The objective function f is infinitely differentiable on manifold R. Using Proposition 9 of Lee et al. (2017), manifold gradient descent will converge to a local minimizer with probability one. Since the only local minima for function f are w1 = w1, w2 = w2 and w1 = w2, w2 = w1, manifold gradient descent converges to the true solutions.

Proof of Theorem 4.1. The proof of the main result is complicated, so let's start with a simpler case, in which both w1 and w2 are in span{w1, w2}. Proposition 4.3. Assume w1 = w2 = 1, w1T w2 = 0 and w1, w2  span{w1, w2}, then there is no spurious local minimizer of the objective function (3) on the manifold R = { w1 = w2 = 1}. Furthermore, every saddle point or local maximizer has a direction of negative curvature.
Proof. The complete proof is given in Appendix B and C, so here we just give a proof sketch. To prove this, we need some observations. The first important observation is that we are always on manifold { w1 = w2 = 1}, and for each vector in the plane with fixed norm, there is only one degree of freedom, which means we can express each vector with only one variable. Thus, we can express the vectors in polar coordinates, where 1 and 2 are the angles for w1 and w2.
3

Under review as a conference paper at ICLR 2018

The second observation is we only need to compute the gradient on the manifold and check whether

it's zero.

Define m(w1)

=

sin

1

f  w11

-

cos

1

f  w12

and m(w2)

=

sin

2

f  w21

-

cos

2

f  w22

.

Then

for w1 and w2, the norm of the manifold gradients are |m(w1)| and |m(w2)|. Thus, we only need to

check whether the value of function m is 0 and get rid of the absolute value sign.

Then we apply the polar coordinates onto the manifold gradients, and obtain:

m(w2)

=

1 (


-

w1,w2 ) sin(2

-

1)

+

cos 2

-

sin 2

1

+ 

w2,w1 sin 2 - w2,w2 cos 2

.

(6) (7)

The last observation we need for this theorem is that we must divide this problem into several cases

because each angle in (312) is a piecewise linear function. If we discuss each case independently, the

resulting functions are linear in the angles. The details are in Appendix B. After the calculation of all

cases, we found the positions of all the critical points: WLOG assume 1  2, then there are four

critical

points

in

the

2D

case:

(1,

2)

=

(0,

 2

),

(

 4

,

 4

),

(

 4

,

5 4

)

and

(

5 4

,

5 4

).

After finding all the critical points, we compute the manifold Hessian matrix for those points and show that there is a direction of negative curvature. The details can be found in Appendix C.

The next step is to reduce to a three dimensional problem. As stated in the two-dimensional case, the gradient is in span{w1, w2, w1, w2}, which is four-dimensional. However, using the following lemma, we can reduce it to three dimensions and simplify the whole problem.
Lemma 4.4. If (w1, w2) is a critical point, then there exists a set of standard orthogonal basis (e1, e2, e3) such that e1 = w1, e2 = w2 and w1, w2 lies in span{e1, e2, e3}.

The second observation is that critical points satisfy the following relation.

Lemma 4.5.

arccos(-w11) = arccos(-w12) = - w23 .

arccos(-w21) arccos(-w22)

w13

(8)

Proposition 4.6. Assume w1 = w2 = 1, w1T w2 = 0 and i  [2], wi / span{w1, w2}, then there is no spurious local minimizer of the objective function (3) on the manifold { w1 = w2 = 1}. Furthermore, every saddle point or local maximizer has a direction of negative curvature.

Proof. The complete proof is given in Appendix D, so here we just give a proof sketch.

The ratio in Lemma 4.5 captures an important property of all critical points. For simplicity, based on D.5, we define k0 = -k, 1 =  - w2,w1 and 2 =  - w2,w2 . Then

 - w1,w1 = k01  - w1,w2 = k02.

(9) (10)

. From this ratio, we can construct a new function F :

Lemma 4.7. Define

F () =

-k0

,

k0 cos(k0) + cos()

then

F (1)

=

F

(2)(1,

2



[0,

 k0

]).

(11)

Then from the properties of that particular function and upper bound the value of k0 we get Lemma 4.8. 1 = 2.

That lemma shows that w1 and w2 bisector of w1 and w2. Combining

must be on a plane whose projection this with the computation of Hessian,

onto span{w1, w2} is the we conclude that we have

found negative curvature for all possible critical points, which completes the proof.

4

Under review as a conference paper at ICLR 2018

Combining both Propositions 4.3 and 4.6, we have proved Theorem 4.1, which is the main result of this paper.

5 PROOFS
Here we provide some detailed proofs which are important for the understanding of the main theorem.

5.1 WHY WE ONLY NEED 3 DIMENSION Lemma 5.1. If (w1, w2) is a critical point, then there exists a set of standard orthogonal basis (e1, e2, e3) such that e1 = w1, e2 = w2 and w1, w2 lies in span{e1, e2, e3}.

Proof. If (w1, w2) is a critical point, then

(I

-

w1w1T

)

f w1

=

0.

where matrix (I - w1w1T ) projects a vector onto the tangent space of w1. Since

(I - w1w1T )w1 = w1 - w1 = 0,

(12) (13)

we get

(I

-

w1w1T

f )
w1

=

1 (I


-

w1w1T )

( - w1,w2 )w2 - ( - w1,w1 )w1 - ( - w1,w2 )w2

,

(14) (15)

which means that ( - If w1,w2 = , i.e., w1

w1,w2 )w2 - = -w2, then

( - w1,w1 )w1 - ( - w1,w2 of course the four vectors have

)w2 rank

lies in the direction of w1. at most 3, so we can find

the proper basis. If w1,w2 < , then we know that there exists a real number r such that

( - w1,w2 )w2 - ( - w1,w1 )w1 - ( - w1,w2 )w2 + r · w1 = 0.

(16)

Since w1,w2 < , we know that the four vectors w1, w2, w1 and w2 are linear dependent. Thus, they have rank at most 3 and we can find the proper basis.

5.2 SOME PROPERTIES OF CRITICAL POINTS

Next we will focus on the properties of critical points. Assume (w1, w2) is one of the critical points,

from e2 =

lemma w2 and

D.1 w1,

we can find a set of standard orthogonal basis (e1, w2 lies in span{e1, e2, e3}. Furthermore, assume w1

e2, e3) such that e1 = w11e1 + w12e2 +

= w1, w13e3

and w2 = w21e1 + w22e2 + w23e3, i.e., w1 = (w11, w12, w13) and w2 = (w21, w22, w23). Since

we have already found out all the critical points when w13 = w23 = 0, in the following we assume

w123 + w223 = 0.

Lemma 5.2. w1,w2 < .

fsProwroo1fm,ouwfr.1(t2h)I0wefr81)ww-teh1,a(kwtn2(o-=w-(w,1,w-twh12e,w)nww21w)2,ww1l12ie=)-swi-1n(w+sp-2(,ansow-{1ew,1w2w,1e1i)2,sww}i21,n)s-wtoh2we(l1diei-rseicsntwpitao1h,nnwe{2ode)fi1wrwe,2ce1t2l.ii}oeWnsaneoindfhtwwahv12ee.dHailrsorepewcaatedinvoy{enerk,o1n(,foeww2-}1n,. Thus, w13 = w23 = 0 and that contradicts with the assumption. In a word, w1,w2 < .
Lemma 5.3. w13  w23 = 0.

5

Under review as a conference paper at ICLR 2018

Proof. We have already known from (208) that ( - w1,w2 )w2 - ( - w1,w1 )w1 - ( - w1,w2 )w2 lies in the direction of w1. Writing it in each dimension and we know that there exists a real number r0 such that

( - w1,w2 )w21 - ( - w1,w1 ) = r0 · w11 ( - w1,w2 )w22 - ( - w1,w2 ) = r0 · w12
( - w1,w2 )w23 = r0 · w13.

(17) (18) (19)

From lemma D.2 we know that w1,w2 < , so we can define k = r0 .  - w1,w2

(20)

Then the equations become

w21

-

 

- -

w1 ,w1 w1 ,w2

= k · w11

w22

-

 

- -

w1 ,w2 w1 ,w2

= k · w12

w23 = k · w13.

(21)
(22) (23)

Similarly, we have

w11

-

 

- -

w2 ,w1 w1 ,w2

=k

· w21

w12

-

 

- -

w2 ,w2 w1 ,w2

=k

· w22

w13 = k · w23.

(24)
(25) (26)

Since w123 + w223 = 0, at least one of those two variables cannot be 0. WLOG, we assume that w13 = 0. If w23 = 0, then from (219) we know that w13 = 0, which contradicts the assumption.
Thus, w23 = 0, which means that w13  w23 = 0.

Lemma 5.4. w13  w23 < 0.

Proof.

Adapting from the proof of lemma D.3, we know that kk

= ·w23
w13

w13 w23

= 1, so k

=

1 k

.

From lemma D.2 we know that w1,w2 < , and from lemma D.3 we know that both w1 and w2 are

outside span{w1, w2}, so i, j

 [2], wi,wj

< .

Thus, i, j



[2],

 -wi ,wj  -w1 ,w2

> 0. Therefore, we

have

That

means

k

<

0,

so

w23 w13

>

0.

In a word, w13  w23 < 0.

w21 > k · w11 1
w11 > k w21.

(27) (28)

Lemma 5.5.

arccos(-w11) = arccos(-w12) = - w23 .

arccos(-w21) arccos(-w22)

w13

(29)

Proof. Adapting from the proof of lemma D.4 and we know that

w - -w1,w1
21 -w1,w2
w11

 -w1 ,w2
= w -22 -w1,w2 w12

= w23 = k. w13

6

(30)

Under review as a conference paper at ICLR 2018

Similarly, we have

w - -w2,w1
11 -w1,w2
w21

 -w2 ,w2
= w -12 -w1,w2 w22

=

w13

=

1 .

w23 k

Taking the first component of (229) and (230) gives us

Thus,

w21

=

k

·

w11

+

 

- -

w1 ,w1 w1 ,w2

w21

=

k

·

w11

-

 k


- -

w2 ,w1 w1 ,w2

.

Similarly, we get

 - w1,w1 = -k.  - w2,w1

 - w1,w2 = -k.  - w2,w2 Since i, j  [2],  - wi,wj = arccos(-wij ), we know that

arccos(-w11) = arccos(-w12) = - w23 .

arccos(-w21) arccos(-w22)

w13

(31) (32) (33) (34) (35) (36)

6 ANALYSIS OF CRITICAL POINTS FOR NON-ORTHOGONAL W 

In this section, we partially characterize the structure of the critical points when w1, w2 are non-

orthogonal, but form an acute angle. In other Let us first consider the 2D cases, i.e., both w1

wanodrdws,2tahreeainngtlheebseptawneoefnww11

aannddww22.

is   (0, Similar to

 2

).

the

original problem, after the technique of changing variables(i.e., using polar coordinates and assume

1 and 2 are the angles of w1 and w2 in polar coordinates), we divide the whole plane into 4 parts,

which are the angle in [0, ], [, ], [,  + ] and [ + , 2). We have the following lemma:

Lemma 6.1. Assume w1 = w2 = 1, w1T w2 > 0 and w1, w2  span{w1, w2}. When w1 and w2 are in the same part(one of four parts), the only critical points except the global minima are those when both w1 and w2 are on the bisector of w1 and w2.

Proof. The complete proof is given in appendix E, the techniques are nearly the same as things in the original problem and a bit harder, so to be brief, we omit the proof details here.

For the three-dimensional cases cases of this new problem, it's interesting that the first few lemmatas are still true. Specifically, Lemma D.1(restated as Lemma 4.4) to Lemma D.5(restated as Lemma 4.5) are still correct. The proof is very similar to the proofs of those lemmas, except we need modification to the coefficients of terms in the expressions of the manifold gradients.

7 EXPERIMENTS
We did experiments to verify the theoretical results. Since our results are restricted to the case of K = 2 hidden units, it is also natural to investigate whether general two-layer ReLU networks also have the property that all local minima are global minima. Unfortunately as we show via numerical simulation, this is not the case. We consider the cases of K from 2 to 11 hidden units and we set the dimension d = K. For each K, the true parameters are orthogonal to each other. For each K, we run projected gradient descent with 300 different random initializations, and count the number of local minimum (critical points where the manifold Hessian is positive definite) with non-zero training error. If we reach a sub-optimal local minimum, we can conclude the loss surface exhibits spurious local minima. The bar plot showing the number of times gradient descent converged to spurious local minima is in Figure 1. From the plot, we see there is no spurious local minima from K = 2 to K = 6. However for K  7, we observe a clear trend that there are more spurious local minima when there are more hidden units.

7

Spurious Local Minima

Under review as a conference paper at ICLR 2018
120
100
80
60
40
20
0 2 3 4 5 6 7 8 9 10 11 Number of Nodes K
Figure 1: Spurious Local Minima for K  2 ReLU Network.
8 CONCLUSION AND FUTURE WORK
In this paper, we provided recovery guarantee of stochastic gradient descent with random initialization for learning a two-layer neural network with two hidden nodes, unit-norm weights, ReLU activation functions and Gaussian inputs. Experiments are also done to verify our results. For future work, here we list some possible directions.
8.1 GENERAL CASE OF NETWORKS This paper focused on a ReLU network with only two hidden units, . And the teaching weights must be orthogonal. Those are many conditions, in which we think there are some conditions that are not quite essential, e.g., the orthogonal assumption. In experiments we have already seen that even if they are not orthogonal, it still has some good properties such as the positions of critical points. Therefore, in the future we can further relax or abandon some of the assumptions of this paper and preserve or improve the result we have.
8.2 BAD LOCAL MINIMA The neural network we discussed in this paper is in some sense very simple and far from practice, although it is already the most complex model when we want to analyze the whole loss surface. By experiments we have found that when it comes to seven hidden nodes with orthogonal true parameters, there will be some bad local minima, i.e., there are some local minima that are not global. We believe that research in this paper can capture the characteristics of the whole loss surface and can help analyze the loss surface when there are three or even more hidden units, which may give some bounds on the performance of bad local minima and help us understand the specific non-convexity of loss surfaces.
REFERENCES
A. Brutzkus and A. Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. International Conference on Machine Learning (ICML), 2017.
A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. In AISTATS, 2015.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121­2159, 2011.
8

Under review as a conference paper at ICLR 2018
R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In Advances in Neural Information Processing Systems, pages 2973­2981, 2016.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points--online stochastic gradient for tensor decomposition. arXiv:1503.02101, 2015.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. arXiv preprint arXiv:1704.00708, 2017.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. arXiv preprint arXiv:1611.10258, 2016.
Benjamin D Haeffele and René Vidal. Global optimality in tensor factorization, deep learning, and beyond. arXiv preprint arXiv:1506.07540, 2015.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231, 2016.
Elad Hazan, Kfir Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex optimization. In Advances in Neural Information Processing Systems, pages 1594­1602, 2015.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems, pages 927­935, 2011.
Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT, 2009.
K. Kawaguchi. Deep learning without poor local minima. In Advances In Neural Information Processing Systems, pages 586­594, 2016.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
J. D. Lee, I. Panageas, G. Piliouras, M. Simchowitz, M. I. Jordan, and B. Recht. First-order methods almost always avoid saddle points. ArXiv e-prints, 2017.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges to minimizers. University of California, Berkeley, 1050:16, 2016.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses. arXiv preprint arXiv:1607.06534, 2016.
Timothy Poston, C-N Lee, Y Choie, and Yonghoon Kwon. Local minima and back propagation. In Neural Networks, 1991., IJCNN-91-Seattle International Joint Conference on, volume 2, pages 173­176. IEEE, 1991.
Shai Shalev-Shwartz, Ohad Shamir, and Karthik Sridharan. Learning kernel-based halfspaces with the 0-1 loss. SIAM Journal on Computing, 40(6):1623­1646, 2011.
Mahdi Soltanolkotabi. Learning relus via gradient descent. arXiv preprint arXiv:1705.04591, 2017.
D. Soudry and Y. Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Forthcoming, 2016.
9

Under review as a conference paper at ICLR 2018
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. International Conference on Machine Learning (ICML), 2017.
Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. arXiv preprint arXiv:1611.03131, 2016.
C. Zhang, S.y Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016a.
Q. Zhang, R. Panigrahy, S. Sachdeva, and A. Rahimi. Electron-proton dynamics in deep learning. arXiv preprint arXiv:1702.00458, 2017a.
Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly learnable in polynomial time. In International Conference on Machine Learning, pages 993­1001, 2016b.
Yuchen Zhang, Jason Lee, Martin Wainwright, and Michael Jordan. On the learnability of fullyconnected neural networks. In Artificial Intelligence and Statistics, pages 83­91, 2017b.
10

Under review as a conference paper at ICLR 2018

A PRELIMINARIES

Consider a neural network with 2 hidden nodes and ReLU as the activation function: F (x) = (w1T x) + (w2T x) , 2
where (x) = max(0, x) is the ReLU function.

(37)

First we study the 2-D case, i.e., the input and all parameters are two dimensional. Assume that the input follows standard normal distribution.

The loss function is population loss:

l(W ) = Ex

(w1T x) + (w2T x) - (w1T x) + (w2T x) 2 . 22

(38)

Define

g(u, v) = Ex (uT x)(vT x) ,

then from Brutzkus and Globerson (2017) we get

1 g(u, v) = u
2

v (sin u,v - ( - u,v) cos u,v) .

Thus,

g 1 =v
u 2

u u

sin u,v

+

1 (
2

-

u,v )v.

(39) (40) (41)

Moreover, from (38) we get

1 l(W ) =
4

g(wi, wj) - 2g(wi, wj) + g(wi, wj) .

i,j[2]

(42)

Assume w1 = w2 and w1T w2 = 0. WLOG, let e1 = w1 and e2 = w2. Then we know that i, j  [2], g(wi, wj) is a constant number. Thus, define the objective function(which equals to 4l(W ) up to an additive constant)

f (W ) = g(w1, w1) + g(w2, w2) + 2g(w1, w2) - 2

g(wi, wj).

i,j[2]

(43)

Thus,

f 1 w1 = w1 +  w2

w1 w1

1

sin

w1 ,w2

+

( 

-

w1,w2 )w2

-1 

w1

w1 w1

sin w1,w1

-

1 (


-

w1,w1 )w1

-1 

w2

w1 w1

sin w1,w2

-

1 (


-

w1,w2 )w2

1 = w1 +  w2

w1 w1

1

sin

w1 ,w2

+

( 

-

w1,w2 )w2

-1 

w1 w1

sin

w1 ,w1

-

1 (


-

w1,w1 )w1

-1 

w1 w1

sin

w1 ,w2

-

1 (


-

w1,w2 )w2.

Similarly, for w2, the gradient is

f 1 w2 = w2 +  w1

w2 w2

1

sin

w1 ,w2

+

( 

-

w1,w2 )w1

-1 

w2 w2

sin

w2 ,w1

-

1 (


-

w2,w1 )w1

-1 

w2 w2

sin

w2 ,w2

-

1 (


-

w2,w2 )w2.

(44) (45) (46) (47) (48) (49)
(50) (51) (52)

11

Under review as a conference paper at ICLR 2018

Assume that w1 = (w11, w12) and w2 = (w21, w22), then the gradient can be expressed in this form:

f 1 w11 = w11 + 

w2 w1

1

w11

sin w1,w2

+

( 

-

w1,w2 )w21

-1 

w11 w1

1

sin w1,w1

-

( 

-

w1,w1 )

-1 

w11 w1

sin w1,w2

(53) (54) (55)

and

f 1 w12 = w12 + 

w2 w1

w12

sin w1,w2

+

1 (


-

w1,w2 )w22

-1 

w12 w1

sin w1,w1

-1 

w12 w1

1

sin w1,w2

-

( 

-

w1,w2 ).

(56) (57) (58)

Because of symmetry, for w2, the gradient is

f 1 w21 = w21 + 

w1 w2

w21

sin w1,w2

+

1 (


-

w1,w2 )w11

-1 

w21 w2

1

sin w2,w1

-

( 

-

w2,w1 )

-1 

w21 w2

sin w2,w2

(59) (60) (61)

and

f 1 w22 = w22 + 

w1 w2

1

w22

sin w1,w2

+

( 

-

w1,w2 )w12

-1 

w22 w2

sin w2,w1

-1 

w22 w2

1

sin w2,w2

-

( 

-

w2,w2 ).

(62) (63) (64)

B CRITICAL POINTS IN 2D CASES

B.1 2D PRELIMINARIES

In 2D cases, we can translate W to polar coordinates and fix w1 = w2 = 1, so there are two variables left: 1 and 2, i.e., w1 = (cos 1, sin 1) and w2 = (cos 2, sin 2).

For manifold gradient, we only need to consider its norm and check whether it's zero. For w1 and w2,

the

(directed)

norm

of

manifold

gradients(expressed

by

m)

are

m(w1)

=

sin

1

f  w11

-

cos

1

f  w12

and

m(w2)

=

sin

2

f  w21

-

cos

2

f  w22

.

12

Under review as a conference paper at ICLR 2018

To make life easier, it's better to simplify the m functions a bit using w1 = (cos 1, sin 1) and w2 = (cos 2, sin 2):

m(w1)

=

f sin 1 w11

-

f cos 1 w12

(65)

= sin 1

cos

1

+

1 

cos 1

sin w1,w2

+

1 (


-

w1,w2 )

cos

2

(66)

-

1 

cos 1 sin w1,w1

-1+

w1 ,w1 

-

1 

cos 1 sin w1,w2

(67)

- cos 1

sin 1

+

1 

sin 1

sin w1,w2

+

1 (


-

w1,w2 )

sin 2

(68)

-

1 

sin 1 sin w1,w1

-1+

w1 ,w2 

-

1 

sin 1 sin w1,w2

=

1 (


-

w1,w2 ) sin(1

-

2)

+

cos 1

-

sin 1

1

+ 

w1,w1 sin 1 - w1,w2 cos 1

.

(69) (70) (71)

Similarly,

1

m(w2)

=

( 

-

w1,w2 ) sin(2

-

1)

+

cos 2

-

sin 2

1

+ 

w2,w1 sin 2 - w2,w2 cos 2

.

(72) (73)

Then we can divide them into several cases and analyze them one by one to specify the positions and properties of the critical points.
WLOG, assume 1  2.

B.2

0



1



2



 2

The norm of the manifold gradient w.r.t. w1 is

m(w1)

=

1 (


-

2

+

1) sin(1

- 2) + cos 1

-

sin 1

1 +


1 sin 1 -

 2

-

1

cos 1

.

Similarly, the norm of m(w2) is

m(w2)

=

1 (


-

2

+

1) sin(2

- 1) + cos 2

-

sin 2

1 +


2 sin 2 -

 2

-

2

cos 2

.

Define

h1()

=

cos



-

sin



+

1 

 sin  -

 - 2

cos 

.

If m(w1) = m(w2) = 0, then

1

h1(1)

=

( 

-

2

+ 1) sin(2

- 1)

and

1

h1(2)

=

( 

-

2

+ 1) sin(1

- 2).

(74) (75)
(76) (77) (78)
(79) (80)

13

Under review as a conference paper at ICLR 2018

Thus,

h1(1) + h1(2) = 0.

Note

that

when

0







 2

,

h1()

=

-

 2

-1 

+



sin 

-



-1 

-



cos



<

0.

Also note that

1 h1() + h1( 2 - ) = cos  - sin  +   sin  -
+ cos  -  - sin  -  22

 - 2

cos 

1 +

 -  sin  -  -  cos  - 

2

2

2

= cos  - sin  + 1  sin  -  -  cos  2

+ sin  - cos  + 1  -  cos  -  sin  2

= 0.

(81)
(82)
(83) (84) (85) (86) (87) (88)

Thus, if m(w1) = m(w2) = 0, then 1 + 2

2

=

 2

-

1

into

(75)

and

we

get

=

 2

.

From 1



2

we know that 1



 4

.

Plug

m(w1)

=

0



h1(1)

=

21 + 

 2

cos(21).

(89)

Lemma B.1.

If 0  



 4

,

then

h1()



2 + 

 2

cos(2)

(90)

and

the

inequality

becomes

equality

only

then



=

0

or



=

 4

.

Proof.

When 0   

 4

,

2 + 

 2

cos(2)

- h1()

1 2 = + cos(2) +
2

1-  

sin  -

1 +
2

cos 

(91) (92)



1 +

cos(2) +

1- 

sin  -

1 +

cos 

2

 2

(93)



1 +

cos(2) + 3 sin  -

1 +

cos 

2

4 2

(94)

=

1 +

(cos(2) - cos ) + 3 sin 

2

4

(95)

 3 (cos(2) - cos ) + 3 sin  44

= 3 (cos(2) - (cos  - sin )) 4

3 =

cos2  - sin2  - (cos  - sin )

4

= 3 (cos  - sin ) (cos  + sin  - 1) 4

 0.

Note

that

(96)

is

because

cos(2)

- cos 

is

always

non-positive

when

0







 4

.

(96) (97) (98) (99) (100)

From (93), the inequality becomes an equality only when  cos(2) = 0, which means that the only

possibilities are



=

0 or



=

 4

.

After

plugging

in

those

two possibilities in (90),

we

know that

h()

=

2+

 2



cos(2)

holds

when



=

0

or



=

 4

.

14

Under review as a conference paper at ICLR 2018

Using the above lemma, we conclude that m(w1) =

m(w1)

=

0

iff

(1,

2)

=

(0,

 2

)

or

(1,

2)

=

(

 4

,

 4

).

0 iff 1 + 2

=

 2

and 

=

0

or

 4

,

i.e.,

In

a

word,

there

are

two

critical

points

in

this

case:

(1,

2)

=

(0,

 2

)

and

(1,

2)

=

(

 4

,

 4

).

B.3

 2



1



2





The norm of the manifold gradient w.r.t. w1 is

1

m(w1)

=

( 

-

2

+

1) sin(1

- 2) + cos 1

-

sin 1

1 +


1 sin 1 -

1

-

 2

cos 1

.

Similarly,

m(w2)

=

1 (


-

2

+

1) sin(2

- 1) + cos 2

-

sin 2

1 +


2 sin 2 -

2

-

 2

cos 2

.

Define

h2()

=

cos



-

sin



+

1 

 sin  -

-  2

cos 

,

Let 

=



-

 2

,

then



h2() = h2

+ 2

= - sin  - cos  + 1

 +

sin

 +

-  cos

 +

2

2

2

= - sin  - cos  + 1

  + cos  +  sin 

2

= - sin  + 1  -  cos  +  sin  2

= - sin  + 1  sin  -  -  cos  2

= h1( ) - cos 

Lemma B.2.

When





[

 2

,

],

1 h2()  - 2 ,

and

the

inequality

becomes

equality

only

then



=

 2

or



=

.

(101) (102)
(103) (104)
(105)
(106) (107) (108) (109) (110) (111) (112)
(113)

Proof.

Let 

=



-

 2

,

then





[0,

 2

]

and

h2() = h1( ) - cos 

= - sin  + 1  sin  -  -  cos  2

=  - 1 cos  +  - 1 sin 

2



(114) (115) (116)

 -1

cos 

-

1 sin 

22

= - 1 (cos  + sin  ) 2

 -1. 2

(117) (118) (119)

Note that the inequality becomes equality only when  cos  = 0 and

 

-

1 2

sin  = 0, i.e.,



=

 2

or



=

.

15

Under review as a conference paper at ICLR 2018

If m(w1) = m(w2) = 0, then

h2(1)

=

1 (


-

2

+ 1) sin(2

- 1)

and

h2(2)

=

1 (


-

2

+ 1) sin(1

- 2).

Thus,

h2(1) + h2(2) = 0.

However, we know that h2(1) < 0 and h2(1) < 0, which makes a contradiction.

In a word, there is no critical point in this case.

B.4





1



2



3 2

The norm of the manifold gradient w.r.t. w1 is

1

m(w1)

=

( 

-

2

+

1) sin(1

- 2) + cos 1

-

sin 1

1 +


(2 - 1) sin 1 -

1

-

 2

cos 1

.

Similarly, the norm of m(w2) is

m(w2)

=

1 (


-

2

+

1) sin(2

- 1) + cos 2

-

sin 2

1 +


(2 - 2) sin 2 -

2

-

 2

cos 2

.

Define

1

h3() = cos  - sin  + 

(2 - ) sin  -

- 2

cos 

.

Let  =  + , then

h3() = h3( + )

= cos( + ) - sin( + )

1 +

( -  ) sin( + ) -

 +

cos( + )

2

= - cos  + sin  + 1 ( -  )(- sin  ) -  +  -  (- cos  ) 2

= - cos  + sin  + 1 - sin  +  sin  +  cos  +  -  cos  2

= - cos  + sin  - sin  + cos  + 1  sin  -  -  cos  2

1 =

 sin  -

 -

cos 

2

= h1( ) - cos  + sin  .

Moreover,





[,

3 2

],

h3()

+

5 h3( 2

-

)

=

h1(

-

)

-

cos(

-

)

+

sin(

-

)

+

5 h1( 2

-



-

)

-

5 cos(
2

-



-

)

+

5 sin(
2

-



-

)

=

h1(

-

)

+

cos 

-

sin 

+

3 h1( 2

-

)

+

sin 

-

cos 

=

h1(

-

)

+

3 h1( 2

-

)

= 0.

(120)
(121) (122)
(123) (124)
(125) (126)
(127)
(128) (129) (130) (131) (132) (133) (134) (135)
(136) (137) (138) (139) (140)

16

Under review as a conference paper at ICLR 2018

Also,

when





[,

3 2

],

h3()

=



- 

-

1

cos 

+



-

3 2


-

1

sin 

>

0,

(141)

so

h3

is

an

increasing

function

when





[,

3 2

].

Thus,

if

m(w1)

=

m(w2)

=

0,

then 1

+ 2

=

5 2

.

From

1



2

we

know that 1



5 4

.

Plug

2

=

5 2

-

1

in

(124)

and

we

get

m(w1)

=

0



h3(1)

=

21 - 

3 2

cos(21).

(142)

From Lemma B.1,

h3(1) = h1(1 - ) - cos(1 - ) + sin(1 - )

(143)



21 - 

3 2

cos(2(1) - ) - cos(1

- ) + sin(1

- )

=

21 - 

3 2

cos(21) - cos(1

- ) + sin(1

- )



21 - 

3 2

cos(21).

(144) (145) (146)

Note

that

(144)

becomes

equality

only

when

1

=



or

1

=

5 4

,

and

(146)

becomes

equality

only

when

1

=

5 4

.

Therefore,

in

this

case,

m(w1)

=

0

if

and

only

if

1

=

5 4

.

In

a

word,

the

only

critical

point

in

this

case

is

(1,

2)

=

(

5 4

,

5 4

).

B.5

3 2



1



2



2

Actually, this is symmetric to the B.3, so in this part I would like to specify this kind of symmetry.

We have already assumed that 1  2 without loss of generality, and under this assumption, we can find another symmetry: From w1 and w2, using line y = x as symmetry axis, we can get two new vectors w1 and w2. w1 is not necessarily the image of w1 because we need to preserve the assumption that 1  2, but there exists one and only one mapping such that 1  2. In this kind of symmetry, the angles, including w1,w2 and wi,wj where i, j  [2], are the same, so the two symmetric cases share the same gradients, thus the symmetric critical points.

We use (i, j) ,where i, j  [4], to represent the case that 1 is in the ith quadrant and 2 is in the jth one. Using this kind of symmetry, we conclude that (1, 2) is equivalent to (1, 4) and (2, 3) is
equivalent to (3, 4), so there are 4 cases left which are (1, 2), (1, 3), (2, 3) and (2, 4).

B.6

0



1



 2



2





Similar to previous cases,

m(w1)

=

1 (


-

2

+

1) sin(1

- 2) + cos 1

-

sin 1

1 +


1 sin 1 -

 2

-

1

cos 1

and

1

m(w2)

=

( 

-

2

+

1) sin(2

- 1) + cos 2

-

sin 2

1 +


2 sin 2 -

2

-

 2

cos 2

.

Using previous definitions, we conclude that

1

m(w1)

=

( 

- 2

+ 1) sin(1

- 2) +

h1(1)

(147) (148)
(149) (150)
(151)

17

Under review as a conference paper at ICLR 2018

and

m(w2)

=

1 (


- 2

+ 1) sin(2

- 1) +

h2(2).

(152)

If m(w1) = m(w2) = 0, then m(w1) + m(w2) = 0, i.e.,

h1(1) + h2(2) = 0.

(153)

From (111) we know that Thus, using lemma B.2,



h1(1)

=

h2(1

+

) 2

+

cos 1.

(154)

 11

h1(1)

+

h2(2)

=

h2(1

+

) 2

+

h2(2)

+

cos 1



- 2

-

2

+

1

=

0.

(155)

That means the only case that h1(1) + h2(2) = 0 is when the inequality (155) becomes equality,

wp2ohi=incthw2mhoeirlaen2tsh=tehaott.hcPeorlsuigs1gni=nogt.1tShaeinnmcdebha(2c(k1,i1n+2()14=28))(=a0n, dh22()(1h52a0)s),b=weee-nc21acn.ouTvnehrtueifsdy,

we must have 1 = 0, and that the first one is a critical
in case 1, there are no new

critical points in this case.

B.7

0



1



 2

,





2



3 2

Similar to previous cases,

1

m(w1)

=

( 

-

w1,w2 ) sin(1

-

2)

+

cos 1

-

sin 1

1 +


1 sin 1 -

 2

-

1

cos 1

(156) (157)

and

1

m(w2)

=

( 

-

w1,w2 ) sin(2

-

1)

+

cos 2

-

sin 2

1 +


(2 - 2) sin 2 -

2

-

 2

cos 2

.

(158) (159)

Thus, using previous definitions

1

m(w1)

=

( 

-

w1,w2 ) sin(1

-

2)

+

h1(1)

(160)

and

1

m(w2)

=

( 

-

w1,w2 ) sin(2

-

1)

+

h3(2).

(161)

If m(w1) = m(w2) = 0, then m(w1) + m(w2) = 0, i.e.,

For

0







 2

,

define

h1(1) + h3(2) = 0.

(162)

H() = h1() + h3( + ).

(163)

Then we have the following lemma:

Lemma points of

B.3. H in

When

[0,

 2

]

0are =0,44,

H ()

and

 2

 .

0,

and when

 4







 2

,

H ()



0.

Besides,

all zero

Proof. From (135), h3( + ) = h1() - cos  + sin . Thus,

H() = 2h1() - cos  + sin 

= cos  - sin  + 2  sin  -  -  cos  2

2 = cos  +

2 - 1

sin 



=

2 (cos 

+

sin )

-

sin .



(164) (165) (166) (167)

18

Under review as a conference paper at ICLR 2018

When

0







 4

,

since

sin 

is

a

concave

function

for

,

we

know

that

sin 



sin

 4



4



=

 22
. 

(168)

Thus,

H ()

=

2 (cos 

+

sin )

-

sin





 2 2  - sin 



 0.



To

make

H ()

=

0,

we

must

have

sin 

=

22 ,

so



=

0

or



=

 4

.

Besides,

when

 4

<





 2

,

note

that

 H(
2

-

)

+

H ()

=

2h1()

-

cos 

+

sin 

+

 2h1( 2

-

)

-

 cos(
2

-

)

+

 sin(
2

-

)

=2

h1()

+

 h1( 2

-

)

-

cos



+

sin



-

 cos(

-

)

+

 sin(

-

)

22

= 0.

(169) (170) (171)
(172) (173) (174) (175) (176)

Thus,

H ()

=

-H

(

 2

-

)



0.

And

to

make

H ()

=

0,

the

only

possibility

is



=

 2

,

which

ends

the proof.

Remember that if m(w1) = m(w2) = 0, then we have h3(2) = -h1(1).

If

h1(1)

>

0,

i.e.,

0



1

<

 4

,

then

from

lemma

B.3,

H (1 )



0,

which

means

that

h3(1 + )  -h1(1).

(177)

Since h3 is a strictly increasing function, we know that if h3(2) = -h1(1), then 2  1 + , so sin(1 - 2)  0, and that means

1

m(w1)

=

( 

-

w1,w2 ) sin(1

-

2) + h1(1)

>

0+0

=

0.

(178)

Similarly,

if

h1(1)

<

0,

i.e.,

 4

<

1



 2

,

then

from

lemma

B.3,

H (1 )



0,

which

means

that

h3(1 + )  -h1(1).

(179)

Thus, if h3(2) = -h1(1), then 2  1 + , so sin(1 - 2)  0, and that means

1

m(w1)

=

( 

-

w1,w2 ) sin(1

-

2) + h1(1)

<

0+0

=

0.

(180)

The

last

possibility

is

h1(1)

=

0,

i.e.,

1

=

 4

.

Plugging

it

into

(162)

and

we

know

that

h3(2)

=

0,

so

2

=

5 4

.

And

that

is

indeed

a

critical

point.

In

a

word,

the

only

critical

point

in

this

case

is

(1,

2)

=

(

 4

,

5 4

).

B.8

 2



1







2



3 2

Like previous cases,

m(w1)

=

1 (


-

w1,w2 ) sin(1

-

2)

+

h2(1)

(181)

19

Under review as a conference paper at ICLR 2018

and

m(w2)

=

1 (


-

w1,w2 ) sin(2

-

1)

+

h3(2).

If m(w1) = m(w2) = 0, then m(w1) + m(w2) = 0, i.e.,

h2(1) + h3(2) = 0.

Let  = 2 - , then from (111) and (135), we know that

h3(2) = h3( + )

= h1( ) - cos  + sin 



= h2(

+

) + sin  . 2

Thus, from lemma B.2,

h2(1)

+

h3(2)

=

h2(1)

+

h2(2

-

 2

)

+

sin(2

-

)

 -1 - 1 + 1 22

= 0.

(182)
(183)
(184) (185) (186)
(187) (188) (189)

Therefore, in order to achieve h2(1) + h3(2) = 0, the only way is let (188) becomes equality,

which

means

that

2

=

3 2

and

1

=

 2

or

.

Plugging

them

into

(181)

and

(182)

we

conclude

that

both of them are not critical points.

In a word, there is no critical point in this case.

B.9

 2



1



,

3 2



2

<

2

Similar to previous cases,

1

m(w1)

=

( 

-

w1,w2 ) sin(1

-

2)

+

h2(1)

(190)

and

1

m(w2)

=

( 

-

w1,w2 ) sin(2

-

1)

+

cos 2

-

sin 2

(191)

1 5

+ 

(2 - 2) sin 2 -

2 - 2 cos 2 .

(192)

From

 2



1





and

3 2



2



2

we

know

that

w1 ,w2



 2

,

so

1  ( - w1,w2 ) sin(1 - 2)



 2


·1

=

1 .

2

(193)

When

1 

(

-

w1,w2 ) sin(1

-

2)

=

1 2

,

we

must

have

w1 ,w2

=

 2

,

so

it

must

be

true

that

(1,

2)

=

(,

3 2

).

However,

when

(1,

2)

=

(,

3 2

),

we

have

1 

(

-

w1,w2 )

sin(1

-

2)

=

-

1 2

.

Thus,

1 (


-

w1,w2 ) sin(1

-

2)

<

1 .
2

(194)

Therefore, using lemma B.2,

m(w1)

<

1 2

+ (- 1 ) 2

=

0.

In a word, there is no critical point in this case.

(195)

B.10 CONCLUSION

In conclusion, based on the assumption that 1  2 there are four critical points in the 2D case:

(1,

2)

=

(0,

 2

),

(

 4

,

 4

),

(

 4

,

5 4

)

and

(

5 4

,

5 4

).

20

Under review as a conference paper at ICLR 2018

C HESSIAN FOR 2D CASES

There

are

4

critical

points:

(

 4

,

 4

),

(

 4

,

5 4

),

(

5 4

,

5 4

),

(0,

 2

).

Obviously,

the

point

(0,

 2

)

is

a

global

minima. Next we want to compute the Hessian on other 3 points.

Assume the manifold is R = {(w1, w2) : w1 2 = w2 2 = 1}, then the Hessian on the manifold is

zT

2Rf z

=

zT

2f

z

-

(w1T

f w1

)

z1

2

-

(w2T

f w2

)

z2

2

=

z1T

2f w1w1T

z1

+

z2T

2f w2w2T

z2

+

2z1T

2f w1w2T

z2

-

(w1T

f w1

)

z1

2

-

(w2T

f w2

)

z2

2

(196) (197) (198)

where z = (z1, z2) satisfies w1T z1 = 0, w2T z2 = 0. Next, we compute each term in Hessian. Since

f w1

=

w1

+

1 

||w2

||

w1 ||w1||

sin w1,w2

+

1  ( - w1,w2 )w2

-

1 

w1 ||w1||

sin w1,w1

-

1 (


-

w1,w1 )w1

-

1 

w1 ||w1||

sin w1,w2

-

1 (


-

w1,w2 )w2.

(199) (200) (201)

and

f w2

=

w2

+

1 

||w1

||

w2 ||w2||

sin w1,w2

+

1  ( - w1,w2 )w1

-

1 

w2 ||w2||

sin w2,w1

-

1 (


-

w2,w1 )w1

-

1 

w2 ||w2||

sin w2,w2

-

1 (


-

w2,w2 )w2.

21

(202) (203) (204)

Under review as a conference paper at ICLR 2018

Then we can get when w1 = w2 and w1 = -w2,

2f w1w1T

= I + ||w2|| 

sin w1,w2 w1

I

-

sin w1,w2 w1 3

w1w1T

- cos w1,w2 w1

1

1-(

w1T w2 w1 w2

)2

w1w2T w1 w2

-

w1T w2 w1 3 w2

w1w1T

+ 

1

1-(

w1T w2 w1 w2

)2

w2w2T w1 w2

-

w1T w2 w1 3 w2

w2w1T

-1 

sin w1,w1 w1

I

-

sin w1,w1 w1 3

w1w1T

- cos w1,w1 w1

1

1-(

w1T w1 w1 w1

)2

w1w1T w1 w1

-

w1T w1 w1 3 w1

w1w1T

- 

1

1 - ( w1T w1
w1 w1

)2

w1w1T w1 w1

-

w1T w1 w1 3 w1

w1w1T

-1 

sin w1,w2 w1

I

-

sin w1,w2 w1 3

w1w1T

- cos w1,w2 w1

1

1-(

w1T w2 w1 w2

)2

w1w2T w1 w2

-

w1T w2 w1 3 w2

w1w1T

- 

1

1 - ( w1T w2
w1 w2

)2

w2w2T w1 w2

-

w1T w2 w1 3 w2

w2w1T

Using the fact that w1T z1 = 0,

Similarly,

z1T

2f w1w1T

z1

=

1+

sin w1,w2 

+



1 1 - w1T w2

2

z1T w2

2

- sin w1,w1 - 

1 1 - w1T w1

2

z1T w1

2

- sin w1,w2 - 

1 1 - w1T w2

2

z1T w2

2

z2T

2f w2w2T

z2

=

1+

sin w1,w2 

+



1 1 - w1T w2

2

z2T w1

2

- sin w2,w1 - 

1 1 - w2T w1

2

z2T w1

2

- sin w2,w2 - 

1 1 - w2T w2

2

z2T w2

2

22

Under review as a conference paper at ICLR 2018

Next,

2f w1w2T

= sin w1,w2  w1 w2

w1w2T

- w2 cos w1,w2  w1

1

1-(

w1T w2 w1 w2

)2

1 w1 w2

w1w1T -

w1T w2 w1 w2

3 w1w2T

+ 

1

1-(

w1T w2 w1 w2

)2

1 w1 w2

w2w1T -

w1T w2 w1 w2

3 w2w2T

1

+

( 

-

w1,w2 )I

and

z1T

2f w1w2T

z2

=



1 1 - w1T w2

2

z1T

w2w1T z2

+

1 (


-

w1,w2 )z1T z2

In conclusion,

zT 2Rf z =



1 1 - w1T w2

2

z1T w2

2- 

1 1 - w1T w1

2

z1T w1

2- 

1 1 - w1T w2

2

z1T w2

2

+ 

1 1 - w1T w2

2

z2T w1

2- 

1 1 - w2T w1

2

z2T w1

2- 

1 1 - w2T w2

2

z2T w2

2

+



2 1 - w1T w2

2 z1T w2w1T z2

+

2 (


-

w1,w2 )z1T z2

-

1 (


-

w1,w2 )w1T w2

-

1 (


-

w1,w1 )w1T w1

-

1 (


-

w1,w2 )w1T w2

-

1 (


-

w1,w2 )w2T w1

-

1 (


-

w2,w1 )w2T w1

-

1 (


-

w2,w2 )w2T w2

.

When w1 = w2 or w1 = -w2, we should consider the limit of the Hessian.

First, let's compute the limit of some functions that we will use later. For simplicity, we just consider the case when w1  w2. The case w1  -w2 will be the same.

Claim: lim
w2 w1

z1T w2 2

=0

2

1- w1T w2

Proof: WLOG, we assume w1 = (1, 0), w2 = (cos , sin ),   0. Otherwise, we can do a rotation which doesn't affect the inner product. Since z1T w1 = 0 , z1 = (0, 1). Then

lim
w2 w1

z1T w2 2

= lim  sin2 

1 - w1T w2 2 0 1 - cos2 

= lim | sin |
0

=0

Similarly, we have the following claims.

Claim: lim
w2 w1

z2T w1 2

=0

2

1- w1T w2

23

Under review as a conference paper at ICLR 2018

Claim: lim = 0z1T w2w1T z2

w2 w1

2
1- w1T w2

Using these claims, we can computet the Hessian when w1 = w2.

lim
w2 w1

z1T

2f w1w1T

z1

=1-

sin w1,w1 

-



1 1 - w1T w1

2

z1T w1

2

- sin w1,w2 - 

1 1 - w1T w2

2

z1T w2

2

lim
w2 w1

z2T

2f w2w2T

z2

=1-

sin w1,w1 

-



1 1 - w1T w1

2

z1T w1

2

- sin w1,w2 - 

1 1 - w1T w2

2

z1T w2

2

lim
w2 w1

z1T

2f w1w2T

z2

=

z1T z2

Now, we can compute the hessian on critical points. For simplicity we just consider the case that k = 1.

C.1

(

 4

,

 4

)

On the direction z = (z1, z2) =

  

2 2

,

-

2 2

,

2 2

,

-

2 2

,



w1T

f w1

=

2-

2  

-

32 4 

w2T

f w2

=

2-

2-3 2 4

So

zT

R2 f z

=

zT

2f z

-

(w1T

f w1

)

z1

2

-

(w2T

f w2

)

z2

2



=

z1Tw12fw1T

z1

+

z2T

2f w2w2T

z2

+

2z1T

2f w1w2T

z2

-

4

+

2 

2

+

3

2

2

3 =

2-2

2 >0

2

  

On the direction z = (z1, z2) =

2 2

,

-

2 2

,

-

2 2

,

2 2

,

zT

R2 f z

=

zT

2f z

-

(w1T

f w1

)

z1

2

-

(w2T

f w2

)

z2

2



=

z1T

2f w1w1T

z1

+

z2T 

2f w2w2T

z2

+ 2z1T 

2f w1w2T

z2

-

4

+

2 

2

+

3

2

2

=1- 2

2 +1- 2

2 -2-4+ 2

23 +

2

 



2

3 =

2-2

2 -4<0

2

So this point is a saddle point.

24

Under review as a conference paper at ICLR 2018

C.2

(

5 4

,

5 4

)

On the direction z = (z1, z2) =

  

2 2

,

-

2 2

,

2 2

,

-

2 2

,



w1T

f w1

=

2-

2- 

2 4



w2T

f w2

=

2-

2- 

2 4

zT

2Rf z

=

zT

2f z

-

(w1T

f w1

)

z1

2

-

(w2T

f w2

)

z2

2



=

z1T

2f w1w1T

z1

+

z2T 



2f w2w2T

z2

+

2z1T

2f w1w2T

z2

-

4

+

22 

+

2 2

=1+1+2-4+ 2

2 +

2



2

22 2

= + >0

2

  

On the direction z = (z1, z2) =

2 2

,

-

2 2

,

-

2 2

,

2 2

,



zT 2Rf z

=

z1T

2f w1w1T

z1

+

z2T 

2f w2w2T

z2

+ 2z1T 

2f w1w2T

z2

-

4

+

22 

+

2 2

=1- 2

2 +1- 2

2 -2-4+ 2

2 +

2

 



2

= 2 - 2 2 -4<0

2

So this point is a saddle point.

C.3

(

 4

,

5 4

)

On the direction z = (z1, z2) =

  

2 2

,

-

2 2

,

2 2

,

-

2 2

,



w1T

f w1

=

1-

2-3 2 4



w2T

f w2

=

1-

2- 

2 4

zT

2Rf z

= = =

zT

2f

z

-

(w1T

f w1

)

z1

2 - (w2T

z1T

2f w1w1T

1+1+2-

z1 +2z2T 2w22fw2T 2+ + 2

z2

+

f )
w2

z2

2

2z1T

2f w1w2T

z2

-

2

+

 22


+

 2

 22





=2+ + 2>0



  

On the direction z = (z1, z2) =

2 2

,

-

2 2

,

-

2 2

,

2 2

,

25

Under review as a conference paper at ICLR 2018

zT

R2 f z

= =

z1T

2f w1w1T

1- 2

2 +

z1

+

z2T 

2f w2w2T

z2

1- 2 2 -2-2+

+ 2z1T 
22

2f
w1w2T 

+2

z2

-

2

+

 22


+

 2

=

 2

 -2 2

-

2

 <0





So this point is a saddle point.

C.4 CONCLUSION In conclusion, we have four critical points: one is global maximal, the other three are saddle points.

D 3D CASES

D.1 WHY WE ONLY NEED 3 DIMENSION

Lemma D.1. If (w1, w2) is a critical point, then there exists a set of standard orthogonal basis (e1, e2, e3) such that e1 = w1, e2 = w2 and w1, w2 lies in span{e1, e2, e3}.

Proof. If (w1, w2) is a critical point, then

(I

-

w1w1T

)

f w1

=

0.

where matrix (I - w1w1T ) projects a vector onto the tangent space of w1. Since

(I - w1w1T )w1 = w1 - w1 = 0,

(205) (206)

we get

(I

-

w1w1T

f )
w1

=

1 (I


-

w1w1T )

( - w1,w2 )w2 - ( - w1,w1 )w1 - ( - w1,w2 )w2

,

(207) (208)

which means that ( - w1,w2 )w2 - ( - w1,w1 )w1 - ( - w1,w2 )w2 lies in the direction of w1. If w1,w2 = , i.e., w1 = -w2, then of course the four vectors have rank at most 3, so we can find the proper basis. If w1,w2 < , then we know that there exists a real number r such that

( - w1,w2 )w2 - ( - w1,w1 )w1 - ( - w1,w2 )w2 + r · w1 = 0.

(209)

Since w1,w2 < , we know that the four vectors w1, w2, w1 and w2 are linear dependent. Thus, they have rank at most 3 and we can find the proper basis.

D.2 SOME PROPERTIES OF CRITICAL POINTS

Next we will focus on the properties of critical points. Assume (w1, w2) is one of the critical points,

from e2 =

lemma w2 and

D.1 w1,

we can find a set of standard orthogonal basis (e1, w2 lies in span{e1, e2, e3}. Furthermore, assume w1

e2, e3) such that e1 = w11e1 + w12e2 +

= w1, w13e3

and w2 = w21e1 + w22e2 + w23e3, i.e., w1 = (w11, w12, w13) and w2 = (w21, w22, w23). Since

we have already found out all the critical points when w13 = w23 = 0, in the following we assume

w123 + w223 = 0.

Lemma D.2. w1,w2 < .

Pfsroroofmoufr.(t2hI0efr8)wwteh1,akwtn2(o=w-(,w-th1e,wnw21w),ww112=)-w-1(w+-2(,sow-1w,w2w11i),swwi21n)-wth2e(ldiei-rseicntwito1h,nwe2od)fiwrwe2c1tl.iioeWns eoinfhtwahv1ee. dHailroreewcatedivoyenrk,on(foww-1n,

26

Under review as a conference paper at ICLR 2018

w1,w1 )w1 - ( - w1,w2 )w2 lies in span{e1, e2}, so w1  span{e1, e2} and w2  span{e1, e2}. Thus, w13 = w23 = 0 and that contradicts with the assumption. In a word, w1,w2 < . Lemma D.3. w13  w23 = 0.

Proof. We have already known from (208) that ( - w1,w2 )w2 - ( - w1,w1 )w1 - ( - w1,w2 )w2 lies in the direction of w1. Writing it in each dimension and we know that there exists a real number r0 such that

( - w1,w2 )w21 - ( - w1,w1 ) = r0 · w11 ( - w1,w2 )w22 - ( - w1,w2 ) = r0 · w12
( - w1,w2 )w23 = r0 · w13.

(210) (211) (212)

From lemma D.2 we know that w1,w2 < , so we can define k = r0 .  - w1,w2

(213)

Then the equations become

w21

-

 

- -

w1 ,w1 w1 ,w2

= k · w11

w22

-

 

- -

w1 ,w2 w1 ,w2

= k · w12

w23 = k · w13.

(214)
(215) (216)

Similarly, we have

w11

-

 

- -

w2 ,w1 w1 ,w2

=k

· w21

w12

-

 

- -

w2 ,w2 w1 ,w2

=k

· w22

w13 = k · w23.

(217)
(218) (219)

Since w123 + w223 = 0, at least one of those two variables cannot be 0. WLOG, we assume that w13 = 0. If w23 = 0, then from (219) we know that w13 = 0, which contradicts the assumption.
Thus, w23 = 0, which means that w13  w23 = 0.

Lemma D.4. w13  w23 < 0.

Proof. Adapting from the proof of lemma D.3, we know that

w21 -

 - w1,w1  - w1,w2

= k · w11

w22 -

 - w1,w2  - w1,w2

= k · w12

w23 = k · w13

and

w11

-

 

- -

w2 ,w1 w1 ,w2

=k

· w21

w12

-

 

- -

w2 ,w2 w1 ,w2

=k

· w22

w13 = k · w23.

Furthermore, kk

= ·w23 w13
w13 w23

= 1, so k

=

1 k

.

(220) (221) (222)
(223) (224) (225)

27

Under review as a conference paper at ICLR 2018

From lemma D.2 we know that w1,w2 < , and from lemma D.3 we know that both w1 and w2 are

outside span{w1, w2}, so i, j

 [2], wi,wj

< .

Thus, i, j



[2],

 -wi ,wj  -w1 ,w2

> 0. Therefore, we

have

That

means

k

<

0,

so

w23 w13

>

0.

In a word, w13  w23 < 0.

w21 > k · w11 1
w11 > k w21.

(226) (227)

Lemma D.5.

arccos(-w11) arccos(-w21)

=

arccos(-w12) arccos(-w22)

=

- w23 . w13

(228)

Proof. Adapting from the proof of lemma D.4 and we know that

Similarly, we have

w -21

 -w1 ,w1  -w1 ,w2

w11

= w -22

 -w1 ,w2  -w1 ,w2

w12

= w23 = k. w13

w -11

 -w2 ,w1  -w1 ,w2

w21

= w -12

 -w2 ,w2  -w1 ,w2

w22

=

w13

=

1 .

w23 k

Taking the first component of (229) and (230) gives us

Thus,

w21

=

k

·

w11

+

 

- -

w1 ,w1 w1 ,w2

w21

=

k

·

w11

-

 k


- -

w2 ,w1 w1 ,w2

.

Similarly, we get

 - w1,w1 = -k.  - w2,w1

 - w1,w2 = -k.  - w2,w2 Since i, j  [2],  - wi,wj = arccos(-wij ), we know that

arccos(-w11) arccos(-w21)

=

arccos(-w12) arccos(-w22)

=

- w23 . w13

(229)
(230) (231) (232) (233) (234) (235)

For simplicity, based on D.5, we define k0 = -k, 1 =  - w2,w1 and 2 =  - w2,w2 . Then

 - w1,w1 = k01

(236)

 - w1,w2 = k02.

(237)

WLOG, assume k0  1, otherwise we can switch w1 and w2.

Thus,

w11 = - cos(k01) w12 = - cos(k02) w21 = - cos(1) w22 = - cos(2).

(238) (239) (240) (241)

28

Under review as a conference paper at ICLR 2018

Lemma D.6.

1 + 2



 2

.

Proof. Since 1 =  - w2,w1 and 2 =  - w2,w2 , we know that 1, 2  [0, ]. Besides,

w121 + w122 = 1 - w123  1 w221 + w222 = 1 - w223  1.

(242) (243)

Thus,

cos2(k01) + cos2(k02)  1 cos2(1) + cos2(2)  1.

(244) (245)

If

one

of

1

and

2

is

larger

than

 2

,

say

1

>

 2

,

then

of

course

1

+

2



 2

.

If

1, 2



[0,

 2

],

then

sin2

 2

-

1

= cos2(1)  1 - cos2(2) = sin2(2),

(246)

so

 2

-

1



2,

which

means

that

1

+

2



 2

.

In

a

word,

1

+ 2



 2

.

Lemma D.7. 1  k0  3.

Proof. First we prove that k0 

1

and

2

is

no

less

than

 4

,

say

contradiction. Thus, k0  4.

4: 1

From



 4

.

lemma D.6, we If k0 > 4, then

know

that

1

+

2



 2

 - w1,w1 = k01 >

, so at least one of , which makes a

Furthermore,

if

3

<

k0



4,

then

1,

2



[0,

 3

]

because

k01,

k02



[0,

].

If

1, 2



[0,

 4

),

then

1

+ 2

<

 2

which

contradicts

lemma

D.6.

If 1, 2



[

 4

,

 3

],

then

k01

,

k02



(

3 4

,

],

which

means

that

cos2(k01)+cos2(k0

2)

>

1 2

+

1 2

=

1

and contradicts (244).

If

1



 4



2

and k01

<

 2

,

then

1

<

 2k0

<

 6

,

so

from

lemma

D.6,

2



 2

- 1

>

 3

,

which

contradicts k02  .

If 1



 4



2

and

k01



 2

,

then

k01, k02



[

 2

,

].

Since

cos2(k01) + cos2(k02)



1,

we

know that

sin2

k01

-

 2

= cos2(k01)  1 - cos2(k02) = sin2( - k02),

(247)

so

k01 -

 2



 - k02,

which

means

that k01 + k02



3 2

.

Thus, 1 + 2

<

 2

,

which

contradicts

lemma D.6.

In a word, 1  k0  3.

Lemma D.8. Define

F () =

-k0

,

k0 cos(k0) + cos()

then

F (1)

=

F

(2)(1,

2



[0,

 k0

]).

(248)

Proof.

Since k01, k02

 [0, ], we know that 1, 2



[0,

 k0

].

From (229), applying the change of variables on the first component and we get

- cos 1

-

k0 1  -w1 ,w2

- cos(k01)

= -k0.

Thus,



-

w1 ,w2

=

k0

-k01 cos(k01) +

cos(1)

=

F (1).

(249) (250)

29

Under review as a conference paper at ICLR 2018

Similarly, if we apply the change of variables onto the second component of (229), we will get



-

w1 ,w2

=

k0

-k02 cos(k02) +

cos(2)

=

F (2).

(251)

Thus,

 F (1) = F (2)(1, 2  [0, k0 ]).

(252)

Lemma D.9.

0



[

 2k0

,

3 4k0

),

s.t.,

 <0  F () = = 

 >0

0   < 0

 = 0

0

<





 k0

.

(253)

Proof.

Note that when 



[0,

 k0

],

-k0

is

always

non-positive.

Define G()

=

k0 cos(k0) +

cos(), then G() is a strict decreasing function w.r.t. . Note that G(0) = k0 + 1 > 0 and

G

 k0

= cos

 k0

-

k0

<

0,

so

there

must

be

an

0



(0,

 k0

)

such

that

G(0)

=

0.

Thus,

when

0





<

0,

G()

>

0,

and

when

0



 k0

,

G()

<

0.

Thus,

 <0 

0   < 0

F () = = 

 = 0 .

 >0

0

<





 k0

(254)

Then

the

only

thing

we

need

to

prove

is

 2k0



0

<

3 4k0

.

Note

that



G( ) = cos

0

2k0 2k0



(255)

3 G( ) = cos

3

- k0 

2-

2 = 0.

(256)

4k0 4k0 2 2 2



Since the inequality (256) holds only when cos

3 4k0

=

2 2

and

k0 2

=

2 2

,

which

means

k0

=

3

and k0 = 1, which makes a contradiction. Thus,

3 G( ) < 0.
4k0

(257)

Therefore,

 2k0



0

<

3 4k0

,

which

completes

the

proof.

Lemma D.10.

F () is either strictly decreasing or first decrease and then increase when 



(0,

 k0

].

Proof.

F

()

=

-

k0

(k0

cos(k0)

+ cos()) - (k0 cos(k0)

k0 -k02 sin(k0 + cos())2

)

-

sin 

(258)

=

-k0

k0

cos(k0) + cos  + k02 sin(k0) (k0 cos(k0) + cos())2

+



sin



.

(259)

Define H()

=

k0 cos(k0)+cos +k02 sin(k0)+ sin (



(0,

 k0

]),

then

H

()

·

F

()

<

0(i.e.,

when H() is positive, F () is decreasing, otherwise F () is increasing), and we know that

H () = -k02 sin(k0) - sin  + k03 cos(k0) + k02 sin(k0) +  cos  + sin  = k03 cos(k0) +  cos  = (k03 cos(k0) + cos )  (k0 cos(k0) + cos ) =  · G()

(260) (261) (262) (263) (264)

< 0. (265)

30

Under review as a conference paper at ICLR 2018

Note

that

(263)

holds

because



>

0



 2k0

.

Thus,

H ()

is

a

strictly

decreasing

function

when





(0,

 k0

].

We can see that

H(0) = G(0) + k020 sin(k00) + 0 sin 0 = k020 sin(k00) + 0 sin 0 > 0.

(266) (267)

Thus,

if

H(

 k0

)



decrease and then

0, then F () is increase when

monotonically





(0,

 k0

].

decreasing

when





(0,

 k0

].

Otherwise,

F

()

first

Lemma D.11.





(

3 4k0

,

 k0

],

F ()

<

F

3 4k0

.

Proof. From lemma D.10 we have already known that F () is either strictly decreasing or first

decrease

and

then

increase

when





(0,

 k0

],

so

the

maximum

of

the

function

value

on

an

interval

can

only

be

at

the

endpoints

of

that

interval,

which

means

that

we

only

need

to

prove

F

(

3 4k0

)

>

F

(

 k0

).

Note that

3  F( ) > F( )
4k0 k0

3



4

2 2

k0

-

cos

3 4k0



>

k0

- cos

 k0

3



4

2 2

k0

-

cos

3 4k0

1

>

k0

- cos

 k0



3 4

k0

-

cos

 k0

2 >
2

k0 - cos

3 4k0





3- 2 42

k0

>

3 4

cos

 k0

- cos

3 4k0

.

(268) (269) (270) (271) (272)

Let

h(x)

=

3 4

cos x

-

cos

3x 4

(x



[

 3

,

]),

then

3 3x h (x) = sin
44

- sin x

.

(273)

Thus, h(x)


is

decreasing

in

[

 3

,47

]

and

increasing

in

[

4 7

,

].

However,

we

know

that

h(

 3

)

=

3 8

-

2 2

<

0

and

h()

=

-

3 4

+

2 2

<

0,

so

h(x)

is

negative

when

x



[

 3

,

].

Therefore,



3- 2 42

k0

>

0

>

3 4

cos

 k0

-

cos

3 4k0

,

(274)

which

means

that

F

(

3 4k0

)

>

F

(

 k0

).

Thus,





(

3 4k0

,

 k0

],

F

()

<

F

3 4k0

.

Lemma D.12. 1 = 2.

Proof. From the proof of lemma D.8 we get

F (1) =  - w1,w2 = F (2).

Thus, F (1), F (2)  [0, ].

Using

lemma

D.9,

1, 2

>

0



 2k0

,

so

that

k01, k02



(

 2

,

].

(275)

31

Under review as a conference paper at ICLR 2018

From

(244),

we

know

that

k0(1 + 2)



3 2

,

which

means

that

at

least

one

of 1

and

2

are

less

than

or

equal

to

3 4k0

,

w.l.o.g.

we

assume

1



3 4k0

.

Note

that

lemma

D.11

tells

us

that

F

(

3 4k0

)

>

F

(

 k0

),

so

at

the

point



=

3 4k0

,

the

function

cannot

be increasing, which combining with lemma D.10 shows that F () is strictly decreasing when





(0,

3 4k0

].

If

2

>

3 4k0

,

then

we

know

that

F (1)



F

3 4k0

> F (2), which contradicts F (1) = F (2).

Thus,

1,

2(0,

3 4k0

].

Since

F ()

is

monotonically

decreasing

when





(0,

3 4k0

],

we

can

conclude

that 1 = 2.

D.3 NEGATIVE CURVATURE
First we compute the Hessian matrix: If z = (tz1, z2), ||z1|| = ||z2|| = 1 and w1T z1 = w2T z2 = 0, then

zT R2 f z =

(276)

t2



1 1 - w1T w2

2

z1T w2

2- 

1 1 - w1T w1

2

z1T w1

2- 

1 1 - w1T w2

2

z1T w2

2

(277)

+ 

1 1 - w1T w2

2

z2T w1

2- 

1 1 - w2T w1

2

z2T w1

2- 

1 1 - w2T w2

2

z2T w2

2

(278)

+t



2 1 - w1T w2

2 z1T w2w1T z2 +

2 

(

-

w1,w2 )z1T

z2

- t2

1 (


-

w1,w2 )w1T w2

-

1 (


-

w1,w1 )w1T w1

-

1 (


-

w1,w2 )w1T w2

-

1 (


-

w1,w2 )w2T w1

-

1 (


-

w2,w1 )w2T w1

-

1 (


-

w2,w2 )w2T w2

.

Lemma D.13. For every critical point (w1, w2) outside span{w1, w2},

1 (


-

w1,w2 )w1T w2

-

1 (


-

w1,w1 )w1T w1

-

1 (


-

w1,w2 )w1T w2

= -k0( - w1,w2 )

1 (


-

w1,w2 )w2T w1

-

1 (


-

w2,w1 )w2T w1

-

1 (


-

w2,w2 )w2T w2

1 = - k0 ( - w1,w2 ).

(279)
(280) (281)
(282) (283) (284) (285)

Proof. In lemma D.3, we have three equations, and we write them again for convenience:
( - w1,w2 )w21 - ( - w1,w1 ) = r0 · w11 ( - w1,w2 )w22 - ( - w1,w2 ) = r0 · w12
( - w1,w2 )w23 = r0 · w13.
Multiply 286 by w11, 287 by w12, 288 by w13, we get ( - w1,w2 )w21w11 - ( - w1,w1 )w11 = r0 · w121 ( - w1,w2 )w22w12 - ( - w1,w2 )w12 = r0 · w122 ( - w1,w2 )w23w13 = r0 · w123.

(286) (287) (288)
(289) (290) (291)

32

Under review as a conference paper at ICLR 2018

Combine these three equations, we know that

1 (


-

w1,w2 )w1T w2

-

1 (


-

w1,w1 )w1T w1

-

1 (


-

w1,w2 )w1T w2

= r0

=

(

-

w1,w2 )

w23 w13

= -k0( - w1,w2 ).

Similarly,

1 (


-

w1,w2 )w2T w1

-

1 (


-

w2,w1 )w2T w1

-

1 (


-

w2,w2 )w2T w2

=

(

-

w1,w2 )

w13 w23

1 = - k0 ( - w1,w2 ).

(292) (293) (294) (295)
(296) (297) (298)

Lemma D.14. For every critical point (w1, w2) outside span{w1, w2}, there is negative curvature.





Proof.

We select z1 = (-

2 2

,

2 2

,

0)

and

z2

=

(

2 2

,

-

2 2

,

0),

then

zT R2 f z = -

1- 1 - w121

1 +
1 - w221

k0

+

1 k0

-

2

From lemma D.7 we know that 1  k0  3.

( - w1,w2 ).

(299)

If 1  k0  2, then

zT

R2 f z



-2

+

1 2

·



<

0.

(300)

If

2

<

k0



3,

from

(244)

and

lemma

D.12

we

get

2 cos2(k01)



1,

so

k01



3 4

,

which

means

that

1



3 4k0

<

3 .
8

(301)

Thus,

|w11|

=

|

cos

1|

>

cos

3 8

.

(302)

Besides,

from

(245)

and

lemma

D.12

we

know

that

2 cos2

1



1,

so

1



 4

,

which

means

that

k01

>

2·

 4

=

 .
2

(303)

Using (301) and (303),

w11 = w12 = - cos(k01) > 0

(304)

w21 = w22 = - cos 1 < 0.

(305)

From lemma D.4, we conclude that

w1, w2 = w11 · w21 + w12 · w22 + w13 · w23 < 0,

(306)

which means that Thus,



w1 ,w2

>

. 2

(307)

zT R2 f z  -

1

-1+

1

-

cos2

3 8

=

 -42

-

1

+

2

3

3+ 1 -2 3

-  2

(308) (309)

< 0. (310)

In a word, for every critical point (w1, w2) outside span{w1, w2}, there is negative curvature.

33

Under review as a conference paper at ICLR 2018

E 2D CASES WITH ASSUMPTION RELAXATION

Since this section is pretty similar to B, I will try my best to make it brief and point out the most important things in the proof.

E.1 PRELIMINARIES

After the changing of variables(i.e., polar coordinates), we know that w1 = (cos 1, sin 1) and w2 =

(cos 2, sin 2).

And

the

manifold

gradient(expressed

by

m)

are

m(w1)

=

sin

1

f  w11

-

cos

1

f  w12

and

m(w2)

=

sin

2

f  w21

-

cos

2

f  w22

.

Applying the changing of variables and multiply it by , we get

m(w1) = ( - w1,w2 ) sin(1 - 2) + ( - w1,w2 ) sin( - 1) - ( - w1,w1 ) sin 1. (311) And

m(w2) = ( - w1,w2 ) sin(2 - 1) + ( - w2,w2 ) sin( - 2) - ( - w2,w1 ) sin 2. (312) Define(where w = (cos , sin ))

h() = ( - w,w2 ) sin( - ) - ( - w,w1 ) sin .

(313)

Then when  is in the first part to the fourth part, the function h will change to four different functions:

h1() = ( -  + ) sin( - ) - ( - ) sin 

(314)

h2() = ( -  + ) sin( - ) - ( - ) sin 

(315)

h3() = ( -  + ) sin( - ) - ( - ) sin 

(316)

h4() = ( -  - ) sin( - ) - ( - ) sin . WLOG, we assume 1  2.

(317)

E.2 0  1  2  

First, it's easy to verify that   [0, ], h1() + h1( - ) = 0.

Besides,

h1() = sin  + sin( - ) - ( - ) cos  - ( -  + ) cos( - )

(318)

=

2

sin



cos(

-

 )

-

(

-

)

cos



-

(

-



+

)

cos(

-

)

22

(319)

 2 sin  -  (cos  + cos( - )) 22

(320)

= 2 sin  -  cos  cos( -  )

(321)

22 2

 2 sin  -  cos  22

< 0. (322)

When m(w1) = m(w2) = 0, we know that h1(1)+h1(2) = 0, and because of those two properties

above,

we

know

that

1

+

2

=

.

Thus,

1



[0,

 2

].

And

we

have

the

following

lemma

Lemma E.1. m(w1)  0.

Proof.

m(w1) = sin( - 21)( -  + 21) - ( -  + 1) sin( - 1) + ( - 1) sin 1

 sin( - 21)( -  + 1) - ( -  + 1) sin( - 1) + ( - 1) sin 1



sin(

-

21)(

-



+

1)

-

(

-



+

1)

sin(

-

1)

+

(

-

 ) sin
2

1

=

(

-



+

1)(sin(

-

21)

-

sin(

-

1))

+

(

-

 )
2

sin

1



(

-

 )(sin(
2

-

21)

-

sin(

-

1)

+

sin

1)

=

(

-

 )(sin(
2

-

21)

-

sin

1

-

sin

1

cos(

-

21)

-

cos 1

sin(

-

21))

 0.

(323) (324) (325)
(326)
(327)
(328) (329)

34

Under review as a conference paper at ICLR 2018

Thus, the only possible critical points are m(w1)

=

0, which are 0 and

 2

.

After verification,

we conclude that there are only two critical points in this case: (1, 2) = (0, ) or (1, 2) =

(

 2

,

 2

).

E.3   1  2  

When m(w1) = m(w2) = 0, we know that h1(1) + h1(2) = 0. However, when   [, ], we know that

h2() = ( -  + ) sin( - ) - ( - ) sin 

 0. (330)

The inequality cannot become equal because the possible values of s such that each term equals zero has no intersection. Thus, h2() is always negative, which means that in this case there are no critical points.

E.4   1  2   + 

It's easy to verify that   [,  + ], h3() + h3(2 +  - ) = 0. Furthermore,

h3() = - sin( - ) - cos( - )( +  - ) - sin  - ( - ) cos 

=

-2

sin



cos(

-

 )

-

(

-

)

cos



-

(

+



-

)

cos(

-

)

22

> 0.

(331) (332) (333)

Thus, from m(w1) = m(w2) = 0, we know that h1(1) + h1(2) = 0 we get 1 + 2 = 2 + ,

which

means

that

1



[, 

+

 2

],

so

we

can

prove

the

following

lemma:

Lemma E.2. m(w1)  0.

Proof. Let  = 1 - , then

m(w1) = ( - 2 + 1) sin(1 - 2) + h3(1)

(334)

= ( +  -  +  ) sin(2 - ) + h1( ) +  sin  -  sin( -  )

(335)

 ( + 2 - ) sin(2 - ) + sin( - 2 )( + 2 - ) + (sin  - sin( -  )) (336)

 (sin  - cos  )  0.

(337) (338)

The first inequality is from lemma E.1.

Thus,

the

only

possible

critical

points

are

m(w1)

=

0,

which

are



and



+

 2

.

After

verification,

we conclude that there are only two critical points in this case: (1, 2) = (,  + ) or (1, 2) =

(

+

 2

,



+

 2

).

35

