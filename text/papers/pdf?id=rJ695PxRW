Under review as a conference paper at ICLR 2018
DISCOVERING ORDER IN UNORDERED DATASETS: GENERATIVE MARKOV NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
The assumption that data samples are independently identically distributed is the backbone of many learning algorithms. Nevertheless, datasets often exhibit rich structures in practice, and we argue that there exist some unknown orders within the data instances. Aiming to find such orders, we introduce a novel Generative Markov Network (GMN) which we use to extract the order of data instances automatically. Specifically, we assume that the instances are sampled from a Markov chain. Our goal is to learn the transitional operator of the chain as well as the generation order by maximizing the generation probability under all possible data permutations. One of our key ideas is to use neural networks as a soft lookup table for approximating the possibly huge, but discrete transition matrix. This strategy allows us to amortize the space complexity with a single model and make the transitional operator generalizable to unseen instances. To ensure the learned Markov chain is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training. Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover orders among data instances and also perform comparably well to state-of-the-art methods on the one-shot recognition benchmark task.
1 INTRODUCTION
Recent advances in deep neural networks offer great potentials for machines to learn automatically without humans interventions. For instance, Convolutional Neural Networks (CNNs) (Krizhevsky et al., 2012) provided an automated way for learning image feature representations. Compared to hand-crafted ones such as SIFT and SURF, these hierarchical deep features demonstrate superior performance in recognition (Xu et al., 2015; Finn & Levine, 2017) and transfer learning (Glorot et al., 2011) problems. Another example would be learning to learn for automatic parameter estimation. Andrychowicz et al. (2016) proposed to update model parameters without any pre-defined update rule such as stochastic gradient descent (SGD) or ADAM (Kingma & Ba, 2014). Surprisingly, this update-rule-free framework showed better performance and faster convergence on both object recognition and image style transformation tasks. In our paper, we investigate the following novel question: given an unordered dataset where instances may be exhibiting some implicit order, can we order a dataset automatically according to this order? We argue that such order often exists even when we are dealing with the data that are naturally thought of as being i.i.d. sampled from a common though complex distribution. For example, let's consider a dataset consisting of the joint locations on the body of the same person taken on different days. The data i.i.d. assumption is justified since postures of a person took on different days are likely unrelated. However, we can arrange the data instances such that the joints follow an articulated motion or a set of motions in a way that makes each pose highly predictable given the previous ones. Although this arrangement depends on the person as ballerinas' poses might obey different dynamics than the poses of tennis players, the simultaneous inference on the pose dynamics can lead to a robust model that explains the correlations among joints. To put it differently, if we reshuffle the frames of a video clip, the data can now be modeled by an i.i.d. model. Nevertheless, reconstructing the order leads to an alternative model where transitions between the frames are easier to fit the links between the latent structures and observations. The ballerina's dancing, if sampled very sparsely, can be thought of as a reshuffled video sequence that needs to be reordered such that a temporal model can generate it.
1

Under review as a conference paper at ICLR 2018
One naive and obvious way to find the order in a dataset is to perform sorting based on a predefined distance metric; e.g., the Euclidean distance between image pixel values. However, the distance metrics have to be predefined differently and empirically according to distinct types/characteristics of the datasets at hand. A proper distance metric for one domain may not be a good one for other domains. For instance, p distance is a good measure for DNA/RNA sequences (Nei & Kumar, 2000) while it does not characterize the semantic distances between images. We argue that the key component of the ordering problem lies in the discovery of proper distance metric in an automatic and adaptive way. To approach this problem, we propose to learn a distance-metric-free model to discover the ordering in the dataset. More specifically, we model the data by treating them as if they were generated from a Markov chain. We propose to simultaneously train the transitional operator and find the best order by a joint optimization over the parameter space as well as all possible permutations. We term our model Generative Markov Networks (GMNs). One of the key ideas in the design of GMNs is to use neural networks as a soft lookup table to approximate the possibly huge but discrete transition matrix. This strategy allows GMNs to amortize the space complexity using a unified model. Furthermore, due to the differentiable property of neural networks, the transitional operator of GMNs can also generalize on unseen but similar data instances. As an additional contribution, to ensure the Markov chain learned by GMNs is ergodic, we propose a greedy batch-wise permutation scheme that allows fast training. One related task is one-shot recognition which has only one labeled data per category in the target domain. Most of the work in this area considered learning a specific distance metric (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017) or category-separation metric (Ravi & Larochelle, 2017; Finn et al., 2017) for the data. During the inference phase, they computed either the smallest distance or highest class prediction score between the support and query instances. Alternatively, from a generative modeling perspective, we can first generate the Markov chain for the support instances, then we fit the query instances into the Markov chain and decide the labels with the highest log-likelihood. Empirically, we evaluate the learned Markov chain by showing that GMNs are able to discover implicit orders among data instances and also perform comparably well to state-of-the-art methods on the benchmark one-shot recognition task.
2 RELATED WORK
The literature on deep generative models and stochastic sampling is abundant. Due to the space limit, we discuss the ones that are most relevant to our work. Deep Generative Models: We consider two classes of deep generative models based on ancestral sampling and iterative sampling, respectively. Variational Autoencoders (VAEs) (Kingma & Welling, 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) can be cast as ancestral sampling-based methods. In the inference phase, these approaches generated one sample from the model by performing a single inference pass from the underlying graphical models. As a comparison, methods based on iterative sampling performed multiple and iterative passes through all the variables in the corresponding graphical models. Usually, these methods involved simulating a Markov chain in the entire state space, and they aimed at improving quality of generated samples by mixing the underlying chain. Recent works on this line of research included (Bengio et al., 2013; 2014; Sohl-Dickstein et al., 2015; Bordes et al., 2017; Song et al., 2017). Our approach can be categorized as an iterative sampling-based model. However, it has three significant differences comparing to previous works. First, all the existing works assumed that training instances are i.i.d. sampled from the stationary distribution of a Markov chain. This assumption is risky since, often the case, it is hard to measure whether a Markov chain has mixed or not. On the contrary, we only assume that data instances are sampled from the chain, without expecting the chain has mixed. As we will see later, the stationarity assumption in previous works often prevents them from observing the implicit data relationships. Second, prior approaches were proposed based on the notion of denoising models. In other words, their goal was generating high-quality images; on the other hand, we aim at discovering orders in datasets. Third, to the best of our knowledge, all the existing works were implicit models in the sense that they only admitted efficient sampling
2

Under review as a conference paper at ICLR 2018

schemes. In contrast, the proposed GMN is an explicit model where besides an efficient sampling procedure, the model maintains a tractable likelihood function that can be computed efficiently. One-Shot Learning: Deep one-shot learning approaches could be divided into two categories: distance-metric-learning and categories-separation-metric-learning approaches. The former aimed at either learning a similarity measurement between instance pairs (Koch et al., 2015) or applying specific metric loss based on cosine distance (Vinyals et al., 2016)/ Euclidean distance (Snell et al., 2017). These methods referred to nonparametric classifiers and relied heavily upon human design. As a comparison, methods in the second category offered more generalities. Typically, this type of methods tackled the problem using a meta-learning framework to train parametric classifiers. Precisely, they considered two levels of learning: the first stage is to update base learners' parameters and the second stage is to update parameters for the meta learner. Recent works (Ravi & Larochelle, 2017; Kaiser et al., 2017; Finn et al., 2017) belonged to this category. The methods mentioned above viewed one-shot recognition as a discriminative task; on the contrary, we hold a generative perspective. Since we consider a Markov chain data generation assumption, we can directly decide the labels for query instances by fitting them into the Markov chain (or the orders we observe) generated from support instances. This generative nature significantly decreases the difficulty of training as we no longer rely on any designed metric. More details will be covered in Sec. 4.

3 GENERATIVE MARKOV NETWORKS

Lcheati{ns. iO}inu=r1gdoeanloitsetooujroitnratliyninregcodvatear

which are assumed being generated the unknown Markov chain as well

from an unknown Markov as the order of generation

process. Note that since the generation order is unknown, even if the true Markov chain was given,

it would still be computationally intractable to find the optimal order that best fits our data. To get

around of this intrinsic difficulty, as we will see in Sec. 3.2 , we propose a greedy algorithm to find

an order given the current estimation of the transitional operator.

We denote the underlying represents the index of the

data order to be a permutation instance that is generated at the

over [n]: t-th step

 of

= the

M{a(rtk)o}vtn=c1h,aiwn.heIrneoth(et)r

words, a Markov chain is formed as follows:

s(1) ! s(2) ! · · · ! s(n). We consider all the possible permutations  and arbitrary distribution over these permutations, which leads to a joint log-likelihood estimation problem:

X X Yn

max log
,



P() P({si}ni=1, ; )

= max log
,

P() P(1)(s(1)) T (s(t)|s(t 1); ) ,
 t=2

where P(1)(·) is the initial distribution of the Markov chain and T (s0|s; ) is the transitional op-

erator parametrized by model parameters . Note that the effect of the initial distribution P(1)(·)

diminishes with the increase of the data size n. Hence, without loss of generality, we assume P(1)(·) is uniform over all possible states, leading to the following optimization problem:
01

X Yn

max, log @

P() T (s(t)|s(t 1); )A ,

(1)

2(n)

t=2

where (n) is the set of all possible permutations over [n]. Unfortunately, direct optimization of (1) is computationally intractable. For each fixed  and P, the number of all possible permutations (i.e., |(n)|) is n!. To approximate this expensive function, we present an efficient greedy algorithm in Sec. 3.3.

3.1 PARAMETRIZED TRANSITIONAL OPERATOR VIA NEURAL NETWORKS In practice, when the state space is huge, often we cannot afford to maintain the tabular transition matrix directly, which takes up to O(d2) space, where d is the number of states in the chain. For example, if the state refers to a binary image I 2 {0, 1}p, the size of the state space is d = 2p

3

Under review as a conference paper at ICLR 2018

which is nearly infeasible to compute. Hence, before optimizing (1), we should first find a family of functions to parametrize the transitional operator T (·|·).
Being universal function approximators (Hornik et al., 1989), neural networks could be used to approximate the discrete structures which led to the recent success of deep reinforcement learning (Mnih et al., 2013). In our case, we utilize neural networks to approximate the discrete tabular transition matrix. The advantages are two-fold: first, it significantly reduces the space complexity by amortizing the space required by each separate state into a unified model. Since all the states share the same model as the transitional operator, there is no need to store the transition vector for each separate state explicitly. Second, neural networks allow better generalization for the transition probabilities across states. The reason is that, in most real-world applications, states, represented as feature vectors, are not independent from each other. As a result, the differentiable approximation to a discrete structure has the additional smoothness properties, which allows the transitional operator to have a good estimate even for the unseen states.
Let  be the parameters of the neural networks and we can define f(s, s0) = T (s0|s; ) : Rp  Rp ! [0, 1]
to be the transition function that takes two states s and s0 as inputs and returns the corresponding transition probability. Note that one can consider each discrete transitional operator as a lookup table; for example, we use s and s0 to locate the corresponding row and column of the table and read out its probability. From this perspective, the neural network works as a soft lookup table that outputs the transition probability given two states (features).

3.2 GREEDY APPROXIMATION OF THE OPTIMAL ORDER

As mentioned above, the direct evaluation of eq. (1) is computationally intractable given P and .

Here, we develop a coordinate ascent style training algorithm to optimize eq. (1) efficiently. The key

insight comes from the following observation: for each fixed , there exists a point mass distribution

over (n) that achieves the maximum value for eq. (1). More precisely,

01

!

X Yn

Yn

max, log @

P() T (s(t)|s(t 1); )A = max log

T (s(t)|s(t 1); )

2(n)

t=2

t=2

with

Xn

 = arg max

log T (s(t)|s(t 1); ).

2(n) t=2

We leave the proof in Supplementary. In other words, given each , the optimization problem over 

now reduces to finding the optimal permutation  that gives the maximum likelihood on generating

the data. However, without further assumption on the structure of the transitional operator, this is

still a hard problem which takes time O(n!). Instead, we propose a greedy algorithm to approximate

the optimal order, which takes time O(n2 log n). We list the pseudocode in Alg. 1.

At first, Alg. 1 enumerates all the possible states appearing in the first time step. For each of the following steps, it finds the next state by maximizing the transition probability at the current step, i.e., a local search to find the next state. The final approximate order is then defined to be the maximum of all these n orders. A naive implementation of this algorithm has time complexity O(n3). However, we can reduce it to O(n2 log n) by pre-computing T (si|sj; ), 8i, j 2 [n] and sorting them so that the maximum finding operation in line 5 can be done in constant time.

Given the approximate order ^, we then proceed to optimize the model parameter  by gradient based optimization. By now it should be clear that the whole algorithm is an instance of the famous coordinate ascent algorithm, where we alternatively optimize over the order  and the model parameters . Since both optimizations over  and  will not decrease the objective function, the algorithm is guaranteed to converge.

3.3 BATCH-WISE PERMUTATION TRAINING The O(n2 log n) computation to find the approximate order in Alg. 1 can be expensive when the size of the data is large. In this section we provide batch-wise permutation training to avoid this

4

Under review as a conference paper at ICLR 2018

Algorithm 1 Greedy Approximate Order

Input: 1: v

Input data {si}in=1 1

and transitional operator T (si|sj; )

2: for i = 1 to n do

3: i(1) i

4: for j = 2 to n do

5: 6: 7: 8:

veifnivdiif>(oPjr)vtn=t2hmleonagxTk62(s{ii((1j),.1.).,|si(ij(t)1;)})T (sk | si(j

1); )

9: ^ i

10: v vi

11: end if

12: end for

13: return ^

Algorithm 2 Optimization with Batch-Wise Permutation Training

Input: {si}ni=1, bo, b, t,

1: 2:

fIonritikal=ize1t(o0)1, {xdi(o0)}bi=1

3: if k  1(mod t) then

4: Sample {xi(k)}bi=1bo  {si}in=1

5: 6:

end{xifi(k)}bi=1 = {{xi(k)}bi=1bo , {xi(k 1)}bi=o 1}

7: 8:

Compute Compute

^ (k) r(k

u1s)inloggthPe(G{xrei}edbi=y1A; p(pkro1x)im) =ate@O rPdebtr=(2Allogg. 1T)

(x^ (k) (t) |x^ (k

1) (t

1); (k

1) )

9: (k) = (k 10: end for

1) +

r(k

1) log P({xi}bi=1; (k

1) )

issue. The idea is to partition the original training set into batches with size b and perform greedy approximate order on each batch. Assuming b  n is a constant, the effective time complexity becomes: O(b2 log b) · n/b = O(nb log b), which is linear in n. However, since training data are partitioned into chunks, the learned transitional operator is not guaranteed to have nonzero transition probabilities between different chunks of data. In other words, the learned transitional operator does not necessarily induce an ergodic Markov chain due to the isolated states. To avoid this problem, we propose a simple strategy to enforce some samples are overlapping between the consecutive batches. We show the pseudocode in Alg. 2. In Alg. 2, b means the batch size, is the learning rate and b0 < b is the number of overlap states between consecutive batches.

3.4 INTRODUCING STOCHASTIC LATENT VARIABLES VIA VARIATIONAL BAYES INFERENCE

In this section we give a detailed description on how to implement the transitional operator where

the state can be both discrete or continuous. At the first step, to prevent our GMNs from simply

memorizing all the training data and their transitions, we introduce stochastic latent variables z 2 Rz

via Variational Bayes Inference (Wainwright et al., 2008). The evidence lower bound (ELBO) of the

log likelihood for the transitional operator (i.e., log T (s0|s; )) becomes:

h i



log T (s0|s; )  EzQ(z|s; ) log P(s0|s, z; ) KL Q(z|s; ) || P(z) ,

(2)

where T (s0|s; ) has been replaced by a distribution P(s0|s, z; ) parametrized by , which allows

us to make the dependence of s on z. Moreover, KL is the KL-divergence, Q(z|s; ) is an encoder

function parametrized by that encodes latent code z given current state s, and P(z) is a fixed

prior which we take its form as Gaussian distribution N (0, I). We use reparametrized trick to

draw Q(z|s;

) from Gaussian N

µQ, (s),

2 Q,

(s)I

where µQ, (s) and

Q, (s) are learnable

functions.

Next, we consider two types of distribution family for P(s0|s, z; ): Bernoulli and Gaussian.

5

Under review as a conference paper at ICLR 2018

GMN: NN: GMN: NN: GMN: NN:
Figure 1: For MNIST, Horse, and MSR SenseCam datasets: the implicit order observed from GMN and the oder implied from Nearest Neighbor sorting.

If s 2 {0, 1}p (i.e., a binary image), we define log P(s0|s, z; ) as:







log P(s0|s, z; ) = s0 log g (s, z) + (1 s0) log 1 g (s, z) ,

hi

where is element-wise multiplication and g (s, z) : {0, 1}p, Rz ! [0, 1]p.

If s 2 Rp (i.e., a real-valued feature vector), we choose P(s0|s, z; ) to be fixed variance factored

Gaussian N

µP, (s, z),

2 P

I

, where µP,

(s, z)

:

Rp+z

!

Rp and

P is a fixed variance. We

simply choose P in all the experiments. log P(s0|s, z; ) can thus be defined as

log P(s0|s, z; ) =

2

1
2 P

||s0

where const. is not related to the optimization of .

µP, (s, z)||22 + const.,

For simplicity, we specify  = { [ }. Therefore, the model parameters update for  in (2) refers to the updates for and .

4 EXPERIMENTS
4.1 DISCOVERING ORDERS IN DATASETS We perform experiments on ordering data in three datasets: MNIST (LeCun et al., 1990), Horse (Borenstein & Ullman, 2002), and MSR SenseCam (Jojic et al., 2010). We also provide another experiment on Moving MNIST (Srivastava et al., 2015) in Supplementary. Among these datasets, MNIST, Horse, and MSR SenseCam do not have explicit orders. On the other hand, Moving MNIST can be seen as a collection of short video clips, and thus each sequence of frames has an explicit order. Due to the space limit, we only show partial ordering results. Please see Supplementary for the full version. <MNIST> MNIST (LeCun et al., 1990) is a well-studied dataset that contains 60,000 training examples. Each example is a digit image with size 28x28. We rescale the pixel values to [0, 1]. Note that since MNIST contains a large number of instances, we perform the ordering in a randomly sampled batch to demonstrate our results. <Horse> Horse dataset (Borenstein & Ullman, 2002) consists of 328 horse images collected from the Internet. Each horse is centered in a 30x40 image. For the preprocessing, the object-background segmentation is applied, and the binary pixel value is set to 1 and 0 for object and background, respectively. Examples are show in Supplementary. <MSR SenseCam> MSR SenseCam (Jojic et al., 2010) is a dataset consisting of images taken by SenseCam wearable camera. It contains 45 classes with approximately 150 images per class. Each image has size 480x640. We resize each image into 224x224 and extract the feature from VGG-19 network (Simonyan & Zisserman, 2014). In this dataset, we consider only office category which has 362 images.
4.1.1 IMPLICIT ORDERS IN DATASETS We apply Alg. 2 to train our Generative Markov Networks. When the training converges, we plot the images following permutation ^ in Alg. 1. Note that ^ can be seen as the implicit order suggested by GMNs. For comparison, we also plot the images following nearest neighbor sorting using

6

Under review as a conference paper at ICLR 2018

GMN: NN: GMN: NN: GMN: NN:
Figure 2: For MNIST, Horse, and MSR SenseCam datasets: data generation from the learned transitional operator in GMN and Nearest Neighbor search.

Euclidean distances. The parameters {boverlap, b, t} in Alg. 2 are {50, 500, 600}, {328, 328, 1}, and {362, 362, 1} for MNIST, Horse, and MSR SenseCam, respectively. Network architectures for parameterizing T (·|·; ) are specified in Supplementary. The results are shown in Fig. 1. We first observe that data following the order suggested by our proposed GMN have visually high autocorrelation. This result implies that our proposed GMN can discover nice implicit orders for the dataset. Comparing to the strong ordering baseline Nearest Neighbor sorting, one could hardly tell which one is better. Nevertheless, GMN is a distance-metricfree model which requires no predefined distance metric. Moreover, the implicit order suggested by GMN considers a generative modeling viewpoint: the order is the optimal permutation under the Markov chain data generation assumption (see Sec. 3.2).

4.1.2 TRANSITIONAL OPERATOR AS A GENERATIVE MODEL

Next, we examine the data generation using the learned transitional operator. Conditioned

on a given sample s, instead of sampling s0  T (s0|s; ) directly, we sample s0 =

aairmg smaatxdsis2c{o{vsei}rnii=n1g\sd}aTtas(ests|'s;or)d.erWs, ewmhialkeeotthhiesr

modification based iterative sampling

on the reason that our model models (Bengio et al., 2013;

2014; Sohl-Dickstein et al., 2015; Bordes et al., 2017; Song et al., 2017) intended to denoise gener-

ated samples. Similar to Sec. 4.1.1, we exploit nearest neighbor search using Euclidean distance for

comparison. distance.

More

precisely,

s0N N

=

arg maxs2{{si}in=1\s} d(s, s)

with

d(·, ·)

denoting

Euclidean

Fig. 2 illustrates the sampling of GMN and Nearest Neighbor search. We can see that Nearest Neighbor search is not able to perform efficient sampling since it would stick between two similar images. On the other hand, our proposed GMN can perform consecutive sampling. This tremendous difference implies the distinction between the discriminative (sampling by a fixed distance metric) and the generative (sampling through the transitional operator in a Markov chain) model.

4.2 ONE-SHOT RECOGNITION Now, we perform one-shot recognition task on the miniImageNet (Vinyals et al., 2016; Ravi & Larochelle, 2017), which is a benchmark dataset designed for the evaluation of few-shot learning (Vinyals et al., 2016; Ravi & Larochelle, 2017). Being a subset of ImageNet (Russakovsky et al., 2015), it contains 100 classes and each class has 600 images. Each image is downsampled to size 84x84. As suggested in (Ravi & Larochelle, 2017), the dataset is divided into three parts: 64 classes for training, 16 classes for validation, and 20 classes for testing. Identical to (Ravi & Larochelle, 2017), we consider the 5 way 1 shot problem. That is, from testing classes, we sample 5 classes with each class containing 1 labeled example. The labeled examples refer to support instances. Then, we randomly sample 500 unlabeled query examples in these 5 classes for evaluation. We repeat this procedure for 10, 000 times and report the average with 95% confidence intervals in Tbl. 1.

4.2.1 TRAINING DETAILS Instead of viewing one-shot recognition as a discriminative task, we hold it as a generative one. To achieve this goal, we train our Generative Markov Networks on training classes and then apply it to testing classes. More precisely, for each training episode, we sample 1 class from the training

7

Under review as a conference paper at ICLR 2018

Table 1: 5-way 1-shot recognition task for miniImageNet. The results are reported averagely in 10, 000 episodes with 95% confidence intervals.

Model Meta-Learner LSTM (Ravi & Larochelle, 2017) Model-Agnostic Meta-Learning (Finn et al., 2017) Meta Networks (Munkhdalai & Yu, 2017) Meta-SGD (Li et al., 2017) Temporal Convolutions Meta-Learning (Mishra et al., 2017) Nearest Neighbor with Cosine Distance Matching Networks FCE (Vinyals et al., 2016) Siamese (Koch et al., 2015) mAP-Direct Loss Minimization (Triantafillou et al., 2017) mAP-Structural Support Vector Machine (Triantafillou et al., 2017) Prototypical Networks (Snell et al., 2017) Attentive Recurrent Comparators (Shyam et al., 2017) Skip-Residual Pairwise Networks (Mehrotra & Dukkipati, 2017) Generative Markov Networks without fine-tuning (ours) Generative Markov Networks with fine-tuning (ours)

Basic/ Advanced Model Basic Basic
Advanced Basic
Advanced Basic Basic Basic Basic Basic Basic
Not Specified Advanced Basic Basic

Discriminative/ Generative Discriminative Discriminative Discriminative Discriminative Discriminative Discriminative Discriminative Discriminative Discriminative Discriminative Discriminative Discriminative Discriminative Generative Generative

Parametric/ Nonparametric Parametric Parametric Parametric Parametric Parametric
Nonparametric Nonparametric Nonparametric Nonparametric Nonparametric Nonparametric Nonparametric Nonparametric Nonparametric Nonparametric

Accuracy 43.44±0.77 48.70±1.84 49.21±0.96 50.47±1.87 55.71±0.99 41.08±0.70 43.56±0.84 48.42±0.79 41.64±0.78 47.89±0.78 49.42±0.78
49.1 55.2 45.36±0.94 48.87±1.10

classes and let {20, 100, 10}.

W{sei}cino=n1sibdeeral3l,t0h0e0dtartaainfrionmg etphiissocdleass.s.

Then,

we

apply

Alg.

2

with

{boverlap,

b,

t}

=

On the other hand, for each testing episode, we apply GMNs to generate a chain from each support instance:
s~c1  T (·|sc0; ) , s~2c  T (·|s~1c; ) , · · · , s~ck  T (·|s~ck 1; ), Mwhaerrkeovs0cchiasinth. e support instance belonging to class c and s~c is the generated samples from the

Next, we fit each query example into each chain by computing the average approximating loglikelihood. Namely, the probability for generating the query sample sq in the chain of class c is

P(sq|c) :=

1 k+1

 log

Xk T (sq|s0c; ) +

log

 T (sq|s~ci ; ) .

i=1

In a generative viewpoint, the predicted class c^ for sq is determined by

c^ = arg max P(sq|c).
c

(3) (4)

For fair comparisons, we use the same architecture specified in (Ravi & Larochelle, 2017) to extract 1600-dimensional features. We pretrain the architecture using standard softmax regression on image-label pairs in training and validation classes. The architecture consists of 4 blocks. Each block comprises a CNN layer with 64 3x3 convolutional filters, Batch Normalization (Ioffe & Szegedy, 2015) layer, ReLU activation, and 2x2 Max-Pooling layer. Then, we train our Generative Markov Networks based on these 1, 600 dimensional features. Network architecture for parameterizing T (·|·; ) is specified in Supplementary.

4.2.2 RESULTS For a comprehensive analysis, we also provide the variant of our GMN with fine-tuning. In other words, we fine-tune GMN by applying Alg. 2 with {boverlap, b, t} = {20, 100, 10} on support and query instances. Note that in eq. (3), k is chosen to be 1 and 5 for the non-fine-tuned and fine-tuned version, respectively. We compare our proposed method and the related approaches in Tbl. 1, in which Basic model refers the architecture in (Ravi & Larochelle, 2017) and Advanced models refer to more complicated designs. Generally, it is not fair to compare the methods using different models; therefore, we only discuss the methods using Basic model in the following. First, we observe that the performance of GMN is comparable to other works. For example, the best result of all methods is reported by Meta-SGD (Munkhdalai & Yu, 2017) with 50.47 ± 1.87. Although GMN suffers from slight performance drop, it requires a much less computational budget. The reason is that the meta-learning (parametric) approaches (Ravi & Larochelle, 2017; Finn et al., 2017; Munkhdalai & Yu, 2017; Li et al., 2017; Mishra et al., 2017) rely on huge networks to manage complicated intersections between meta and base learners, while parameters for GMN exist only in  which is a relatively tiny network. On the other hand, the best performance reported in the distance-metric learning (nonparametric) approaches is Prototypical Networks (Snell et al., 2017) with 49.42 ± 0.78. Sacrificing from little performance deterioration, our proposed GMN enjoys

8

Under review as a conference paper at ICLR 2018 more flexibility without the need of defining any distance metric as in (Vinyals et al., 2016; Koch et al., 2015; Triantafillou et al., 2017; Snell et al., 2017; Shyam et al., 2017; Mehrotra & Dukkipati, 2017). More importantly, except for our proposed GMN, all the works belong to discriminative models, which means they are optimized based on carefully chosen objectives for one-shot learning purpose. Next, our proposed GMN enjoys a significant improvement (45.36 ± 0.94 ! 48.87 ± 1.10) from fine-tuning over support and query instances. This result verifies that GMN is able to simulate the Markov chain data generation process since the query instances can be better fitted in the chains generated from the support instances.
5 CONCLUSION
In this paper, we argue that data i.i.d. assumption is not always the case in most of the datasets. Often, data instances are exhibiting some implicit orders which may benefit our understanding and analysis of the dataset. To observe the implicit orders, we propose a novel Generative Markov Network which considers a Markov chain data generation scheme. Specifically, we simultaneously learn the transitional operator as a generative model in the Markov chain as well as find the optimal orders of the data under all possible permutations. In lots of experiments, we show that our model is able to observe implicit orders from unordered datasets and also perform well on the one-shot recognition task.
9

Under review as a conference paper at ICLR 2018
REFERENCES
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pp. 3981­3989, 2016.
Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as generative models. In Advances in Neural Information Processing Systems, pp. 899­907, 2013.
Yoshua Bengio, Eric Laufer, Guillaume Alain, and Jason Yosinski. Deep generative stochastic networks trainable by backprop. In International Conference on Machine Learning, pp. 226­234, 2014.
Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion training. arXiv preprint arXiv:1703.06975, 2017.
Eran Borenstein and Shimon Ullman. Class-specific, top-down segmentation. In European conference on computer vision, pp. 109­122. Springer, 2002.
Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pp. 2786­2793. IEEE, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 513­520, 2011.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672­2680, 2014.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359­366, 1989.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448­456, 2015.
Nebojsa Jojic, Alessandro Perina, and Vittorio Murino. Structural epitome: a way to summarize ones visual experience. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 1027­1035. Curran Associates, Inc., 2010. URL http://papers.nips.cc/paper/ 4092-structural-epitome-a-way-to-summarize-ones-visual-experience. pdf.
Lukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events. arXiv preprint arXiv:1703.03129, 2017.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In ICML Deep Learning Workshop, volume 2, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097­1105, 2012.
10

Under review as a conference paper at ICLR 2018
Yann LeCun, Bernhard E Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne E Hubbard, and Lawrence D Jackel. Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems, pp. 396­404, 1990.
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few shot learning. arXiv preprint arXiv:1707.09835, 2017.
Akshay Mehrotra and Ambedkar Dukkipati. Generative adversarial residual pairwise networks for one shot learning. arXiv preprint arXiv:1703.08033, 2017.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. Meta-learning with temporal convolutions. arXiv preprint arXiv:1707.03141, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. arXiv preprint arXiv:1703.00837, 2017. Masatoshi Nei and Sudhir Kumar. Molecular evolution and phylogenetics. Oxford university press,
2000. Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. ICLR, 2017. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211­252, 2015. Pranav Shyam, Shubham Gupta, and Ambedkar Dukkipati. Attentive recurrent comparators. arXiv preprint arXiv:1703.00767, 2017. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175, 2017. Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015. Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. arXiv preprint arXiv:1706.07561, 2017. Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video representations using LSTMs. In ICML, 2015. Eleni Triantafillou, Richard Zemel, and Raquel Urtasun. Few-shot learning through an information retrieval lens. arXiv preprint arXiv:1707.02610, 2017. Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. In Advances in Neural Information Processing Systems, pp. 3630­3638, 2016. Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1­2):1­305, 2008. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, pp. 2048­2057, 2015.
11

Under review as a conference paper at ICLR 2018

SUPPLEMENTARY FOR DISCOVERING ORDER IN UNORDERED DATASETS: GENERATIVE MARKOV NETWORKS
Anonymous authors Paper under double-blind review

1 PROOF FOR SEC. 3.2

Here, we prove that

01

X Yn

max, log @

P() T (s(t)|s(t 1); )A = max log

2(n)

t=2

! Yn
T (s(t)|s(t 1); )
t=2

with

Xn

 = arg max

log T (s(t)|s(t 1); ).

2(n) t=2

Proof:

For any  over (n), we have:

X Yn

!

log P() T (s(t)|s(t 1); )  log

 t=2

= log

= log

X Yn

!

P() T (s(t)|s(t 1); )

 t=2

!

X Yn

I( = ) T (s(t)|s(t 1); )


Yn

t=2 !

T (s(t)|s(t 1); ) ,

t=2

where I( = ) is the indicator function that takes value 1 iff  =  otherwise 0. Realize that I( = ) also defines a valid distribution over (n), which proves our claim.

2 EXAMPLE IMAGES FOR HORSE DATASET

Figure 1: RGB images of Horse Dataset.
Figure 2: Pre-processed images of Horse Dataset. 1

Under review as a conference paper at ICLR 2018 Horse dataset Borenstein & Ullman (2002) consists of images collected from Internet. Fig. 1 illustrates these RGB images. Object-background segmentations are applied on these images and the horses are centered in 30x40 images. The processed images are shown in Fig. 2.
3 FULL ORDERING RESULTS FOR MNIST, HORSE, AND MSR SENSECAM
Fig. 3, 4, and 5 show the results of the implicit order observed from GMN the order implied from Nearest Neighbor sorting. On the other hand, Fig. 6, 7, and 8 illustrate the sampling of GMN and Nearest Neighbor search. (a)
(b) Figure 3: For MNIST dataset: (a) implicit order observed from GMNs (b) order implied from nearest neighbor sorting using Euclidean distance.
(a) (b)
Figure 4: For Horse dataset: (a) implicit order observed from GMNs (b) order implied from nearest neighbor sorting using Euclidean distance.
2

Under review as a conference paper at ICLR 2018
(a)
(b)
Figure 5: For office category in SenseCam dataset: (a) implicit order observed from GMNs (b) order implied from nearest neighbor sorting using Euclidean distance.
(a)
(b)
Figure 6: For MNIST dataset, data generation from (a) learned transition operator in GMNs (b) nearest neighbor search using Euclidean distance.
4 MOVING MNIST
<Moving MNIST> Moving MNIST (Srivastava et al., 2015) contains 10, 000 sequences each of length 20 showing 2 digits moving in a 64x64 frame. We rescale the pixel values to [0, 1]. For each training episode, we apply Alg. 2 to train GMN on one randomly chosen sequence with parameters {boverlap, b, t} set as {0, 20, 10}. We consider 6, 000 training episodes. For evaluation, we randomly sample a disjoint sequence from training sequences and observe the optimal permutation (implicit order) from Alg. 1. Fig. 9 illustrates the results for the implicit order observed from Generative Markov Networks, the order inferred from Nearest Neighbor sorting using Euclidean distance, and the suggested explicit order. We find that both the orders observed from GMN and NN sorting manifest smooth motions for two digits in the frame. It is worth noting that our proposed GMN enjoys the freedom of not defining any distance metric. The sampling results for Moving MNIST dataset are shown in Fig. 10. We consider two approaches: the proposed Generative Markov Networks and Nearest Neighbor search. We find that, by learning the transition operator in a Markov chain as a generative model, GMN performs much better sampling results than Nearest Neighbor search which is a discriminative model.
5 NETWORK ARCHITECTURES FOR TRANSITION OPERATOR
We elaborate the design of the transition operator in Fig. 11. In our design, U can be seen as a gating mechanism between input Xt and the learned update X~ . More precisely, the output can be written
3

Under review as a conference paper at ICLR 2018

(a)
(b)
Figure 7: For Horse dataset, data generation from (a) learned transition operator in GMNs (b) nearest neighbor search using Euclidean distance.
(a)

(b)

Figure 8: For office category in SenseCam dataset, data generation from (a) learned transition operator in GMNs (b) nearest neighbor search using Euclidean distance.

as

Xt+1 = U X~ + (1 U ) Xt,

(1)

where denotes element-wise product. We specify each function f in Tbl. 1, 2, 3, 4, and 5. Note that we omit the bias term for simplicity. We use ADAM (Kingma & Ba, 2014) with learning rate

0.001 and 0.2 dropout rate to train our T (·|·; ).

Table 1: Details of functions for MNIST experiments.

function f1 f21 f22 f3 f41 f42

details 784x512 FC layer with ReLU
512x128 FC layer 512x128 FC layer 912x512 FC layer with ReLU 512x784 FC layer with sigmoid 512x784 FC layer with sigmoid

REFERENCES
Eran Borenstein and Shimon Ullman. Class-specific, top-down segmentation. In European conference on computer vision, pp. 109­122. Springer, 2002.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video representations using LSTMs. In ICML, 2015.

4

Under review as a conference paper at ICLR 2018

(a)

(b)

(c)
Figure 9: For Moving MNIST dataset: (a) implicit order observed from GMNs (b) order implied from nearest neighbor sorting using Euclidean distance (c) suggested explicit order.

(a)

(b)

Figure 10: For Moving MNIST dataset, data generation from (a) learned transition operator in GMNs (b) nearest neighbor search using Euclidean distance.

Xt f1

#~&(0,1)

f21  f22 !

+

Z ./0.12

+
f3



5- U

+ Xt+1

f41 U
f42 34

Figure 11: Network design for T (·|·; ).

Table 2: Details of functions for Horse experiments.

function f1 f21 f22 f3 f41 f42

details 1200x512 FC layer with ReLU
512x128 FC layer 512x128 FC layer 912x512 FC layer with ReLU 512x1200 FC layer with sigmoid 512x1200 FC layer with sigmoid

Table 3: Details of functions for MSR SenseCam experiments.

function f1 f21 f22 f3 f41 f42

details 4096x1024 FC layer with ReLU
1024x256 FC layer 1024x256 FC layer 4352x1024 FC layer with ReLU 1024x4096 FC layer with sigmoid 1024x4096 FC layer

Table 4: Details of functions for Moving MNIST experiments.

function f1 f21 f22 f3 f41 f42

details 4096x1024 FC layer with ReLU
1024x256 FC layer 1024x256 FC layer 4352x1024 FC layer with ReLU 1024x4096 FC layer with sigmoid 1024x4096 FC layer with sigmoid

5

Under review as a conference paper at ICLR 2018

function f1 f21 f22 f3 f41 f42

Table 5: Details of functions for miniImageNet experiments.
details 1600x1024 FC layer with ReLU // 1024x512 FC layer with ReLU // 512x256 FC layer with ReLU
256x64 FC layer 256x64 FC layer 1664x256 FC layer with ReLU 256x512 FC layer with ReLU // 512x1024 FC layer with ReLU // 1024x1600 FC layer with sigmoid 256x512 FC layer with ReLU // 512x1024 FC layer with ReLU // 1024x1600 FC layer

6

