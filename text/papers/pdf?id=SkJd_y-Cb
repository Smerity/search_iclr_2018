Under review as a conference paper at ICLR 2018

W N :D R
Anonymous authors Paper under double-blind review

L

A
Word embeddings extract semantic features of words from large datasets of text. Most embedding methods rely on a log-bilinear model to predict the occurrence of a word in a context of other words. Here we propose word2net, a method that replaces their linear parametrization with neural networks. For each term in the vocabulary, word2net posits a neural network that takes the context as input and outputs a probability of occurrence. Further, word2net can use the hierarchical organization of its word networks to incorporate additional meta-data, such as syntactic features, into the embedding model. For example, we show how to share parameters across word networks to develop an embedding model that includes part-of-speech information. We study word2net with two datasets, a collection of Wikipedia articles and a corpus of U.S. Senate speeches. Quantitatively, we found that word2net outperforms popular embedding methods on predicting heldout words and that sharing parameters based on part of speech further boosts performance. Qualitatively, word2net learns interpretable semantic representations and, compared to vector-based methods, better incorporates syntactic information.

I

Word embeddings are an important statistical tool for analyzing language, processing large datasets of text to learn meaningful vector representations of the vocabulary (Bengio et al., 2003; 2006; Mikolov et al., 2013b; Pennington et al., 2014). Word embeddings rely on the distributional hypothesis, that words used in the same contexts tend to have similar meanings (Harris, 1954). More informally (but equally accurate), a word is defined by the company it keeps (Firth, 1957).

While there are many extensions and variants of embeddings, most rely on a log-bilinear model. This model posits that each term is associated with an embedding vector and a context vector. Given a corpus of text, these vectors are fit to maximize an objective function that involves the inner product of each observed word's embedding with the sum of the context vectors of its surrounding words. With useful ways to handle large vocabularies, such as negative sampling (Mikolov et al., 2013a) or Bernoulli embeddings (Rudolph et al., 2016), the word embedding objective resembles a bank of coupled linear binary classifiers.

Here we introduce word2net, a word embedding method that relaxes this linear assumption. Word2net still posits a context vector for each term, but it replaces each word vector with a term-specific neural network. This word network takes in the sum of the surrounding context vectors and outputs the occurrence probability of the word. The word2net objective involves the output of each word's network evaluated with its surrounding words as input. The word2net objective resembles a bank of coupled non-linear binary classifiers.

How does word2net build on classical word embeddings? The main difference is that the word networks can capture non-linear interaction effects between co-occurring words; this leads to a better model of language. Furthermore, the word networks enable us to share per-term parameters based on word-level meta-data, such as syntactic information. Here we study word2net models that share parameters based on part-of-speech ( ) tags, where the parameters of certain layers of each network are shared by all terms tagged with the same tag.

Figure 1a illustrates the intuition behind word2net. Consider the term

. The top of the

figure shows one observation of the word, i.e., one of the places in which it appears in the data. (This

excerpt is from U.S. Senate speeches.) From this observation, the word2net objective contains the

probability of a binary variable wn;

conditional on its context (i.e., the sum of the context

vectors of the surrounding words). This variable is whether

occurred at position n.

1

Under review as a conference paper at ICLR 2018
Under review as a conference paper at ICLR 2018

... were opposed to the increase of the circulation of ...

verb (v) noun (n)

decrease v

cut n

doubling v amount n

(a) increases v saving n

supply v

limit n

cost v

amounts n

estimates v decreases n

cut v

increases n

raising v decrease n

amount v declines n
(b) half adj amounting n

(c)
Figure 1: An illustration of deep Bernoulli embeddings. (top left) In word2net, each vocabulary

Figure 1: Anwthioelrlsduusvmtrioasftrietohpnerecosoefnnwtteexodtrbvdye2cnatoenrtse.u(rvaa0l)onIfentthwweoowrkrodrp2dasnraienmt,teheteearcciozhendtteexbrytmanvvd.ioTsuhtrpeeupwtsroetrhsdeennoetcetcwduorbrrekyntcaaeknpeesruoabrsaaiblninpliuetyttwork

wioncictbhulruwreee)nifcgrehotpmswstoroiofomibattrv.hisda`lgeas/bicrv.twioaewlrTiinngtothyhterqedtaueox/wetgfatwroiygvt(ro.hdegpIrne.arndie(trbtaeshnornefetgo)ttetero.waxtmtga.hTo.me(lhr(ebpfrkeofli)tegr)pimW,hnWrttehep)oroeduUrirqindtdscuc22iitlnensunsrgediettteehthseemcenaaarospebnssuhlrtelimaoynsrbicvsnoaheogrbafr,brpiwtsilnhoisegteruy/cacoacthnoenoeafnfisstayhnt"eedndatxntealtaathcdrycregveetemeariscstcoeatwis,oc"ntrorfswosoirsmhdrsaimila/nle(aladlrfotowitrcohoctnheru.derbTo/rltyeaahuntgettscephpmerusaa,toihtrosrsiesfthinsogtwhanne

entire layer (omroasntgseim) iblaertwreesuelntswaroerndosuwnsitshucthhaess"acmute." tag (

in this case). (c) The fitted word2net

with sharing can be queried for semantic similarities of the word networks for each word/tag pair.

In this example, we list the most similar networks to

/ and

/.

neural network that outputs the probability of that word (Figure 1a). If we are given the tags

of the words, we may use parameter sharing instead in order to form a per-word per-tag neural

network (Figure 1b). Finally, we also propose a method for computing similarities between the neural
The idea behinnedtwworokrrde2pnreestenistathioantstohfethceownodridtisoannadldpermoobnasbtrialtietythoatftwheny;capture seimsatnhteico(auntdpuevteonfsaynmtauctlitci)-layer network thatstiamkileasrittihees (cFoignutreex1tca).s input. Each layer of the network transforms the context into a new

hidden repreIsnenoutarteimonp,irriceawl estiugdhyt,iwnge sthhoewlathtaetnptafreamateutreer sshaacricnogridninwgortdo2nthetepirerrfeolremvsabnectteerfothranpraepdpilycitningg the

occurrence owford2vec or stan.daNrdoBteentohualtli nemotbeidllduinsgtrsaotnedthearaeugtmheent0e-dvvaorciaabbulearsy, oif.ew.,ortdh/etagnpeagiarst.ivWee saalsmo ples, wthheiicrhcocrorrersepstdpooeonmwndoodinnrdstgt2orvawwetecootorhrdrdastsntdaetenhtedwapatroBdarerkBrenseo.rnunololtui laelmit epbmeodbsdeiitdnidogisnnpgnrso..vIidnewbeottredr2pnreedti,ctthiveeilropg-rloikbealibhioloidtiewsheanlscoomcopamreed from

Now suppose weRhave tagged the corpus with . Figure 1b shows how to incorporate this syntactic

information into word2net. The network is specific to

as a noun (as opposed to a verb).

The parametfejrrsr:omfotvheed tfihrissttolathyeeirnt(roordaunctgioen), anreeedsshraewrerditinagmhoernegWaolrldneomubnedsdiinngthmeodceolsllleecartniosnem; athnteicother layers (blue) faeraetusrepseocfiwfiocrdtos by exploiting.thTehcou-so,ctchuerrennectewpoatrtkersnfsoorf words in a col/lection oaf ndodcuments. Ther/e dfseeiaffptaeurrraeitnselhayro. ewFmiM22agtr00heoou01esrm73rtee;;fiaoMLrnr1feseyctivtlkheeyoselxvashl&toeyaeovennawGsreptitospopflarondtrolhso.rba,eemace2rnh0amgode1c,st3ov2heraa0sse;rt1blity4as;hcn;ioe;mtVnsPltiieaoaallnnfatglienowrs.ingnow&T-tgrbodahtiMolsrieindspncmeCseembatcatreaotlodllsmu.dd,emooie2andfl0,cge12thfslh40,u(e;s1irBneM5tcehn;wnoneBshginerhaiitcorleo&khexefattttTnsh,a,eeluih2..e,,se0m221.c,006ios0a1;s3n2cBi;o;csan2Miomd0rp;n0eldrei6troihrh;nbt&&eMaghbemMnKtisloiiaheeatvinwte&htudswhkotH,dciodc2euip0nhoce1tgoocn7lrnudo)r,,.nettcaetgxlyst picks out tagogneda dwoot rpdrosdruecltaotefdthteowtohred veemrbbedadnidngrevleacteodrs taondthtehencoounnte.xt vectors, as opposed to the deep

neural network architectures proposed by Bengio et al. (2003; 2006) and Mnih & Hinton (2007). Our
Below, we demvoedloelpdtiheersdfertoamilsthoesfewdoeerpd2nneuertaal nnedtwsoturkdyarcithsitpecetrufroesrminatnwcoewwayisth. Ftiwrsot, dwaethaasveetsa, saepcaorlalteection

of Wikipedianeatwrtoircklefosraenacdh avoccoabrupluarsyowfoUrd.,Sin.sSteeadnaotfea sipnegelecnheetsw.orWk tehaftoouuntpduttshtahte wloogirtds 2fonreatllowuotrpdes rinforms

popular embethdedvioncgabmuleatrhy.oOdsuropnerpspreecdtiivcetionfgahbealndk-oofuptawraollredl sb,inaanrydctlhasastifischaatiroinngprpobalreammseatlelorwssbfaosrefadstoenr afunrdt,hceor mbopoasretafosusdprtpptithmaoeerritrzvfosaoetftrrciusomtpcnoetaruoe-nrfcbecht)ahettsea.eegnndQsde)tmuwtionaoeclrstaikphtpsea.otcutSdiirfiveescce,sollebaynmye,deta,wtrnoestouriocrrfidapntr2hrccoenhopinereteeprtcttoiwlteeurosraareortkefne.sntsRhaseebiynclweantsoeltlrairdntpchsctar,oietarctwpnaoidonbrrtafldhtoeieenrsmsgmyebsnmaietdtadiaecdontiiinnnctfi.gopcsrrmo(rwepaepittirrhoteionesus(etstnuahncetahyytions

encode are typically redundant (Andreas & Klein, 2014), so there is room for improvement with a

Related workm.odeWl thoartda2llnowetsbfouriladdsdiotinonwalosrydnteamctibcesdtrducintugrse.methods. Though originally designed as deep

neural netwoWrkeaardcohpittethcetupreerssp(eBcteivnegoifoeextpoanl.e,n2ti0al0f3a;m2il0y0e6m;bMedndihng&s (RHuidnotolpnh,e2t0a0l.,72),01m6o),swt haipcphleixctaetniodns of

word embeddwionrgdsenmobweddreinlgysotondlaotags-ebtsilbineyeoanrdmteoxtd. eTlhse(rMe airkeoallsoovseotmaelv.,ar2i0an1t3s aan;bd;ecx;tePnesnionnisnogftoexnpoenteanlt.i,al2014;

Mnih & Barkan,

2T0e1h6,fb;a2umB0t ti1ahly2mey;elmMealrblne&hdiahdvMien&giansnKc(dRoatmuv,d2umo0kolp1cnhu7ao&)n.geBTlxulphe,oie,2n2ke00ne11tyi37a;il;nfRLanumeodvviolyyalpt&lhiiokeenGtliabholoe.l,doh2dbi0new1dr7hg;wo,Lso2eiur0nd&1a2t4unB;realVeltip,iisla2nr0taih1msa7e&;ttLietriMuriesecpdtCreaetale.sl,rlem2un0imt1ns7e,d)w2, o0r1d5s;

with functions, instead of vectors (Rumelhart et al., 1986) or distributions (Vilnis & McCallum, 2015). Word2net keeps context vectors, but it replaces the em2 bedding vector with a neural network.

Previous work has also used deep neural networks for word embeddings (Bengio et al., 2003; 2006; Mnih & Hinton, 2007); these methods use a single network that outputs the unnormalized log probabilities for all words in the vocabulary. Word2net takes a different strategy: it has a separate network for each vocabulary word. Unlike the previous methods, word2net's approach helps maintain the objective as a bank of binary classifiers, which allows for faster optimization of the networks.

To develop word2net, we adopt the perspective of exponential family embeddings (Rudolph et al., 2016), which extend word embeddings to data beyond text. There are several extensions to exponential

2

Under review as a conference paper at ICLR 2018

family embeddings (Rudolph & Blei, 2017; Rudolph et al., 2017; Liu & Blei, 2017), but they all have in common an exponential family likelihood whose natural parameter has a log-bilinear form. Word2net extends this framework to allow for non-linear relationships. Here we focus on Bernoulli embeddings, which are related to word embeddings with negative sampling, but our approach easily generalizes to other exponential family distributions (e.g., Poisson).
Finally, word embeddings can capture semantic properties of the word, but they tend to neglect most of the syntactic information (Andreas & Klein, 2014). Word2net introduces a simple way to leverage the syntactic information to improve the quality of the word representations.

WN

In this section we develop word2net as a novel extension of Bernoulli embeddings (Rudolph et al.,

2016). Bernoulli embeddings are a conditional model of text, closely related to word2vec. Specifically,

they are related to continuous bag-of-words (

) with negative sampling.1 We first review Bernoulli

embeddings and then we present word2net as a deep Bernoulli embedding model.

.B

:B

Exponential family embeddings learn an embedding vector v 2 RK and a context vector v 2 RK for each unique term in the vocabulary, v D 1; : : : ; V . These vectors encode the semantic properties of words, and they are used to parameterize the conditional probability of a word given its context. Specifically, let wn be the V -length one-hot vector indicating the word at location n, such that wnv D 1 for one term (vocabulary word) v, and let cn be the indices of the words in a fixed-sized window centered at location n (i.e., the indices of the context words). Exponential family embeddings parameterize the conditional probability of the target word given its context via a linear combination of the embedding vector and the context vectors,

p.wnv j cn/ D Bernoulli

.

> v

n

/

;

with

X

n ,

v0 :

v 0 2cn

(1)

Here,

.x/ D

1 1Ce

x

is the sigmoid function, and we have introduced the notation n for the sum of

the context vectors at location n. Note that Eq. 1 does not impose the constraint that the sum over the

vocabulary

words

P
v

p.wnv

D

1

j

cn/

must

be

1.

This

significantly

alleviates

the

computational

complexity (Mikolov et al., 2013b; Rudolph et al., 2016).

This type of exponential family embedding is called Bernoulli embedding, named for its conditional distribution. In Bernoulli embeddings, our goal is to learn the embedding vectors v and the context vectors v from the text by maximizing the log probability of words given their contexts. The data contains N pairs .wn; cn/ of words and their contexts, and thus we can form the objective function L. ; / as the sum of log p.wnv j cn/ for all instances and vocabulary words. The resulting objective can be seen as a bank of V binary classifiers, where V is the vocabulary size. To see that, we make use of Eq. 1 and express the objective L. ; / as a sum over vocabulary words,

NV

0
V

L.

; / D X X log p.wnv

X j cn/ D @

X log

.

> v

n

/

C

X

log

.

nD1 vD1

vD1 nW wnv D1

nW wnv D0

1

> v

n

/A

:

(2)

If we hold all the context vectors v fixed, then Eq. 2 is the objective of V independent logistic regressors, each predicting whether a word appears in a given context or it does not. The positive examples are those where word v actually appeared in a given context; the negative examples are those where v did not appear. It is the context vectors that couple the V binary classifiers together.

In practice, we need to either downweight the contribution of the zeros in Eq. 2, or subsample the set of negative examples for each n (Rudolph et al., 2016). We follow the latter case here, which leads to negative sampling (Mikolov et al., 2013b). (See the connection in more detail in Appendix B.)

1See Appendix B for more details on the connections.

3

Under review as a conference paper at ICLR 2018

.W N

B

Word2net replaces the linear classifiers in Eq. 2 with non-linear classifiers. In particular, we replace the linear combination v>n with a neural network that is specific to each vocabulary word v, so that

p.wnv D 1 j cn/ D f .nI v/ ;

(3)

where f . I v/ W RK ! R is a feed-forward neural network with parameters (i.e., weights and intercepts) v. The number of neurons of the input layer is K, equal to the length of the context vectors v. Essentially, we have replaced the per-term embedding vectors v with a per-term neural network v. We refer to the per-term neural networks as word networks.

The word2net objective is the sum of the log conditionals,

0
V

XX

Lword2net. ; / D @

log

vD1 nW wnv D1

X

f .nI v/ C

log

nW wnv D0

1 f .nI v/ A ; (4)

where we choose the function f . I v/ to be a three-layer neural network,2

h.n1v/ D tanh n>v.1/ ; h.n2v/ D tanh .hn.1v//>v.2/ ; f .nI v/ D .hn.2v//>v.3/:

(5)

Replacing vectors with neural networks has several implications. First, the bank of binary classifiers has additional model capacity to capture nonlinear relationships between the context and the cooccurrence probabilities. Specifically, each layer consecutively transforms the context to a different representation until the weight matrix at the last layer can linearly separate the real occurrences of the target word from the negative examples.
Second, for a fixed dimensionality K, the resulting model has more parameters.3 This increases the model capacity, but it also increases the risk of overfitting. Indeed, we found that without extra regularization, the neural networks may easily overfit to the training data. We regularize the networks via either weight decay or parameter sharing (see below). In the empirical study of Section 3 we show that word2net fits text data better than its shallow counterparts and that it captures semantic similarities. Even for infrequent words, the learned semantic representations are meaningful.

Third, we can exploit the hierarchical structure of the neural network representations via parameter sharing. Specifically, we can share the parameters of a specific layer of the networks of different words. This allows us to explicitly account for tags in our model (see below).

Regularization through parameter sharing enables the use of tags. One way to regularize word2net is through parameter sharing. For parameter sharing, each word is assigned to one of T
groups. Importantly, different occurrences of a term may be associated to different groups.

We share specific layers of the word networks among words in the same group. In this paper, all neural network representations have 3 layers. We use index ` 2 f1; 2; 3g to denote the layer at which we apply the parameter sharing. Then, for each occurrence of term v in group t we set v.`/ D t.`/.
Consider now two extreme cases. First, for T D 1 group, we have a strong form of regularization by forcing all word networks to share the parameters of layer `. The number of parameters for layer ` has been divided by the vocabulary size, which implies a reduction in model complexity that might help prevent overfitting. This parameter sharing structure does not require side information and hence can be applied to any text corpus. In the second extreme case, each word is in its own group and T D V . This set-up recovers the model of Eqs. 4 and 5, which does not have parameter sharing.

When we have access to a corpus annotated with tags, parameter sharing lets us use the

information to improve the capability of word2net by capturing the semantic structure of the data.

Andreas & Klein (2014) have shown that word embeddings do not necessarily encode much syntactic

information, and it is still unclear how to use syntactic information to learn better word embeddings.

The main issue is that many words can appear with different tags; for example, can be both

a

and refer to the animal or a

and refer to the activity of catching the animal. On the

one hand, both meanings are related. On the other hand, they may have differing profiles of which

2Three layers performed well in our experiments, allowing for parameter sharing to include tags. 3For fairness, in Section 3 we also compare to shallow models with the same number of parameters.

4

Under review as a conference paper at ICLR 2018

contexts they appear in. Ideally, embedding models should be able to capture the difference. However,

the simple approach of considering /

and /

as separate terms fails because there

are few occurrences of each individual term/tag pair. (We show that empirically in Section 3.)

Exploiting the hierarchical nature of the network representations of word2net, we incorporate information through parameter sharing as follows. Assume that for location n in the text we have a one-hot vector sn 2 f0; 1gT indicating the tag. To model the observation at position n, we use a neural network specific to that term/tag combination,

p.wnv D 1; snt D 1 j cn/ D

f

nI

v.:`/; t.`/

Á :

(6)

That is, the neural network parameters are combined to form a neural network in which layer ` has parameters t.`/ and the other layers have parameters v.:`/. Thus, we leverage the information about the tag t by replacing v.`/ with t.`/ in layer `, resulting in parameter sharing at that layer. If the same term v appears at a different position n0 with a different tag t 0, at location n0 we replace the parameters v.`/ of layer ` with t.`0 /. Figure 1b illustrates parameter sharing at ` D 1.
Even though now we have a function f . / for each term/tag pair, the number of parameters does not scale with the product V T ; indeed the number of parameters of the network with information is smaller than the number of parameters of the network without side information (Eq. 5). The reason is that the number of parameters necessary to describe one of the layers has been reduced from V to T due to parameter sharing (the other layers remain unchanged).

Finally, note that we have some flexibility in choosing which layer is tag-specific and which layers are word-specific. We explore different combinations in Section 3, where we show that word2net with
information improves the performance of word2net. The parameter sharing approach extends to side information beyond tags, as long as the words can be divided into groups, but we focus on parameter sharing across all words (T D 1) or across tags.

Semantic similarity of word networks. In standard word embeddings, the default choice to compute semantic similarities between words is by cosine distances between the word vectors. Since word2net
replaces the word vectors with word networks, we can no longer apply this default choice. We next
describe the procedure that we use to compute semantic similarities between word networks.

After fitting word2net, each word is represented by a neural network. Given that these networks parameterize functions, we design a metric that accounts for the fact that two functions are similar if they map similar inputs to similar outputs. So the intuition behind our procedure is as follows: we consider a set of K-dimensional inputs, we evaluate the output of each neural network on this set of inputs, and then we compare the outputs across networks. For the inputs, we choose the V context vectors, which we stack together into a matrix  2 RV K. We evaluate each network f . / row-wise on  (i.e., feeding each v as a K-dimensional input to obtain a scalar output), obtaining a V -dimensional summary of where the network f . / maps the inputs. Finally, we use the cosine distance of the outputs to compare the outputs across networks. In summary, we obtain the similarity of two words w and v as

dist .w; v/ D f .I w />f .I v/ : jjf .I w /jj2 jjf .I v/jj2

(7)

If we are using parameter sharing, we can also compare -tagged words; e.g., we may ask how

similar is /

to / . The two combinations will have different representations under

the word2net method trained with -tag sharing. Assuming that layer ` is the shared layer, we

compute the semantic similarity between the word/tag pair OEw; t  and the pair OEv; s as

dist.OEw; t ; OEv; s/

D

jjf

f .I w.:`/; t.`//>f .I v.:`/; s.`// .I w.:`/; t.`//jj2 jjf .I v.:`/; s.`//jj2

:

(8)

E

In this section we study the performance of word2net on two datasets, Wikipedia articles and Senate speeches. We show that word2net fits held-out data better than existing models and that the learned network representations capture semantic similarities. Our results also show that word2net is superior

5

Under review as a conference paper at ICLR 2018

Table 1: Summary of the two corpora analyzed in Section 3.

corpus size vocabulary tagger tags tagged vocabulary

Wikipedia

17M words 15K terms NLTK 11 tags 49K tagged terms

Senate speeches 24M words 15K terms CoreNLP 11 tags 38K tagged terms

at incorporating syntactic information into the model, which improves both the predictions and the quality of the word representations.

Data. We use word2net to study two data sets, both with and without tags:

Wikipedia: The text8 corpus is a collection of Wikipedia articles, containing 17M words. We form

a vocabulary with the 15K most common terms, replacing less frequent terms with the

token. We annotate text8 using the

tagger and the universal tagset.4 Table 7 in Appendix C

shows a description of the tagset. We also form a tagged dataset in which each term/tag combination

has a unique token, resulting in a vocabulary of 49K tagged terms.

Senate speeches: These are the speeches given in the U.S. Senate in the years 1916-2009. The data is a transcript of spoken language and contains 24M words. Similarly as above, we form a vocabulary of 15K terms. We annotate the text using the Stanford CoreNLP tagger (Manning et al., 2014), and we map the tags to the universal tagset. We form a tagged dataset with 38K tagged terms.
Table 1 summarizes the information about both corpora. We split each dataset into a training, a validation, and a test set, which respectively contain 90%, 5%, and 5% of the words. Additional details on preprocessing are in Appendix C.

Methods. We compare word2net to its shallow counterpart, the

model (Mikolov et al., 2013b),

which is equivalent to Bernoulli embeddings ( - )5 (Rudolph et al., 2016). We also compare with

the skip-gram model.6 (Mikolov et al., 2013b) We run - /

and skip-gram on the data and

also on the augmented data of -tagged terms. In detail, the methods we compare are:

- / : Learns vector representations for each word (or tagged word) by optimizing Eq. 2. Skip-gproasm: Learns vector representations for each word (or tagged word) by optimizing Eq. 12. Word2nept:osLearns a neural network representation for each word by optimizing Eq. 4. We study
the followingpopsarameter sharing schemes:

1. all

: no parameter sharing.

2. pos all : layer ` shared between all networks.

3. pos all : layer ` shared between terms with the same part-of-speech ( ) tag.

For woradl2lnet, wppooessexperiment with the context dimensions K 2 f20; 100g. The context dimension is

also theaadllillmaelnlsion of the input layer. For K D 20, we use H1 D 10 hidden units in the first hidden

cKlHaoy1nCetrDexKotfHHaaelln2a1lldcDaaChwllllwoH2raaao0dlll1rlllHhdviedn2cdetCotewrnsHo)u.r2nkS.itiaTsnn.chdeWeHwsihte2hawoDlluaotnw1tp0tamohraoicdmdodemeeltsnperahurasnevhiemtasr2oiinKdnget,lhpsteahbresoaetmnhcuoeimtnnedbrteselrarpmyeoesrfr.otpefFarcrmoaormn(KtteehtxeeDtresdn1ipmt0re0iere,nswwsioooefrnduthKsiees

and We

eixnpteerrimmsaaellonllftaawtllollittahl

parameters, we fit the methods with context sizes jcnj 2 f2; 4; 8g and we

K2 train

f20; 165; 100; 1260g. all methods using stochastic

gradient

descent ( ) (Robbins & Monro, 1951) with jSnj D 10 negative samples on the Wikipedia data and

with jSnj D 20 negative samples on the Senate speeches. We use regularization with standard

deviation 10 for the word and context vectors, as well as weight decay for the neural networks. We

use Adam (Kingma & Ba, 2015) with Tensorflow's default settings (Abadi et al., 2016) to train all

methods for up to 30000 iterations, using a minibatch size of 4069 or 1024. We assess convergence

by monitoring the loss on a held-out validation set every 50 iterations, and we stop training when the

average validation loss starts increasing. We initialize and freeze the context vectors of the word2net

methods with the context vectors from a pretrained Bernoulli embedding with the same context

dimension K. Network parameters are initialized according to standard initialization schemes of

4See http://nltk.org.

5See Appendix B for the detailed relationship between - and

with negative sampling.

6The skip-gram objective is related to

/ - through Jensen's inequality (see Appendix B).

6

Under review as a conference paper at ICLR 2018

Table 2: Word2net outperforms existing word embedding models (skip-gram and - /

)

in terms of test log-likelihood on the Wikipedia data, both with and without tags. We compare

models with the same context dimension K and the same total number of parameters p=V for different

context sizes (cs). (Results on more configurations are in Appendix A.) For word2net, we study

different parameter sharing schemes, and the color coding indicates which layer is shared and how, as

in Figure 1. Parameter sharing improves the performance of word2net, especially with tags.

vocabulary K p=V cs 2 cs 4 cs 8

Mikolov et al. (2013b):

skip-gram

words

20 40 1:061 1:062 1:071

skip-gram

tagged words 20 240 2:994 3:042 3:042

Mikolov et al. (2013b); Rudolph et al. (2016):

-/

words

20 40 1:023 0:976 0:941

-/ -/
this work: word2net word2net

tappgoosgsswheppaodoorriwdssnsoppgroodsss
aapolllls

165 20
20 20

330 1:432 1:388 1:381 240 1:411 1:437 1:461
330 0:940 0:912 0:937 120 1:040 1:003 0:964

word2net

paaolllls 20 230 1:191 1:141 1:111

word2net word2net word2net word2net

aaappoollllllss aaallllll

ppaaaoollllssll aallll

paaolllls ppaaaoollllllss

20 20 20 20

320 0:863 0:881 0:890
120 0:918 0:914 0:871
230 0:844 0:801 0:793 320 0:840 0:822 0:862

dfeiestdr-ifbourtwioanrdwniethurbaol unnetdwsorkps (6G=aalpllollrHoatiln&lCaaaBlllHlllenoguito. , 2010), i.e., the weights are initialized from a uniform

Quantitative log-likelihood

roefstuhletsw:oWrdos rind2thneettehstaaasllsellbtaa,ellltolltgerpp.wrendvicj tcivne/.pFeorrfoskrimp-agnracme.,

We compute the predictive which was trained to predict

the context words from the target, we average the context vectors v for a fair comparison.7

Table 2 shows the results for the Wikipedia dataset. We explore different model sizes: with the same number of parameters as word2net, and with the same dimensionality K of the context vectors. For word2net, we explore different parameter sharing approaches. Table 5 in Appendix A shows the results for other model sizes (including K D 100). In both tables, word2net without parameter sharing performs at least as good as the shallow models. Importantly, the performance of word2net improves with parameters sharing, and it outperforms the other methods.

Tables 2 and 5 also show that - /

and skip-gram perform poorly when we incorporate

information by considering an augmented vocabulary of tagged words. The reason is that

each term becomes less frequent, and these approaches would require more data to capture the co-

occurrence patterns of tagged words. In contrast, word2net with parameter sharing provides the

best predictions across all methods (including other versions of word2net).

Finally, Table 6 in Appendix A shows the predictive performance for the U.S. Senate speeches. On

this corpus, skip-gram performs better than - /

and word2net without parameter sharing;

however, word2net with sharing also provides the best predictions across all methods.

Qualitative results: Word2net captures similarities and leverages syntactic information. Table 3

displays the similarity between word networks (trained on Wikipedia with parameter sharing at layer

` D 1), compared to the similarities captured by word embeddings ( - /

). For each query

word, we list the three most similar terms, according to the learned representations. The word vectors

are compared using cosine similarity, while the word networks are compared using Eq. 7. The table

shows that word2net can capture latent semantics, even for less frequent words such as

.

Table 4 shows similarities of models trained on the Senate speeches. In particular, the table compares:

-/

without information, - /

trained on the augmented vocabulary of tagged

words, and word2net with parameter sharing at the input layer (` D 1). We use Eq. 8 to compute

the similarity across word networks with sharing. We can see that word2net is superior at

incorporating syntactic information into the learned representations. For example, the most similar

7If we do not average, the held-out likelihood of skip-gram becomes worse.

7

Under review as a conference paper at ICLR 2018

Table 3: The word networks fitted using word2net capture semantic similarities. We compare the top

3 similar words to several query words (shaded in gray) for

/ - and word2net, trained on

the Wikipedia dataset. The numbers in parenthesis indicate the frequency of the query words.

(3000)

expectancy per
increase

word2net capacity
amount
energy

votes voting election

(1000)
word2net elect
candidate candidates

bananas potatoes pepper

(500)
word2net beans juice poultry

turtle beaver pratchett

(70)
word2net dolphin
crow dodo

Table 4: Word2net learns better semantic representations by exploiting syntactic information. The

top 3 similar words to several queries are listed for different models fitted to the Senate speeches.

We compare

trained without tags (left),

with tags (center), and word2net with

parameter sharing (right). The tags are noted in orange. Parameter sharing helps word2net

capture better semantic similarities, while adding the information to

hurts its performance.

like senator
just

(pron)
governor n senator from alabama n
used adj

word2net myself pron
my pron
himself pron

but reason
that

(sc)

unemployed n annuity n shelled v

word2net as sc
that sc
through sc

fatal consequences
coupled

(n)
pro adj enough adv positions n

word2net consequences n
clash n
handicaps n

think what just

(v)
time v best adj favour n

word2net think v
know v
answer v

networks to the pronoun are other pronouns such as

, , and

. Word networks

are often similar to other word networks with the same tag, but we also see some variation. One

such example is in Figure 1c, which shows that the list of the 10 most similar words to the verb

contains the adjective .

D
We have presented word2net, a method for learning neural network representations of words. The word networks are used to predict the occurrence of words in small context windows and improve prediction accuracy over existing log-bilinear models. We combine the context vectors additively, but this opens the door for future research directions in which we explore other ways of combining the context information, such as accounting for the order of the context words and their tags.
We have also introduced parameter sharing as a way to share statistical strength across groups of words and we have shown empirically that it improves the performance of word2net. Another opportunity for future work is to explore other types of parameter sharing besides sharing, such as sharing layers across documents or learning a latent group structure together with the word networks.
R
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Jacob Andreas and Dan Klein. How much do word embeddings encode about syntax? In Proceedings of Association for Computational Linguistics, 2014.
Robert Bamler and Stephan Mandt. Dynamic word embeddings. In International Conference on Machine Learning, 2017.
Oren Barkan. Bayesian neural word embedding. arXiv preprint arXiv:1603.06571, 2016.

8

Under review as a conference paper at ICLR 2018
Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of machine learning research, 3(Feb):1137­1155, 2003.
Yoshua Bengio, Holger Schwenk, Jean-Sébastien Senécal, Fréderic Morin, and Jean-Luc Gauvain. Neural probabilistic language models. In Innovations in Machine Learning, pp. 137­186. Springer, 2006.
J. R. Firth. A synopsis of linguistic theory 1930-1955. In Studies in Linguistic Analysis (special volume of the Philological Society), volume 1952­1959, 1957.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Aistats, volume 9, pp. 249­256, 2010.
Zellig S Harris. Distributional structure. Word, 10(2-3):146­162, 1954.
Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on, pp. 263­272. Ieee, 2008.
D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Neural Information Processing Systems, pp. 2177­2185, 2014.
Liping Liu and David M. Blei. Zero-inflated exponential family embeddings. In International Conference on Machine Learning, 2017.
Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations, 2014. URL http://www.aclweb.org/anthology/P/P14/P14-5010.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR Workshop Proceedings. arXiv:1301.3781, 2013a.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Neural Information Processing Systems, pp. 3111­3119, 2013b.
Tomas Mikolov, Wen-T au Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In HLT-NAACL, pp. 746­751, 2013c.
Andriy Mnih and Geoffrey E. Hinton. Three new graphical models for statistical language modelling. In International Conference on Machine Learning, 2007.
Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estimation. In Neural Information Processing Systems, pp. 2265­2273, 2013.
Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language models. In International Conference on Machine Learning, pp. 1751­1758, 2012.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Conference on Empirical Methods on Natural Language Processing, volume 14, pp. 1532­1543, 2014.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pp. 400­407, 1951.
Maja Rudolph and David Blei. Dynamic bernoulli embeddings for language evolution. arXiv preprint at arXiv:1703.08052, 2017.
Maja Rudolph, Francisco Ruiz, Stephan Mandt, and David Blei. Exponential family embeddings. In Advances in Neural Information Processing Systems, pp. 478­486, 2016.
Maja Rudolph, Francisco Ruiz, Susan Athey, and David Blei. Structured embedding models for grouped data. In Advances in Neural Information Processing Systems, 2017.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. Nature, 323:9, 1986.
Luke Vilnis and Andrew McCallum. Word representations via Gaussian embedding. In International Conference on Learning Representations, 2015.
9

Under review as a conference paper at ICLR 2018

A

AA

For completeness, we show here some additional results that we did not include in the main text for space constraints.

In particular, Table 5 compares the test log-likelihood of word2net with the competing models--

namely, skip-gram and - /

. All methods are trained with negative sampling, as described

in the main text. This table shows the results for the Wikipedia dataset, similarly to Table 2, but

it includes other model sizes (i.e., another value of K). In this table, word2net with no parameter

sharing performs similarly to - /

with the same number of parameters, but its performance

can be further improved with part-of-speech ( ) parameter sharing.

Table 6 shows the test log-likelihood for the U.S. Senate speeches. Here, skip-gram is the best method that does not use tags, but it is outperformed by word2net with parameter sharing.

Table 5: Comparison of the test log-likelihood across different models on the Wikipedia dataset. We compare models with the same context dimension K and the same total number of parameters p=V
for different context sizes ("cs"). For word2net, we explore different parameter sharing schemes. The
color coding of the parameter sharing (same as Figure 1) indicates which layer is shared and how.

vocabulary K p=V cs 2 cs 4 cs 8

Mikolov et al. (2013b):

skip-gram

words

100 200 1:107 1:053 1:043

skip-gram

tagged words 100 1200 3:160 3:151 3:203

Mikolov et al. (2013b); Rudolph et al. (2016):

-/

words

100 200 1:212 1:160 1:127

-/ -/
this work: word2net

tapgosgswhepaodorriwdsnsogrds
pos

100 1260
100

1200 2520
2520

1:453 3:772
1:088

3:387 2:397
1:049

3:433 2:506
1:012

word2net

apolls 100 520 1:041 0:988 1:001

word2net word2net word2net

paolls pos paolls aall pos

100 100 100

2120 521 2120

1:114
0:828 0:892

1:059
0:807 0:850

1:016
0:770 0:822

aallll pos

BR

all allll
B all all all all

Word2vec (Mikolov et al., 2013ba)llis onaelolf the most widely used method for learning vector representations of words. There are mualtlilple ways to implement word2vec. First, there is a choice of

the objective. Second, there are several awlalys of how to approximate the objective to get a scalable

algorithm. In this section, we describe the two objectives, continuous bag-of-words (

) and

skip-gram, and we focus on negative sampling as the method of choice to achieve scalability. We

describe the similarities and differences between Bernoulli embeddings (Rudolph et al., 2016) and

these two objectives. In summary, under certain assumptions Bernoulli embeddings are equivalent to

with negative sampling, and are related to skip-gram through Jensen's inequality.

-Á

(negative sampling)

First we explain how Bernoulli embeddings and the Bernoulli embedding full objective,

with negative sampling are related. Consider

01

L. ; / D X @ X log . v>n/ C X log . v>n/A :

n vW wnv D1

vW wnv D0

(9)

In most cases, the summation over negative examples (wnv D 0) is computationally expensive to compute. To address that, we form an unbiased estimate of that term by subsampling a random set Sn

10

Under review as a conference paper at ICLR 2018

Table 6: Comparison of the test log-likelihood across different models on the Senate speeches. We compare models with the same context dimension K and the same total number of parameters p=V
for different context sizes ("cs"). For word2net, we explore different parameter sharing schemes. The
color coding of the parameter sharing (same as Figure 1) indicates which layer is shared and how.

vocabulary K p=V cs 2 cs 4 cs 8

Mikolov et al. (2013b):

skip-gram

words

20 40 1:052 1:080 1:061

skip-gram

tagged words 20 240 1:175 1:199 1:227

Mikolov et al. (2013b); Rudolph et al. (2016):

-/

words

20 40 1:274 1:246 1:222

-/ -/
this work: word2net

tapgogsweodrwdsords shpaorisng
pos

20 165
20

240 1:352 1:340 1:339 330 1:735 1:734 1:744
330 1:406 1:555 1:401

word2net

apolls 20 120 1:276 1:256 1:243

word2net

paolls 20

word2net word2net

pos paolls aalll pos

20 20

aallll pos

of

terms

and

rescaling

by

V1 jSn j

,

0

all aalll all all
all all

X Lb. ; / D @

Xall allolgal.l v>n/ C

n vW wnv D1 all

230 1:462 120 0:873 230 1:057
V 1X log
jSnj v2Sn

1:435
0:860 1:034

1:413
0:850 1:015

1 . v>n/A :

(10)

Here, we have introduced an auxiliary coefficient . The estimate is unbiased only for D 1;

however, Rudolph et al. (2016) showed that downweighting the contribution of the zeros works better

in practice.8 In particular, if we set the downweight factor as

D

jSn j V1

,

we

recover

the

objective

of

with negative sampling,

01

Lb. ; / D X @ X

log . v>n/ C X log .

> v

n

/A

Á

LCBOW.

; /

n vW wnv D1

v2Sn

(11)

There are two more subtle theoretical differences between both. The first difference is that Bernoulli

embeddings include a regularization term for the embedding vectors, whereas

does not. The

second difference is that, in Bernoulli embeddings, we need to draw a new set of negative samples

Sn at each iteration of the gradient ascent algorithm (because we form a noisy estimator of the

downweighted objective). In contrast, in

with negative sampling, the samples Sn are drawn

once in advance and then hold fixed. In practice, for large datasets, we have not observed significant

differences in the performance of both approaches. For simplicity, we draw the negative samples Sn

only once.

(negative sampling) skip-gram (negative sampling)

Now we show how

and skip-gram are related (considering negative sampling for both). Recall

that the objective of

is to predict a target word from its context, while the skip-gram objective

is to predict the context from the target word. Negative sampling breaks the multi-class constraint

that the sum of the probability of each word must equal one, and instead models probabilities of the

individual entries of the one-hot vectors representing the words.

When we apply negative sampling, the given by

objective becomes Eq. 11. The skip-gram objective is

0

XX

Lskip-gram. ; / D

@ log

.n;v/W wnv D1 v02cn

> v

v0

C

X log

v 0 2Sn

1 v>v0 A ;

(12)

8This is consistent with the approaches in recommender systems (Hu et al., 2008).

11

Under review as a conference paper at ICLR 2018

Table 7: Universal tagset.
Tag Description
adj adjective adp adposition adv adverb conj conjuction det determiner, article n noun num numeral prt particle pron pronoun sc preposition or subordinating conjuction v verb x other

That is, for each target term wnv, the

objective has one term while the skip-gram objective

has jcnj terms. Consider a term .n; v/ for which wnv D 1. We take the corresponding

term

from Eq. 11 and we apply Jensen's inequality to obtain the corresponding skip-gram term in Eq. 12:

01

log

.

> v

n

/

D

log

@

> v

X

v0 A

X log

v 0 2cn

v 0 2cn

> v

v0

:

(13)

Here, we have made use of the concavity of the log . / function. In general, this is a consequence of the convexity of the log-normalizer of the (Bernoulli) exponential family distribution.

This holds for the "positive" examples wnv. As for the negative examples (wnv D 0), the comparison is not as straightforward, because the choice of terms in Eqs. 11 and 12 is not exactly the same. In particular, Eq. 11 holds v0 fixed and draws v from the noise distribution, while Eq. 12 holds v fixed and draws v0 from the noise distribution.

CD

In this paper we study Wikipedia articles (text8) and a corpus of U.S. Senate speeches. On both

corpora, we restrict the vocabulary to the 15K most frequent words, replacing all the remaining words

with a designated token. We annotate the data using

tagger9 or the Stanford CoreNLP tagger

(Manning et al., 2014), using the universal tagset shown in Table 7.

The Senate speeches contain a lot of boilerplate repetitive language; for this reason, we tokenize

around 350 frequent phrases, such as

or , considering the

entire phrase an individual vocabulary term. We apply the tagger before this tokenization step,

and then we assign the

tag to all phrases.

We split the data into training (90%), testing (5%), and validation (5%) sets. We use the validation set to assess convergence, as explained in the main text. We subsample the frequent words following Mikolov et al. (2013b); i.e., each word wn in the training set is discarded with probability

s

t

Prob.wn is discarded/ D 1

; frequency.wn/

(14)

where frequency.wn/ denotes the frequency of word wn, and t D 10 5.
For each method, we use jSnj D 10 negative samples on the Wikipedia articles and jSnj D 20 negative samples on the Senate speeches. Following Mikolov et al. (2013b), we draw the negative samples from the unigram distribution raised to the power of 0:75.

9See http://nltk.org

12

