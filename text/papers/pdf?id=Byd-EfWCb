Under review as a conference paper at ICLR 2018
DECODING DECODERS: FINDING OPTIMAL REPRESENTATION SPACES FOR UNSUPERVISED SIMILARITY TASKS
Anonymous authors Paper under double-blind review
ABSTRACT
Experimental evidence indicates that shallow bag-of-words models outperform complex deep networks on many unsupervised similarity tasks. Introducing the concept of an optimal representation space, we provide a simple theoretical resolution to this apparent paradox. In addition, we present a straightforward procedure that, without any retraining or architectural modifications, allows deep recurrent models to perform equally well (and sometimes better) when compared to shallow models. To validate our analysis, we conduct a set of consistent empirical evaluations and introduce several new sentence embedding models in the process. While the current work is presented within the context of natural language processing, the insights are applicable to the entire field of representation learning.
1 INTRODUCTION
Distributed representations have played a pivotal role in the current success of machine learning. In contrast with the symbolic representations of classical AI, distributed representation spaces can encode rich notions of semantic similarity in their distance measures, which in turn can allow generalisation to novel inputs, a critical component of any learning system. Methods to learn these representations have recently gained significant traction, in particular for modelling words (Mikolov et al., 2013a). They have since been successfully applied to many other domains, including images (Girod et al., 2011; Razavian et al., 2014) and graphs (Kipf & Welling, 2016; Grover & Leskovec, 2016; Narayanan et al., 2017).
Learning representations from unlabelled data that are useful in other tasks is at the forefront of modern machine learning research. The Natural Language Processing (NLP) community in particular, has invested significant efforts in the construction (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2016; Joulin et al., 2017), evaluation (Baroni et al., 2014) and theoretical analysis (Levy & Goldberg, 2014) of distributed representations for words.
Recently, attention has shifted towards learning representations of larger pieces of text, such as phrases (Yin & Schu¨tze, 2015; Zhang et al., 2017), sentences (Kalchbrenner et al., 2014; Kiros et al., 2015; Tai et al., 2015; Hill et al., 2016; Arora et al., 2017), and entire paragraphs (Le & Mikolov, 2014). Some of this work has relied on a sentence-level version of the distributional hypothesis (Harris, 1954), an assumption that sentences which occur in close proximity have a similar meaning. Models trained in an unsupervised manner on large corpora of text are usually applied to supervised transfer tasks, where the representation for a sentence forms the input to a supervised classification problem, or to unsupervised similarity tasks, where the (cosine) similarity of two inputs informs some downstream process, such as information retrieval.
When evaluating different architectures, Hill et al. (2016) observed that, while deep complex models are preferable in supervised settings, shallow log-linear bag-of-words (BOW) models outperform deep learning approaches on unsupervised similarity tasks. However, these unsupervised tasks are more interesting from a general AI point of view, as they test whether the machine truly understands the human notion of similarity, without being explicitly told what is similar. Current experimental evidence suggests that shallow models are somehow better at this, contradicting the established view of superiority of deep models in abundance of data.
1

Under review as a conference paper at ICLR 2018
In this work we attempt to address these observations. Our main contributions are as follows:
· We introduce the formalism of an optimal representation space, in which the similarity measure is optimal with respect to the objective function.
· We show that models with log-linear decoders are usually evaluated in the optimal space, while recurrent models are not. This effectively explains the performance gap on unsupervised similarity tasks.
· We show that, when evaluated in their optimal space, recurrent models close that gap. · We validate our findings with a series of consistent empirical evaluations based on a single
publicly available codebase. In the process, we introduce new hybrid models with promising performance characteristics for transfer tasks.
2 UNSUPERVISED SENTENCE REPRESENTATIONS
In contrast to the supervised setting, where the input and output signals are usually well-defined, learning sentence representations in an unsupervised manner is not so straightforward. It is not entirely clear what the goals of a learning system should be in this setting, or whether there are any guarantees that the resulting embeddings will be useful in any downstream tasks.
To overcome these difficulties, it is sometimes possible to reformulate the original problem as a supervised learning task by introducing some domain-dependent assumptions. For instance, word embedding models such as Word2Vec rely on the distributional hypothesis to learn to predict a word given its context, or alternatively, predict a context given a word. Such representations are well-known to capture some degree of similarity as perceived by humans (Mikolov et al., 2013b).
Based on the success of word-embeddings, many distributed sentence representations are derived from straightforward functions of constituent word vectors, such as the sum or average (Mitchell & Lapata, 2010; Milajevs et al., 2014; Wieting et al., 2015; Arora et al., 2017). Such models can yield surprisingly high-quality representations (Arora et al., 2017), but naturally cannot leverage any contextual information for building sentence representations, leading some researchers to move towards more powerful embedding models.
Inspired by the word vector learning procedure by Mikolov et al. (2013b) and the encoder-decoder framework of Cho et al. (2014), Kiros et al. (2015) showed that it is possible to learn high-quality generic sentence representations from unlabelled data by leveraging a sentence-level distributional hypothesis (Polajnar et al., 2015). Their model, SkipThought, uses a Recurrent Neural Network (RNN) encoder and two RNN decoders to predict for any given sentence its two adjacent sentences.
SkipThought has enjoyed impressive results on many supervised transfer benchmarks, particularly when implemented with layer normalisation (Ba et al., 2016). In these benchmarks, the encoder is fixed after training on a separate unannotated dataset and then used as a feature extractor for a simpler model (such as logistic regression) for the task in question. In this setting, SkipThought with layer normalisation is currently considered to be one of the best general purpose sentence-level encoders (Conneau et al., 2017).
In contrast, the SkipThought model has proved notably less successful on unsupervised similarity tasks (Hill et al., 2016; Conneau et al., 2017). These benchmarks measure how the cosine distance between sentence pairs correlates with corresponding human judgements of semantic similarity. Since no additional model is being trained in this scenario, the encoder is evaluated on its own merits. While this shortcoming of SkipThought and RNN-based models in general has been pointed out, to the best of our knowledge, it has never been systematically addressed in the literature before.
Motivated by the success and, at the same time, dissatisfied with the slow training time of SkipThought, Hill et al. (2016) proposed a conceptually similar approach, FastSent. This log-linear model replaces computationally expensive RNN encoder and decoders with much simpler BOW versions, admittedly sacrificing word order information as a consequence. Interestingly, this change made FastSent among state-of-the-art on unsupervised similarity tasks, but dramatically reduced the performance on supervised benchmarks (Hill et al., 2016; Conneau et al., 2017).
Others in the literature have recognised the need to analyse the geometric characteristics of representation spaces (Almahairi et al., 2015; Schnabel et al., 2015). Most relevantly, Hill et al. (2016)
2

Under review as a conference paper at ICLR 2018

concluded that "Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics." Our work provides the beginnings of this analysis via an extension and refinement of above statement.

3 OPTIMAL REPRESENTATION SPACE

3.1 NOTATION
Before we describe each model separately, we formally introduce the setting. Let S = (s1, s2, . . . , sN ) be an ordered corpus of contiguous sentences where each sentence si = wi1wi2 . . . wii consists of words from a pre-defined vocabulary V of size |V |. Additionally, xw denotes a one-hot encoding of w and vw is the corresponding (input) word embedding. We transform the corpus into a set of pairs D = {(si, ci)}iN=1, where si  S and ci is a context of si. Throughout this paper we assume that contexts ci are given by ci = si-1  si+1; our analysis readily generalises to different definitions of the context.

3.2 FASTSENT AND LOG-LINEAR DECODERS
FastSent consists of an additive BOW encoder and two log-linear BOW decoders. Due to the model's simplicity, it is particularly fast to train and evaluate, yet has shown state-of-the-art performance in unsupervised similarity tasks (Hill et al., 2015). Here we describe and analyse the model in three parts: the encoder, decoders, and objective.

Encoder. A simple BOW encoder represents a sentence si as a sum of the input word embeddings:

hi =

vw.

wsi

(1)

Decoders. The two decoders share weights and can be considered as a single decoder that outputs a probability distribution over the vocabulary conditional on a sentence si

(y^)w = pmodel(w|si) =

exp (uw · hi) , w V exp (uw · hi)

(2)

where uw  Rd is the output word embedding for a word w. (Biases are omitted for brevity.)

Objective. The objective is to maximise the model probability of contexts ci given sentences si
across the corpus D which corresponds to finding the Maximum Likelihood Estimator (MLE) for the trainable parameters :1

MLE = arg max

pmodel(ci|si; ).

 (si,ci)D

(3)

In the case of the log-linear BOW decoder above, the context ci contains words from both si-1 and si+1 and the probabilities of words are independent, yielding

pmodel(ci|si; ) =

pmodel(w|si; ) =

wci

wci

exp (uw · hi)

=

wci exp (uw · hi) .

w V exp (uw · hi) |ci| w V exp (uw · hi)

(4)

Switching to the negative log-likelihood and inserting the above expression, we arrive at the follow-

ing optimisation problem:



MLE = arg min -

uw · hi + |ci| log

exp (uw · hi)  .

 (si,ci)D wci

w V

(5)

1In practice, we minimise the Kullback-Leibler Divergence (KLD) between the data distributions y and model distributions y^, which is known to be an equivalent problem.

3

Under review as a conference paper at ICLR 2018

Noticing that

uw · hi =

uw · hi = ci · hi,

wci

wci

(6)

we see that the objective in Equation (5) forces the sentence representation hi to be similar under dot product to its context representation ci, which is simply the sum of the output embeddings of the context words. Simultaneously, output embeddings of words that do not appear in the context
of a sentence are forced to be dissimilar to its representation. We explicitly demonstrate this in
Appendix A.

Finally, using cos to denote close under cosine similarity, we find that if two sentences si and sj have similar contexts, then ci cos cj. Additionally, the objective function in Equation (5) ensures that hi cos ci and hj cos cj. Therefore, it follows that hi cos hj.

Putting it differently, sentences that occur in related contexts are assigned representations that are
similar under cosine similarity cos (·, ·) and thus cos (·, ·) is the appropriate similarity measure in the case of log-linear decoders.2

Interestingly, the objective in Equation (5) only depends on the sum encoder from Equation (1) through the encoder output hi. Thus we could exchange the encoder with other functions, such as a deep or even recurrent neural network, and would still arrive at the same conclusion.

These observations lead us to a simple but important statement: in any model where the decoder is log-linear with respect to the encoder, the space induced by the encoder and equipped with cos (·, ·) as the similarity measure is what we call an optimal distributed representation space: "a space in which semantically close concepts (or inputs) are close in distance" (Goodfellow et al., 2016) and that distance is optimal with respect to the model's objective.

As a practical corollary, FastSent is among the best on unsupervised similarity tasks because these tasks use cos (·, ·) for similarity and hence evaluate the models in their optimal representation space. Admittedly, evaluating a model in its optimal space does not by itself guarantee any good performance downstream as the tasks might deviate from the model's assumptions. For example, if sentences "my cat likes my dog" and "my dog likes my cat" are labelled as dissimilar, FastSent will stand no chance of succeeding. However, as we show later, evaluating the model in a suboptimal space may very well hurt its performance.

3.3 SKIPTHOUGHT AND RECURRENT SEQUENCE DECODERS
SkipThought consists of a recurrent RNN encoder along with two RNN decoders that effectively predict, word for word, the context of a sentence. While computationally complex, it is currently the state-of-the-art model for supervised transfer tasks (Hill et al., 2016). The same cannot be said about its performance on unsupervised similarity tasks, where SkipThought lags behind much simpler models. In this section we explain why and show how to close the performance gap. Its encoder, decoders, and objective are defined as follows:

Encoder. The encoder is a Gated Recurrent Unit (GRU)3 (Cho et al., 2014):

rt =  Wrvt + Urht-1 , zt =  Wzvt + Uzht-1 , h~t = tanh Wvt + U rt ht-1 ht = 1 - zt ht-1 + zt h~t,

,

where denotes the element-wise (Hadamard) product.

(7) (8) (9) (10)

2Evidently, the correct measure is actually the dot product. However, the unboundedness of the dot product
makes it impossible to tell what is similar and antisimilar in absolute terms, so we use cosine similarity instead. 3We follow the notation of Kiros et al. (2015) where z  1 - z compared to standard conventions.

4

Under review as a conference paper at ICLR 2018

Decoders. The previous and next sentence decoders are also GRUs. The initial state for both is given by the final state of the encoder

hi0-1 = hi0+1 = hi i .

(11)

and the update equations are the same as in Equations (7) to (10). Note that the original implementation in Kiros et al. (2015) additionally biases the gates by the encoder output, while our version makes no such provision. We discuss this change and its performance impact in Appendix B.

The time unrolled states of the previous sentence decoder are converted to probability distributions over the vocabulary conditional on the sentence si and all the previously occurring words

(y^it-1)w = pmodel(wit-1|wit--11, . . . , wi1-1, si; ) =

w

exp (uw · hit-1) V exp (uw · hti-1)

,

(12)

The outputs y^ti+1 of the next sentence decoder are computed analogously.

Objective. We define the probability of a context ci given a sentence si as

pmodel(ci|si; ) = pmodel(si-1|si; ) × pmodel(si+1|si; ).

(13)

where

i-1

i-1

pmodel(si-1|si; ) =

p(wit-1|si; ) =

t=1 t=1

and similarly for pmodel(si+1|si; ).

exp (uwit-1 · hti-1) w V exp (uw · hit-1)

Similarly to Equation (5), MLE for the model parameters  can be found as

(14)

 j
MLE = arg min -
 siD j{i-1,i+1} t=1



uwjt · htj + log

exp (uw · htj)  .

w V

(15)

Using  to denote vector concatenation, we note that

j  j   j 

uwjt · hjt = 

uwjt  · 

htj  = ci · hiD,

j{i-1,i+1} t=1

j{i-1,i+1} t=1

j{i-1,i+1} t=1

(16)

where the sentence representation hDi is now an ordered concatenation of the hidden states of both decoders, and the context representation ci is an ordered concatenation of the output embeddings of
the context words. Hence we can come to the same conclusion as in the log-linear case, except we have order-sensitive representations as opposed to unordered ones. As before, hiD is forced to be similar under dot product to the context ci, and is made dissimilar to sequences of uw that do not
appear in the context.

The "transitivity" argument from Section 3.2 remains intact, except the decoder hidden state se-
quences might differ in length from sentence to sentence. To avoid this problem, we can formally treat them as infinite-dimensional vectors in 2 with only a finite number of initial components occu-
pied by the sequence and the rest set to zero. Alternatively, we can agree on the maximum sequence
length, which in practice can be determined from the training corpus.

Regardless, the above space of unrolled concatenated decoder states, equipped with cosine similarity, is the optimal representation space for models with recurrent decoders. Consequently, this space should be a much better candidate for unsupervised similarity tasks, a fact we experimentally confirm in Section 5.

We refer to the method of accessing the decoder states at every time step as unrolling the decoder. Note that accessing the decoder output does not require re-architecting or retraining the model, giving a potential performance boost on unsupervised similarity tasks almost "for free". We will demonstrate the effectiveness of this technique empirically in Section 5.

In practice, models such as SkipThought are evaluated in the space induced by the encoder, where cosine similarity is not an optimal measure with respect to the objective. However, by using D

5

Under review as a conference paper at ICLR 2018

Table 1: Performance of different architectures and sentence representations on unsupervised similarity tasks. On each task, the highest performing setup for each decoder type is highlighted in bold and the highest performing setup overall is underlined. All reported values indicate Pearson/Spearman correlation coefficients for the task. RNN encoder: Using the raw encoder output (RNN-RNN/SkipThought) achieves the lowest performance across all tasks as observed in Hill et al. (2016); Conneau et al. (2017). Unrolling the RNN decoders using either the average (RNN-mean) or the concatenation of the decoder outputs (RNN-concat) dramatically improves the performance across all tasks compared to using the raw encoder RNN output, validating the theoretical justification presented in Section 3.3. We note that Conneau et al. (2017) report higher scores for SkipThought; we attribute the discrepancy to a different decoder type used in Kiros et al. (2015) and elaborate on it in Appendix C. BOW encoder: We do not observe the same uplift in performance from unrolling the RNN encoder compared to the encoder output. We believe this is caused by the simplicity of the BOW encoder forcing its outputs to obey sentence-level distributional hypothesis irrespective of decoder type, resulting in multiple candidates for the optimal representation space. It is not clear a priori which space would be perform the best on the transfer task, and should be investigated with a detailed analysis in future work.

Encoder RNN
BOW

Decoder
BOW RNN RNN-mean RNN-concat BOW RNN RNN-mean RNN-concat

STS12
0.466/0.496 0.323/0.357 0.430/0.458 0.419/0.445 0.497/0.517 0.508/0.526 0.533/0.551 0.521/0.540

STS13
0.376/0.414 0.320/0.319 0.457/0.446 0.426/0.414 0.526/0.520 0.483/0.489 0.509/0.517 0.491/0.498

STS14
0.478/0.482 0.345/0.345 0.499/0.481 0.466/0.452 0.576/0.561 0.575/0.562 0.578/0.565 0.561/0.554

STS15
0.424/0.454 0.402/0.409 0.511/0.516 0.497/0.503 0.604/0.605 0.644/0.641 0.637/0.635 0.627/0.625

STS16
0.552/0.586 0.373/0.408 0.528/0.542 0.511/0.529 0.592/0.592 0.585/0.585 0.605/0.601 0.584/0.581

to denote the decoder part of the model, the encoder space equipped with the induced similarity measure cos (D(·), D(·)) is again an optimal space. While in some ways this is a simple change of notation, it shows that a model may have many optimal spaces and they can be constructed using the layers of the network itself.
As a downside, concatenating hidden states of the decoder leads to very high dimensional vectors, which might be undesirable in applications. We find that averaging hidden states also works and actually improves the results slightly. Intuitively, this corresponds to destroying the word order information the decoder has learned. The performance gain might be due to the nature of the downstream tasks. Additionally, because of the way we unroll decoders during inference time, we observe the "softmax drifting effect," which causes a drop in performance for longer sequences. We elaborate on this effect in Section 5.
4 EXPERIMENTAL SETUP
To support the theory in Section 3, we train several models with the same overall architecture but different combinations of encoders and decoders. We use the SentEval tool (Conneau et al., 2017) to benchmark sentence embeddings on both supervised and supervised transfer tasks.
Models and training. Each model has an encoder for the current sentence, and decoders for the previous and next sentences. Using the notation ENC-DEC, we train RNN-RNN, RNN-BOW, BOW-BOW, and BOW-RNN. Note that RNN-RNN corresponds to SkipThought, and BOW-BOW to FastSent. In addition, for models that have RNN decoders, we unroll between 1 and 10 decoder hidden states and report on the best-performing one (with results for all given in Appendix C). We refer to these as *-RNN-concat for the concatenated states and *-RNN-mean for the averaged states. All models are trained on the Toronto Books Corpus (Zhu et al., 2015), a dataset of 70 million ordered sentences from over 7,000 books. The sentences are pre-processed such that tokens are lower case and splittable on space.
Evaluation tasks. The supervised tasks in SentEval include paraphrase identification (MSRP) (Dolan et al., 2004), movie review sentiment (MR) (Pang & Lee, 2005), product review sentiment (CR), (Hu & Liu, 2004)), subjectivity (SUBJ) (Pang & Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), and question type (TREC) (Voorhees, 2002). In addition, there are two supervised tasks on the SICK dataset, entailment and relatedness (denoted SICK-E and SICK-R) (Marelli et al., 2014). For the supervised tasks, SentEval trains a logistic regression model with 10-fold crossvalidation using the model's embeddings as features.
The unsupervised similarity tasks are STS12-16 (Cer et al., 2017; Agirre et al., 2012; 2013; 2014; Agirre, 2015; Agirre et al., 2016), which are scored in the same way as SICK-R but without train-

6

Under review as a conference paper at ICLR 2018

Table 2: Performance of different architectures and sentence representations on supervised transfer tasks. On each task, the highest performing setup for each decoder type is highlighted in bold and the highest performing setup overall is underlined. All reported values indicate test accuracy on the task, except for SICK-R where we report the Pearson correlation with human-provided scores. SICK-R and SICK-E scores for RNN-concat are omitted due to memory constraints. Note that the analysis in Section 3 is not readily applicable here, as instead of using a similarity measure in the representation space directly, the supervised transfer tasks train an entirely new model on top the chosen representation.

Encoder RNN
BOW

Decoder
BOW RNN RNN-mean RNN-concat BOW RNN RNN-mean RNN-concat

MR
75.78 77.06 76.55 76.20 76.16 76.05 75.85 77.27

CR
79.34 81.77 81.03 82.07 81.14 82.07 81.30 82.04

MPQA
86.25 88.59 87.35 85.96 87.03 85.80 85.54 88.74

SUBJ
90.77 92.56 92.29 91.80 92.77 92.13 90.80 92.88

SST
81.99 82.65 81.11 80.83 81.66 80.83 80.12 81.82

TREC
84.60 86.60 84.80 87.20 84.20 87.20 84.00 89.60

MRPC
70.55 71.94 73.51 71.59 71.07 72.99 71.13 73.68

SICK-R
0.80 0.83 0.84
- 0.84 0.82 0.81
-

SICK-E
78.81 81.10 78.22
- 80.58 78.87 77.76
-

ing a new supervised model; in other words, the embeddings are used to directly compute cosine similarity. For more details on all of tasks and the evaluation strategy, see Conneau et al. (2017).
Implementation and hyperparameters. Our goal is to study how different decoder types affect the performance of sentence embeddings on various tasks. To this end, we use identical hyperparameters and architecture for each model (except encoder and decoder types), allowing for a fair headto-head comparison. Specifically, for RNN encoders and decoders we use a single layer GRU with layer normalisation (Ba et al., 2016). All the weights (including word embeddings) are initialised uniformly over [-0.1, 0.1] and trained with Adam without weight decay or dropout (Kingma & Ba, 2014). Sentence length is clipped or zero-padded to 30 tokens and the end-of-sentence tokens are used throughout training and evaluation. Following Kiros et al. (2015), we use a vocabulary-size of 20k, 620-dimensional word embeddings, and 2400 hidden units in RNN encoders / decoders.
5 RESULTS
Performance across unsupervised similarity tasks is presented in Table 1 and performance across supervised transfer tasks is presented in Table 2. When the encoder is an RNN, the supervised transfer results validate our claims in Section 3.3. The results are less conclusive when the encoder is a BOW, see the caption of Table 1 for more details.
When we look at the performance on supervised transfer in Table 2, combined with the similarity results in Table 1, we see that the notion models cannot be good at both supervised and transfer tasks needs refining. Our results show that, for example, the raw encoder output for SkipThought (RNN-RNN) achieves strong performance on supervised transfer, whilst its mean decoder output (RNN-mean) achieves strong performance on supervised transfer. Instead, our results demonstrate that a single model can perform well on both types of transfer task if the representation spaces chosen for each task are allowed to be different.
Curiously, the unusual combination of a BOW encoder and concatenation of the RNN decoders leads to the best performance on most benchmarks, even slightly exceeding that of InferSent (Conneau et al., 2017) on SUBJ. This architecture may be worth investigating.
Finally, the performance of the unrolled models is presented in Figure 1, with the numeric values of all tasks and unrolls up to 10 presented in Appendix C . We observe that the performance to peak at around 2-3 hidden states and fall off afterwards. In principle, one might expect the peak to be around the average sentence length of the corpus. A possible explanation of this behaviour is the "softmax drifting effect". As there is no target sentence during inference time, we generate the word embedding for the next time step using the softmax output pt-1 from the previous time step, i.e. v^t = VT p^t-1, where V is the input word embedding matrix. Given the ambiguity about what the surrounding sentences might be, a potentially softmax output might "drift" the sequence of v^t away from the word embeddings expected by the decoder. It is likely that beam search would stabilise this behaviour, but further work is needed to understand this and other possible causes in detail.

7

Under review as a conference paper at ICLR 2018

Spearman correlation coefficient Spearman correlation coefficient

0.48

0.46

0.44

0.42

0.40

0.38 0.36 0.34
1

Mean decoder output Concatenated decoder output Encoder output with RNN decoder Encoder output with BOW decoder
2 3 4 5 6 7 8 9 10 Number of unroll steps

0.56

0.54

0.52

0.50

0.48 0.46 0.44
1

Mean decoder output Concatenated decoder output Encoder output with RNN decoder Encoder output with BOW decoder
2 3 4 5 6 7 8 9 10 Number of unroll steps

(a) RNN encoder

(b) BOW encoder

Figure 1: Performance on the STS14 task depending on a number of unrolled hidden states of the decoders. In case of RNN encoder, RNN-RNN-mean at its peak matches the performance of RNNBOW and both unrolling strategies strictly outperform RNN-RNN. In case of BOW encoder, only BOW-RNN-mean outperforms competing models (possibly because the BOW encoder is unable to preserve word order information).

6 CONCLUSION
In this work we have presented a simple explanation for the observed performance gap between FastSent (BOW-BOW) and SkipThought (RNN-RNN) architectures when using encoder output as a sentence embedder on unsupervised similarity tasks. Specifically, we note that the encoder-decoder training objective induces a similarity measure between embeddings on an optimal representation space. When the task uses the same similarity measure, we observe improved performance.
Assuming the use of cosine similarity, we show that the optimal representation space for FastSent is precisely its encoder output, whereas in the SkipThought case it is not, but is instead constructed by concatenating the decoder output states. The observed performance gap can then be explained by noting that previous uses of log-linear architectures for unsupervised similarity tasks correctly have leveraged their optimal representation space, but RNN architectures like SkipThought have not.
We then validate our claims by comparing the empirical performance of different architectures across transfer tasks. In general, we observe that unrolling the RNN decoder for different numbers of hidden states yields a performance that interpolates between the lower performance of the raw encoder output with RNN decoder and higher performance of the BOW decoder across all similarity tasks. This demonstrates how our insights into optimal similarity spaces can motivate novel ways of extracting vector embeddings from neural networks.
Ultimately, a good representation is one that makes a subsequent learning task easier. For unsupervised similarity tasks, either within or outside of the context of NLP, this essentially reduces to how well the model separates objects in the chosen representation space, and how appropriately the similarity measure compares objects in that space. Our findings lead us to the following practical advice: i) Use a simple model architecture where the optimal representation space is clear by construction, or ii) use an arbitrarily complex model architecture and analyse the objective function to reveal, for a given vector representation of choice, an appropriate similarity metric.
We hope that future work will utilise a careful understanding of what similarity means and how it is linked to the objective function, and that our analysis can be applied to help boost the performance of other complex models.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Eneko Agirre. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. SemEval2015, (SemEval):252­263, 2015.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. Proc. 6th Int. Work. Semant. Eval. (SemEval 2012), conjunction with First Jt. Conf. Lex. Comput. Semant. (* SEM 2012), (3):385­393, 2012.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. SEM 2013 shared task : Semantic Textual Similarity. Second Jt. Conf. Lex. Comput. Semant. (*SEM 2013), 1: 32­43, 2013.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. Proc. 8th Int. Work. Semant. Eval. (SemEval 2014), (SemEval): 81­91, 2014.
Eneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation. Proc. 10th Int. Work. Semant. Eval., pp. 497­511, 2016. URL http://aclweb.org/anthology/S16-1081.
Amjad Almahairi, Kyle Kastner, Kyunghyun Cho, and Aaron Courville. Learning Distributed Representations from Reviews for Collaborative Filtering. In Proc. 9th ACM Conf. Recomm. Syst. RecSys '15, pp. 147­154, New York, New York, USA, 2015. ACM Press.
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A Simple but Tough-to-Beat Baseline for Sentence Embeddings. Int. Conf. Learn. Represent., pp. 1­14, 2017.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. jul 2016. ISSN 1607.06450. URL http://arxiv.org/abs/1607.06450.
Marco Baroni, Georgiana Dinu, and Germa´n Kruszewski. Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proc. 52nd Annu. Meet. Assoc. Comput. Linguist. (Volume 1 Long Pap., pp. 238­247, Stroudsburg, PA, USA, 2014. Association for Computational Linguistics. URL http://aclweb.org/anthology/ P14-1023.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching Word Vectors with Subword Information. jul 2016. URL http://arxiv.org/abs/1607.04606.
Daniel Cer, Mona Diab, Eneko Agirre, In~igo Lopez-Gazpio, and Lucia Specia. SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation. Proc. 11th Int. Work. Semant. Eval., pp. 1­14, jul 2017.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation. In Proc. 2014 Conf. Empir. Methods Nat. Lang. Process., pp. 1724­1734, Stroudsburg, PA, USA, 2014. Association for Computational Linguistics. URL http://arxiv.org/abs/1406.1078.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. may 2017. URL http://arxiv.org/abs/1705.02364.
Bill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora. In Proc. 20th Int. Conf. Comput. Linguist. - COLING '04, pp. 350­es, Morristown, NJ, USA, 2004. Association for Computational Linguistics.
Bernd Girod, Vijay Chandrasekhar, David Chen, Ngai-Man Cheung, Radek Grzeszczuk, Yuriy Reznik, Gabriel Takacs, Sam Tsai, and Ramakrishna Vedantham. Mobile Visual Search. IEEE Signal Process. Mag., 28(4):61­76, jul 2011. URL http://arxiv.org/abs/1112.6209.
9

Under review as a conference paper at ICLR 2018
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning, 2016. ISSN 1548-7091. URL https://mitpress.mit.edu/books/deep-learning.
Aditya Grover and Jure Leskovec. node2vec: Scalable Feature Learning for Networks. jul 2016. doi: 10.1145/2939672.2939754. URL http://arxiv.org/abs/1607.00653.
Zellig S. Harris. Distributional Structure. WORD, 10(2-3):146­162, aug 1954.
Felix Hill, Roi Reichart, and Anna Korhonen. SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation. Comput. Linguist., 41(4):665­695, dec 2015. URL http: //arxiv.org/abs/1408.3456.
Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning Distributed Representations of Sentences from Unlabelled Data. feb 2016. URL http://arxiv.org/abs/1602.03483.
Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proc. 2004 ACM SIGKDD Int. Conf. Knowl. Discov. data Min. - KDD '04, pp. 168, New York, New York, USA, 2004. ACM Press.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of Tricks for Efficient Text Classification. In Proc. 15th Conf. Eur. Chapter Assoc. Comput. Linguist. Vol. 2, Short Pap., pp. 427­431, Stroudsburg, PA, USA, jul 2017. Association for Computational Linguistics. URL http://arxiv.org/abs/1607.01759.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A Convolutional Neural Network for Modelling Sentences. In Proc. 52nd Annu. Meet. Assoc. Comput. Linguist. (Volume 1 Long Pap., pp. 655­665, Stroudsburg, PA, USA, apr 2014. Association for Computational Linguistics. URL http://arxiv.org/abs/1404.2188.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. pp. 1­15, dec 2014. URL http://arxiv.org/abs/1412.6980.
Thomas N Kipf and Max Welling. Variational Graph Auto-Encoders. Nipsw, (2):1­3, nov 2016. URL http://arxiv.org/abs/1611.07308.
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. Skip-Thought Vectors. jun 2015. URL http://arxiv.org/abs/ 1506.06726.
Quoc V. Le and Tomas Mikolov. Distributed Representations of Sentences and Documents. 32, 2014. URL http://arxiv.org/abs/1405.4053.
Omer Levy and Yoav Goldberg. Neural Word Embedding as Implicit Matrix Factorization. In Z Ghahramani, M Welling, C Cortes, N D Lawrence, and K Q Weinberger (eds.), Adv. Neural Inf. Process. Syst. 27, pp. 2177­2185. Curran Associates, Inc., 2014.
M Marelli, S Menini, Marco Baroni, L Bentivogli, R Bernardi, and R Zamparelli. A SICK cure for the evaluation of compositional distributional semantic models. Lrec, (May):216­223, 2014.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. pp. 1­12, jan 2013a. URL http://arxiv.org/abs/1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. pp. 1­9, oct 2013b. URL http://arxiv.org/abs/1310.4546.
Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and Matthew Purver. Evaluating Neural Word Representations in Tensor-Based Compositional Settings. pp. 708­719, aug 2014. URL http://arxiv.org/abs/1408.6179.
Jeff Mitchell and Mirella Lapata. Composition in Distributional Models of Semantics. Cogn. Sci., 34(8):1388­1429, nov 2010.
10

Under review as a conference paper at ICLR 2018
Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal. graph2vec: Learning Distributed Representations of Graphs. jul 2017. URL http://arxiv.org/abs/1708.04357.
Bo Pang and Lillian Lee. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proc. 42nd Annu. Meet. Assoc. Comput. Linguist. - ACL '04, pp. 271­es, Morristown, NJ, USA, 2004. Association for Computational Linguistics. URL http://arxiv.org/abs/cs/0409058.
Bo Pang and Lillian Lee. Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales. In Proc. 43rd Annu. Meet. Assoc. Comput. Linguist. - ACL '05, number June, pp. 115­124, Morristown, NJ, USA, 2005. Association for Computational Linguistics. URL http://arxiv.org/abs/cs/0506075.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global Vectors for Word Representation. In Proc. 2014 Conf. Empir. Methods Nat. Lang. Process., pp. 1532­1543, Stroudsburg, PA, USA, 2014. Association for Computational Linguistics.
Tamara Polajnar, Laura Rimell, and Stephen Clark. An Exploration of Discourse-Based Sentence Spaces for Compositional Distributional Semantics. In Proc. First Work. Link. Comput. Model. Lexical, Sentential Discourse-level Semant., pp. 1­11, Stroudsburg, PA, USA, 2015. Association for Computational Linguistics.
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN Features Offthe-Shelf: An Astounding Baseline for Recognition. In 2014 IEEE Conf. Comput. Vis. Pattern Recognit. Work., pp. 512­519. IEEE, jun 2014.
Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. Evaluation methods for unsupervised word embeddings. In Proc. 2015 Conf. Empir. Methods Nat. Lang. Process., number September, pp. 298­307, Stroudsburg, PA, USA, 2015. Association for Computational Linguistics.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks. feb 2015.
Ellen M Voorhees. Overview of the TREC 2001 question answering track. NIST Spec. Publ., (0): 42­51, 2002.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating Expressions of Opinions and Emotions in Language. Lang. Resour. Eval., 39(2-3):165­210, may 2005.
John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Towards Universal Paraphrastic Sentence Embeddings. pp. 1­17, nov 2015. URL http://arxiv.org/abs/1511.08198.
Wenpeng Yin and Hinrich Schu¨tze. Discriminative Phrase Embedding for Paraphrase Identification. In Proc. 2015 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol., volume 21, pp. 1368­1373, Stroudsburg, PA, USA, sep 2015. Association for Computational Linguistics.
Yizhe Zhang, Dinghan Shen, Guoyin Wang, Zhe Gan, Ricardo Henao, and Lawrence Carin. Deconvolutional Paragraph Representation Learning. aug 2017. URL http://arxiv.org/abs/ 1708.04729.
Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books. Proc. IEEE Int. Conf. Comput. Vis., 2015 Inter:19­27, jun 2015. URL http://arxiv.org/abs/1506.06724.
11

Under review as a conference paper at ICLR 2018

A OPTIMISATING OBJECTIVE
The task is to maximise the quantity Q found in eq. (5)

Q=

uw · hi - log

exp (uw · hi) =

qiw ,

(si,ci)D wci

w VW

(si,ci)D wVW

(17)

where

qiw = log (x) - log (x + y),

(18)

where we drop the sentence and word subscript on x and y for brevity (but in the following equations it is understood we are referring to a specific given word w given specific sentence s), and

x = exp (uw · hi),

y = exp (uw · hi).
w |VW |\{w}

(19)

We find the derivatives

qsiw =

y ,

x x(x + y)

qsiw = - 1 , y (x + y)

(20)

and conclude that since both x and y are therefore positive, that for a given word w and sentence si, the quantity qsiw is made larger by

· Increasing x, leading to an increase in the dot product of the word present in the context with the context vector, and
· Reducing y, leading to a decrease in the dot products of all other words.

Performing this analysis across all words in a context leads to the maximisation of the dot products of the context representation ci with the sentence representation hi

uw · hi = ci · hi
wci

and a minimisation of the dot product of the sentence representation hi word vectors uw that are not in the context ci

log exp (uw · hi).

wci

w |VW |\{w}

B THE ORIGINAL SKIPTHOUGHT DECODER

Kiros et al. (2015) additionally bias each decoder gate by the encoder output hii .

rt =  WrDvt + UDr ht-1 + Crhii ,
zt =  WzDvt + UDz ht-1 + Czhii , h~t = tanh WDvt + UD rt ht-1 + Chii , h = 1 - zt ht-1 + zt h~t,

(21) (22) (23) (24)

We suspect this change has substantial impact on similarity performance as the encoder contributes more directly to the decoder hidden states and is therefore "closer" to the optimal space.

12

Under review as a conference paper at ICLR 2018

C UNROLLED RNN RESULTS

Table 3: Performance of different architectures and sentence representations on unsupervised similarity tasks with different length of unrolled sentence. On each task, for each encoder-decoder combination, the highest performing setup is highlighted in bold. All reported values indicate Pearson/Spearman correlation coefficients for the task.

Unroll

STS12

STS13

STS14

RNN encoder, concatenated RNN decoder output
1 0.408/0.434 0.410/0.400 0.446/0.434 2 0.419/0.445 0.426/0.414 0.466/0.452 3 0.404/0.430 0.410/0.404 0.444/0.434 4 0.391/0.416 0.395/0.390 0.433/0.424 5 0.374/0.397 0.382/0.375 0.421/0.411 6 0.354/0.380 0.384/0.374 0.415/0.403 7 0.337/0.363 0.366/0.355 0.401/0.387 8 0.321/0.347 0.353/0.339 0.387/0.371 9 0.309/0.337 0.343/0.325 0.380/0.362 10 0.300/0.328 0.331/0.310 0.372/0.354
RNN encoder, mean RNN decoder output
1 0.414/0.439 0.415/0.406 0.452/0.440 2 0.429/0.456 0.441/0.430 0.485/0.469 3 0.430/0.458 0.457/0.446 0.499/0.481 4 0.425/0.453 0.454/0.443 0.497/0.478 5 0.420/0.447 0.450/0.440 0.496/0.477 6 0.416/0.445 0.445/0.436 0.496/0.476 7 0.413/0.444 0.440/0.433 0.495/0.474 8 0.407/0.441 0.432/0.425 0.490/0.467 9 0.401/0.437 0.424/0.417 0.483/0.459 10 0.393/0.430 0.413/0.405 0.476/0.452
BOW encoder - concatenated RNN decoder output
1 0.518/0.538 0.484/0.491 0.556/0.550 2 0.521/0.540 0.491/0.498 0.561/0.554 3 0.512/0.532 0.486/0.495 0.554/0.548 4 0.506/0.526 0.469/0.479 0.542/0.537 5 0.494/0.514 0.461/0.470 0.531/0.526 6 0.471/0.499 0.453/0.461 0.515/0.512 7 0.452/0.480 0.436/0.443 0.496/0.495 8 0.429/0.458 0.424/0.428 0.474/0.475 9 0.412/0.442 0.423/0.423 0.462/0.461 10 0.399/0.425 0.412/0.411 0.447/0.444
BOW encoder - mean RNN decoder output
1 0.528/0.548 0.488/0.496 0.564/0.556 2 0.533/0.551 0.504/0.511 0.575/0.564 3 0.533/0.551 0.509/0.517 0.578/0.565 4 0.530/0.548 0.503/0.512 0.575/0.560 5 0.529/0.546 0.501/0.510 0.574/0.558 6 0.522/0.541 0.500/0.507 0.571/0.555 7 0.516/0.537 0.494/0.501 0.566/0.550 8 0.507/0.530 0.483/0.491 0.558/0.543 9 0.500/0.525 0.482/0.488 0.553/0.538 10 0.494/0.519 0.481/0.486 0.549/0.533

STS15
0.486/0.492 0.497/0.503 0.476/0.483 0.459/0.468 0.443/0.451 0.430/0.435 0.416/0.419 0.404/0.405 0.394/0.396 0.386/0.387
0.489/0.495 0.510/0.516 0.511/0.516 0.504/0.509 0.498/0.503 0.498/0.504 0.498/0.505 0.496/0.503 0.489/0.497 0.481/0.489
0.627/0.625 0.627/0.625 0.616/0.616 0.600/0.602 0.585/0.589 0.565/0.573 0.548/0.558 0.525/0.537 0.512/0.524 0.498/0.507
0.633/0.630 0.638/0.636 0.637/0.635 0.630/0.628 0.624/0.624 0.617/0.619 0.611/0.613 0.601/0.604 0.595/0.599 0.588/0.593

STS16
0.493/0.514 0.511/0.529 0.487/0.513 0.478/0.505 0.461/0.493 0.440/0.478 0.418/0.458 0.396/0.426 0.378/0.402 0.364/0.385
0.499/0.519 0.526/0.541 0.528/0.542 0.527/0.542 0.520/0.538 0.516/0.535 0.509/0.532 0.501/0.523 0.490/0.512 0.480/0.503
0.578/0.575 0.584/0.581 0.571/0.573 0.562/0.564 0.550/0.554 0.522/0.536 0.504/0.522 0.486/0.509 0.471/0.492 0.459/0.477
0.594/0.590 0.605/0.600 0.605/0.601 0.601/0.598 0.599/0.598 0.591/0.592 0.585/0.588 0.576/0.581 0.568/0.574 0.562/0.568

13

