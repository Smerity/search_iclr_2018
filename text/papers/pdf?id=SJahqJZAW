Under review as a conference paper at ICLR 2018
Stabilizing GAN Training with
Multiple Random Projections
Anonymous authors Paper under double-blind review
Abstract
Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training. Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.
1 Introduction
Generative adversarial networks (GANs), introduced by Goodfellow et al. (2014), endow neural networks with the ability to express distributional outputs. The framework includes a generator network that is tasked with producing samples from some target distribution, given as input a (typically low dimensional) noise vector drawn from a simple known distribution, and possibly conditional side information. The generator learns to generate such samples, not by directly looking at the data, but through adversarial training with a discriminator network that seeks to differentiate real data from those generated by the generator. To satisfy the objective of "fooling" the discriminator, the generator eventually learns to produce samples with statistics that match those of real data.
In regression tasks where the true output is ambiguous, GANs provide a means to simply produce an output that is plausible (with a single sample), or to explicitly model that ambiguity (through multiple samples). In the latter case, they provide an attractive alternative to fitting distributions to parametric forms during training, and employing expensive sampling techniques at the test time. In particular, conditional variants of GANs have shown to be useful for tasks such as in-painting (Denton et al., 2016), and super-resolution (Ledig et al., 2016). Recently, Isola et al. (2016) demonstrated that GANs can be used to produce plausible mappings between a variety of domains--including sketches and photographs, maps and aerial views, segmentation masks and images, etc. GANs have also found uses as a means of un-supervised learning, with latent noise vectors and hidden-layer activations of the discriminators proving to be useful features for various tasks (Denton et al., 2016; Chen et al., 2016; Radford et al., 2016).
Despite their success, training GANs to generate high-dimensional data (such as large images) is challenging. Adversarial training between the generator and discriminator involves optimizing a min-max objective. This is typically carried out by gradient-based updates to both networks, and the generator is prone to divergence and mode-collapse as the discriminator begins to successfully distinguish real data from generated samples with high confidence. Researchers have tried to address this instability and train better generators
1

Under review as a conference paper at ICLR 2018
Figure 1: Overview of our approach. We train a single generator against an array of discriminators, each of which receives lower-dimensional projections--chosen randomly prior to training--as input. Individually, these discriminators are unable to perfectly separate real and generated samples, and thus provide stable gradients to the generator throughout training. In turn, by trying to fool all the discriminators simultaneously, the generator learns to match the true full data distribution.
through several techniques. Denton et al. (2015) proposed generating an image by explicitly factorizing the task into a sequence of conditional generations of levels of a Laplacian pyramid, while Radford et al. (2016) demonstrated that specific architecture choices and parameter settings led to higher-quality samples. Other techniques include providing additional supervision (Salimans et al., 2016), adding noise to the discriminator input (Arjovsky & Bottou, 2016), as well as modifying or adding regularization to the training objective functions (Nowozin et al., 2016; Arjovsky et al., 2017; Zhao et al., 2016). We propose a different approach to address this instability, where the generator is trained against an array of discriminators, each of which looks at a different, randomly-chosen, low-dimensional projection of the data. Each discriminator is unable to perfectly separate real and generated samples since it only gets a partial view of these samples. At the same time, to satisfy its objective of fooling all discriminators, the generator learns to match the true full data distribution. We describe a realization of this approach for training image generators, and discuss the intuition behind why we expect this training to be stable--i.e., the gradients to the generator to be meaningful throughout training--and consistent--i.e., for the generator to learn to match the full real data distribution. Despite its simplicity, we find this approach to be surprisingly effective in practice. We demonstrate this efficacy by using it to train generators on standard image datasets, and find that these produce higher-quality samples than generators trained against a single discriminator.
1.1 Related Work
Researchers have explored several approaches to improve the stability of GANs for training on higher dimensional images. Instead of optimizing Jensen-Shannon divergence as suggested in the original GAN framework, Energy based GAN (Zhao et al., 2016) and Wasserstein GAN (Arjovsky et al., 2017) show improvement in the stability by optimizing total variation distance and Earth mover distance respectively, together with regularizing the discriminator to limit the discriminator capacity. Nowozin et al. (2016) further extended GAN training to any choice of f -divergence as objective. Salimans et al. (2016) proposed several heuristics to improve the stability of training. These include modifying the objective function, virtual batch-normalization, historical averaging of parameters, semi-supervised learning, etc.. All of these methods are designed to improve the quality of gradients, and provide additional supervision to the generator. They are therefore complementary to, and can likely be used in combination with, our approach.
2

Under review as a conference paper at ICLR 2018

Note that prior methods have involved training ensembles of GANs (Wang et al., 2016), or ensembles of discriminators (Durugkar et al., 2016). However, their goal is different from ours. In our framework, each discriminator is shown only a low-dimensional projection of the data, with the goal of preventing it from being able to perfectly reject generated samples. Crucially, we do not combine the outputs of discriminators directly, but rather compute losses on individual discriminators and average these losses.
Indeed, the idea of combining multiple simple discriminators, e.g., with boosting (Freund et al., 1996), or mixture-of-experts Jacobs (1995), is a common approach to learning. While these combinations are aimed at building a single strong discriminator, our objective is to maintain a flow of gradients from individual discriminators to the generator. It is also worth noting the work of Bonneel et al. (2015), who also use low-dimensional projections to deal with high-dimensional probability distributions. However, while they use such projections to efficiently compute Wasserstein distances and optimal transport between two distributions, our goal is to enable stable GAN training in high dimensions.

2 Training with Multiple Random Projections

GANs (Goodfellow et al., 2014) traditionally comprise of a generator G that learns to
generate samples from a data distribution Px, through adversarial training against a single discriminator D as:

min
G

max
D

V (D, G) = ExPx [log D(x)] + EzPz [log(1 - D(G(z)))],

(1)

for x  Rd, Px : Rd  R+, Px = 1, and with Pz a fixed distribution (typically uniform or Gaussian) for noise vectors z  Rd , d  d. The optimization in (1) is carried out using stochastic gradient descent (SGD), with alternating updates to the generator and the discriminator. While this approach works surprisingly well, instability is common during training, especially when generating high-dimensional data.

Theoretically, the optimal stationary point for (1) is one where the generator matches the data distribution perfectly, and the discriminator is forced to always output 0.5 (Goodfellow et al., 2014). But usually in practice, the discriminator tends to "win" and the cost in (1) saturates at a high value. While the loss keeps increasing, the gradients received by the generator are informative during the early stages of training. However, once the loss reaches a high enough value, the gradients are dominated by noise, at which point generator quality stops improving and in fact begins to deteriorate. Training must therefore be stopped early by manually inspecting sample quality, and this caps the number of iterations over which the generator is able to improve.

Arjovsky & Bottou (2016) discuss one possible source of this instability, suggesting that it is because natural data distributions Px often have very limited support in the ambient domain of x and G(z). Then, the generator G is unable to learn quickly enough to generate samples from distributions that have significant overlap with this support. This in turn makes it easier for the discriminator D to perfectly separate the generator's samples, at which point the latter is left without useful gradients for further training.

We propose to ameliorate this problem by training a single generator against an array of discriminators. Each discriminator operates on a different low-dimensional linear projection, that is set randomly prior to training. Formally, we train a generator G against multiple discriminators {Dk}Kk=1 as:

KK

min max
G {Dk}

V (Dk, G) = ExPx [log Dk(WkT x)] + EzPz [log(1 - Dk(WkT G(z)))], (2)
i=k i=k

where Wk, k  {1, · · · , K} is a randomly chosen matrix in Rd×m with m < d. Therefore, instead of a single discriminator looking at the full input (be it real or fake) in (1), each of the K discriminators in (2) sees a different low-dimensional projection. Each discriminator tries to maximize its accuracy at detecting generated samples from its own projected version, while the generator tries to ensure that each sample it generates simultaneously fools all disciminators for all projected versions of that sample.

3

Under review as a conference paper at ICLR 2018

When the true data X and generated samples G(z) are images, prior work (Radford et al., 2016) has shown that it is key that both the generator and discriminator have convolutional architectures. While individual discriminators in our framework see projected inputs that are lower-dimensional than the full image, their dimensionality is still large enough (a very small m would require a large number of discriminators) to make it hard to train discriminators with only fully-connected layers. Therefore, it is desirable to employ convolutional architectures for each discriminator. To do so, the projection matrices WkT must be chosen to produce "image-like" data. Accordingly, we use strided convolutions with random filters to embody the projections WkT (as illustrated in Fig. 1). The elements of these convolution filters are drawn i.i.d. from a Gaussian distribution, and the filters are then scaled to have unit 2 norm. To promote more "mixing" of the input coordinates, we choose filter sizes that are larger than the stride (e.g., we use 8 × 8 filters when using a stride of 2). While still random, this essentially imposes a block-Toeplitz structure on WkT .

3 Motivation

In this section, we motivate our approach, and provide the reader with some intuition for why we expect our approach to improve the stability of training while maintaining consistency.

Stability. Each discriminators Dk in our framework in (2) works with low dimensional projections of the data and can do no better than the traditional full discriminator D in (1). Let y  Rd and l  {0, 1} denote the input and ideal output of the original discriminator D, where l = 1 if y is real, and 0 if generated. However, each Dk is provided only a lower dimensional projection WkT y. Since WkT y is a deterministic and non-invertible function of y, it can contain no more information regarding l than Y itself, i.e., I(l; Y )  I(l; WkT Y ), where I(x; y) denotes the mutual information between the random variables x and y. In other words, taking a low-dimensional projection introduces an information bottleneck which interferes with the ability of the discriminator to determine the real/fake label l.

While strictly possible, it is intuitively unlikely that all the information required for classification is present in the WkT y (for all, or even most of the different WkT ), especially in the adversarial setting. We already know that GAN training is far more stable in lower dimensions, and we expect that each of the discriminators Dk will perform similarly to a traditional discriminator training on lower (i.e., m-) dimensional data. Moreover, Arjovsky & Bottou (2016) suggest that instability in higher-dimensions is caused by the true distribution Px being concentrated in a small fraction of the ambient space--i.e., the support of Px occupies a low volume relative to the range of possible values of x. Again, we can intuitively expect this to be ameliorated by performing a random lower-dimensional projection (and provide analysis for a simplistic Px in the supplementary).
When the discriminators Dk are not perfectly saturated, Dk(WkT G(z)) will have some variation in the neighborhood of a generated sample G(z), which can provide meaningful gradients to the generator. But note that gradients from each individual discriminator Dk to G(z) will lie entirely in the lower dimensional sub-space spanned by WkT .
Consistency. While impeding the discriminator's ability benefits stability, it could also have trivially been achieved by other means--e.g., by severely limiting its architecture or excessive regularization. However, these would also limit the ability of the discriminator to encode the true data distribution and pass it on to the generator. We begin by considering the following modified version of Theorem 1 in Goodfellow et al. (2014):
Theorem 3.1. Let Pg denote the distribution of the generator outputs G(z), where z  Pz, and let PWkT g be the marginals of Pg along Wk. For fixed G, the optimal {Dk} are given by

Dk(y) = PWkT x(y)/ PWkT x(y) + PWkT g(y) , for all k  {1, · · · , K}. The optimal G under (2) is achieved iff PWkT x = PWkT g, k.

(3)

This result, proved in the supplementary, implies that under ideal conditions the generator
will produce samples from a distribution whose marginals along each of the projections Wk match those of the true distribution.

4

Under review as a conference paper at ICLR 2018
Figure 2: Training Stability. We plot the evolution of the "generator loss" across training-- against a traditional single discriminator (DC-GAN), and the average and individual losses against multiple discriminators (K = 48) in our setting. For the traditional single discriminator, this loss rises quickly to high value, indicating that the discriminator saturates to rejecting generated samples with very high confidence. In contrast, the loss in our case remains lower, allowing our discriminators to provide meaningful gradients to the generator.
Figure 3: Evolution of sample quality across training iterations. With our approach, the generator improves the visual quality of its samples quickly and throughout training. Meanwhile, the generator trained in the traditional setting with a single discriminator shows slower improvement, and indeed quality begins to deteriorate after a point.
Thus, each discriminator adds an additional constraint on the generator, forcing it to match Px along a different marginal. As we show in the supplementary, matching along a sufficiently high number of such marginals--under smoothness assumptions on the true and generator distributions Px and Pg--guarantees that the full joint distributions will be close (note that it isn't sufficient for the set of projection matrices {Wk} to simply span Rd to guarantee this). Therefore, even though viewing the data through random projections limits the ability of individual discriminators, with enough discriminators acting in concert, the generator learns to match the full joint distribution of real data in Rd.
4 Experimental Results
We now evaluate our approach with experiments comparing it to generators trained against a single traditional discriminator, and demonstrate that it leads to higher stability during training, and ultimately yields generators that produce higher quality samples. Dataset and Architectures. For evaluation, we primarily use the dataset of celebrity faces collected by Liu et al. (2015)--we use the cropped and aligned 64 × 64-size version of the images--and the DC-GAN (Radford et al., 2016) architectures for the generator and discriminator. We make two minor modifications to the DC-GAN implementation that we find empirically to yield improved results (for both the standard single discriminator setting, as well as our multiple discriminator setting). First, we use different batches of generated images to update the discriminator and generator--this increases computation per iteration, but yields better quality results. Second, we employ batch normalization in the generator
5

Under review as a conference paper at ICLR 2018

Figure 4: Random sets of generated samples from traditional DC-GAN and the proposed framework. For DC-GAN, we show results from the model both at 40k iterations (when the samples are qualitatively the best) and at the end of training (100k iterations). For our setting, we show samples from the end of training for generator models trained with K = 12, 24, 48 projections. Our generator produces qualitatively better samples, with finer detail and fewer distortions. Quality is worse with subtle high-frequency noise when K is smaller, but these decrease with increasing K to 24 and 48.

but not in the discriminator (the original implementation normalized real and generated batches separately, which we found yielded poorer generators).

For our approach, we train a generator against K discriminators, each operating on a dif-

ferent single-channel 32 × 32 projected version of the input, i.e., d/m = 12. The projected

images are generated through convolution with 8 × 8 filters and a stride of two. The filters

are generated randomly and kept constant throughout training. We compare this to the

standard DC-GAN setting of a single discriminator that looks at the full-resolution 64 × 64

color image. We use identical generator architectures in both settings--that map a 100 di-

mensional uniformly distributed noise vector to a full resolution image. The discriminators

also have similar architectures--but each of the discriminator in our setting has one less layer

as it operates on a lower resolution input (we map the number of channels in the first layer

in our setting to those of the second layer of the full-resolution single-discriminator, thus

matching the size of the final feature vector used for classification). As is standard practice,

we compute generator gradients in both settings with respect to minimizing the "incorrect"

classification

loss

of

the

discriminator--in

our

setting,

this

is

given

by

-

1 K

k log Dk(G(z)).

As suggested in (Radford et al., 2016), we use Adam (Kingma & Ba, 2014) with learning

rate 2 × 10-4, 1 = 0.5, and a batch size of 64.

Stability. We begin by analyzing the evolution of generators in both settings through training. Figure 2 shows the generator training loss for traditional DC-GAN with a single discriminator, and compares it the proposed framework with K = 48 discriminators. In both settings, the generator losses increase through much of training after decreasing in the initial iterations (i.e., the discriminators eventually "win"). However, DC-GAN's generator loss

6

Under review as a conference paper at ICLR 2018
Figure 5: Interpolating in latent space. For selected pairs of generated faces (with K = 48), we generate samples using different convex combinations of their corresponding noise vectors. Every combination generates a plausible face, and these appear to smoothly interpolate between various facial attributes--age, gender, expression, hair, etc. Note that the  = 0.5 sample always corresponds to an individual clearly distinct from the original pair.
rises quickly and remains higher than ours in absolute terms throughout training. Figure 3 includes examples of generated samples from both generators across iterations (from the same noise vectors). We observe that DC-GAN's samples improve mostly in the initial iterations while the training loss is still low, in line with our intuition that generator gradients become less informative as discriminators get stronger. Indeed, the quality of samples from traditional DC-GAN actually begins to deteriorate after around 40k iterations. In contrast, the generator trained in our framework improves continually throughout training. Consistency. Beyond stability, Fig. 3 also demonstrates the consistency of our framework. While the average loss in our framework is lower, we see this does not impede our generator's ability to learn the data distribution quickly as it collates feedback from multiple discriminators. Indeed, our generator produces higher-quality samples than traditional DC-GAN even in early iterations. Figure 4 includes a larger number of (random) samples from generators trained with traditional DC-GAN and our setting. For DC-GAN, we include samples from both roughly the end (100k iterations) of training, as well as from roughly the middle (40k iterations) where the sample quality are approximately the best. For our approach, we show results from training with different numbers of discriminators with K = 12, 24, and 48--selecting the generator models from the end of training for all. We see that our generators produce face images with higher-quality detail and far fewer distortions than traditional DC-GAN. We also note the effect of the number of discriminators on sample quality. Specifically, we find that setting K to be equal only to the projection ratio d/m = 12 leads to subtle high-frequency noise in the generator samples, suggesting these many projections do not sufficiently constrain the generator to learn the full data distribution. Increasing K diminishes these artifacts, and K = 24 and 48 both yield similar, high-quality samples. Training Time. Note that the improved generator comes at the expense of increased computation during training. Traditional DC-GAN with a single discriminator takes only 0.6s per training iteration (on an NVIDIA Titan X), but this goes up to 3.2s for K = 12, 5.8s for K = 24, and 11.2s for K = 48 in our framework. Note that once trained, all generators produce samples at the same speed as they have identical architectures.
7

Under review as a conference paper at ICLR 2018
Figure 6: Examples from training on canine images from Imagenet. We show manually selected examples of 128 × 128 images produced by a generator trained on various canine classes from Imagenet. Although not globally plausible, the samples contain realistic lowlevel textures, and reproduce rough high-level composition.
Latent Embedding. Next, we explore the quality of the embedding induced by our generator (K = 48) in the latent space of noise vectors z. We consider selected pairs of randomly generated faces, and show samples generated by linear interpolation between their corresponding noise vectors in Fig. 5. Each of these generated samples is also a plausible face image, which confirms that our generator is not simply memorizing training samples, and that it is densely packing the latent space with face images. We also find that the generated samples smoothly interpolate between semantically meaningful face attributes-- gender, age, hair-style, expression, and so on. Note that in all rows, the sample for  = 0.5 appears to be a clearly different individual than the ones represented by  = 0 and  = 1. Results on Imagenet-Canines. Finally, we show results on training a generator on a subset of the Imagenet-1K database (Deng et al., 2009). We use 128 × 128 crops (formed by scaling the smaller side to 128, and taking a random crop along the other dimension) of 160k images from a subset of Imagenet classes (ids 152 to 281) of different canines. We use similar settings as for faces, but feed a higher 200-dimensional noise vector to the generator, which also begins by mapping this to a feature vector that is twice as large (2048), and which has an extra transpose-convolution layer to go upto the 128 × 128 resolution. We again use 8 × 8 convolutional filters with stride two to form {WkT }--in this case, these produce 64 × 64 single channel images. We use only K = 12 discriminators, each of which has an additional layer because of the higher-resolution--beginning with fewer channels in the first layer so that the final feature vector used for classification is the same length as for faces. Figure 6 shows manually selected samples after 100k iterations of training (see supplementary material for a larger random set). We see that since it is trained on a more diverse and unaligned image content, the generated images are not globally plausible photographs. Nevertheless, we find that the produced images are sharp, and that generator learns to reproduce realistic low-level textures as well as some high-level composition.
5 Conclusion
In this paper, we proposed a new framework to training GANs for high-dimensional outputs. Our approach employs multiple discriminators on random low-dimensional projections of the data to stabilize training, with enough projections to ensure that the generator learn the true data distribution. Experimental results demonstrate that this approaches leads to more stable training, with generators continuing to improve for longer to ultimately produce higher-quality samples. Source code and trained models for our implementation is available at the project page [anonymized for review]. In our current framework, the number of discriminators is limited by computational cost. In future work, we plan to investigate training with a much larger set of discriminators, employing only a small subset of them at each iteration, or every set of iterations. We are also interested in using multiple discriminators with modified and regularized objectives (e.g., (Nowozin et al., 2016; Arjovsky et al., 2017; Zhao et al., 2016)). Such modifications are complementary to our approach, and deploying them together will likely be beneficial.
8

Under review as a conference paper at ICLR 2018
References
Martin Arjovsky and L´eon Bottou. Towards principled methods for training generative adversarial networks. In NIPS Workshop on Adversarial Training, 2016.
Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein gan. arXiv:1701.07875 [stat.ML], 2017.
Nicolas Bonneel, Julien Rabin, Gabriel Peyr´e, and Hanspeter Pfister. Sliced and radon wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 2015.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In NIPS, 2016.
Sanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science. IEEE, 1999.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In CVPR, 2009.
Emily Denton, Sam Gross, and Rob Fergus. Semi-supervised learning with contextconditional generative adversarial networks. arXiv:1611.06430 [cs.CV], 2016.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS, 2015.
Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan. Generative multi-adversarial networks. arXiv:1611.01673 [cs.LG], 2016.
Yoav Freund, Robert E Schapire, et al. Experiments with a new boosting algorithm. In ICML, 1996.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. arXiv:1611.07004 [cs.CV], 2016.
Robert A Jacobs. Methods for combining experts' probability assessments. Neural computation, 1995.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Christian Ledig, Lucas Theis, Ferenc Husza´r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. arXiv:1609.04802 [cs.CV], 2016.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In NIPS, 2016.
Alec Radford, Luke Metz, and Soumit Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, 2016.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv:1011.3027 [math.PR], 2010.
9

Under review as a conference paper at ICLR 2018 Yaxing Wang, Lichao Zhang, and Joost van de Weijer. Ensembles of generative adversarial
networks. arXiv:1612.00991 [cs.CV], 2016. Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial net-
work. arXiv:1609.03126 [cs.LG], 2016.
10

Under review as a conference paper at ICLR 2018

Supplementary Material

A Theory
In this section we provide additional theoretical results showing the benefits of random projections, for some particular choice of distributions, for both improving stability and guaranteeing consistency of GAN training.

Stability
Here, we consider several simplifying assumptions on the distribution of x, to gain intuition as to how a random low-dimensional projection affects the "relative" volume of the support of Px. Let us assume that the range of x is the d-dimensional ball Bd of radius 1 centered at 0. We define the support supp(Px)  Bd of a distribution Px to be the set where the density is greater than some small threshold . Assume that the projection W  Rd×m is entry-wise random Gaussian (rather than corresponding to a random convolution). Denote as BWd the projection of the range on W , and PW T x as the marginal of Px along W .
Theorem A.1. Assume Px = j jN (x|µj, j) is a mixture of Gaussians, such that the individual components are sufficiently well separated (in the sense that there is no overlap between their supports or the projections thereof, see (Dasgupta, 1999)). If supp(Px)  Bd and Vol(supp(Px)) > 0, then Vol(supp(PW T x))/Vol(BWd ) > Vol(supp(Px))/Vol(Bd) with high probability.
This result implies that the projection of the support of Px, under this simplified assumptions, occupies a higher fraction of the volume of the projection of the range of x. This aids in stability because it makes it more likely that a larger fraction of the generator's samples (which also lie within the range of x) will, after projection, overlap with the projected support supp(Px), and can not be rejected absolutely by the discriminator.

Proof of Theorem A.1. We first show that we can assume that the columns of the

projection W are orthonormal. Since W  Rd×m is entry-wise Gaussian distributed, it

has rank m with high probability. Then, there exists a square invertible matrix A such

that W  = AW where W  is orthonormal. In that case, Vol(supp(PW T x))/Vol(BWd ) =

Vol(supp(PW



T x

))/Vol(BWd



)

because

the

numerator

and

denominator

terms

for

both

can

be related by det(A) for the change of variables, which cancels out. Note that under this

orthonormal assumption, BWd = Bm.

Next, we consider the case of an individual Gaussian distribution Px = N (x|µ, ), and prove that the ratio of supports (defined with respect to a threshold ) does not decrease with the
projection. The expression for these ratios is given by:

Vol(supp(Px)) = Vol(Bd) × det() ×

log

1 2

- d log 2

- log det()



Vol(supp(Px)) Vol(Bd)

=

det()

×

log

1 2

- d log 2

- log det()

.

Vol(supp(PW T x)) Vol(BWd )

=

det(W T W ) ×

log

1 2

- m log 2

- log det(W T W )

.

(4) (5)

For sufficiently small , the volume ratio of a single Gaussian will increase with projection if det(W T W ) > det(). Note that all eigenvalues of   1, with at-least one eigenvalue strictly < 1 (since supp(Px)  Bd). First, we consider the case when  is not strictly positive
definite and one of the eigenvalues is 0. Then, Vol(supp(Px)) = 0 and Vol(supp(PW T x))  0, i.e., the volume ratio either stays the same or increases.

11

Under review as a conference paper at ICLR 2018

For the case when all eigenvalues are strictly positive, consider a co-ordinate transform where the first m co-ordinates of x correspond to the column vectors of W , such that

=

W TW W 

W W  W 

,

(6)

where W = W T W . Then,

det() = det(W ) det(W  - WT W  -W1W W  )  det(W ) det(W  ),
 det(W )  det()/ det(W  ).

(7)

Note that det(W  )  1, since all eigenvalues of  are  1, with equality only when W is completely orthogonal to the single eigenvector whose eigenvalue is strictly < 1, which has
probability zero under the distribution for W . So, we have that det(W  ) < 1, and

det(W T W ) = det(w) > det().

(8)

The above result shows that the volume ratio of individual components never decrease, and
always increase when their co-variance matrices are full rank (no zero eigenvalue). Now,
we consider the case of the Gaussian mixture. Note that the volume ratio of the mixture equals the sum of the ratios of individual components, since the denominator Vol(Bm) is
the same, where the support volume in these ratios for component j is defined with respect
to a threshold /j. Also, note that since mixture distribution has non-zero volume, at least one of the Gaussian components must have all non-zero eigenvalues. So, the volume ratios
of Px and PW T x are both sums of individual Gaussian component terms, and each term for PW T x is greater than or equal to the corresponding term for Px, and at least one term is strictly greater. Thus, the support volume ratio of PW T x is strictly greater than that of Px.

Consistency

Proof of Theorem 3.1. The proof follows along the same steps as that of Theorem 1 in Goodfellow et al. (2014).

V (Dk, G)

= ExPx [log Dk(WkT x)] + ExPg [log(1 - Dk(WkT x))]

=

EY

PWkT x [log

Dk(y)]

+

EyPWkT

[log(1
g

-

Dk(y))].

(9)

For any point y  supp(PWkT x)supp(PWkT g), differentiating V (Dk, G) w.r.t. Dk and setting to 0 gives us:

Dk (y )

=

PWkT

PWkT x x(y) +

(y) PWkT

g

(y)

.

(10)

Notice we can rewrite V (Dk, G) as

V (Dk, G) = -2 log(2) + KL

PWkT x|| PWkT x

+ 2

PWkT g

+ KL

PWkT

g|| PWkT x

+ 2

PWkT g

.

(11)

Here KL is the Kullback Leibler divergence, and it is easy to see that the above expression achieves the minimum value when PWkT x = PWkT g.

Next, we present a result that shows that given enough random projections K, the full distribution Pg of generated samples will closely match the full true distribution Px.

12

Under review as a conference paper at ICLR 2018

Def: A function f : Rd  R+ is L-Lipschitz, if  x1, x2  Rd, |f (x1)-f (x2)|  L·d(x1, x2).
Theorem A.2. Let Px and Pg be two compact distributions with support of radius B, such that PWkT x = PWkT g, k  {1, · · · , K}. Let {Wk} be entrywise random Gaussian matrices in Rd×m. Let R = Px - Pg be the residual function of the difference of densities. Then, with high probability, for any x  Rd,

|R(x)| = |Px(x) - Pg(x)|  O where LR is the Lipschitz constant of R.

BLR K1
d-m

,

This theorem captures how much two probability densities can differ if they match along K marginals of dimension m, and how the error decays with increasing number of discriminators K. In particular, if we have a smooth residual function R (with small Lipschitz constant LR), then at any point  Rd, it is constrained to have small values--smaller with increasing number of discriminators K, and higher dimension m. The dependence on LR suggests that the residual can take larger values if it is not smooth--which can result from either the true density Px or the generator density Pg, or both, being not smooth.
Again, this result gives us rough intuition about how we may expect the error between the true and generator distribution to change with changes to the values of K and m. In practice, the empirical performance will depend on the nature of the true data distribution, the parametric form / network architecture for the generator, and the ability of the optimization algorithm to get near the true optimum in Theorem 3.1.

Proof of Theorem A.2. Let R = Px - Pg, be the residual function that captures the difference between the two distributions. R satisfies the following properties:

and for any set S,

R(x)dx = (Px(x) - Pg(x))dx = 1 - 1 = 0,
xx

(12)

R(x)dx 

Px(x)dx  1.

xS

xS

(13)

Further, since both the distributions have same marginals along k different directions {Wk}, we have, for any x,

RWkT y(x) =

R(y) = 0.
y|x=WkT y

We first prove this result for discrete distributions supported on a compact set S with  points along each dimension. Let P~ denote such a discretization of a distribution P and R~ = P~x - P~g.

Each of the marginal equation P~WkT x = P~WkT g (R~WkT y = 0) is equivalent to m linear equations of the distribution P~x of the form, x:WkT x=y P~x(x) = P~WkT g(y). Note that we have d choices for x and m choices for y. Let Ak  Rm×d denote the coefficient matrix AkP~x = P~WkT g, such that Ak(i, j) = 1 if WkT xi = yj, and 0 otherwise.
The rows of Ak for different values of yj are clearly orthogonal. Further, since different Wk are independent Gaussian matrices, rows of Ak corresponding to different Wk are linearly independent. In particular let A  RmK×d denote the vertical concatenation of Ak. Then, A has full row rank of m ·K with probability  1-c·m·e-d (Vershynin, 2010), with c some arbitrary positive constant. Since P~x is a d dimensional vector, d linearly independent equations determine it uniquely. Hence m · K  d, guarantees that P~x = P~g or R~ = 0.

Now we extend the results to the continuous setting. Without loss of generality, let the

compact support S of the distributions be contained in a sphere of radius B. Let N  be

L

an

 LR

net

of

S,

with

Then for every point

d x1

points (see Lemma 5.2 in Vershynin (2010)), where  = 2B



S,

there

exists

a

x2



N LR

such that, d(x1, x2) 

 LR

.

·

LR/.

13

Under review as a conference paper at ICLR 2018

Further

for

any

x1, x2

with

d(x1, x2)



 LR

,

if

R

is

LR

Lipschitz

we

know

that,

|R(x1) - R(x2)|



LR

·

 LR

=

.

(14)

Finally, notice that the marginal constraints do not guarantee that the distributions P~WkT x and P~WkT g match exactly on the -net (or that R is 0), but only that they are equal upto

an additive factor of . Hence, combining this with equation 14 we get, |R(x)| = |Px(x) -

Pg(x)|  O(), for any x with probability  1 - c · m · K · e-d. Since we have d-m =

BL 

d-m

=

O(

1 K

),

we

get



=

O(

BL
1

K d-m

).

14

Under review as a conference paper at ICLR 2018
B Additional Experimental Results
Face Images: Proposed Method (K = 48)
15

Under review as a conference paper at ICLR 2018 Face Images: Proposed Method (K = 24)
16

Under review as a conference paper at ICLR 2018 Face Images: Proposed Method (K = 12)
17

Under review as a conference paper at ICLR 2018 Face Images: Traditional DC-GAN (Iter. 40k)
18

Under review as a conference paper at ICLR 2018 Face Images: Traditional DC-GAN (Iter. 100k)
19

Under review as a conference paper at ICLR 2018 Random Imagenet-Canine Images: Proposed Method
20

