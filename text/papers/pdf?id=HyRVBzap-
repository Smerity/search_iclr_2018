Under review as a conference paper at ICLR 2018
CASCADE ADVERSARIAL MACHINE LEARNING REGULARIZED WITH A UNIFIED EMBEDDING
Anonymous authors Paper under double-blind review
ABSTRACT
Deep neural network classifiers are vulnerable to small input perturbations carefully generated by the adversaries. Injecting adversarial inputs during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). This additional regularization encourages two similar images (clean and perturbed versions) to produce the same outputs, not necessarily the true labels, enhancing classifier's robustness against pixel level perturbation. Next, we show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we also propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.
1 INTRODUCTION
Deep neural networks and other machine learning classifiers are shown to be vulnerable to small perturbations to inputs Biggio et al. (2013); Szegedy et al. (2014); Goodfellow et al. (2015); Papernot et al. (2017); Kurakin et al. (2017a). Previous works Goodfellow et al. (2015); Kurakin et al. (2017a); Huang et al. (2015) have shown that injecting adversarial examples during training (adversarial training) increases the robustness of a network against adversarial attacks. The adversarial examples can be generated by perturbing the inputs in one step or iteratively to either minimize confidence on true labels or increase confidence of a target false label (section 2). The networks trained with one-step methods have shown noticeable robustness against one-step attacks, but, limited robustness against iterative attacks at test time. In Kurakin et al. (2017a), authors explained that the use of iterative methods during training didn't help improve robustness against iterative attacks at test time.
To address this challenge, we (1) propose adversarial training regularized with a unified embedding for both classification and low (pixel) level similarity learning to efficiently ignore pixel level perturbation; (2) provide insights into adversarial training and label leaking phenomenon reported in the previous work Kurakin et al. (2017a) by analyzing correlation between gradients w.r.t. the clean images and gradients w.r.t. their corresponding adversarial images; (3) provide through transferability analysis between purely trained networks and defended networks; and (4) propose cascade adversarial training which transfers the knowledge of the end results of adversarial training.
Low level similarity learning: we advance the previous data augmentation approach by adding additional regularization in deep features to encourage a network to be insensitive to adversarial perturbation and its vicinity. In particular, we inject adversarial images in the mini batch without
1

Under review as a conference paper at ICLR 2018
replacing their corresponding clean images and penalize distance between embeddings from the clean and the adversarial examples.
There are past examples of using embedding space for learning similarity of high level features like face similarity between two different images Schroff et al. (2015); Parkhi et al. (2015); Wen et al. (2016). Instead, we use the embedding space where the high level features are typically represented, for learning similarity of the pixel level differences between two similar images. The intuition of using this regularization is that small difference on input should not drastically change the high level feature representation.
We train ResNet models He et al. (2016) on MNIST LeCun & Cortes (2010) and CIFAR10 dataset Krizhevsky (2009) using the proposed adversarial training. The experimental results demonstrate improved robustness of the network against adversarial images generated by one-step and iterative methods compared to the prior work. We show that modifying the weight of the distance measure in the loss function can help control trade-off between accuracies for the clean and adversarial examples.
Analysis of adversarial training: we provide fruitful insights into adversarial training by analyzing correlation between gradients w.r.t. the clean images and gradients w.r.t. their corresponding adversarial images as a measure of error surface similarity. We show negative correlation between the gradient w.r.t. the clean image and the gradient w.r.t. its corresponding adversarial image is the reason for the label leaking phenomenon, reported in the previous work Kurakin et al. (2017a). We argue that label leaking phenomenon can always happen as long as we train networks by injecting one-step adversarial images due to the negative correlation effect and the similarity between gradients w.r.t. one-step adversarial images.
Transferability analysis: we also analyze transferability between defended networks, and between pure and defended networks by testing accuracies for the target network with the adversarial examples crafted from different networks (black box attack). We show one-step adversarial images from defended networks become weaker than those from undefended networks since adversarial training makes gradient seen from the clean image not to point strong adversarial image by distorting the error surface. We also make interesting observation that iter FGSM images (section 2) transfer more between networks when the source and the target networks are trained with the same training method.
Cascade adversarial training: inspired by this observation, we propose cascade adversarial training which transfers the knowledge of the end results of adversarial training. In particular, we train a network by injecting iter FGSM images crafted from already defended network in addition to the one-step adversarial images crafted from the network being trained. The concept of using already trained networks for adversarial training is also introduced in Trame`r et al. (2017). In their work, purely trained network is merely used as another source network for one-step adversarial examples generation for training. On the contrary, our cascade adversarial training uses already trained defended networks for iter FGSM images generation.
Together with cascade adversarial training and low-level similarity learning, we achieve accuracy increase against unknown iterative attacks, but at the expense of decreased accuracy for one-step attacks. We also show our cascade adversarial training and low level similarity learning gives much better robustness against black box attack.
2 BACKGROUND ON ADVERSARIAL ATTACKS
One-step fast gradient sign method (FGSM), referred to as "step FGSM", tries to generate adversarial images to minimize confidence on true label (Goodfellow et al. (2015)). Adversarial image Xadv is generated by adding sign of the gradients w.r.t. the clean image X multiplied by  [0, 255] as shown below:
Xadv = X + sign(X J (X, ytrue))
One-step target class method tries to generate adversarial images to maximize the confidence on a target false label as follows:
Xadv = X - sign(X J (X, ytarget))
2

Under review as a conference paper at ICLR 2018

Clean images

Adversarial images w/o replacing the clean images

' ' ''

CNN Architecture

car ship
airplane
dog Clean embeddings
car' ship'
airplane' dog'
Adversarial embeddings

Softmax, Cross entropy
loss
+ Total loss
Distance based loss

Figure 1: Adversarial training framework regularized with a unified embedding. Adversarial images are used without replacing the clean images. Distance based loss is added to ensure the distance between the two embeddings per each image minimized.

We use least likely class yLL as a target class and refer this method as "step ll" as in Kurakin et al. (2017b;a).
Basic iterative method, referred to as "iter FGSM", applies FGSM with small  multiple times. X0adv = X, XNadv = ClipX, XNad-v1 +  sign(XNad-v1 J (XNad-v1, ytrue))
We use  = 1, number of iterations N to be min( + 4, 1.25 ). ClipX, is elementwise clipping function where the input is clipped to the range [max(0, X - ), min(255, X + )].
Iterative least-likely class method, referred to as "iter ll", is to apply "step ll" with small  multiple times.
X0adv = X, XNadv = ClipX, XNad-v1 -  sign(XNad-v1 J (XNad-v1, yLL))
Carlini and Wagner attack Carlini & Wagner (2017) referred to as "CW" solves an optimization problem which minimizes both an objective function f (such that attack is success if and only if f (Xadv) < 0) and a distance measure between Xadv and X.
Black box attack is performed by testing accuracy on a target network with the adversarial images crafted from a source network different from the target network. Lower accuracy means successful black-box attack. When we use the same network for both target and source network, we call this as white-box attack.
Adaptive attack is another form of black box attack where adversaries can collect input and output pairs for a target network by querying arbitrary inputs. This can be considered as stronger/weaker attack than pure black box/white box attack model. In this paper, we don't consider this since it will be covered by considering both white-box and black box attacks.
3 LOW LEVEL SIMILARITY LEARNING
3.1 REGULARIZATION WITH A UNIFIED EMBEDDING
Basic idea of adversarial training proposed in Kurakin et al. (2017a) is to inject adversarial examples during training. In this method, k examples are taken from the mini batch B (size of m) and the adversarial examples are generated with one of step methods discussed in section 2. The k adversarial examples replaces the corresponding clean examples when making mini batch. Below we refer this adversarial training method as "Kurakin's".
We advance the algorithm proposed in Kurakin et al. (2017a) by adding low level similarity learning. Unlike Kurakin et al. (2017a), we include the clean examples used for generating adversarial images in the mini batch as shown in figure 1. Once one step forward pass is performed with the mini batch, embeddings are followed by the softmax layer for the cross entropy loss for the standard classification. At the same time, we take clean embeddings and adversarial embeddings, and minimize the distance between the two with the distance based loss.
The distance based loss encourages two similar images (clean and adversarial) to produce the same outputs, not necessarily the true labels. Thus, low-level similarity learning can be considered as

3

Under review as a conference paper at ICLR 2018

Clean embeddings
Learning [bidirectional loss]

Adversarial embeddings

Learning Pivots [pivot loss]

Figure 2: (Left) Bidirectional loss. (Right) Pivot loss.

an unsupervised learning. By adding regularization in higher embedding layer, convolution filters gradually learn how to ignore such pixel-level perturbation. We have applied regularization on lower layers with an assumption that low level pixel perturbation can be ignored in lower hierarchy of networks. However, adding regularization term on higher embedding layer right before the softmax layer showed best performance. The more convolutional filters have chance to learn such similarity, the better the performance. Note that cross entropy doesn't encourage two similar images to produce the same output labels. Standard image classification using cross entropy compares ground truth labels with outputs of a network regardless of how similar training images are.

We define the total loss as follows:

1 Loss = (m - k) + k

m-k

k

L(Xi|yi) +  L(Xiadv|yi)

i=1 i=1

k
+ 2 Ldist(Eiadv, Ei)
i=1

where, Ei and Eiadv are the resulting embeddings from Xi and Xiadv, respectively. m is the size of the mini batch, k ( m/2) is the number of adversarial images in the mini batch.  is the parameter to control the relative weight of classification loss for adversarial images. 2 is the parameter to control the relative weight of the distance based loss Ldist in the total loss. We use  = 0.3, 2 = 0.0001, m = 128, k = 64 for the experiments. We use one step methods when we generate adversarial
examples from the current state of the network and tested the network for all the examples generated
by one step or iterative attack methods. As in Kurakin et al. (2017a), we used randomly chosen in the interval [0, max e] with clipped normal distribution N (µ = 0,  = max e/2), where max e is
the maximum used in training.

3.2 DISTANCE BASED LOSS
Bidirectional loss minimizes the distance between the two embeddings by moving both clean and adversarial embeddings as shown in the left side of the figure 2
Ldist(Eiadv, Ei) = ||Eiadv - Ei||NN , N  1, 2 i = 1, 2, ..., k
We tried N = 1, 2 and found not much difference between the two. We report the results with N = 2 for the rest of the paper otherwise noted. When N = 2, Ldist becomes L2 loss. Pivot loss minimizes the distance between the two embeddings by moving only the adversarial embeddings as shown in the right side of the figure 2
Ldist(Eiadv|Ei) = ||Eiadv - Ei||NN , N  1, 2 i = 1, 2, ..., k
In this case, clean embeddings ( Ei ) serve as pivots to the adversarial embeddings. In particular, we don't back-propagate through the clean embeddings for the distance based loss. 1 The intuition behind the use of pivot loss is that the embedding from a clean image can be treated as the ground truth embedding. We found not much difference between two distance based losses in terms of accuracy. However, we introduce these two losses for those who might be interested in applying our proposed method to give possible knobs for finetuning.
1In Tensorflow, we create non-trainable variable and update this with clean embedding Ei at every training step. Then, adversarial embedding Eiadv is compared with the non-trainable variable.

4

Under review as a conference paper at ICLR 2018

Table 1: MNIST test results (%) for 20-layer ResNet models ( = 0.3*255 at test time). { R20M:
standard training, R20MK: Kurakin's adversarial training, R20MB: Bidirectional loss, R20MP : P ivot loss.} CW L attack is performed with 100 test samples (10 samples per each class) and the number of adversarial examples with > 0.3*255 is reported. Additional details for CW attack can
be found in Appendix E

Model
R20M R20MK R20MB (Ours) R20MP (Ours)

clean
99.6 99.6 99.5 99.5

step ll
9.7 96.7 97.3 97.1

step FGSM
10.3 94.5 96.2 95.7

iter ll
0.0 89.0 97.2 96.9

iter FGSM
0.0 60.2 88.5 88.9

CW
0 46 81 82

40 20 0 20 40 40 30 20 10 0 10 20 30
(a) Standard

20 10 0 10 20 30
20

10 0 10 20 (b) Kurakin

10 5 0 5 10
10

5 0 5 10 (c) P ivot (Ours)

3987654210

Figure 3: Embedding space visualization for modified ResNet models trained on MNIST. x-axis and y-axis show first and second dimension of embeddings respectively. Scatter plot shows first 100 clean embeddings per each class on MNIST test set. Each arrow shows difference between two embeddings (one from iter FGSM images with and the other one from iter FGSM images with +4). We draw arrows from = 0 to = 76 ( 0.3*255) for one sample image per each class. We observe differences between clean and corresponding adversarial embeddings are minimized for the network trained with pivot loss.

3.3 EXPERIMENTAL RESULTS ON MNIST
We use ResNet He et al. (2016) for MNIST classification. We train networks with different methods (standard training, Kurakin's adversarial training and adversarial training with our distance based loss). Experimental details can be found in Appendix A.
Table 1 shows the accuracy results for MNIST test dataset for different types of attack methods. As shown in the table, our method achieves better accuracy than Kurakin's method for all types of attacks with a little sacrifice on the accuracy for the clean images. Even though adversarial training is done only with "step ll", additional regularization increases robustness against unknown "step FGSM", "iter ll", "iter FGSM" and CW L attacks. This shows that our low-level similarity learning can successfully regularize the one-step adversarial perturbation and its vicinity for simple image classification like MNIST.
3.4 EMBEDDING SPACE VISUALIZATION
To visualize the embedding space, we modify 20-layer ResNet model where the last fully connected layer (64x10) is changed to two fully connected layers (64x2 and 2x10). We re-train networks with standard training, Kurakin's method and our pivot loss on MNIST. 2 In figure 3, we draw embeddings (dimension=2) between two fully connected layers. As seen from this figure, adversarial images from the network trained with standard training cross the decision boundary easily as increases. With Kurakin's adversarial training, the distances between clean and adversarial embeddings are
2Modified ResNet models showed slight decreased accuracy for both clean and adversarial images compared to original ResNet counterparts, however, we observed similar trends (improved accuracy for iterative attacks for the network trained with pivot loss) as in table 1.
5

Under review as a conference paper at ICLR 2018

Table 2: CIFAR10 test results (%) for 20-layer ResNet models. { R20: standard training, R20K: Kurakin's adversarial training, R20B: Bidirectional loss, R20P : P ivot loss.} CW L attack is performed with 100 test samples (10 samples per each class) and the number of adversarial examples
with > 2 or 4 is reported.

Model
R20 R20K R20B (Ours) R20P (Ours)

clean
90.9 91.0 91.0 90.9

step ll
=2 =16
44.5 11.5 85.8 84.4 86.1 87.3 86.2 88.3

step FGSM
=2 =16
28.7 12.2 78.9 81.5 78.7 90.0 79.7 91.7

iter FGSM
=2 =4
14.0 0.4 50.6 9.8 52.0 11.2 52.0 11.3

CW
=2 =4
80 13 2 19 3 18 4

minimized compared to standard training. And our pivot loss further minimizes distance between the clean and adversarial embeddings. Note that our pivot loss also decreases absolute value of the embeddings, thus, higher 2 will eventually result in overlap between distinct embedding distributions. We also observe that intra class variation of the clean embeddings are also minimized for the network trained with our pivot loss as shown in the scatter plot in figure 3 (c).

4 ANALYSIS OF ADVERSARIAL TRAINING

In this section, we apply our low-level similarity learning on CIFAR10 dataset and provide in-depth analysis of adversarial training. We also use ResNet models and train those with different methods. Additional details for the experiments can also be found in Appendix A.

4.1 EXPERIMENTAL RESULTS ON CIFAR10
Table 2 shows the accuracy results for CIFAR10 test dataset. Again, we also observe our low-level similarity learning further improves robustness against all types of attacks compared to Kurakin's adversarial training. However, the accuracy improvements against iterative attacks (iter FGSM, CW) are limited, showing regularization effect of low-level similarity learning is not sufficient for the iterative attacks on complex color images like CIFAR10. This deserves substantive analysis and we will revisit in later sections 5 and 6 where cascade adversarial training is proposed as a possible solution after transferability analysis is presented.

4.2 EFFECT OF 2
We train networks with pivot loss and various 2s for CIFAR10 dataset to study effects of the weight of the distance measure in the loss function. Figure 4 shows that a higher 2 increases accuracy of the iteratively generated adversarial images. However, it reduces accuracy on the clean images, and increasing 2 above 0.3 even results in divergence of the training. This is because embedding distributions of different classes will eventually overlap since absolute value of the embedding will be decreased as 2 increases as seen from the section 3.4. In this experiment, we show there exists clear trade-off between accuracy for the clean images and that for the adversarial images, and we recommend using a very high 2 only under strong adversarial environment.

Accuracy (%)

65577898600505050510 4

cisstttleeeerapp_n__FlFGlGSS=MM2 ==22
10 3 10 2
2

Figure 4: Accuracy vs. 2

4.3 LABEL LEAKING ANALYSIS
We observe accuracies for the "step FGSM" adversarial images become higher than those for the clean images ("label leaking" phenomenon) by training with "step FGSM" examples as in Kurakin et al. (2017a). Interestingly, we also observe "label leaking" phenomenon even without providing

6

correlation correlation correlation

Under review as a conference paper at ICLR 2018

1.0

0.8

0.6

0.4

0.2 (1) step_ll, clean

0.0 01.20-2

(((342))) srsatteenppd__olFml,Gs,StcMelep,a_cFnleGaSnM
10-1 100

101

1.0
0.8
0.6 0.4 0.51 0.2 0.47
0.0
01.20-2 10-1

0.81 0.67
100 101

1.0
0.8
0.6 0.4 0.44 0.2 0.40
0.0
01.20-2 10-1

0.75 0.72
100 101

(a) R20: Baseline

(b) R20K : Kurakin's

(c) R20P : Pivot (Ours)

Figure 5: Averaged Pearson's correlation coefficient between the gradients w.r.t. two images. Trained networks are the same in table 2. Correlation were measured by changing for each adversarial image and averaged over randomly chosen 128 images from CIFAR10 test-set. Shaded region represents ± 0.5 standard deviation of each line.

true labels for adversarial images generation as shown in "R20P " in table 2. We argue that "label leaking" is a natural result of the adversarial training.
To understand the nature of adversarial training, we measure correlation between gradients w.r.t. different images (i.e. clean vs. adversarial) as a measure of error surface similarity. We measure correlation between gradients w.r.t. (1) clean vs. "step ll" image, (2) clean vs. "step FGSM" image, (3) clean vs. "random sign" added image, and (4) "step ll" image vs. "step FGSM" image for three trained networks (a) R20, (b) R20K and (c) R20P (Ours) in table 2. Figure 5 draws average value of correlations for each case.
Meaning of the correlation: In order to make strong adversarial images with "step FGSM" method, correlation between the gradient w.r.t. the clean image and the gradient w.r.t. its corresponding adversarial image should remain high since the "step FGSM" method only use the gradient w.r.t. the clean image ( =0). Lower correlation means perturbing the adversarial image at further to the gradient (seen from the clean image) direction is no longer efficient.
Results of adversarial training: We observe that (1) and (2) become quickly lower than (3) as increases. This means that, when we move toward the steepest (gradient) direction on the error
surface, gradient is more quickly uncorrelated with the gradient w.r.t. the clean image than when we move to random direction. As a result of adversarial training, this uncorrelation is observed at a lower making one-step attack less efficient even with small perturbation. (1), (2) and (3) for our case are slightly lower than Kurakin's method at the same which means that our method is better at defending one-step attacks than Kurakin's.
Error surface similarity between "step ll" and "step FGSM" images: We also observe (4) remains high with higher for all trained networks. This means that the error surface (gradient) of the "step ll" image and that of its corresponding "step FGSM" image resemble each other. That is the reason why we get the robustness against "step FGSM" method only by training with "step ll" method and vice versa. (4) for our case is slightly higher than Kurakin's method at the same and that means our similarity learning tends to make error surfaces of the adversarial images with "step ll" and "step FGSM" method to be more similar.
Analysis of label leaking phenomenon: Interestingly, (2) becomes slightly negative in certain range (1 < < 3 for Kurakin's, and 1 < < 4 for Pivot (Ours) ) and this could be the possible reason for "label leaking" phenomenon. For example, let's assume that we have a perturbed image (by "step FGSM" method) at where the correlation between the gradients w.r.t. that image and the corresponding clean image is negative. Further increase of with the gradient (w.r.t. the clean image) direction actually decreases the loss resulting in increased accuracy (label leaking phenomenon). Due to the error surface similarity between "step ll" and "step FGSM" images and this negative correlation effect, however, label leaking phenomenon can always happen for the networks trained with one-step adversarial examples.
7

Under review as a conference paper at ICLR 2018

Table 3: CIFAR10 test results (%) under black box attacks for =16. {Target: same networks in table 2,
Source: R202: standard training, R20K2: Kurakin's, R20P 2: w/ P ivot loss. Source networks share the same initialization which is different from the target networks.} Additional details can be found in Ap-
pendix B

correlation

0.09 0.08 0.07

sisitttteeeerrpp____FFFFGGGGSSSSMMMM,, ,,((RR((RR22220000KKKK,, ,,RRRR22220000K22K2)2)))

0.06

0.05

0.04

2 4 6 8 10 12 14 16

Target Source: step FGSM Source: iter FGSM R202 R20K2 R20P 2 R202 R20K2 R20P 2

R20 16.2 31.6
R20K 66.7 82.7 R20P 57.2 89.2

29.1 2.7 60.1 79.4 55.8 28.5 85.4 52.0 24.3

54.4 28.5 21.7

Figure 6: Correlation between adversarial noises from different networks for each . Correlation is averaged over randomly chosen 128 images from CIFAR10 testset. Shaded region shows ± 0.1 standard deviation of each line.

5 TRANSFERABILITY ANALYSIS
In table 3, we report black box attack accuracies (defined in section 2) to analyze the transferability between the networks trained with different methods. We use the same networks in table 2 as target networks. We re-train 20-layer ResNets (standard training, Kurakin's method and with Pivot loss) with the different initialization from the target networks, and use the trained networks as source networks.
Transferability (step attack): We first observe that high robustness against one-step attack between defended networks (R20K2/R20P 2 -> R20K/R20P ), and low robustness between undefended networks (R202 -> R20). This observation shows that error surfaces of neural networks are driven by the training method and networks trained with the same method end up similar optimum states. The network once showed robustness against white-box attack (R20K, R20P ) tends to show similar robustness against black-box attacks from networks trained with the same method (R20K2, R20P 2). Similarly, undefended network originally poor at white-box attack (R20) tends to be poor at blackbox attack from another undefended network (R202).
It is noteworthy to observe that the accuracies against step attack from the undefended network (R202) are always lower than those from defended networks (R20K2, R20P 2). Possible explanation for this would be that adversarial training tweaks gradient seen from the clean image to point toward weaker adversarial point along that gradient direction. As a result, one-step adversarial images from defended networks become weaker than those from undefended network. Thus, when we mount black box attack with one-step method, it is more efficient to use undefended network as a source network than a defended network. Note that defended network in this case means the network trained with one-step adversarial images. There is no guarantee that undefended networks are always better source for black box one-step attack when the target is not trained with one-step adversarial examples.
Another observation here is that, even though our method showed better performance than Kurakin's for white-box attack, our method didn't help increase robustness against black box attacks. Table 3 shows that our method (R20P ) is better or worse than Kurakin's method (R20K) at one-step black box attack when the source networks are defended networks (R20K2, R20P 2) or undefended network (R202) respectively. However, it is a statistical variation since we found not much difference when we switch the source and the target. Additional details can be found in Appendix D.
Transferability (iterative attack): First, we also didn't find any meaningful differences between our method and Kurakin's method for black box attacks with iterative methods. This is because the changes that similarity learning makes are subtle in terms of black box attacks.
Interestingly, "iter FGSM" attack remains very strong even under the black box attack scenario but only between undefended networks or defended networks. This is because iter FGSM noises (Xadv-X) from defended networks resemble each other. As shown in figure 6, we observe higher correlation between iter FGSM noises from a defended network (R20K) and those from another defended network (R20K2).
8

Under review as a conference paper at ICLR 2018

correlation correlation

0.09

0.08

0.07

0.06

0.05

0.04 0.03

sssaaammmeee iiinnniiittt,,, (((RRR1256100KK,,KRR, R526011PP))0P)

2 4 6 8 10 12 14 16

0.09 0.08

dddddiiiiiffffffffff iiiiinnnnniiiiittttt,,,,, (((((RRRRR21225600100KKKK,,,,KRRRR, R51251610610KKK22)0K)))K2)

0.07

0.06

0.05

0.04

0.03 2 4 6 8 10 12 14 16

(a) Corr: same initialization

(b) Corr: different initialization

Figure 7: Correlation between iter FGSM noises crafted from different networks for each . Corre-
lation is averaged over randomly chosen 128 images from CIFAR10 test-set. Shaded region shows ± 0.1 standard deviation of each line.

Transferability (iterative attack, various architectures): We further study the transferability of iter FGSM images between various architectures with the same or different initialization. To this end, we first train 56-layer ResNet networks (Kurakin's, pivot loss) with the same initialization. Then we train another 56-layer ResNet network (Kurakin's) with different initialization. We repeat the training for the 110-layer ResNet networks. We measure correlation between iter FGSM noises from different networks.
Figure 7 (a) shows correlation between iter FGSM noises crafted from Kurakin's network and those from Pivot network with the same initialization. As expected, we observe high correlation between iter FGSM noises from networks with the same initialization. Correlation between iter FGSM noises from the networks with different initialization, however, becomes lower as the network is deeper as shown in figure 7 (b). Since the degree of freedom increases as the network size increases, adversarially trained networks prone to end up with different states, thus, making transfer rate lower.
Difficulty of defense/attack under the black box attack scenario: As seen from the previous observation, iter FGSM images crafted from a network remain strong between the networks trained with the same strategy. Thus, it is efficient to attack an undefended/defended network with iter FGSM examples crafted from another undefended/defended networks. The problem is we (as attackers/defenders) don't have information about the target/source network under the black box attack scenario. Defenders always have to consider the worst case scenario, and this creates chicken and egg problem. We recommend reporting accuracies for the adversarial examples crafted from other networks trained with the same strategy under the black box scenario.
6 CASCADE ADVERSARIAL TRAINING
Inspired by the observation that iter FGSM images transfer well between defended networks, we propose cascade adversarial training, which trains a network by injecting iter FGSM images crafted from an already defended network. We hypothesize that the network being trained with cascade adversarial training will learn to avoid such adversarial perturbation, enhancing robustness against iter FGSM attack. The intuition behind this proposed method is that we transfer the knowledge of the end results of adversarial training.
Experimental results: We train networks with/without low-level similarity learning by injecting iter FGSM images crafted from the already defended network (R20P in table 3) in addition to one-step adversarial images generated from the network being trained. In particular, for cascade training, iter FGSM images are crafted from R20P with CIFAR10 training images for = 1,2, ..., 16, and those are used randomly together with step ll examples from the network being trained. 3 We train networks with the same initialization used for R20P since the transferability becomes
332 "step ll" examples from the network being trained, 32 "iter FGSM" examples from the defended network, and 64 clean examples are used in one mini-batch.
9

Under review as a conference paper at ICLR 2018

Table 4: CIFAR10 test results (%) for cascade/ensemble networks. { R20P : P ivot loss, R20E:
Ensemble training, R20K,C: Kurakin's and Cascade training, R20P,E: P ivot loss and Ensemble training, and R20P,C: P ivot loss and Cascade training }

Model
R20P (Ours) R20E R20K,C (Ours) R20P,E (Ours) R20P,C (Ours)

clean
90.9 91.0 91.2 90.4 90.3

step ll
=2 =16 86.2 88.3
84.6 69.5
84.9 71.7 84.2 61.3 84.7 74.0

step FGSM
=2 =16 79.7 91.7
80.6 60.9
79.4 64.3 78.2 50.8 80.4 65.6

iter FGSM
=2 =4 52.0 11.3
62.1 23.8
64.8 24.9 64.9 32.6 67.4 38.9

CW
=2 =4 18 4
21 4
17 2 30 9 24 12

Table 5: CIFAR10 test results (%) for cascade/ensemble 110-layer ResNet models. {R110P : P ivot
loss, R110E: Ensemble training, R110K,C : Kurakin's and Cascade training, R110P,E: P ivot loss and Ensemble training, and R110P,C: P ivot loss and Cascade training}

Model
R110K R110P (Ours)
R110E R110K,C (Ours) R110P,E (Ours) R110P,C (Ours)

clean
92.3 92.3
92.3 92.3 91.3 91.5

step ll
=2 =16
88.3 90.7 86.0 89.4
86.3 74.3 86.2 72.8 84.0 65.7 85.7 76.4

step FGSM
=2 =16
86.0 95.2 81.6 91.6
84.1 72.9 82.6 66.7 77.6 54.5 82.4 69.1

iter FGSM
=2 =4
59.4 9.2 64.1 20.9
63.5 21.1 69.3 33.4 66.8 38.3 73.5 42.5

CW
=2 =4
25 4 32 7
24 6 20 5 38 16 27 15

higher within the networks sharing the same initialization. In this experiment, we also train networks with ensemble adversarial training (Trame`r et al. (2017)) with/without low-level similarity learning for comparison. The implementation details for the trained models can be found in Appendix B.
Table 4 shows experiment results for the cascade/ensemble models with/without pivot loss. We find several meaningful observations in table 4. First, ensemble and cascade models show improved accuracy against iterative attack although at the expense of decreased accuracy for one-step attacks compared to the baseline defended network (R20P ). Additional data augmentation from other networks enhances the robustness against iterative attack, weakening label leaking effect caused by one-step adversarial training.
Second, our low-level similarity learning (R20P,E, R20P,C ) further enhances robustness against iterative attacks including fully unknown CW attack. Additional knowledge learned from data augmentation through cascade/ensemble adversarial training enables networks to learn partial knowledge of perturbations generated by an iterative method. And the learned iterative perturbations become regularized further with our low-level similarity learning making networks robust against unknown iterative attacks.
Third, our low-level similarity learning serves as a good regularizer for adversarial images, but not for the clean images for ensemble/cascade models (reduced accuracy for the clean images for R20P,E and R20P,C in table 4). Compared to pure adversarial training with one-step adversarial examples, the network sees more various adversarial perturbations during training as a result of ensemble and cascade training. Those perturbations are prone to end up embeddings in the vicinity of decision boundary more often than perturbations caused by one-step adversarial training. Pivot loss pulls the vicinity of those adversarial embeddings toward their corresponding clean embeddings. During this process, clean embeddings from other classes might also be moved toward the decision boundary which results in decreased accuracy for the clean images.
Deeper networks: We study the application of similarity learning and cascade adversarial training to higher capacity networks to see whether we can recover the accuracy against one-step attacks.
10

Under review as a conference paper at ICLR 2018

Table 6: CIFAR10 test results (%) for 110-layer ResNet cascade/ensemble networks under black box attacks ( =16). {Target: same networks in table 5, Source: re-trained baseline, Kurakin's, cascade
and ensemble networks with/without pivot loss. Source networks use the different initialization from the target networks. Additional details of the models can be found in Appendix B.}

Target

Source: iter FGSM R1102 R110K2 R110E2 R110P 2 R110K,C2 R110P,E2 R110P,C2

R110K

70.5 73.2 27.9 77.0

67.3

54.6

80.8

R110E

77.9 79.5 55.8 79.0

68.2

54.7

82.7

R110P (Ours) 75.9 75.6 39.6 78.5

68.3

61.3

83.3

R110K,C (Ours) 56.4 80.2 61.1 79.5

67.4

62.6

82.1

R110P,E (Ours) 78.2 82.1 67.7 81.7

73.4

68.4

83.8

R110P,C (Ours) 71.9 80.4 63.9 80.1

71.1

64.2

83.0

First, we report accuracy for 110-layer adversarially trained ResNet models (R110K and R110P used in in table 5). We observe higher network capacity improves overall accuracy compared to 20-layer counter-part (R20K and R20P ) as expected from Kurakin et al. (2017a).
Next, we use the defended network, R110P , for cascade adversarial training and train networks with the same initialization used for R110P . We choose R110P since the iter FGSM images transfer well between networks with the same architecture and initialization as seen from the previous section 5. We also train ensemble models as a comparison. Again, additional training details can be found in Appendix B. Overall, accuracy against one-step and iterative attacks are improved as shown in table 5 compared to 20-layer counterparts. Interestingly, ensemble model (R110E) doesn't show meaningful accuracy improvement against iter FGSM images compared to 20-layer ensemble model (R20E). However, together with pivot loss, all ensemble/cascade networks show improved accuracy against iter FGSM attack as well as CW attack. The results show that deeper networks with our methods (low-level similarity learning and cascade adversarial training) can help further increase robustness against both one-step and iterative adversarial attacks.
Black box attack analysis for the cascade networks: We finally perform black box attack analysis for the cascade/ensemble networks with/without pivot loss. As explained in section 5, we report black box attack accuracy with the source networks trained with the same method, but with different initialization from the target networks. We re-train 110-layer ResNet models using Kurakin's/cascade/ensemble adversarial training with/without low-level similarity learning and use those networks as source networks for black-box attacks. Baseline 110-layer ResNet model is also included as a source network. Target networks are the same networks used in table 5. We found iter FGSM attack resulted in lower accuracy than step FGSM attack, thus, report iter FGSM attack results only in table 6.
We first observe that iter FGSM attack from ensemble models (R110E2, R110P,E2) is strong (results in lower accuracy) compared to that from any other trained networks. 4 Since ensemble models learn various perturbation during training, adversarial noises crafted from those networks might be more general for other networks making them transfer easily between defended networks.
Second, cascade adversarial training breaks chicken and egg problem. (In section 5, we found that it is efficient to use a defended network as a source network to attack another defended network.) Even though the transferability between defended networks is reduced for deeper networks, cascade network (R110K,C) shows worst case performance against the attack not from a defended network, but from a purely trained network (R1102). Possible solution to further improve the worst case robustness would be to use more than one network as source networks (including pure/defended networks) for iter FGSM images generation for cascade adversarial training.
Third, ensemble/cascade networks together with our low-level similarity learning (R110P,E, R110P,C) show better worst case accuracy under black box attack scenario. This shows that enhanc-
4We also observed this when we switch the source and the target networks. Additional details can be found in Appendix D.
11

Under review as a conference paper at ICLR 2018
ing robustness against iterative white box attack also improves robustness against iterative black box attack.
7 CONCLUSION
We proposed adversarial training regularized with a unified embedding for classification and lowlevel similarity learning by penalizing distance between the clean and their corresponding adversarial embeddings. The networks trained with low-level similarity learning showed higher robustness against one-step and iterative attacks under white box attack.
We showed that modifying the weight of the distance measure can help control trade-off between accuracies for clean and adversarial examples. We analyzed error surface similarities between the clean and their corresponding adversarial images and found label leaking phenomenon is a natural result of adversarial training.
We performed through transfer analysis and showed iter FGSM images transfer easily between networks trained with the same strategy. We exploited this and proposed cascade adversarial training, a method to train a network with iter FGSM adversarial images crafted from already defended networks. Combining those two techniques (low level similarity learning + cascade adversarial training) with deeper networks further improved robustness against iterative attacks for both white-box and black-box attacks.
However, there is still a gap between accuracy for the clean images and that for the adversarial images. Improving robustness against both one-step and iterative attacks still remains challenging since it is shown to be difficult to train networks robust for both one-step and iterative attacks simultaneously. Future research is necessary to further improve the robustness against iterative attack without sacrificing the accuracy for step attacks or clean images under both white-box attack and black-box attack scenarios.
REFERENCES
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In ECML/PKDD (3), volume 8190 of Lecture Notes in Computer Science, pp. 387­402. Springer, 2013. ISBN 978-3-642-40993-6. URL http://dblp.uni-trier.de/db/conf/pkdd/ pkdd2013-3.html#BiggioCMNSLGR13.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy, 2017.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770­778, 2016. doi: 10.1109/CVPR.2016.90. URL http://dx.doi.org/10.1109/CVPR.2016.90.
Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesva´ri. Learning with a strong adversary. CoRR, abs/1511.03034, 2015. URL http://arxiv.org/abs/1511.03034.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In Proceedings of the International Conference on Learning Representations (ICLR), 2017a.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In Proceedings of the International Conference on Learning Representations (ICLR) Workshop, 2017b.
12

Under review as a conference paper at ICLR 2018
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann. lecun.com/exdb/mnist/.
Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical black-box attacks against deep learning systems using adversarial examples. In Proceeedings of the ACM Asia Conference on Computer and Communications Security (ASIACCS'17), 2017.
O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proceedings of the British Machine Vision Conference (BMVC), 2015.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Proceedings of the International Conference on Learning Representations (ICLR), 2014.
Florian Trame`r, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. CoRR, abs/1705.07204, 2017. URL http: //arxiv.org/abs/1705.07204.
Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recognition. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VII, pp. 499­ 515, 2016. doi: 10.1007/978-3-319-46478-7 31. URL http://dx.doi.org/10.1007/ 978-3-319-46478-7_31.
13

Under review as a conference paper at ICLR 2018
A EXPERIMENTAL SETUP
We scale down the image values to [0,1] and don't perform any data augmentation for MNIST. For CIFAR10, we scale down the image values to [0,1] and subtract per-pixel mean values. We perform 24x24 random crop and random flip on 32x32 original images. We generate adversarial images with "step ll" after these steps otherwise noted. We use stochastic gradient descent (SGD) optimizer with momentum of 0.9, weight decay of 0.0001 and mini batch size of 128. For adversarial training, we generate k = 64 adversarial examples among 128 images in one mini-batch. We start with a learning rate of 0.1, divide it by 10 at 4k and 6k iterations, and terminate training at 8k iterations for MNIST, and 48k and 72k iterations, and terminate training at 94k iterations for CIFAR10. 5 We also found that initialization affects the training results slightly as in Kurakin et al. (2017a), thus, we pre-train the networks 2 and 10 epochs for MNIST and CIFAR10, and use these as initial starting points for different configurations. We use max e = 0.3*255 and 16 for MNIST and CIFAR10 respectively.
B MODEL DESCRIPTIONS
We summarize the model names used in this paper in table 7. For ensemble adversarial training, pre-trained networks as in table 9 together with the network being trained are used to generate onestep adversarial examples during training. For cascade adversarial training, pre-trained defended networks as in table 8 are used to generate iter FGSM images, and the network being trained is used to generate one-step adversarial examples during training.
5We found that the adversarial training requires longer training time than the standard training. Authors in the original paper He et al. (2016) changed the learning rate at 32k and 48k iterations and terminated training at 64k iterations.
14

Under review as a conference paper at ICLR 2018

Table 7: Model descriptions

Dataset MNIST
CIFAR10

ResNet 20-layer 20-layer
56-layer
110-layer

Initialization Group A
B
C D E F G
H
I
J K

Training
standard training Kurakin's
Bidirection loss P ivot loss
standard training Kurakin's
Ensemble training Bidirection loss P ivot loss
Kurakin's & Cascade training P ivot loss & Ensemble training P ivot loss & Cascade training
standard training Kurakin's P ivot loss
standard training
standard training
Kurakin's P ivot loss
Kurakin's
standard training Kurakin's P ivot loss
Ensemble training Kurakin's & Cascade training P ivot loss & Ensemble training P ivot loss & Cascade training
standard training Kurakin's
Ensemble training P ivot loss
Kurakin's & Cascade training P ivot loss & Ensemble training P ivot loss & Cascade training
standard training
standard training

Model
R20M R20MK R20MB R20MP
R20 R20K R20E R20B R20P R20K,C R20P,E R20P,C
R202 R20K2 R20P 2
R203
R204
R56K R56P
R56K2
R110 R110K R110P R110E R110K,C R110P,E R110P,C
R1102 R110K2 R110E2 R110P 2 R110K,C2 R110P,E2 R110P,C2
R1103
R1104

Table 8: Ensemble model description

Ensemble models
R20E, R20P,E, R110E, R110P,E R110E2, R110P,E2

Pre-trained models
R203, R1103 R204, R1104

Table 9: Cascade model description

Cascade models
R20K,C , R20P,C R110K,C , R110P,C R110K,C2, R110P,C2

Pre-trained model
R20P R110P R110P 2

15

argument to softmax

argument to softmax

Under review as a conference paper at ICLR 2018

C ALTERNATIVE VISUALIZATION ON EMBEDDINGS

25 20

TFarulseecclalassss

15

10

5

0

5

1060 40 20 0 20 40 60

argument to softmax

25 20 15 10 5 0 5 1060 40 20 0 20 40 60

argument to softmax

25 20 15 10 5 0 5 1060 40 20 0

20 40

60

(a) R20, step ll

25 20

FTarulseecclalassss

15

10

5

0

5

1060 40 20 0 20 40 60

argument to softmax

(b) R20K , step ll
25 20 15 10 5 0 5 1060 40 20 0 20 40 60

argument to softmax

(c) R20P (Ours), step ll

25 20 15 10 5 0 5 1060 40 20 0

20 40

60

(d) R20, step FGSM

25 20

FTarulseecclalassss

15

10

5

0

5

1060 40 20 0 20 40 60

argument to softmax

(e) R20K , step FGSM
25 20 15 10 5 0 5 1060 40 20 0 20 40 60

argument to softmax

(f) R20P (Ours), step FGSM

25 20 15 10 5 0 5 1060 40 20 0

20 40

60

(g) R20, random sign

(h) R20K , random sign

(i) R20P (Ours), random sign

Figure 8: Argument to the softmax vs. in test time. For the trained networks in table 2, "step ll", "step FGSM" and "random sign" methods were used to generate test-time adversarial images. Arguments to the softmax were measured by changing for each test method and averaged over randomly chosen 128 images from CIFAR10 test-set. Blue line represents true class and the red line represents mean of the false classes. Shaded region shows ± 1 standard deviation of each line.

We draw average value of the argument to the softmax layer for the true class and the false classes to visualize how the adversarial training works as in figure 8. Standard training, as expected, shows dramatic drop in the values for the true class as we increase in "step ll" or "step FGSM direction. With adversarial training, we observe that the value drop is limited at small and our method even increases the value in certain range upto =10. Note that adversarial training is not the same as the gradient masking.As illustrated in figure 8, it exposes gradient information, however, quickly distort gradients along the sign of the gradient ("step ll" or "step FGSM) direction. We also observe improved results (broader margins than baseline) for "random sign" added images even though we didn't inject random sign added images during training. Overall shape of the argument to the softmax layer in our case becomes smoother than Kurakin's method, suggesting our method is good for pixel level regularization. Even though actual value of the embeddings for the true class in our case is smaller than that in Kurakin's, the standard deviation of our case is less than Kurakin's, making better margin between the true class and false classes.

argument to softmax

16

Under review as a conference paper at ICLR 2018

D ADDITIONAL BLACK BOX ATTACK RESULTS

Table 10: CIFAR10 test results (%) under black box attacks for =16. {Target and Source: same networks in table 2}

Target
R20 R20K R20P

Source: step FGSM
R20 R20K R20P
12.2 27.4 27.5 65.7 81.5 81.8 58.1 89.3 91.7

Source: iter FGSM
R20 R20K R20P
0.0 45.9 44.7 51.5 0.0 18.2 48.9 13.4 0.0

Table 10 shows that black box attack between trained networks with the same initialization tends to be more successful than that between networks with different initialization as explained in Kurakin et al. (2017a).
Table 11: CIFAR10 test results (%) under black box attacks for =16. {Target and Source networks are switched from the table 3}

Target
R202 R20K2 R20P 2

Source: step FGSM

R20 R20K R20P

17.9 33.9 65.0 84.6 66.4 88.2

34.5 84.5 87.2

Source: iter FGSM

R20 R20K R20P

4.1 54.8 61.2 25.3 61.6 27.7

54.3 30.4 36.1

In table 11, our method (R20P 2) is always better at one-step and iterative black box attack from defended networks (R20K, R20P ) and undefended network (R20) than Kurakin's method (R20B2). However, it is hard to tell which method is better than the other one as explained in the main paper.
Table 12: CIFAR10 test results (%) for cascade networks under black box attacks for =16. {Target and Source: Please see the model descriptions in Appendix B.}

Target

Source: iter FGSM R110 R110K R110E R110P R110K,C R110P,E R110P,C

R110K2

80.5 72.7 49.3 68.0 49.6 41.0 67.9

R110E2

82.7 59.1 39.5 59.6 51.5 40.3 69.6

R110P 2 (Ours) 80.3 75.9 54.2 72.2 54.9 44.3 72.4

R110K,C2 (Ours) 62.1 74.7 61.5 72.3 46.5 39.0 67.9

R110P,E2 (Ours) 81.5 79.0 50.0 77.3 56.9 45.5 75.2

R110P,C2 (Ours) 72.2 76.4 60.6 73.8 51.0 40.9 72.0

In table 12, we show black box attack accuracies with the source and the target networks switched
from the table 6. We also observe that networks trained with both low-level similarity learning and
cascade/ensemble adversarial training (R110P,C2, R110P,E2) show better worst-case performance than other networks. Overall, iter FGSM images crafted from ensemble model families (R110E, R110P,E) remain strong on the defended networks.

17

CDF CDF CDF

Under review as a conference paper at ICLR 2018

E IMPLEMENTATION DETAILS FOR CARLINI-WAGNER L ATTACK

100

80 =0.3*255

60 40

RRRR22220000MMMMPBK ((OOuurrss))

20

00 20 40 60 80 100

100 100

80 =4 80 =4

60 40 20 00

=2 12

RRRRRR222222000000KKEPPP,,,(CECO(((uOOOruuusrrr)sss))) 345

60 40 20 00

=2 12

RRRRRR111111111111000000KPEKPP,,,(CECO(((uOOOruuusrrr)sss))) 345

(a) MNIST

(b) CIFAR10, 20-layer

(c) CIFAR10, 110-layer

Figure 9: Cumulative distribution function vs. for 100 test adversarial examples generated by CW L attack. Lower CDF value for a fixed means the better defense.

Carlini and Wagner (CW) L attack solves the following optimization problem for every input X.
minimize c · f (X + ) + [(|i| -  )+]
i
such that X +   [0, 1]n
where, the function f is defined such that attack is success if and only if f (X + ) < 0,  is the target perturbation defined as Xadv -X, c is the parameter to control the relative weight of function f in the total cost function, and  is the control threshold used to penalize any terms that exceed  .
Since CW L attack is computationally expensive, we only use 100 test examples (10 examples per each class). We search adversarial example Xadv with c  {0.1, 0.2, 0.5, 1, 2, 5, 10, 20} and   {0.02, 0.04, ..., 0.6} for MNIST and c  {0.1, 0.3, 1, 3, 10, 30, 100} and   {0.001, 0.002, ..., 0.01, 0.012, ..., 0.02, 0.024, ..., 0.04, 0.048, ..., 0.08} for CIFAR10. We use Adam optimizer with an initial learning rate of 0.01/c since we found constant initial learning rate for c · f (X + ) term is critical for successful adversarial images generation. We terminate the search after 2,000 iterations for each X, c and  . If f (X + ) < 0 and the resulting |||| is lower than the current best distance, we update Xadv.
Figure 9 shows cumulative distribution function of for 100 successful adversarial examples per each network. We report the number of adversarial examples with > 0.3*255 for MNIST and that with > 2 or 4 for CIFAR10. As seen from this figure, our approaches provide robust defense against CW L attack compared to other approaches.

18

