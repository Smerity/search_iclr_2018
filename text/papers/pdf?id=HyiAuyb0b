Under review as a conference paper at ICLR 2018
ANALYZING THE ROLE OF TEMPORAL DIFFERENCING IN DEEP REINFORCEMENT LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
Wide adoption of deep networks as function approximators in modern reinforcement learning (RL) is changing the research environment, both with regard to best practices and application domains. Yet, our understanding of RL methods has been shaped by theoretical and empirical results with tabular representations and linear function approximators. These results suggest that RL methods using temporal differencing (TD) are superior to direct Monte Carlo (MC) estimation. In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that each control for a specific factor that affects performance, such as reward sparsity, reward delay or the perceptual complexity of the task. When comparing TD with infinite horizon MC, we are able to reproduce the results from the past in modern settings characterized by perceptual complexity and deep nonlinear models. However, we also find that finite horizon MC methods are not inferior to TD, even in sparse or delayed reward tasks, making MC a viable alternative to TD. We discuss the role of perceptual complexity in reconciling these findings with classic empirical results.
1 INTRODUCTION
The use of deep networks as function approximators has significantly expanded the range of problems that can be successfully tackled with reinforcement learning (RL). However, there is little understanding of when and why certain deep reinforcement learning (DRL) algorithms work well. Theoretical results are mainly based on tabular environments or linear function approximators (Sutton & Barto, 2017). Their assumptions do not cover the typical application domains of DRL, which feature extremely high input dimensionality (typically in the tens of thousands) and the use of nonlinear function approximators. Thus, our understanding of DRL is based primarily on empirical results, and these empirical results guide the design of DRL algorithms.
One such design decision common to the vast majority of existing value-based DRL methods is the use of temporal difference (TD) learning ­ training predictive models by bootstrapping based on their own predictions. This design decision is primarily based on evidence from the pre-DRL era (Sutton, 1988; 1995). The results of those experimental studies are well-known and clearly demonstrate that simple supervised learning, also known as Monte Carlo (MC) prediction, is outperformed by pure TD learning, which, in turn, is outperformed by TD() ­ a method that can be seen as a mixture of TD with MC (Sutton, 1988).
However, recent research has shown (Dosovitskiy & Koltun, 2017) that an algorithm based on Monte Carlo prediction can outperform TD-based methods on complex sensorimotor control tasks in threedimensional environments. These results suggest that the classic understanding of the relative performance of TD and MC may not hold in modern settings. This evidence is not conclusive: the algorithm proposed by Dosovitskiy & Koltun (2017) involves custom components such as parametrized goals and decomposed rewards, and therefore cannot be directly compared to TD-based baselines.
In this paper, we perform a controlled experimental study aiming at better understanding the role of temporal differencing in modern deep reinforcement learning, characterized by essentially infinitedimensional state spaces, extremely high observation dimensionality, and deep nonlinear models used as function approximators. We focus on environments with visual inputs and discrete action sets, and algorithms that involve prediction of value or action-value functions. This is in contrast to value-free policy optimization algorithms (Schulman et al., 2015; Levine & Koltun, 2013) and
1

Under review as a conference paper at ICLR 2018

tasks with continuous action spaces and low-dimensional vectorial state representations that have been extensively benchmarked by Duan et al. (2016) and Henderson et al. (2017). We base our study on deep Q-learning (Mnih et al., 2015), where the Q-function is learned either via temporal differencing or via a finite-horizon Monte Carlo method. To ensure that our conclusions are not limited to pure value-based methods, we additionally evaluate asynchronous advantage actor-critic (A3C), which combines temporal differencing with a policy gradient method (Mnih et al., 2016).
Our main focus is on performing controlled experiments, both in terms of algorithm configurations and environment properties. This is in contrast to previous works, which typically benchmark a number of existing algorithms on a set of standard environments. While proper benchmarking is crucial for tracking the progress of the field, it is not always sufficient for understanding the reasons behind good or poor performance of the algorithms. In this work, we ensure that the algorithms are comparable by implementing them in a common software framework. By varying the parameters such as the balance between TD and MC in the learning update or the prediction horizon, we are able to clearly isolate the effect of these parameters on learning. Moreover, we designed a series of controlled scenarios that focus on specific characteristics of RL problems: reward sparsity, reward delay, perceptual complexity, and properties of terminal states. Results in these environments shed light on strengths and weaknesses of algorithms under investigation.
Our findings in modern DRL scenarios both support and contradict previous results on merits of TD. On the one hand, value-based infinite-horizon methods perform best in the regime which is a mixture of TD and MC, similar to the TD() results of Sutton (1988). On the other hand, in sharp contrast with previous belief, we observe that Monte Carlo algorithms can perform very well on challenging RL tasks. This is made possible by simply limiting the prediction to a finite horizon. Surprisingly, finite-horizon Monte Carlo training is successful in dealing with sparse and delayed rewards, which are generally assumed to impair this class of methods. Monte Carlo training is also more stable to noisy rewards and is particularly robust to perceptual complexity and variability.

2 PRELIMINARIES

We work in a standard reinforcement learning setting of an agent acting in an environment over

discrete time steps. At each time step t, the agent receives an observation ot and selects an action at. We assume partial observability: the observation ot need not carry complete information about the environment and can be seen as a function of the environment's "true state". We assume an

episodic setup, where an episode starts with time step 0 and concludes at a terminal time step T . We

denote by st the tuple of all observations collected by the agent from the beginning of the episode: st = o0, . . . , ot . (In practice we will only include a set of recent observations in s.) The objective is to find a policy (at|st) that maximizes the expected return ­ the sum of all future rewards through
the remainder of the episode:

T

Rt = ri.

(1)

i=t

This sum can become arbitrarily large for long episodes. To avoid divergence, temporally distant

rewards can be discounted. This is typically done in one of two ways: by introducing a discount

factor  or by truncating the sum after a fixed number of steps (horizon)  .

T t+
Rt = i-tri = rt + rt+1 + 2rt+2 + ... ; Rt = ri.
i=t i=t

(2)

The parameters  and  regulate the contribution of temporally distant rewards to the agent's objective. In what follows R^t stands for Rt or Rt .

For a given policy , the value function and the action-value function are defined as expected returns that are conditioned, respectively, on the observation or the observation-action pair:

V (st) = E[R^t|st], Q(st, at) = E[R^t|st, at].

(3)

Optimal value and action-value functions are defined as the maxima over all possible policies:

V

(st)

=

max


V

 (st ),

Q

(st, at)

=

max


Q

(st,

at).

(4)

2

Under review as a conference paper at ICLR 2018

In value-based, model-free reinforcement learning, the value or action value are estimated by a function approximator V with parameters . The function approximator is typically trained by minimizing a loss between the current estimate and a target value:

L() = V (st; ) - Vtarget 2.

(5)

The learning procedure for the action-value function is analogous. Hence, we focus on the value function in the remainder of this section.

Reinforcement learning methods differ in how the target value is obtained. The most straightforward approach is to use the empirical return as target: i.e., Vtarget = Rt or Vtarget = Rt . This is referred to as Monte Carlo (MC) training, since the empirical loss becomes a Monte Carlo estimate of the

expected loss. Using the empirical return as target requires propagating the environment forward

before a episode

ftoraridniisncgosutnetpedcarnettuarkneRplta.ceTh­isbyincrsetaespessftohrefivnairtiea-nhcoerizoof nthreettuarrngeRt tvaolur eunfotirl

the end of the long horizons

and large discount factors.

An alternative to Monte Carlo training is temporal difference (TD) learning (Sutton, 1988). The idea is to estimate the return by bootstrapping from the function approximator itself, after acting for a fixed number of steps n:

t+n-1

Vtarget =

i-tri + nV (st+n; ).

(6)

i=t

TD learning is typically used with infinite-horizon returns. When the rollout length n approaches infinity (or, in practice, maximal episode duration Tmax), TD becomes identical to Monte Carlo training. TD learning applied to the action-value function is known as Q-learning (Watkins, 1989; Watkins & Dayan, 1992; Peng & Williams, 1996; Mnih et al., 2015).

An alternative to value-based methods are policy-based methods, which directly parametrize the policy (a|s; ). An approximate gradient of the expected return is computed with respect to the
policy parameters, and the return is maximized using gradient ascent. Williams (1992) has shown that an unbiased estimate of the gradient can be computed as  log (a|s; ) (Rt - bt(st)), where the function bt(st) is called a baseline and can be chosen so as to decrease the variance of the estimator. A common choice for the baseline is the value function: bt(st) = V (st). A combination of policy gradient with a baseline value function learned via TD is referred to as an actor-critic
method, with policy  being the actor and the value function estimator being the critic.

3 EXPERIMENTAL SETUP
3.1 ALGORITHMS
In our analysis of temporal differencing we focus on three key characteristics of RL algorithms. The first is the balance between TD and MC in the learning update. The second is the prediction horizon, in particular infinite versus finite horizon. The third is the use of pure value-based learning versus an actor-critic approach which includes an explicitly parametrized policy.
To study the first aspect, we use asynchronous n-step Q-learning (n-step Q) (Mnih et al., 2016). In this algorithm, an action-value function is learned with n-step TD (Eq. (6)), and actions are selected greedily according to this function. By varying the rollout length n, we can smoothly interpolate between pure TD and pure MC updates. In order to analyze the second aspect, we implemented a finite-horizon Monte Carlo version of n-step Q, which we call QMC. This algorithm can be seen as a simplified version of Direct Future Prediction (Dosovitskiy & Koltun, 2017). Finally, we select asynchronous advantage actor-critic (A3C) (Mnih et al., 2016) to study the third aspect. In A3C, the value function estimate is learned with n-step TD, and a policy is trained with policy gradient. This allows us to evaluate the interplay of TD learning and policy gradient learning.
To ensure that the comparison is fully controlled and fair, we implemented all algorithms in the asynchronous training framework proposed by Mnih et al. (2016). Multiple actor threads are running in parallel and send the weight updates asynchronously to a parameter server. For A3C and n-step Q, we use the algorithms as described by Mnih et al. (2016). QMC is the n-step Q algorithm where the n-step TD targets are replaced by finite-horizon MC targets.

3

Under review as a conference paper at ICLR 2018
Note that switching to finite horizon necessitates a small additional change in the QMC algorithm. In practice, in n-step Q each parameter update is not just an n-step TD update, but a sum of all updates for rollouts from 1 to n. This improves the stability of training. In QMC such accumulation of updates is impossible, since predictions for different horizons are not compatible. We therefore always predict several Q-values corresponding to different horizons, similar to Dosovitskiy & Koltun (2017). Specifically, for horizon  = 2K, we additionally predict Q-values for horizons {2k}0k<K . This design choice is further explained and supported with experiments in the supplement. Apart from this, there is no difference between n-step Q and QMC .
3.2 ENVIRONMENTS
To calibrate our implementations against results available in the literature, we begin by conducting experiments on several standard benchmark environments: five Atari games from the Arcade Learning Environment (Bellemare et al., 2013) and two environments based on first-person-view 3D simulation in the ViZDoom framework (Kempka et al., 2016). We used a set of Atari games commonly analyzed in the literature: Space Invaders, Pong, Beam Rider, Sea Quest, and Frostbite (Mnih et al., 2015; Schulman et al., 2015; Lake et al., 2016). For the ViZDoom environments, we used the Navigation, Battle and Battle2 scenarios from Dosovitskiy & Koltun (2017).
Our main experiments are on sequences of specialized environments. Each sequence is designed such that a single factor of variation is modified in a controlled fashion. This allows us to study the effect of this factor. Factors of variation include: reward sparsity, reward delay, reward type, and perceptual complexity. These environments form the heart of our analysis and will be publicly released for reproducibility.
For the controlled environments, we used the ViZDoom platform. This platform is compatible with existing map editors with built-in scripting, which allows for flexible and controlled specification of different scenarios. In comparison to Atari games, ViZDoom offers a more realistic setting with a three-dimensional environment and partially observed first-person navigation. We now briefly describe the tasks. Further details are provided in the supplement.
Basic health gathering. The basis for our controlled scenarios is the health gathering task. In this scenario, the agent's aim is to collect health kits while navigating through a maze using visual input. Figure 1(b) shows a typical image observed by the agent. The agent's health level is constantly declining. Health kits add to the health level. The goal is to survive and maintain as much health as possible by collecting health kits. To be precise, the agent loses 6 health units every 8 steps, and obtains 20 health units when collecting a health pack. The agent's total health cannot exceed 100. The reward is +1 when the agent collects a health kit and 0 otherwise. There are 16 health kits in the labyrinth at any given time. When the agent collects one of them, a new one appears at a random location. An episode is terminated after 525 steps, which is equivalent to 1 minute of in-game time.
Terminal states. To test the effect of terminal states on the performance of the algorithms, we modified the health gathering scenario so that each episode terminates after m health kits are collected. For m = 1, all useful training signals come from the terminal state. With larger m, the importance of terminal states diminishes.
Delayed rewards. In this sequence of scenarios we introduce a delay between the act of collecting a health kit and its effect ­ an increase in health and a reward of 1. We have set up environments with delays of 2, 4, 8, 16, and 32 steps.
Sparse rewards. To examine the effect of reward sparsity, we varied the number of available health kits on the map. We created two variations of the basic health gathering environment with increasingly sparse rewards. In the `Sparse' setting, there are 4 health kits in the labyrinth ­ four times fewer than in the basic setting. In the `Very Sparse' setting, only 2 health kits are in the labyrinth ­ eight times fewer than in the basic setting. In order to isolate the effect of sparsity and keep the general difficulty of the task fixed, we accordingly adjusted the amount of health the agent loses per time period: 3 in the Sparse configuration and 2 in Very Sparse. In the Very Sparse scenario under random exploration, the agent gathers a health kit on average every 6,440 steps.
Reward type. In this scenario, we compare the standard binary reward with its more natural but more noisy counterpart. In the basic scenario above, the reward is +1 for gathering a health kit and 0 otherwise. A more natural measure of success in the health gathering task is the actual change in
4

Under review as a conference paper at ICLR 2018

(a) (b)

(c)

Figure 1: Different levels of perceptual complexity in the health gathering task. (a) Map view of a grid world. (b) First-person view of a three-dimensional environment, fixed textures. (c) First-person view of a three-dimensional environment, random textures.

health. With this reward, the agent would directly aim to maximize its health. In this configuration we therefore use a scaled change in health as the reward signal. This reward is more challenging than the basic binary reward due to its noisiness (health is decreased only every eighth step) and the variance in the reward after collecting a health kit due to the total health limit.
Perceptual complexity. To analyze the effect of perceptual complexity, we designed variants of the health gathering task with different input representations. First, to increase the perceptual complexity of the task, we replaced the single maze used in the basic health gathering scenario by 90 randomly textured versions, some of which are shown in Figure 1(c). The labyrinth's texture is changed after each episode during both training and evaluation.
We also created two variants of the health gathering task with reduced visual complexity. These are the only controlled scenarios not using the ViZDoom framework. Both are based on a grid world, where the agent is navigating an 8×8 room with 5 available actions: wait, up, down, left, and right. There are 4 randomly placed health kits in the room, and the aim of the agent is to collect these, with reward +1 for collecting a health kit and 0 otherwise. Each time a health kit is collected a new one appears in a random location. The two variants differ in the representation that is fed to the agent. In one, the agent's input is a 10-dimensional vector that concatenates the 2D Cartesian coordinates of the agent itself and the 4 health kits, sorted by their distance to the agent. In the other variant, the agent gets as input an 8×8 image of the grid world. This is a raster RGB image of a map of the grid world, as shown in Figure 1(a). This latter representation is compatible with the input representation used in the first-person variants (i.e., an RGB image), but the content of the input image is drastically simplified.
3.3 ALGORITHM DETAILS
We used identical network architectures for the three algorithms in all experiments. For experiments in Atari and ViZDoom domains we used deep convolutional networks similar to the one used by Mnih et al. (2015). For gridworld experiments we used fully-connected networks with three hidden layers. For QMC and n-step Q we used dueling network architectures, similar to Wang et al. (2016). The exact architectures are specified in the supplement.
For experiments in Atari environments we followed a common practice and fed the 4 most recent frames to the networks. In all other environments the input was limited to the observation from the current time step. In ViZDoom scenarios, in addition to the observed image we fed a vector of measurements to all networks. The measurements are the agent's scalar health in the health gathering scenarios and a three-dimensional vector of the agent's health, ammo, and frags in the battle scenario.
We trained all models with 16 asynchronous actor threads, for a total of 60 million steps. We identified optimal hyperparameters for each algorithm via a hyperparameter search on a subset of environments and used these fixed hyperparameters for all environments, unless noted otherwise.
5

Under review as a conference paper at ICLR 2018

A3C (Mnih et al., 2016) DFP (Dosovitskiy & Koltun, 2017)
QMC 20-step Q 20-step A3C

#steps
80M 50M
60M 60M 60M

Seaquest
2300 -
12708 4276 2021

S. Invaders
2215 -
1221 1888 1952

Atari Frostbite
180 -
1311 3875 202

Pong
11.4 -
-4.2 8.9 20.6

BeamRider
13236 -
1839 9088 7190

ViZDoom Navigat. Battle Battle 2

- -- 84.1 33.5 16.5

84.4 75.7 70.8

35.9 32.4 22.1

17.5 16.0 11.0

Table 1: Calibration against published results on standard environments. We report the average score at the end of an episode for Atari games, health for the Navigation scenario, and frags for the Battle scenarios. In all cases, higher is better.

For evaluation, we trained three models on each task, selected the best-performing snapshot for each training run, and averaged the performance of these three best-performing snapshots. Further details are provided in the supplement.
4 RESULTS
4.1 CALIBRATION
We start by calibrating our implementations of the methods against published results reported in the literature. To this end, we train and test our implementations on standard environments used in prior work. The results are summarized in Table 1. Our implementations perform similarly to corresponding results reported in prior work.
For A3C the results are significantly different only for BeamRider. However, in Mnih et al. (2016) the evaluation used the average over the best 5 out of 50 experiments with different learning rates. We used the average over 3 runs with a fixed learning rate. Since the results for BeamRider have a high variance even for very small learning rate changes, this explains the difference between the results.
On the ViZDoom scenarios, the QMC implementation performs on par with the DFP algorithm. This shows that DFP does not crucially depend on a decomposition of the reward into a vector of measurements, and can perform equally well given a standard RL setup with a scalar reward. Our A3C implementation achieves significantly better results than those reported by Dosovitskiy & Koltun (2017) on the ViZDoom scenarios. We attribute this to (a) using a rollout value of 20 in our experiments instead of 5 as used by Mnih et al. (2016) and Dosovitskiy & Koltun (2017), and (b) providing the measurements as input to the network. Dosovitskiy & Koltun (2017) have not tested DFP on Atari games. We find that in these environments QMC performs worse overall than 20-step Q and 20-step A3C.
4.2 VARYING THE ROLLOUT IN TD-BASED ALGORITHMS
By changing the rollout length n in n-step Q and A3C, we can smoothly transition between TD and MC training. 1-step rollouts correspond to pure bootstrapping as used in the standard Bellman equation. Infinite rollouts (until the terminal state), on the other hand, correspond to pure Monte Carlo learning of discounted infinite-horizon returns.
Results on three environments ­ Basic health gathering, Sparse health gathering, and Battle ­ are presented in Figure 2. Rollout length of 20 is best on all tasks for n-step Q. Both very short and very long rollouts lead to decreased performance. These findings are in agreement with prior results of TD() experiments (Sutton, 1988; 1995), considering that longer rollouts increase the MC portion of the value target, converging to a full MC update for infinite rollout. A mixture of TD and MC yields the best performance. The results for A3C are qualitatively similar, and again the 20-step rollout is overall near-optimal.
6

Under review as a conference paper at ICLR 2018

Figure 2: Effect of rollout length on TD learning for n-step Q and A3C. We report average health at the end of an episode for health gathering and average frags in the Battle scenario. Higher is better.

4.3 CONTROLLED EXPERIMENTS

We now proceed to a series of controlled experiments on a set of specifically designed environments and compare TD-based methods to QMC , a purely Monte Carlo approach. The motivation is as follows. In the previous section we have seen that very long rollouts lead to deteriorated performance of n-step Q and A3C. This can be attributed to large variance in target values. The variance can be reduced by using a finite horizon, as is the case in QMC. However, the use of a finite horizon means that rewards that are further away than the horizon will not be part of the value target, resulting in
a disadvantage in tasks with sparse or delayed rewards. In order to evaluate this we run controlled experiments designed to isolate the reward delay, sparsity, and other factors. We test 20-step Q and A3C (optimal rollout for TD-based methods), 5-step Q and A3C (more TD in the update), and QMC (finite horizon Monte Carlo).

Reward type. We contrast the standard binary reward with the more natural reward signal propor-
tional to the change in the health level of the agent. Figure 3 (left) shows that in the scenario with binary reward the performance of QMC, 20-step Q, and 20-step A3C is nearly identical, within 4% of each other. However, when trained with the noisier health-based reward, QMC performs within 1% of the result with binary reward, but the performance of TD-based algorithms decreases significantly, especially for the 5-step rollouts. These results suggest that Monte Carlo training is more
robust to noisy rewards than TD-based methods.

Terminal states. Table 2 shows that in environments where terminal states play a crucial role, QMC is outperformed by TDbased methods. This is due to the finite-horizon nature of QMC. A terminal reward only contributes to a single update per episode,
while in TD it contributes to every update in the episode. If nonterminal rewards are present (m = 2), QMC approaches the TDbased algorithms, but still does not reach the performance of 20-
step Q. Difficulties with terminal states can partially explain poor performance of QMC on some Atari games.

m=1 m=2

QMC 20-step Q 5-step Q 20-step A3C 5-step A3C

43.3 75.9 74.3 64.7 61.1

64.0 75.5 71.3 58.2 52.3

Table 2: Terminal states.

Delayed rewards. Figure 3 (middle) shows that the performance
of all algorithms declines even with moderate delays in the reward signal. A delay of 2 steps, or
approximately 0.2 seconds of in-game time, already leads to a 8­12% relative drop in performance
for QMC and 20-step TD algorithms and a 30­40% drop for 5-step TD algorithms. With a delay of 8 steps, or approximately 1 second, the performance of QMC and 20-step TD algorithms drops by 30­70% and 5-step TD agents are essentially unable to survive until the end of an episode. With a
delay of 32 steps, all algorithms degrade to a trivial score. Interestingly, the performance of QMC declines less rapidly than the performance of the other algorithms and QMC consistently outperforms the other algorithms in the presence of delayed rewards.

Sparse rewards. TD-based infinite-horizon approaches should theoretically be effective at propagating distal rewards, and are therefore supposed to be advantageous in scenarios with sparse rewards. The results on the Sparse and Very Sparse scenarios however, do not support this expectation (Figure 3 (right)): QMC performs on par with 20-step Q, and noticeably better than 20step A3C and 5-step algorithms. We believe the reason for the unexpectedly good performance of

7

Under review as a conference paper at ICLR 2018

Figure 3: Effect of reward properties. Left to right: reward type, reward delay, reward sparsity. We report the average health at the end of an episode. Higher is better. MC training (QMC, green) performs well on all environments.

QMC is that Monte Carlo approaches are well suited for training perception systems, as discussed in more detail in Section 4.4. A video of a QMC agent trained on the Very Sparse task is available at https://youtu.be/OJ1eBzW7cJ0.

Perceptual complexity. We test the algorithms on a series of environments of gradually increasing perceptual complexity. The results are summarized in Figure 4. In simple gridworld environments, the locations of the agent and the health kits are given to the agent directly in the form of their coordinates (Grid Vec.) or a map (Grid Map). In these scenarios, TD-based methods perform well. The Grid Vec. task is successfully solved by all methods. 5-step unrolling outperforms the 20-step versions and QMC in both setups.

However, the situation is completely different in the Basic and Multi-texture setups, in which the perceptual input is much more complex. In the Basic setup, all methods perform roughly on par, but 5-step unrolling drops behind the other methods. In the Multi-texture setup, QMC outperforms other algorithms.

To further analyze the effect of perception on DRL, we conduct
an additional experiment where we separate the learning of per-
ception and control. We first train two perception systems on the Battle task by predicting Q-values under a fixed policy with 20-step Q or QMC . We then re-initiailize the weights in the top two layers, freeze the weights in the rest of the the networks, and re-train the top two layers on the Battle task with 20-step Q or QMC . Further details are provided in the supplement. The results are shown in Table 3. Both Q and QMC control reach higher score with a perception system trained with QMC. This supports the hypothesis that Monte Carlo training is efficient at
training deep perception systems from raw pixels.

Perception
20-step Q QMC

Control 20-step Q QMC

18.0 31.8

19.9 35.2

Table 3: Separate training of perception and control on the Battle scenario. Higher is better.

Figure 4: Effect of perceptual complexity. Perceptual complexity increases from left to right. We report average cumulative reward per episode for grid worlds and average health at the end of the episode for ViZDoom-based setups.
8

Under review as a conference paper at ICLR 2018
4.4 DISCUSSION
Temporal differencing methods are generally considered superior to Monte Carlo methods in reinforcement learning. This opinion is largely based on empirical evidence from domains such as gridworlds (Sutton, 1995), cart pole (Barto et al., 1983), and mountain car (Moore, 1990). Our results agree: in gridworlds and on Atari games we find that n-step Q learning outperforms QMC. We further find, similar to the TD() experiments from the past (Sutton, 1988), that a mixture of MC and TD achieves best results in n-step Q and A3C.
However, the situation changes in perceptually complex environments. In our experiments in immersive three-dimensional simulations, a finite-horizon MC method (QMC) matches or outperforms TD-based methods. Especially interesting are the results of the sparse reward experiments. Sparse problems are supposed to be specifically challenging for finite-horizon Monte Carlo estimation: in our Very Sparse setting, average time between health kits is 44 time steps when a human is controlling the agent. This exceeds QMC's finite prediction horizon of 32 steps, making it seemingly impossible for the algorithm to achieve nontrivial performance. Yet QMC is able to keep up with the results of the 20-step Q algorithm and clearly outperforms A3C.
What is the reason for this contrast between classic findings and our results? We believe that the key difference is in the complexity of perception in immersive three-dimensional environments, which was not present in gridworlds and other classic problems, and is only partially present in Atari games. In immersive simulation, the agent's observation is a high-dimensional image that represents a partial view of a large (mostly hidden) three-dimensional environment. The dimensionality of the state space is essentially infinite: the underlying environment is specified by continuous surfaces in three-dimensional space. Memorizing all possible states is easy and routine in gridworlds and is also possible in some Atari games (Blundell et al., 2016), but is not feasible in immersive three-dimensional simulations. Therefore, in order to successfully operate in such simulations, the agent has to learn to extract useful representations from the observations it receives. Encoding a meaningful representation from rich perceptual input is where Monte Carlo methods are at an advantage due to the reliability of their training signals. Monte Carlo methods train on ground-truth targets, not "guess from a guess", as TD methods do (Sutton & Barto, 2017).
These intuitions are supported by our experiments. Figure 4 shows that increasing the perceptual difficulty of the health gathering scenario hurts the performance of QMC less than it does the TDbased approaches. Table 3 shows that QMC is able to learn a better perception network than 20-step Q. In Figure 3, 20-step TD algorithms perform better than their 5-step counterparts in all tested scenarios. Longer rollouts bring TD closer to MC, in agreement with our hypothesis.
5 CONCLUSION
For the past 30 years TD-based methods have dominated the field of reinforcement learning. Our experiments on a range of complex tasks in perceptually challenging environments show that in deep reinforcement learning finite-horizon MC can be a viable alternative to TD. We find that while TD is at advantage in tasks with simple perception, long planning horizons, or terminal rewards, MC training is more robust to noisy rewards, effective for training perception systems from raw sensory inputs, and surprisingly successful at dealing with delayed and sparse rewards. Thus, a key challenge that can be derived from our results is to find ways on how to combine the advantages of noise-free supervised MC learning with those of TD. We hope that our results will contribute to a set of best practices for deep reinforcement learning that are consistent with the empirical reality of modern application domains.
REFERENCES
Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13(5), 1983.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. JAIR, 47, 2013.
9

Under review as a conference paper at ICLR 2018
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman, Joel Z. Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis. Model-free episodic control. arXiv:1606.04460, 2016.
Alexey Dosovitskiy and Vladlen Koltun. Learning to act by predicting the future. In ICLR, 2017. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In ICML, 2016. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017. Michal Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jas´kowski. ViZ-
Doom: A Doom-based AI research platform for visual reinforcement learning. In IEEE Conference on Computational Intelligence and Games, 2016. Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building machines that learn and think like people. Behavioral and Brain Sciences, 2016. Sergey Levine and Vladlen Koltun. Guided policy search. In ICML, 2013. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, et al. Human-level control through deep reinforcement learning. Nature, 518(7540), 2015. Volodymyr Mnih, Adria` Puigdome`nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, 2016. Andrew William Moore. Efficient memory-based learning for robot control. Technical Report 209, University of Cambridge, Computer Laboratory, 1990. Jing Peng and Ronald J. Williams. Incremental multi-step Q-learning. Machine Learning, 1996. John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, 2015. Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3, 1988. Richard S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse coding. In NIPS, 1995. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2nd edition, 2017. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. Dueling network architectures for deep reinforcement learning. In ICML, 2016. Christopher J. C. H. Watkins. Learning from delayed rewards. PhD thesis, University of Cambridge, England, 1989. Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8, 1992. Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Machine Learning, 1992.
10

Under review as a conference paper at ICLR 2018
SUPPLEMENTARY MATERIAL
S1 FURTHER RESULTS
Effect of the rollout length and the prediction horizon In Table 5 of the main paper we have shown that the performance of n-step Q decreases for rollouts larger than 20. For the QMC algorithm a similar phenomenon is observed for large horizons, as shown in Table S1. The performance is decreasing for a horizon larger than 32. In both cases, the decrease is likely caused by the high variance of large sums of future rewards. The high variance in reward sums increases the variance of the gradients and leads to higher noise when training the value predictions. This hinders the action selection process, which relies on fine differences between values of different actions.

Figure S1: Performance of the QMC algorithm using different value prediction horizons.

Difference between asynchronous n-step Q and QMC. As mentioned in the main paper, apart from the different targets to learn the Q-function there is another difference between the n-step Q and QMC algorithms. It is caused by the usage of multiple unrolling values in the n-step Q algorithms. In n-step Q instead of only using the n-step rollout, multiple values are used within every batch (every value from 1 to n (Mnih et al., 2016)). This results in an increased performance and stability of the n-step Q algorithm. It is not directly applicable to QMC since different unrolling values result in different finite horizons. Instead in QMC multiple Q-function heads exist to predict the different finite horizons (Dosovitskiy & Koltun, 2017). The difference between the trivial implementation and the multiple unrolling modifications are shown in Table S1.
There are no further differences between the two algorithms. They use the same architecture and asynchronous training. Both even perform best under the same hyperparameters like the learning rate.

QMC Constant rollout QMC
20-step Q Constant rollout 20-step Q

Health Basic
77.1 78.2
78.3 69.0

Health Sparse
59.5 55.9
55.3 45.5

Battle
35.9 30.9
32.4 23.5

Table S1: Difference between using multiple or constant rollouts within one train step.

Separate training of perception and control In order to perform the perception freezing experiments we first train two perception systems on the Battle task with 20-step Q and QMC for 20 million steps (1/3 of the usual training) by predicting Q values under a fixed policy (we tried using a fully trained QMC or 20-step Q policy). Thereafter we freeze the perception and the measurements part of the network. (The full architecture of the perception and the measurements are shown
in Table S3). We then reinitialize the remaining layers and retrain the networks with the frozen perception with QMC and 20-step Q, both using each of the two available perceptions (for 40 million steps).

11

Under review as a conference paper at ICLR 2018

20-step Q perception QMC perception

Pretrained using QMC policy

20-step Q

QMC

18.0 31.8

19.9 35.2

Pretrained using 20-step Q policy

20-step Q

QMC

16.2 30.6

20.3 30.4

Table S2: Performance of 20-step Q and QMC with a pretrained and frozen perception, higher is better.

The full results are shown in Table S2. Both Q and QMC are able to reach higher score with a QMC perception, on both of the used initial policies. The results in the main paper correspond to perception systems trained under the QMC policy.

S2 ADDITIONAL ALGORITHM AND ENVIRONMENT DETAILS
Batch size for small rollouts In algorithms with asynchronous n-step TD targets the batch size is usually equal to the unrolling length n. However decreasing the batch size in A3C could also effect the performance of the policy gradient of the A3C loss. To make sure that we only measure the effect of different n-step TD targets and do not alternate the policy gradient part we keep the batch size at the constant value of 20 (for all rollouts smaller than 20). This is realized by using multiple n-step rollouts within one batch (e.g. for a 5-step rollout the batch consists of 4 rollouts). Overall those batches lead to improved performance of A3C. For n-step Q using the constant batch size of 20 results in similar performance and significantly reduced the execution time. Therefore we used those batches for both A3C and n-step Q in our experiments.
Network details In each experiment we used the same network architecture for all algorithms. For tasks with visual input ­ in ViZDoom and ALE ­ we used a convolutional network with architecture similar to Mnih et al. (2015). For all experiments in the ViZDoom environment, in addition to the image the networks got a vector of measurements as input: agent's health level and current time step for Health gathering and Navigation, and agent's health, ammo and frags for Battle. For QMC and n-step Q we used the dueling architecture (Wang et al., 2016), splitting value prediction into an action independent expectation E(st, ) and an action dependent part for the advantage of using a specific action A(st, a, ). For l actions, the value prediction emitted by the network is computed as:

Q(st, a, ) = E(st, ) + A(st, a, )

;

A(st,

a,

)

=

A(st,

a,

)

-

1 l

A(st, a , ) (7)

a

The architecture of the QMC Network is shown in Table S3. QMC is predicting the Q value for multiple finite horizons at once: 1, 2, 4, 8, 16 and 32. Predictions for all horizons are emitted at once. Therefore, for l actions, the network has 6 outputs for the expectation values and 6 × l outputs
for the action advantages. We used greedy action selection according to an objective function which
is a linear combination of predictions at different horizons, same as in DFP (Dosovitskiy & Koltun,
2017):

a(st) = arg max 0.5 · Q(8)(st, a ) + 0.5 · Q(16)(st, a ) + 1.0 · Q(32)(st, a )
a

(8)

The network for n-step Q was identical, except that instead of 6 predictions, a single value function was predicted for each action. The A3C architecture was also identical, except that the network was not split in the last hidden layer like it was for the dueling networks. Both the policy and the value output shared the same last hidden layer as in Mnih et al. (2016). The network we are using for A3C is larger than that used by Mnih et al. (2016). We found that the larger network matches or exceeds the performance of the smaller network used by Mnih et al. (2016) on our tasks.
For both gridworlds we used fully connected networks. For all results reported in the paper the three algorithms used three hidden fully connected layers with size of 512.

12

Under review as a conference paper at ICLR 2018

Network part Perception (P)
Measurements (M) Expectation Action advantage

Input type image
vector P+M P+M

Input size
84 × 84 × {1 or 4} 20 × 20 × 32 9 × 9 × 64 7 × 7 × 64 3136
{2 or 3} 128 128
512 + 128 512
512 + 128 512

Channels
32 64 64 3136 512
128 128 128
512 6
512 6·l

Kernel
8×8 4×4 3×3

Stride
4 2 1

Layer type convolutions flatting fully connected fully connected
fully connected fully connected

Table S3: Network architecture of QMC for l actions.
Training and evaluation details We found that for each of the three asynchronous algorithms the learning rate of 7 × 10-4 leads to the best result in most of the tested environments. Further we found that, in general, ViZdoom scenarios are less sensitive to learning rate changes than different Atari games. We decreased the learning rate linearly to zero over the training procedure. As the optimizer we used shared RMSProp with the same parameters as in Mnih et al. (2016).
In all experiments we used a total of 60 million steps for training. This means all actor threads together processed 60 million steps. We used frame skip of 4, therefore 60 million frame-skipped steps correspond to 240 million non-frame-skipped environment steps. For QMC each asynchronous agent performed a parameter update every 20 steps with a batch size of 20. Each time the most recent 20 frames with available value targets were used. Every 2.5 million steps we evaluated the network over 500 episodes for Vizdoom Environments and over 200 episodes for Atari games. For one training run the best result out of all 24 evaluations was considered as its final score. For each experiment three runs were performed for each algorithm. The average of the three run scores was considered as the final performance of that algorithm on the task.
Additional environment details The Navigation scenario is identical to the "Health Gathering Supreme" scenario included in the ViZDoom environment. The aim of the agent is to navigate a maze, collect health kits and avoid vials with poison. A map of the maze is shown in Figure S2.
All other Health gathering scenarios are set up in the same labyrinth, but differ in the presence and the number of objects in the maze: no poison vials, and a different number of health kits depending on the variant of the Health gathering scenario. In each Health gathering scenario a constant number of health kits is present on the map at any given point in time. Once a health kit is gathered, another one is created at a random location in the maze.
To make sparse health gathering map results comparable to each other we kept the health d that the agent looses every 8 time steps to be proportional to the density of health kits on the map: d  #health kits.
In the Battle scenario we used the same reward as in Dosovitskiy & Koltun (2017). It is a weighted sum of changes in measurements: r = f + h/60 + a/20 where f are the amount of eliminated monsters, h the change in health and a the change in ammunition. For Basic health gathering we either used a binary reward r  {0, 1}, or the change in health: r = h/30.

13

Under review as a conference paper at ICLR 2018

Figure S2: Map of the health gathering labyrinth.

Name
Pong Seaquest S. Invaders Frostbite BeamRider
Battle Battle 2 Navigation Health gathering
Grid Vec. Grid Map

State
4 images 4 images 4 images 4 images 4 images
image + 3D vector image + 3D vector image + 2D vector image + 2D vector
10D vector 300D vector

Action
3 18 6 18 9
256 256 6 6
5 5

Table S4: List of network input and action amounts of all considered environments.

14

