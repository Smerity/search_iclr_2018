Under review as a conference paper at ICLR 2018
THREE FACTORS INFLUENCING MINIMA IN SGD
Anonymous authors Paper under double-blind review
ABSTRACT
We focus on the importance of noise in stochastic gradient descent (SGD) based training of deep neural networks (DNNs). We develop theory that studies SGD training as a stochastic differential equation and show that its stationary distribution is related to the loss surface. Our analysis suggests that the combination of batch size, learning rate, and the variance of the true loss gradients acts as a hyperparameter steering the behavior of SGD and determines the trade-offs between the depth and width of the minima that SGD converges to. In a nutshell, a higher ratio of learning rate to batch size leads to wider minima. We validate our theory by examining the correlation between these three factors and the final performance and sharpness of the minimum found. As a verification of our theory, we empirically demonstrate that the learning dynamics is similar between experiments with different learning rates and batch sizes in SGD if the ratio of learning rate to batch size is the same.
1 INTRODUCTION
Despite being massively over-parameterized models (Zhang et al., 2016), deep neural networks (DNNs) have demonstrated good generalization ability and achieved state-of-the-art performances in many application domains such as image (He et al., 2016) and speech recognition (Amodei et al., 2016). The reason for this success has been a focus of research recently but still remains an open question. Our work provides new theoretical insights and useful suggestions for deep learning practitioners.
The standard way of training DNNs corresponds to minimizing a loss function using stochastic gradient descent (SGD) and its variants (Bottou, 1998). In SGD, parameters are updated by taking a small discrete step depending on the learning rate (LR) in the direction of the negative loss gradient, which is approximated based on a small subset of training examples (called mini-batch). Since DNNs are highly non-convex functions with multiple minima, SGD in general converges to different local minima depending on optimization hyper-parameters, initialization and the loss curvature.
Recently, several works (Arpit et al., 2017; Advani & Saxe, 2017; Shirish Keskar et al., 2016) have investigated how SGD impacts generalization in DNNs. It has been argued that local minima with low curvature (wide minima) can generalize better than minima whose curvature is large (sharp minima) (Hochreiter & Schmidhuber, 1997; Shirish Keskar et al., 2016). Specifically, (Shirish Keskar et al., 2016) found that batch size (BS) strongly affects the curvature of minima that are reached by SGD (i.e., smaller batch sizes usually lead to wider minima). We generalize this notion to talk about the controllable noise level in SGD which we theoretically show is determined by the ratio of learning rate over batch size. As such, we discuss how SGD with appropriate noise level tends to converge to deeper and wider minima. In this vein, while (Dinh et al., 2017) discuss the existence of minima with different widths but which behave similarly in terms of predictions, we argue that SGD naturally tends to find wider minima when appropriate noise levels are used.
We approximate SGD as a continuous stochastic differential equation (Bottou, 1998; Mandt et al., 2017). Based on this, we derive the stationary distribution of this stochastic process, and further derive the relative probability of landing in one local minima as compared to another depending on their depth and width. Our main finding is that the ratio of learning rate and batch-size along with noise about the true gradient (due to the choice of batch sample) influences the trade-off between the depth and sharpness of the final minima, with a high ratio of learning rate to batch size favouring flatter minima. In addition, our analysis provides a theoretical justification for the empir-
1

Under review as a conference paper at ICLR 2018
ical observation that scaling the learning rate linearly with batch size leads to good performance of DNNs (Krizhevsky, 2014; He et al., 2016).
We verify our theoretical insights experimentally on several models and datasets. In particular, we demonstrate that high learning rate to batch size ratio (due to either high learning rate or low batchsize) leads to wider minima and correlates well with better validation performance. We also observe that a high learning rate to batch size ratio helps to prevent memorization. Furthermore, we show that multiplying each of the learning rate and the batch size by the same scalar factor results in similar training dynamics, but if the scalar factor gets too large, then the assumptions of the theory are violated and performance drops.
2 RELATED WORK
The relationship between stochastic gradient descent (SGD) and sampling a posterior distribution via stochastic Langevin methods has been the subject of discussion in a number of papers (Chen et al., 2014; Ding et al., 2014; Vollmer et al., 2015; Welling & Teh, 2011; Shang et al., 2015; Sato & Nakagawa, 2014). In particular, Mandt et al. (2017) describe the dynamics of stochastic gradient descent (SGD) as a stochastic process that can be divided into three distinct phases. In the first phase, weights diffuse and move away from the initialization. In the second phase the gradient magnitude dominates the noise in the gradient estimate. In the final phase, the weights are near the optimum. (Shwartz-Ziv & Tishby, 2017) make related observations from an information theoretic point of view and suggest the diffusion behavior of the parameters in the last phase leads to the minimization of mutual information between the input and hidden representation. In a similar vein, we relate the SGD dynamics to the stationary distribution of the stochastic differential equation. Our derivation bears similarity with Mandt et al. (2017). However, while Mandt et al. (2017) study SGD as an approximate Bayesian inference method in the final phase of optimization in a locally convex setting, our end goal is to analyse the stationary distribution over the entire parameter space reached by SGD. Further, our analysis allows us to compare the probability of SGD ending up in one minima over another, which is novel in our case.
Our work is closely related to the ongoing discussion about the role of large batch size and sharpness of found minima in generalization (Shirish Keskar et al., 2016). Goyal et al. (2017); Hoffer et al. (2017) empirically show that scaling up the learning rate, and training for more epochs, leads to good generalization using large batch size. Our novelty is in explaining from the theoretical point of view the importance of the specific ratio of learning rate to batch size. We also observe that the final minima found by large batch size can be wide if a larger learning rate is used.
Our work is also related to the importance of noise in SGD, which has been previously explored. The main inspiration behind learning rate schedule has been shown to be noise annealing (Bottou, 1998). Neelakantan et al. (2015) observe empirically that adding noise can aid optimization of very deep networks. Our analysis allows us to derive the impact of the gradient's noise in the SGD stationary distribution. Additionally, our work also provides intuitions toward explaining the recently proposed Cyclic learning rate (CLR) schedule (Smith, 2015). Cyclic learning rate schedules have demonstrated good optimization and generalization performances, but are grounded on empirical observation. We also show that one can replace learning rate annealing with an equivalent batch size schedule. It suggests that the benefit of cyclic learning rate relates to the noise that it induces.
3 THEORETICAL RESULTS
Our focus in this section is on finding the relative probability with which we end optimization in a certain minimum, as compared to another minimum. We will find that the relative probability depends on the local geometry of the loss function at each minimum, and on the BS, the LR and the covariance of the stochastic gradients.
3.1 SETUP
We follow a theoretical setup similar to Mandt et al. (2017), approximating SGD with a continuoustime stochastic process, which we now outline.
2

Under review as a conference paper at ICLR 2018

Let us consider a model parametrized by  = {1, . . . , q}. For N training examples xi, i  {1, ..., N }, the loss function, L(), and the corresponding gradient g(), are defined based on the
sum over the loss values for all training examples. Stochastic gradients g(S)() arise when we consider a batch B of size S < N of random indices drawn uniformly from {1, ..., N } and form an
(unbiased) estimate of loss and gradient based on the corresponding subset of training examples

L(S)() = 1 S

l(, xn) ,

nB

g(S)() =  L(S)() . 

We consider stochastic gradient descent (SGD) with learning rate , as defined by the update rule

(t + 1) = (t) - g(S)() .

We now make the following assumptions:

(1) By the central limit theorem (CLT), we assume the gradient noise is Gaussian with covari-

ance

matrix

1 S

C()

g(S)() = g() + 1 g(), where g()  N (0, C()) . S

We note that the covariance is symmetric positive-semidefinite, and so can be decomposed into the product of two matrices C() = B()B () .

(2) We assume the discrete process of SGD can be approximated by the continuous time limit of the following stochastic differential equation (known as a Langevin equation)

d = -g() +  B()f (t) dt S

(1)

where f (t) is a normalized Gaussian time-dependent stochastic term.

3.2 MAIN RESULTS

The Langevin equation is a stochastic differential equation, and we are interested in its stationary distribution to gain insights into the behavior of SGD and the quality of minima it converges to. It can be derived by finding the stationary solutions of a partial differential equation known as the Fokker-Planck equation, which governs the evolution of the probability density of the value of the parameters with time1. The Fokker-Planck equation and its derivation can be found in Appendix A in equation (5). We arrive at the following theorem (all the notations are defined in the setup section above):

Theorem 1 (Stationary Distribution). Assume that the gradient covariance is proportional to the identity, i.e. C() = 2()I and that |(2())| 2S|g|. Then the stationary distribution of
the stochastic differential equation 6 is given by

P () = P0e-L()/n ,

(2)

where

n



 2 () 2S

and

P0

is

a

normalization

constant.

Here P () defines the density over the parameter space. The above result says that if we run SGD

long enough (under the assumptions made), then the likelihood of the parameters being in a particu-

lar

state

asymptotically

follows

the

above

density.

Note,

that

n



 2 () 2S

is

a

measure

of

the

noise

in

the system and it depends on two parts: nc  /S, which is the controllable noise set by the choice

of hyper-parameters, and the gradient variance 2(), which is not a tunable hyper-parameter.

Given the probability density P (), we are now interested in deriving the probability of ending at a given minimum, A, which we will denote by lowercase pA = p~AC, where C is a normalization constant which is the same for every mimnima (the unnormalized probability p~A is all we are interested in when estimating the relative probability of finishing in a given minimum compared to another one). This probability is derived in Appendix D, and given in the following theorem, which is the core result of our theory.

1The Fokker-Plank equation was also used in the context of the stochastic gradient langevin dynamics (SGLD) algorithm in the Bayesian learning setting (Sato & Nakagawa, 2014). While in this setting it was found that the stochastic noise can be ignored, we arrive at a different conclusion.

3

Under review as a conference paper at ICLR 2018

Theorem 2 (Probability of ending in minima A). Assume the loss is locally convex with Hessian HA and loss LA at the minimum A. Then the unnormalized probability of ending in minima A is

p~A =

(2nA)q exp - LA

det HA

nA

where q is the number of parameters and nA  nc2(A) is the noise at the minimum.

(3)

Here we see that the probability of landing in a given minimum depends on the geometry of the loss
surface (captured by the height LA and the determinant of the Hessian det HA, which determines the width of the minimum) as well as on the noise nA at A.

For larger noise, nA, the factor exp

-

LA nA

is down-weighted compared to

(2 nA )q det HA

,

which

grows

with larger nA. Thus, with larger noise the width gets more important than the height of the mini-

mum, while the converse holds if the noise is low enough. Also notice, that with nA the probability

p~A directly depends on the ratio of learning rate and batch size nc = /S. We will explore the

implications of this experimentally in Section 4.

To see which kind of minima is preferred, it is instructive to consider the ratio of probabilities pA and pB given by

pA = pB

nAp npB

det HB exp LB - LA

det HA

nB nA

.

Let us now look at the special case where nA = n = nB (other cases can be found in Appendix E). Without loss of generality we can set LA  LB. If LA = LB the minimum with lower determinant of the Hessian (i.e. the flatter minima) is more probable. If LA  LB, it holds that pA  pB if and
only if

1  log

det HB det HA

n (LA - LB)

Y .

That is, there is an upper bound on the inverse of the noise for A to be favored in the case that its loss is higher than at B, and this upper bound depends on the difference in the heights compared to the ratio of the widths. In particular we can see that if det HB < det HA, then Y < 0, and so no amount of noise will result in A being more probable than B ­ that is, if the minimum at A is both higher and sharper than the minimum at B, it is never reached with higher probability than B, regardless of the amount of noise. However, if det HB > det HA then Y > 0, and there is a lower bound on the noise

n > (LA - LB)

log

det HB det HA

(4)

to make A more probable than B­ that is if the minimum at A is higher but flatter than the minimum at B, it is favored over B, as long as the noise is large enough, as defined by eq. (4).

To summarize, the presented theory shows that the noise level in SGD (which is defined by the learning rate, batch size and gradient covariance) controls the extent to which optimization favors wider over lower minima. Increasing the noise by increasing the ratio of learning rate and batch size, increases the probability of wider compared to lower optima.

4 EXPERIMENTS
4.1 IMPACT OF SGD ON MINIMA
In this section, we empirically study the impact of the learning rate , batch size S on the local minimum that SGD finds. We first focus on an MLP with 20 layers with ReLU activation functions (similar to one used in (Neelakantan et al., 2015)) and trained on FashionMNIST. We study how the controllable noise

4

Under review as a conference paper at ICLR 2018

Hessian norm Validation performance

6.8E+00 6.6E+00 6.4E+00 6.2E+00 6.0E+00 5.8E+00 5.6E+00
0.0E+00

L1.R0E/B-0S3

2.0E-03

89% 88% 87% 86% 85% 84%
LR/BS0E+00 2E-04 4E-04 6E-04 8E-04 1E-03

(a)

Correlation

of

 S

with logarithm of

(b) Correlation of

 S

with validation accu-

norm of Hessian.

racy.

Figure 1: Impact on SGD with ratio of learning rate (LR)  and batch size (BS) S for 20 layer ReLU Network on FashionMNIST.

Accuracy (and scaled cross-entropy) Accuracy (and scaled cross-entropy) Accuracy (and scaled cross-entropy)

CIFAR10 (Resnet56): LR=0.1/BS=128, LR=0.1/BS=1024 100
80
60 train accuracy val accuracy train loss
40 val loss
20
0 1.0 0.5 0.0 0.5 1.0 1.5 2.0

CIFAR10 (Resnet56): LR=0.1/BS=128, LR=0.01/BS=128 100 train accuracy
val accuracy train loss 80 val loss
60
40
20
0 1.0 0.5 0.0 0.5 1.0 1.5 2.0

CIFAR10 (Resnet56): LR=0.1/BS=1024, LR=0.01/BS=128 100
80
60
40
20 train accuracy val accuracy train loss
0 val loss 1.0 0.5 0.0 0.5 1.0 1.5 2.0

(a)

left

=0.1 S=128

,

right

=0.1 S=1024

(b) left

=0.1 S=128

,

right

=0.01 S=128

(c) left

=0.1 S=1024

,

right

=0.01 S=128

Figure 2: Interpolation of Resnet56 Networks trained with different with different learning rate to

batch size ratio,

 N

.

 (x-axis) corresponds to the interpolation coefficient.

Lower

 N

ratio leads to

sharper minima.

ratio nc

=

 S

leads to minima with different curvatures and validation accuracy.

To measure the

curvature at that minimum, we compute the norm of its Hessian using a finite difference (Wu et al.,

2017) (higher Hessian norm imples higher sharpness of minima). In Figure 1a, we report the norm wodanfeicdtthhrSeeTaHsheeeso[s,5srs0eiuam,ng1g0f2eo0,sr0(t3l]io)n).c,gAathtlhsamatntihcnhiiig=mghhaeSerorgbSSrtaofpiawnvuesos,dhrwsebsfleytaoShtbteGesorDeprovtfvioemerrthidszaihaftfatiterohrpneeenrnttoomnwrimcanrwidmoisftahat.hflSeaH(twteehsrsemiraeinniamtuth[m5ee.m-Tihn3ii,sm5aaeg-arelse2os]

In Figure 1b, we explore the impact of nc

=

 S

on the final validation performance and confirm

that better generalization correlates with higher nc. Taken together, Fig. 1a and Fig. 1b imply wider

minima correlate well with better

generalization.

As nc

=

 S

increases,

SGD finds local

minima that

generalize better. In the Appendix, we report similar results for Resnet applied on CIFAR10 and the

20 layer ReLU network with bad initialization.

To further illustrate the behavior of SGD with different noise levels, we train three Resnet56 models

on

CIFAR10

using

naive

SGD,

but

with

different

 S

.

Our

baseline

model

uses

=0.1 S=128

.

We

define

a

large

batch

model

=0.1 S=1024

and

a

small

learning rate model have approximately

learning rate model

the

same

 S

ratio.

In

=0.01 S=128

.

The

Figure 2, we

large batch and the follow (Goodfellow

small et al.,

2014) and look at the loss between interpolation of different pairs of models. More specifically, let

f1

and

f2

be

the

early

stop

models

of

two

SGD

optimizers

using

different

 S

.

In

Fig.

2,

we

report

L((1 - )f1 + f2 where L is the loss function and   [-1, 2].

We observe that both model with large batch or low learning rate end in a sharper minimum relatively

to

the

baseline

model,

both

having

a

lower

 S

than

the

baseline.

Each

of

the

three

plots

in

Figure

2

adds empirical weight to our theoretical prediction that higher nc = /S gives preference to wider

minima over sharper minima.

Finally we perform a similar experiment on VGG-11 on CIFAR-10, but in this case, we train all the

models with the same noise level but different values of learning rate and batch size. Specifically,

in

this

case

we

use

=0.1× S=50×

,

where

we

use



=

0.25, 1, 4.

We

then

interpolate

between

the

model

5

Under review as a conference paper at ICLR 2018

Accuracy (and scaled cross-entropy) Accuracy (and scaled cross-entropy)

CIFAR10 (VGG11): = 1, = 4
100

80

60

train accuracy val accuracy

train loss

40 val loss

20

0 1.0 0.5 0.0 0.5 1.0 1.5

2.0

100 80 60 40 20 0
1.0

CIFAR10 (VGG11): = 1, = 0.25
train accuracy val accuracy train loss val loss 0.5 0.0 0.5 1.0 1.5 2.0

(a)  = 1 corresponds to model at  = 0 and  = 4 (b)  = 1 corresponds to model at  = 0 and  = 4

corresponds to model at  = 1

corresponds to model at  = 0.25

Figure 3:

Left:

=0.1× S=50×

,

right:

=0.1× S=50×

.

Interpolation between parameters of models trained with

the same learning rate () to batch-size (S) ratio, but different  and S values determined by .

Hence our theory predicts the minima for these models should be qualitatively similar as can be

seen by these plots.

Accuracy Accuracy

100 80 60 40 20
0

100

80

60

Cyclic BS Train Cyclic LR Train Cyclic BS Test Cyclic LR Test

100 Epoch 200

300

40
20 0

bs=128, lr=0.001 train bs=640, lr0.005 train bs=128, lr=0.001 test bs=640, lr=0.005 test

100 Epoch 200

300

Figure 4: Learning rate schedule can be replaced by an equivalent batch size schedule. The ratio of learning rate to batch size is equal at all times in both red and blue in each plot. Train and test accuracy for experiments involving VGG-11 architecture on CIFAR10 dataset. Left: cyclic batch size schedule (blue) in range 128 to 640, compared to cyclic learning rate schedule (red) in range 0.001 to 0.005. Right: constant batch size 128 and constant learning rate 0.001 (blue), compared to constant batch size 640 and constant learning rate 0.005 (red).

parameters of the pair of models and record loss and accuracy (shown in figure 3). We find that all the minima in this case have similar width and depth qualitatively showing that for the same noise ratio, SGD ends up in minima of similar quality.

4.2

 S

RATIO DETERMINES LEARNING DYNAMICS

OF

SGD

A prediction of the theory is that the learning rate  and batch size S only appear in the ratio nc = /S in the expressions for the probability of finishing in a given minimum. Based on this, the endpoint of SGD with a learning rate schedule   /a, for some a > 0, and a constant batch size S, should be the same as the endpoint of SGD with a constant learning rate and a batch size schedule S  aS. We see in Fig. 4 that exchanging a learning rate schedule for a batch size schedule leads to similar train and test accuracy, for both cyclic and constant schedules, in the case where the controllable noise nc = /S is the same in each case. In particular we notice very similar dynamics in the case of a cyclic learning rate when compared to a cyclic batch size. Full loss curves and learning rate and batch size schedules for this experiment can be found in Appendix G.2 in Fig. 10. The test accuracy for cyclic batch size and cyclic learning rate was 89.39% and 89.24% respectively. For constant (batch size, learning rate) = (128, 0.001) and constant (batch size, learning rate) = (640, 0.005) the test accuracy was 87.25% and 86.92% respectively.

Note that exchanging batch size for learning rate such that the ratio /S remains constant is different to the common assumption that one should trade them so as to keep the ratio / S constant. This commonly used heuristic of scaling the learning rate with the square root of the batch size is used for example in (Hoffer et al., 2017), as a way of keeping the covariance matrix of the parameter update step the same for any batch size. However, our point here is that our theory and experiments

6

Under review as a conference paper at ICLR 2018

Validation accuracy Validation accuracy

25.0% random labels 55% 50.0% random labels
76%

74% 50%

72% 70% 45%

68% 65%

40%

63% 35% 62%

60%0%Ran20d%om40l%ab6e0l%ac8c0u%ra1c00y% 30%0%Ran20d%om40l%ab6e0l%ac8c0u%ra1c00y%

25.0% random labels 55% 50.0% random labels
76%

74% 50%

72% 70% 45%

68% 65%

40%

63% 35% 62%

60%0%Ran20d%om40l%ab6e0l%ac8c0u%ra1c00y% 30%0%Ran20d%om40l%ab6e0l%ac8c0u%ra1c00y%

Figure 5:

 S

impact on memorization on dataset with added 25% and 50% random label noiseon

the training set. The two left columns (and right columns) use 0.9 momentum (and 0.0 momen-

tum) respectively.

We

observe that given a specific

level of memorization,

high

 S

leads to better

generalization.

suggest exchanging learning rate and batch size in a way that keeps the ratio of controllable noise nc = /S constant will result in the same stationary distribution.

4.3 IMPACT OF SGD ON MEMORIZATION

To generalize well, a model must identify the underlying pattern in the data instead of simply perfectly memorizing each training example. An empirical approach to test for memorization is to see if a deep neural network can fit a training set with random noise labels rather than the true labels (Zhang et al., 2016; Arpit et al., 2017). In this section, we highlight that SGD with a sufficient amount of noise reduces the amount of memorization in a network.

Experiments are performed using the MNIST dataset and an MLP similar to the one in (Arpit et al.,

2017), but with 256 hidden units. We train the MLP on the dataset with different amounts of random

labels in the training set.

For

each label noise level, we evaluate the impact of

 S

,

controlling

the

amount of noise in the SGD and its impact on the generalization performances. Specifically, we

run a grid of batch size in 50, 100, 200, 400, 800, learning rate in 0.05, 0.1, 0.2 and momentum in

0.0 and 0.9. Models are trained for 300 epochs. We report in Fig. 5 the MLP performances for

both the noisy training set and the clean validation set (without random label, to measure the model

generalization ability).

Our analysis highlights that SGD with low controllable noise nc

=

 S

steers the endpoint of op-

timization towards a minima with low generalization ability, associated with a sharp minima, as

predicted by our theory. We observe in Fig. 5 that larger noise in SGD (regardless if controlled us-

ing smaller batch size or larger learning rate) leads to solutions which generalize better for the same

amount of memorized random labels on the training set. We also reproduce the observation reported

in (Arpit et al., 2017): that randomization roughly starts after reaching maximum generalization -

for more details see the curves in Fig. 11 included in Appendix G.3. They show that lower control-

lable

noise

 S

optimize

much

slower

and

do

not

reach

larger

than

20%

accuracy

on

random

labels.

For runs with momentum we exclude higher learning rates than 0.02 as they lead to divergence (see

appendix).

4.4 BREAKING POINT OF THE THEORY IN PRACTICE
Our analysis relies on the fact that the gradient step is sufficiently small so that first order approximation of a Taylor expansion is a good estimate of the loss function. In the case where the learning rate becomes too high, the first order approximation of a Taylor expansion is no longer suitable, in which case the continuous limit of the discrete SGD update equation will no longer be valid. In this case, the stochastic differential equation doesn't hold, and hence neither does the Fokker-Planck equation, and so our theory will not be predictive. In particular, we don't expect to arrive at the same stationary distribution as governed just by the ratio /S. This is exemplified in Fig 6, where similar learning dynamics and final performance is seen when simultaneously multiplying the learning rate and batch size by a factor  up to a certain limit. This is done for different train-set size to see if the breaking point depends on this size. This plots seem to suggest that this happens for smaller 

7

Under review as a conference paper at ICLR 2018

validation accuracy validation accuracy validation accuracy

80

70

60 50

=1 =2 =3

40 = 4

30 = 7

20

10
0 25 50 75 ep1o00ch 125 150 175 200

(a) Train dataset size 12000

90

80

70 = 1

60 = 2

50 = 3

40 30

=4 =5

20

10

0 25 50 75 ep1o00ch 125 150 175 200

(b) Train dataset size 22500

80 = 1

=3

60 = 4

=5

40

=7 =9

20 = 10

0 25 50 75 ep1o00ch 125 150 175 200

(c) Train dataset size 45000

Figure 6: Breaking point of the theory: Experiments involving VGG-11 architecture on CIFAR10

dataset. Validation accuracy for different dataset sizes, and different  values. In each experiment,

we multiply the learning rate () and batch size (S) with  such that the ratio

× S×

is fixed.

We

observe that for the same ratio increasing the learning rate and batch size yields similar performance

up to a  and then performance drops significantly.

values when dataset size is smaller. We highlight other limitations due to our theory assumption in appendix F. A similar experiment is performed on Resnets in figure 7.

4.5 CYCLICAL BATCH AND LEARNING RATE SCHEDULES

It has been observed that a cyclic learning rate (CLR) schedule leads to better generalization (Smith,

2015). In Sec. 4.2 we demonstrated that one can exchange cyclic learning rate schedule (CLR) with

batch size (CBS) and approximately preserve the practical benefit of CLR. This inspired us to hy-

pothesize

that

by

changing

between

controllable

noise

levels

(

 S

)

CLR

switches

between

sharp/deep

and wide/shallow minima. To validate that, we run VGG-11 on CIFAR10 using 4 training sched-

ules: CLR with stepsize of 4 and 15 epochs in each stage, CBS with stepsize 4 and 15 epochs in

each stage. Each run is repeated 8 times and for each variant we track evolution of sharpness and

accuracy. We observe that CBS and CLR with longer stages lead to 89.9 ± 0.20% and 90.2 ± 0.10%

test accuracy, in both cases 0.2 - 0.3% above variance with shorter range. CBS and CLR with

long stepsize each seem to be promising schedules for training DNNs. Finally, we validate that

CBS and CLR switch between sharp/deep and wide/shallow minima, suggesting that CLR improves

convergence time to stationary distribution, as seen in Fig. 13 of Appendix G.4.

5 CONCLUSIONS
We shed light on the role of noise in SGD optimization of DNNs and argued that three factors (batch size, learning rate and gradient variance) strongly influence the properties (loss and width) of the final minima at which SGD converges. Learning rate and batch size of SGD can be viewed as one effective hyper-parameter that act as a factor controllable noise nc = /S, which, together with the gradient covariance influences the trade-off between the loss and width of the final minima (high noise favors wide minima), which in turn tunes the robustness of the prediction function. 2.
Further, we experimentally verify that the controllable noise nc = /S determines the width and height of the minima towards which SGD converges. We also show the impact of this controllable noise on the memorization phenomenon. We discussed the limitations of the theory and in what situations it breaks down, exemplified by when the learning rate gets too large. We also experimentally verify that  and S can vary in linear proportion as long as the controllable noise /S remains the same. In addition, our experiments suggest that cyclical learning rates oscillate between sharp/deep and wide/shallow minima as long as the stage of increased noise is long enough to allow for mixing.

REFERENCES
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural networks. arXiv preprint arXiv:1710.03667, 2017.
2Robustness of model can also be controlled using regularization methods, e.g. L2, dropout or adversarial training.

8

Under review as a conference paper at ICLR 2018
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: Endto-end speech recognition in english and mandarin. In International Conference on Machine Learning, pp. 173­182, 2016.
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 233­242, International Convention Centre, Sydney, Australia, 06­11 Aug 2017. PMLR. URL http://proceedings.mlr.press/ v70/arpit17a.html.
Léon Bottou. Online learning and stochastic approximations. On-line learning in neural networks, 17(9):142, 1998.
T. Chen, E. B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In Proceedings of the 31st International Conference on Machine Learning, pp. 1683­1691, 2014.
N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, and H. Neven. Bayesian sampling using stochastic gradient thermostats. In Advances in Neural Information Processing Systems 27, pp. 3203­3211, 2014.
L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio. Sharp Minima Can Generalize For Deep Nets. ArXiv e-prints, March 2017.
Ian J Goodfellow, Oriol Vinyals, and Andrew M Saxe. Qualitatively characterizing neural network optimization problems. arXiv preprint arXiv:1412.6544, 2014.
P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. ArXiv e-prints, June 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770­778, 2016.
Sepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural Computation, 9(1):1­42, 1997.
E. Hoffer, I. Hubara, and D. Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. ArXiv e-prints, May 2017.
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014.
S. Mandt, M. D. Hoffman, and D. M. Blei. Stochastic Gradient Descent as Approximate Bayesian Inference. ArXiv e-prints, April 2017.
A. Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens. Adding Gradient Noise Improves Learning for Very Deep Networks. ArXiv e-prints, November 2015.
Issei Sato and Hiroshi Nakagawa. Approximation analysis of stochastic gradient langevin dynamics by using fokker-planck equation and ito process. In Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp. 982­990, Bejing, China, 22­24 Jun 2014. PMLR. URL http://proceedings.mlr.press/v32/satoa14.html.
Xiaocheng Shang, Zhanxing Zhu, Benedict Leimkuhler, and Amos J Storkey. Covariancecontrolled adaptive Langevin thermostat for large-scale Bayesian sampling. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 37­45, 2015. URL http://papers.nips.cc/paper/ 5978-covariance-controlled-adaptive-langevin-thermostat-for-large-scale-bayesian-s pdf.
9

Under review as a conference paper at ICLR 2018

N. Shirish Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. ArXiv e-prints, September 2016.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. CoRR, abs/1703.00810, 2017. URL http://arxiv.org/abs/1703.00810.
L. N. Smith. Cyclical Learning Rates for Training Neural Networks. ArXiv e-prints, June 2015.
S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh. (Non-) asymptotic properties of stochastic gradient Langevin dynamics. arXiv preprint arXiv:1501.00438, 2015.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning, pp. 681­688, 2011.
Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.

APPENDIX

A DERIVATION OF THE FOKKER-PLANCK EQUATION

In this appendix we derive the Fokker-Planck equation. For any stochastic differential equation,
since the evolution is noisy we can't say exactly where in parameter space the parameter values will be at any given time. But we can talk about the probability P (, t|0, t0) that a parameter takes a certain value  at a certain time t, given that it started at 0, t0. That is captured by the Fokker-Planck equation, which reads

P (, t) t

=



·

g()P

(,

t)

+

2 2S



·

[C()P

(,

t)]

.

(5)

In this appendix we will derive the above equation from the stochastic differential equation (6). We will not be interested in pure mathematical rigour, but intend the proof to add intuition for the machine learning audience.

For brevity we will sometimes write the probability just as P (, t). We will sometimes make use of tensor index notation, where a tensor is denoted by its components, (for example i are the components of the vector ), and we will use the index summation convention, where a repeated index is to be summed over.

We start with the stochastic differential equation

d = -g() +  B()f (t). dt S

(6)

A formal expression for the probability for a given noise function f is given by P (, t) = ( - f ), but since we don't know the noise, we instead average over all possible noise functions to get

P (, t) = E[( - f )].

(7)

While this is just a formal solution we will make use of it later in the derivation.

We now consider a small step in time t, working at first order in t, and ask how far the parameters move, denoted , which is given by integrating (6)

 = -gt +  B

t+t
f (t )dt

St

(8)

10

Under review as a conference paper at ICLR 2018

where we've assumed that  is small enough that we can evaluate g at the original . We now look at expectations of . Using the fact that the noise is normalized Gaussian, E(f (t)) = 0, we get (switching to index notation for clarity)

E(i) = -git

(9)

and using that the noise is normalized Gaussian again, we have that E(f (t)f (t )) = I(t - t ), leading to

E(i j )

=

2 S Cijt +

O(t2).

(10)

If at time t we are at position  and end up at position  =  +  at time t + t, then we can take (7) with f =  and Taylor expand it in i

P (, t + t| , t) =

 1 + E(i) i

+

1 2 2 E(ij) ij

+ O(t2)

( -  ). (11)

We deal with the derivatives of the delta functions in the following way. We will use the following identity, called the Chapman-Kolmogorov equation, which reads

+

P (, t + t|0, t0) =

d P (, t + t| , t )P ( , t |0, t0)

-

(12)

for any t , such that t0  t  t + t. This identity is an integral version of the chain rule of probability, stating there are multiple paths of getting from an initial position 0 to  at time t + t and one should sum all of these paths.

We will now substitute (11) into the first term on the right hand side of (12) with t set to be t, and apply integration by parts (assuming vanishing boundary conditions at infinity) to move the derivatives off of the delta functions and onto the other terms. We end up with

P (,

t

+

t|

,

t)

=

P (,

t|0, t0)

-

 i

(E(i)P (,

t|0,

t0))

+

1 2

2 ij

(E(ij)P (,

t|0, t0))

.

(13)

We can then take the first term on the right hand side of (13) to the other side, insert (9) and (10), divide by t and take the limit t  0, getting a partial derivative with respect to time on the left hand side, leading directly to the Fokker-Planck equation quoted in the text (5) (where we have reverted back to vector notation from index notation for conciseness)

P (, t) t

=



·

g()P

(,

t)

+

2 2S



·

[C()P

(,

t)]

.

(14)

B INTUITIONS FROM FOKKER-PLANCK

In this appendix we will give some supplementary comments about the intuition we can gain from
the Fokker-Planck equation (5). If the learning rates, batch size and covariance all change slowly, and the covariance is proportional to the identity C = 2I then we can rewrite the Fokker-Planck
equation in the following form

P (, t) t

=



·

[g()P (, t)]

+

2 2S

2

P

(,

t

)

,

(15)

where we have rescaled the time coordinate to be t  t. One can now see that in terms of this rescaled time coordinate, the ratio between the drift and diffusion terms is governed by the following
quantity

2 n
2S

(16)

11

Under review as a conference paper at ICLR 2018

which we call the noise. In terms of the balance between drift and diffusion, we see that higher n gives rise to a more diffusive evolution, while a lower n allows the potential drift term to dominate. In the next section we will see how n controls the stationary distribution at which SGD converges to. For now we highlight that in terms of this rescaled coordinate, only n controls the evolution towards this stationary distribution (not just its endpoint), in terms of the rescaled time t. That is, learning rate and batch size are interchangable in the sense that n is invariant under transformations S  aS,   /a, for a > 0. But note that the time it takes to reach the stationary distribution depends on  as well, because of the rescaled time variable. For example, for a higher learning rate, but constant n, one arrives at the same stationary distribution, but in a quicker time by a factor of 1/.
However, a caution here is necessary. The first order SGD update equation only holds for small enough  that a first order approximation to a Taylor expansion is valid, and hence we expect the first order approximation of SGD as a continuous stochastic differential equation to break down for high . Thus, we expect learning rate and batch size to be interchangable up to a maximum value of  at which the approximation breaks.

C STATIONARY DISTRIBUTION

In this appendix we will prove equation (2) quoted in the main text, which claims that the stationary (ie late time, independent of time) solution of the Fokker-Planck equation has the following form

P () = P0e-L()/n.

(17)

where

n



2 2S

,

under

the

assumptions

that

the

covariance

is

proportional

to

the

identity

C()

=

2()I and further that |(2)| 2S|g|. Here P0 is a normalization constant.

In order to prove this is the stationary distribution, we need to solve the Fokker-Planck equation (5) with the left hand side set equal to zero (since we are looking for the stationary solution). To do this, we begin by writing the Fokker-Planck equation in a slightly different form, making use of a probability current J , defined

J



g()P (, t)

+

2 2S



·

(C()P (, t))

in which the Fokker-Planck equation becomes

(18)

P (, t) t

=



·

J.

(19)

At this point we use the assumptions that C() = 2()I and that |(2)| 2S|g| in order to bring C() outside of the divergence in (18) and turn the divergence into a gradient of P . We get

J

=

g()P (, t)

+

22 2S



P

(,

t)

(20)

Now we notice we can write this in the following way J = ne-L()/n eL()/nP

(21)

where

we've

defined

n

=

2 2S

.

Then

we

consider

that



·

J

=

0

and

hence

 · ne-L()/n eL()/nP = 0.

(22)

This equation has the rather easy to spot solution, simply P () = P0e-L()/n
which is the desired stationary solution we intended to find.

(23)

12

Under review as a conference paper at ICLR 2018

D DERIVATION OF PROBABILITY OF ENDING IN A GIVEN MINIMA

In this appendix we derive the discrete set of probabilities of ending at each minima, as given in (3). We work locally near A and take the following approximation of the loss function, since it is near a minimum,

1

L()



LA

+

( 2

-

A)

HA( - A).

(24)

where HA is the Hessian, and is positive definite for a minimum.
The distribution PA() is a probability density, while we are interested in the discrete set of probabilities of ending at a given minimum, which we will denote by lowercase pA. To calculate this discrete set of probabilities, for each minimum we need to integrate this stationary distribution over an interval containing the minimum. We take 2 to take the constant value A2 within this minimum and hence denote n by nA. Integrating (2) in some region RA around A and using (24)

1

pA  P0

exp
RA

- L() nA

11

 P0

exp
RA

- nA

LA

+

( 2

-

A)

HA( - A)

 P0 exp

- LA nA

(2nA)p

1 det HA

(25) (26) (27)

where in the last line we assume the region is large enough that an approximation to the full Gaussian integral can be used (i.e. that the tails don't contribute, which is fine as long as the region is sufficiently larger than det HA) ­ note that the region can't be too large, otherwise we would invalidate our local assumption. The picture is that the minima are sufficiently far apart that the region can be taken sufficiently large for this approximation to be valid.

Since we are interested in relative probabilities between different minima, so we can consider the unnormalized probability, dropping factors that are common amongst all minima we get

p~A =

npA exp - LA

det HA

nA

(28)

This is the required expression given in the main text.

As an aside, we note that the derivation used above talked about true minima, with a positive definite Hessian, i.e. a minima in all directions. In the real world application to deep neural networks with a large number of parameters, it is unrealistic to expect the endpoint of training to be in a true minimum. Instead, it is more likely to be a point in which the Hessian has some positive eigenvalues Ai in just a few directions, say d q, with the other eigenvalues (approximately) zero. In this case, the relative probabilities are given by

p~A =

(2nA)d exp - LA .

A1 · · · Ad

nA

(29)

In the main paper we only consider the simplest case of true minima, though much of what we say can be extended to the case of points with some positive Hessian eigenvalues with some flat directions.

E FURTHER SPECIAL CASES
In this appendix we explore other special cases in which the ratio of probabilities for ending in one or another minima simplifies, allowing for an intuitive picture of the way SGD favours some minima over others. We begin this appendix with the special case of equal height minima, and then look at equal width minima.

13

Under review as a conference paper at ICLR 2018

80

Valid Acc

60 40 20
0

baseline times 2 times 3 times 4 times 5 times 6 times 7 times 8 times 9 50 100 150 200 250 300 Epoch

Figure 7: Experiments involving Resnet56 architecture on CIFAR10 dataset. In each curve, we

multiply

the

 S

ratio

by

a

given

factor

(increasing

both

batch

size

and

learning

rate).

We

observe

that

multiplying the ratio by a factor up to 5 results in similar performances. However, the performances

degrades for factor superior to 5.

E.1 EQUAL HEIGHT MINIMA

Consider now the case where the minima are the same height LA = L = LB. The simplest case is to look at equal width minima, in which case A is favoured over B if and only if nA > nB, with the relation being exponential ­ ie a noisier minima is exponentially more probable than a less noisy
one.

If on the other hand the minima are not equal width, but do have equal height, we can without loss of generality take det HB > det HA. We find that pA > pB if and only if the following inequality is satisfied

log

npA nBp

11 >L -
nA nB

+ log det HB det HA

11 >L -
nA nB

(30)

This places an interesting constraint on the noises and the height L.

E.2 EQUAL WIDTH MINIMA

The case of equal width minima has been covered in the previous sections, but with either equal width or equal height presupposed. Here, if we don't assume the noise or height are equal, but that the widths are the same, then pA > pB if and only if

nAp

e-

LA nA

>

nBp e-

LB nB

.

(31)

It is not clear how to simplify this further, but one can see that if LA < LB then this inequality will be satisfied for most reasonable values of nA and nB, unless nB is exponentially larger than nA.

F LIMITATION OF OUR ANALYSIS
Due to our assumptions, we expect the theory to become unreliable when the discrete to continuous approximation fails, when the variance in gradients changes as fast as the loss, when the covariance in gradients cannot be written as proportional to the identity matrix, the finite size of the training set and when momentum is considered.
We discussed limit of the discrete to continous approximation in 4.4 (further illustrated in 7).
When the gradient variance changes rapidly compared to the loss, there will be a different, more complicated stationary solution that arises when solving the stationary Fokker-Planck equation. We expect this to happen in certain datasets and in using certain architectures, though it is hard to say an exact relation between datasets, architectures and gradient variances.
When the covariance in gradients can't be written as proportional to the identity matrix, the stationary solution to the Fokker-Planck equation is the solution to a complicated partial differential

14

Under review as a conference paper at ICLR 2018

Validation Accuracy

92 91 90 89 88 87 86 85 84
10 4

10 3 LR/BS

Figure 8: Validation accuracy of Resnet56 networks against different ratios learning rate (LR) to batch size (BS) on CIFAR10. Trained with naive SGD.

equation, and one can't easily spot the solution by inspection as we do in Appendix C. We expect this approximation to break down especially in the case of complicated architectures where different gradient directions will be have very different gradient covariances.

Our theory does not involve the finite size of the training set, which is a drawback of the theory. This may be especially apparent when the batch size becomes large, compared to the training set size, and we expect the theory to break down at this point.

Finally, we mention that momentum is used in practical deep learning optimization algorithms. Our theory does not consider momentum, which is a drawback of the theory and we expect it to break down in models in which momentum is important. We can write a Langevin equation for the case of momentum, with momentum damping coefficient µ,
dv = -µv - g + B f (t) dt S
where the velocity is v = d/dt.

The Fokker-Planck equation corresponding to this Langevin equation with momentum is

 t

+

v

·



P =  ·

µ vP + gP


+

22 2S

2

P

where P is now a function of velocity v as well as  and t, and we've again assumed the gradient covariance varies slowly compared to the loss.

From this more complicated Fokker-Planck equation, it is hard to spot the stationary distribution.
We leave the study of this for further work. For now, we just note that a factor of  can be taken
from the right hand side, and then the ratio between the diffusion and drift terms will again be the noise n = 2/(2S)

G OTHER EMPIRICAL RESULTS
G.1 MORE SGD IMPACTS ON MINIMA
Figure 8 reports the validation accuracy for Resnet models trained on CIFAR with different LR/BS ratio. Figure 9 reports 20 layer ReLU experiment variant with scaled initialization by 0.7 , which explores case in which model is driven to bad local minima (and is not addressed by our theory).
G.2 MORE LEARNING RATE & BATCH SIZE EXCHANGEABILITY
In this appendix we show in more detail the experiments of Section 4.2 which show the exchangeability of learning rate and batch size. In Fig. 10 we show the log cross entropy loss, the epochaveraged cross entropy loss, train, validation and test accuracy as well as the batch size schedule,

15

Under review as a conference paper at ICLR 2018

Validation performance

89%

88%

87%

86%

85%
LR/BS0E+001E-04 2E-04 3E-04 4E-04 5E-04 6E-04

Figure

9:

Correlation

of

 S

with

validation

accuracy

for

20

layer

ReLU

network

on

Fashion

MNIST

with bad initialization.

learning rate schedule. We see in the lr/bs plot that the orange and blue lines (cyclic schedules) have the same ratio of lr/bs as each other throughout, and that their dynamics are very similar to each other. The same holds for the green and red lines, of constant batch size and learning rate. This supports the theoretical result of this paper, that the ratio S/ governs the stationary distribution of SGD.
We note that it is not just in the stationary distribution that the exchangeability holds in these plots - it appears throughout training, highlighted especially in the cyclic schedules. We postulate that due to the scaling relation in the Fokker-Planck equation, the exchangeability holds throughout learning as well as just at the end, as long as the learning rate does not get so high as to ruin the approximations under which the Fokker-Planck equation holds.
G.3 DETAILS ON MEMORIZATION EXPERIMENT
We report learning curves from memorization experiment with 0.0 momentum, see Fig. 11. We additionally confirm similar result as in previous experiments and show correlation between batch size and learning rate ratio and norm of Hessian, see Fig. 12.
G.4 CYCLIC BATCH AND LEARNING SCHEDULE
We report sharpness and accuracy evolution for learning schedules discussed in Sec. 4.5, see Fig. 13.

16

Under review as a conference paper at ICLR 2018
Figure 10: We show in more detail exchangeability of batch size and learning rate in a one-to-one ratio. In blue, cyclic batch size schedule between size 128 and 640 and fixed learning rate 0.005, is exchangeable with orange cyclic learning rate schedule between learning rates 0.001 and 0.005 with fixed batch size 128. In green, constant batch size 640 and constant learning rate 0.005 is exchangeable with, in red, constant batch size 128 and constant learning rate 0.005.
17

Under review as a conference paper at ICLR 2018

Accuracy

25.0% random labels 25.0% random labels
100% 100%

80% 80%

60% 60%
40% 40% 20% 20%
050.500%10r0an15d0om200la2b50el3s00 050.500%10r0an15d0om200la2b50el3s00
100% 100%

80% 80%

60% 60%

40% 40%

20% 20%

0 50 100Ep1o50ch200 250 300

0 50 100Ep1o50ch200 250 300

Accuracy

Figure 11: Learning curves for memorization experiment with momentum 0.0. Solid lines represent

training

accuracy,

dotted

validation

accuracy.

Warm

color

indicates

higher

 S

ratio.

Hessian norm

6.8E+00 6.6E+00 6.4E+00 6.2E+00 6.0E+00 5.8E+00 5.6E+00
0.0E+00

L1.R0E/B-0S3

2.0E-03 0.0E+00

L1.R0E/B-0S3

2.0E-03

Figure 12: Correlation between (approximate) norm of Hessian of best validation minima and LR/BS for 0.0 (left) and 0.9 (right) momentum.

Train loss

Norm. hessian norm

4.0E+07 Cyclical BS + long

Cyclical LR + long
4.0E+07

Norm. Hessian norm

3.5E+07

Loss 3.5E+07

3.0E+07

3.0E+07

2.5E+07

2.5E+07

2.0E+07

2.0E+07

1.5E+07

1.5E+07

1.0E+07

1.0E+07

5.0E+06

5.0E+06

600.0E+00 80 1E0p0och120 140 600.0E+00 80 1E0p0och120 140

4.0E+07 3.5E+07 3.0E+07 2.5E+07 2.0E+07 1.5E+07 1.0E+07 5.0E+06 0.0E+00

Constant LR

4.0E+07

3.5E+07

3.0E+07

2.5E+07

2.0E+07

1.5E+07

1.0E+07

5.0E+06

60 80 Ep1o0ch0 120 140 0.0E+00 60

Cyclical LR
0.3 0.2 0.2 0.2 0.1 0.1
80 E1p0o0ch 120 140

Figure 13: Cyclical learning rate switches between sharp/deep and wide/shallow minima if stage of increased noise are long enough to allow for mixing. Plots from left to right: cyclical BS with long stage (15 epochs), cyclical LR with long stage, constant learning rate, cyclical LR with short stage (5 epochs). On vertical axis we report loss (red) and norm of Hessian normalized by loss (blue).

18

