Under review as a conference paper at ICLR 2018
DNA-GAN: LEARNING DISENTANGLED REPRESENTATIONS FROM MULTI-ATTRIBUTE IMAGES
Anonymous authors Paper under double-blind review
ABSTRACT
Disentangling factors of variation has always been a challenging problem in representation learning. Existing algorithms suffer from many limitations, such as unpredictable disentangling factors, bad quality of generated images from encodings, lack of identity information, etc. In this paper, we proposed a supervised algorithm called DNA-GAN trying to disentangle different attributes of images. The latent representations of images are DNA-like, in which each individual piece represents an independent factor of variation. By annihilating the recessive piece and swapping a certain piece of two latent representations, we obtain another two different representations which could be decoded into images. In order to obtain realistic images and also disentangled representations, we introduced the discriminator for adversarial training. Experiments on Multi-PIE and CelebA datasets demonstrate the effectiveness of our method and the advantage of overcoming limitations existing in other methods.
1 INTRODUCTION
The success of machine learning algorithms depends on data representation, because different representations can entangle different explanatory factors of variation behind the data. Although prior knowledge can help us design representations, the vast demand of AI algorithms in various domains cannot be met, since feature engineering is labor-intensive and needs domain expert knowledge. Therefore, algorithms that can automatically learn good representations of data will definitely make it easier for people to extract useful information when building classifiers or predictors.
Of all criteria of learning good representations as discussed in Bengio et al. (2013), disentangling factors of variation is an important one that helps separate various explanatory factors. For example, given a human-face image, we can obtain various information about the person, including gender, hair style, facial expression, with/without eyeglasses and so on. All of these information are entangled in a single image, which renders the difficulty of training a single classifier to handle different facial attributes. If we could obtain a disentangled representation of the face image, we may build up only one classifier for multiple attributes.
In this paper, we propose a supervised method called DNA-GAN to obtain disentangled representations of images. The idea of DNA-GAN is motivated by the DNA double helix structure, in which different kinds of traits are encoded in different DNA pieces. We make a similar assumption that different visual attributes in an image are controlled by different pieces of encodings in its latent representations. In DNA-GAN, an encoder is used to encode an image to the attribute-relevant part and the attribute-irrelevant part, where different pieces in the attribute-relevant part encode information of different attributes, and the attribute-irrelevant part encodes other information. For example, given a facial image, we are trying to obtain a latent representation that each individual part controls different attributes, such as hairstyles, genders, expressions and so on. Though annihilating recessive pieces and swapping certain pieces, we can obtain novel crossbreeds that can be decoded into new images. By the adversarial discriminator loss and the reconstruction loss, DNA-GAN can reconstruct the input images and generate new images with new attributes. Each attribute is disentangled from others gradually though iterative training. Finally, we are able to obtain disentangled representations in the latent representations.
The summary of contributions of our work is as follows:
1

Under review as a conference paper at ICLR 2018
1. We propose a supervised algorithm called DNA-GAN, that is able to disentangle multiple attributes as demonstrated by the experiments of interpolating multiple attributes on MultiPIE (Gross et al., 2010) and CelebA (Liu et al., 2015) datasets.
2. We introduce the annihilating operation that prevents from trivial solutions: the attributerelevant part encodes information of the whole image instead of a certain attribute.
3. We employ iterative training to address the problem of unbalanced multi-attribute image data, which was theoretically proved to be more efficient than random image pairs.
2 RELATED WORK
Traditional representation learning algorithms focus on (1) probabilistic graphical models, characterized by Restricted Boltzmann Machine (RBM) (Smolensky, 1986), Autoencoder (AE) and their variants; (2) manifold learning and geometrical approaches, such as Principal Components Analysis (PCA) (Pearson, 1901), Locally Linear Embedding (LLE) (Roweis & Saul, 2000), Local Coordinate Coding (LCC) (Yu et al., 2009), etc. However, recent research has actively focused on developing deep probabilistic models that learn to represent the distribution of data. Kingma & Welling (2013) employs an explicit model distribution and uses variational inference to learn its parameters. As the generative adversarial networks (GAN) (Goodfellow et al., 2014) has been invented, many implicit models are developed.
In the semi-supervised setting, Siddharth et al. (2016) learns a disentangled representations by using an auxiliary variable. Bouchacourt et al. (2017) proposes the ML-VAE that can learn disentangled representations from a set of grouped observations. In the unsupervised setting, InfoGAN (Chen et al., 2016) tries to maximize mutual information between a small subset of latent variables and observations by introducing an auxiliary network to approximate the posterior. However, it relies much on the a-priori choice of distributions and suffered from unstable training. Another popular unsupervised method -VAE (Higgins et al., 2016), adapted from VAE, lays great stress on the KL distance between the approximate posterior and the prior. However, unsupervised approaches do not anchor a specific meaning into the disentanglement.
More closely with our method, supervised methods take the advantage of labeled data and try to disentangle the factors as expected. DC-IGN (Kulkarni et al., 2015) asks the active attribute to explain certain factor of variation by feeding the other attributes by the average in a mini-batch. TD-GAN (Wang et al., 2017) uses a tag mapping net to boost the quality of disentangled representations, which are consistent with the representations extracted from images through the disentangling network. Besides, the quality of generated images is improved by implementing the adversarial training strategy. However, the identity information should be labeled so as to preserve the id information when swapping attributes, which renders the limitation of applying it into many other datasets without id labels. IcGAN (Perarnau et al., 2016) is a multi-stage training algorithm that first takes the advantage of cGAN (Mirza & Osindero, 2014) to learn a map from latent representations and conditional information to real images, and then learn its inverse map from images to the latent representations and conditions in a supervised manner. The overall effect depends on each training stage, therefore it is hard to obtain satisfying images. Unlike these models, our model requires neither explicit id information in labels nor multi-stage training.
Many works have studied the image-to-image translation between unpaired image data using GANbased architectures, see Isola et al. (2016), Taigman et al. (2016), Zhu et al. (2017), Liu et al. (2017) and Zhou et al. (2017). Interestingly, these models require a form of 0/1 weak supervision that is similar to our setting. However, they are circumscribed in two image domains which are opposite to each other with respect to a single attribute. Our model differs from theirs as we generalize to the case of multi-attribute image data. Specifically, we employ the strategy of iterative training to overcome the difficulty of training on unbalanced multi-attribute image datasets.
3 DNA-GAN APPROACH
In this section, we formally outline our method. A set X of multi-labeled images and a set of labels Y are considered in our setting. Let {(X1, Y1), . . . , (Xm, Ym)} denote the whole training dataset, where Xi  X is the i-th image with its label Yi  Y . The small letter m denotes the number
2

Under review as a conference paper at ICLR 2018

of samples in set X and n denotes the number of attributes. The label Yi = (y1i , . . . , yni ) is a n-dimensional vector where each element represents whether Xi has certain attribute or not. For
example, in the case of labels with three candidates [Bangs, Eyeglasses, Smiling], the facial image Xi whose label is Yi = (1, 0, 1) should depict a smiling face with bangs and no eyeglasses.

3.1 MODEL

As shown in Figure 1, DNA-GAN is mainly composed of three parts: an encoder (Enc), a decoder (Dec) and a discriminator (D). The encoder maps the real-world images A and B into two latent disentangled representations

Enc(A) = [a1, . . . , ai, . . . , an, za], Enc(B) = [b1, . . . , bi, . . . , bn, zb]

(1)

where [a1, . . . , ai, . . . , an] is called the attribute-relevant part, and za is called the attribute-irrelevant part. ai is supposed to be a DNA piece that controls yi, the i-th attribute in the label, and za is for keeping other silent factors which do not appear in the attribute list as well as image identity
information. The same thing applies for Enc(B).

Encoder

Discriminator loss

... ... ... ...
... ... ... ... ... ... ... ...

Decoder

Reconstruction loss

annihilating

Figure 1: DNA-GAN architecture.

We focus on one attribute each time in our framework. Let's say we are at i-th attribute. A and B are required to have different labels, i.e. (y1A, . . . , 1iA, . . . , ynA) and (y1B, . . . , 0iB, . . . , ynB), respectively. In our convention, A is always for the dominant pattern, while B is for the recessive
pattern. We copy Enc(A) directly as the latent representation of A1, and annihilate bi in the copy of Enc(B) as the latent representation of B1. The annihilating operation means replacing all elements with zeros, and plays a key role in disentangling the attribute, which we will discuss in detail in Sec-
tion 3.3. By swapping ai and 0i, we obtain two new latent representations [a1, . . . , 0i, . . . , an, za] and [b1, . . . , ai, . . . , bn, zb] that are supposed to be decoded into A2 and B2, respectively. Though a decoder Dec, we can get four newly generated images A1, B1, A2 and B2.

Dec([a1, . . . , ai, . . . , an, za]) = A1, Dec([b1, . . . , 0i, . . . , bn, zb]) = B1 Dec([a1, . . . , 0i, . . . , an, za]) = A2, Dec([b1, . . . , ai, . . . , bn, zb]) = B2

(2)

Out of these four children, A1 and B1 are reconstructions of A and B, while A2 and B2 are novel crossbreeds. The reconstruction losses between A and A1, B and B1 ensure the quality of reconstructed samples. Besides, using an adversarial discriminator D that helps make generated samples
A2 indistinguishable from B, and B2 indistinguishable from A, we can enforce attribute-related information to be encoded in ai.

3

Under review as a conference paper at ICLR 2018

3.2 LOSS FUNCTIONS

Given two images A and B and their labels YA = (y1A, . . . , 1iA, . . . , ynA) and YB = (y1B, . . . , 0iB, . . . , ynB) which are different at the i-th position, the data flow can be summarized by (1) and (2). We force the i-th latent encoding of B to be zero in order to prevent from trivial
solutions as we will discuss in Section 3.3.

The encoder and decoder receive two types of losses: (1) the reconstruction loss,

Lreconstruct = A - A1 1 + B - B1 1

(3)

which measures the reconstruction quality after a sequence of encoding and decoding; (2) the standard GAN loss,

LGAN = -E[log(D(A2|yiA = 1))] - E[log(D(B2|yiB = 0))]

(4)

which measures how realistic the generated images are. The discriminator takes the generated image and the i-th element of its label as inputs, and outputs a number which indicates how realistic the input image is. The larger the number is, the more realistic the image is. Omitting the coefficient, the loss function for the encoder and decoder is

LG = Lreconstruct + LGAN .

(5)

The discriminator D receives the standard GAN discriminator loss LD1 = -E[log(D(A|yiA = 1))] - E[log(1 - D(B2|yiA = 1))] LD0 = -E[log(D(B|yiB = 0))] - E[log(1 - D(A2|yiB = 0))] LD = LD1 + LD0
where LD1 drives D to tell A from B2, and LD0 drives D to tell B from A2.

(6) (7) (8)

3.3 ANNIHILATING OPERATION PREVENTS FROM TRIVIAL SOLUTIONS

Through experiments, we observe that there exist trivial solutions to our model without the annihilating operation. We just take the single-attribute case as an example. Suppose that Enc(A) = [a, za] and Enc(B) = [b, zb], we can get four children without annihilating operation
A1 = Dec([a, za]), B1 = Dec([b, zb]), A2 = Dec([b, za]), B2 = Dec([a, zb]) (9)
The reconstruction loss makes it invertible between the latent encoding space and image space. The adversarial discriminator D is supposed to disentangle the attribute from other information by telling whether A2 looks as real as B and B2 looks as real as A or not. As we know that the generative adversarial networks give the best solution when achieving the Nash equilibrium. But without the annihilating operation, information of the whole image could be encoded into the attribute-relevant part, which means

Enc(A) = [a, 0], Enc(B) = [b, 0]

(10)

Therefore, we obtain the following four children

A1 = Dec([a, 0]), B1 = Dec([b, 0]), A2 = Dec([b, 0]), B2 = Dec([a, 0]) (11)
In this situation, the discriminator D cannot discriminate A2 from B, since they share the same latent encodings. By reconstruction loss, A2 and B are exactly the same image, which is against our expectation that A2 should depict the person from A with the attribute borrowed from B. The same thing happens to B2 and A as well.

To prevent from learning trivial solutions, we adopt the annihilating operation by replacing the recessive pattern b with a zero tensor of the same size1. If information of the whole image were
encoded into the attribute-relevant part, the four children in this case are

A1 = Dec([a, 0]), B1 = Dec([0, 0]), A2 = Dec([0, 0]), B2 = Dec([a, 0]) (12)
The encodings of B1 and A2 contain no information at all, thus neither the person in B1 nor A2 who is supposed to be the same as in B can be reconstructed by Dec. This forces the attribute-irrelevant part to encode some information of images.

1Use tf.zeros like() in TensorFlow implementation.

4

Under review as a conference paper at ICLR 2018

3.4 ITERATIVE TRAINING

To reduce the difficulty of disentangling multiple attributes, we take the strategy of iterative training:
we update our model using a pair of images with opposite labels at a certain position each time. Suppose that we are at the i-th position, the label of image A is (y1A, . . . , 1iA, . . . , ynA), while the label of image B is (y1B, . . . , 0Bi , . . . , ynB). During each iteration, as i goes through from 1 to n repeatedly, our model fed with such a pair of images can disentangle multiple attributes one-by-one.

Compared with training with random pairs of images, iterative training is proved to be more effective. Random pairs of images means randomly selecting pairs of images each time without label constraints. A pair of images with different labels is called a useful pair.

We theoretically show that our iterative training is much more efficient than random image pairs

especially when the dataset is unbalanced. All proofs can be found in the Appendix.

Theorem 1. Let X = {(X1, Y1), . . . , (Xm, Ym)} denote the whole multi-attribute image dataset,

where Xi is a multi-attribute image and its label Yi = (y1i , . . . , yni ) is an n-dimensional vector.

There are totally 2n kinds of labels, denoted by L = {l1, . . . , l2n }. The number of images with

label li is mi, and

2n i=1

mi

=

m.

To

select

all

useful

pairs

at

least

once,

the

expected

numbers

of

iterations needed for randomly selecting pairs and for iterative training are denoted by E1 and E2

respectively. Then,

E1 = m2

11

1+ +иии+ 2 m2 -

2n i=1

m2i

(13)

1

E2  2n и max

mimj

s=1,...,n

iIs ,j Js

1+ +иии+ 2 m2 -

1

(m2n-1
k1 =1

ik1

+ mjk1 )2

(14)

where Is represents the indices of labels where the s-th element is 1, and Js represents the indices of labels where the s-th element is 0.

Definition 1. (Balancedness) Define the balancedness of a dataset X described above with respect

to the s-th attribute as follows:

s =

iIs mi jJs mj

(15)

where Is represents the indices of labels where the s-th element is 1, and Js represents the indices

of labels where the s-th element is 0.

Theorem 2. We have E2  E1, when

n  min (s + 1)2 . s 2s

(16)

Specifically, E2  E1 holds true for all n  2.

The property of the function ( + 1)2/(2) suits well with the definition of balancedness, because it attains the same value for  and 1/, which is invariant to different labeling methods. Its value gets larger as the dataset becomes more unbalanced. The minimum is obtained at  = 1, which is the case of a balanced dataset.

Theorem 2 demonstrates that the iterative training mechanism is always more efficient than random
pairs of images when the number of attributes met the criterion (16). As the dataset becomes more unbalanced, (s + 1)2/(2s) goes larger, which means (16) can be more easily satisfied. More importantly, iterative training helps stabilize the training process on unbalanced datasets. For example,
given a two-attribute dataset, the number of data of each kind is as follows:

Table 1: The example of an unbalanced two-attribute dataset.

Label

(0, 0) (0, 1) (1, 0) (1, 1)

Number of data 1 1 m m

If m 1 is a very large number, then it is highly likely that we will select a pair of images whose labels are (1, 0) and (1, 1) each time by randomly selecting pairs. We ignore the pair of images

5

Under review as a conference paper at ICLR 2018
A B A2 B2 A1 B1
Figure 2: Manipulating illumination factors on the Multi-PIE dataset. From left to right, the six images in a row are: original images A with light illumination and B with the dark illumination, newly generated images A2 and B2 by swapping the illumination-relevant piece in disentangled representations, and reconstructed images A1 and B1.
whose labels are (1, 0) and (1, 0) or (1, 1) and (1, 1), though these two cases have equal probabilities of being chosen. Because they are not useful pairs, thus do not participated in training. In this case, most of the time the model is trained with respect to the second attribute, which will cause the final learnt model less effective to the first attribute. However, iterative training can prevent this from happening, since we update our model evenly with respect to two attributes.
4 EXPERIMENTS
In this section, we perform different kinds of experiments on two real-world datasets to validate the effectiveness of our methods. We use the RMSProp (Sutskever et al., 2013) optimization method initialized by a learning rate of 5e-5 and momentum 0. All neural networks are equipped with Batch Normalization (Ioffe & Szegedy, 2015) after convolutions or deconvolutions. We used Leaky Relu (Maas et al., 2013) as the activation function in the encoder. Besides, we adopt strategies mentioned in Wasserstein GAN (Arjovsky et al., 2017) for stable training. More details will be available online. We divide all images into training images and test images according to the ratio of 9:1. All of the following results are from test images without cherry-picking. 4.1 MULTI-PIE DATABASE The Multi-PIE (Gross et al., 2010) face database contains over 750,000 images of 337 subjects captured under 15 view points and 19 illumination conditions. We collecte all front faces images of different illuminations and align them based on 5-point landmarks on eyes, nose and mouth. All aligned images are resized into 128 О 128 as inputs in our experiments. We label the light illumination face images by 1 and the dark illumination face images by 0. As shown in Figure 2,
6

Under review as a conference paper at ICLR 2018
the illumination on one face is successfully transferred into the other face without modifying any other information in the images. This demonstrates that DNA-GAN can effectively disentangle the illumination factor from other factors in the latent space.
4.2 CELEBA DATASET
CelebA (Liu et al., 2015) is a dataset composed of 202599 face images and 40 attribute binary vectors and 5 landmark locations. We use the aligned and cropped version and scaled all images down to 64 О 64. To better demonstrate the advantage of our method, we choose TD-GAN (Wang et al., 2017) and IcGAN (Perarnau et al., 2016) for comparisons. As we mentioned before, TD-GAN requires the explicit id information in the label, thus cannot be applied to the CelebA dataset directly. To overcome this limitation, we use some channels to encode the id information in its latent representations. In our experiments, the id information is preserved when swapping the attribute information in the latent encodings. We also compared the experimental results of IcGAN with ours in the celebA dataset. The following results are obtained using the the official code and pre-trained celebA model provided by the author2.

(a) TD-GAN

(b) IcGAN

Figure 3: The experimental results of TD-GAN and IcGAN on CelebA dataset. Three rows indicates the swapping attributes of Bangs, Eyeglasses and Smiling. For each model, the four images in a row are: two original images, and two newly generated images by swapping the attributes. The third image is generated by adding the attribute to the first one, and the fourth image is generated by removing the attribute from the second one.

As displayed in Figure 3a, modified TD-GAN encounters the problem of trivial solutions. Without id information explicitly contained in the label, TD-GAN encodes the information of the whole image into the attribute-related part in the latent representations. As a result, two faces are swapped directly. Whereas in Figure 3b, the quality of images generated by IcGAN are very bad, which is probably due to the multi-stage training process of IcGAN. Since the overall effect of the model relies much on the each stage.
DNA-GAN is able to disentangle multiple attributes in the latent representations as shown in Figure 4. Since different attributes are encoded in different DNA pieces in our latent representations, we are able to interpolate the attribute subspaces by linear combination of disentangled encodings. Figure 4a, 4b and 4c present disentangled attribute subspaces spanned by any two attributes of Bangs, Eyeglasses and Smiling. They demonstrate that our model is effective in learning disentangled representations. Figure 4d shows the hairstyle transfer process among different Bangs styles. It is worth mentioning that the top-left image in Figure 4d is outside the CelebA dataset, which further validate the generalization potential of our model on unseen data. Please refer to Figure 5 in the Appendix for more results.
2https://github.com/Guim3/IcGAN

7

Under review as a conference paper at ICLR 2018

(a) Bangs and Eyeglasses

(b) Bangs and Smiling

(c) Eyeglasses and Smiling

(d) Different Bangs

Figure 4: The interpolation results of DNA-GAN. Figure 4a, 4b and 4c display the disentangled attribute subspaces spanned by any two attributes of Bangs, Eyeglasses and Smiling. Figure 4d shows the attribute subspaces spanned by several Bangs feature vectors. Besides, the top-left image in Figure 4d is outside the CelebA dataset.

5 CONCLUSION
In this paper, we propose a supervised algorithm called DNA-GAN that can learn disentangled representations from multi-attribute images. The latent representations of images are DNA-like, consisting of attribute-relevant and attribute-irrelevant parts. By the annihilating operation and attribute hybridization, we are able to create new latent representations which could be decoded into novel images with designed attributes. The iterative training strategy effectively overcomes the difficulty of training on unbalanced datasets and helps disentangle multiple attributes in the latent space.
The experimental results not only demonstrate that DNA-GAN is effective in learning disentangled representations and image editing, but also point out its potential in interpretable deep learning, image understanding and transfer learning.
There also exist some limitations of our model. Without strong guidance on the attribute-irrelevant parts, some background information is encoded into the attribute-relevant part. As we can see in Figure 4, the background color gets changed when swapping attributes. Besides, our model may fail when several attributes are highly correlated with each other. For example, Male and Mustache are statistically dependent, which are hard to disentangle in the latent representations. These are left as our future work.

8

Under review as a conference paper at ICLR 2018
REFERENCES
Mart┤in Arjovsky, Soumith Chintala, and Le┤on Bottou. Wasserstein GAN. CoRR, abs/1701.07875, 2017.
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798Г1828, 2013.
Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-level variational autoencoder: Learning disentangled representations from grouped observations. CoRR, abs/1705.08841, 2017.
Xi Chen, Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems 29, pp. 2172Г2180, 2016.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pp. 2672Г2680, 2014.
Ralph Gross, Iain A. Matthews, Jeffrey F. Cohn, Takeo Kanade, and Simon Baker. Multi-pie. Image Vision Comput., 28(5):807Г813, 2010.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, pp. 448Г456, 2015.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. CoRR, abs/1611.07004, 2016.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013.
Tejas D. Kulkarni, William F. Whitney, Pushmeet Kohli, and Joshua B. Tenenbaum. Deep convolutional inverse graphics network. In Advances in Neural Information Processing Systems 28, pp. 2539Г2547, 2015.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. CoRR, abs/1703.00848, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, pp. 3730Г3738, 2015.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural network acoustic models. In Proc. ICML, volume 30, 2013.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.
Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559Г572, 1901.
Guim Perarnau, Joost van de Weijer, Bogdan Raducanu, and Jose M. A┤ lvarez. Invertible conditional gans for image editing. CoRR, abs/1611.06355, 2016.
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embedding. science, 290(5500):2323Г2326, 2000.
N Siddharth, Brooks Paige, Alban Desmaison, Jan-Willem van de Meent, Frank Wood, Noah D Goodman, Pushmeet Kohli, and Philip HS Torr. Learning disentangled representations in deep generative models. 2016.
9

Under review as a conference paper at ICLR 2018
Paul Smolensky. Information processing in dynamical systems: Foundations of harmony theory. Technical report, COLORADO UNIV AT BOULDER DEPT OF COMPUTER SCIENCE, 1986.
Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, pp. 1139Г1147, 2013.
Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. CoRR, abs/1611.02200, 2016.
Chaoyue Wang, Chaohui Wang, Chang Xu, and Dacheng Tao. Tag disentangled generative adversarial network for object image re-rendering. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, pp. 2901Г2907, 2017.
Kai Yu, Tong Zhang, and Yihong Gong. Nonlinear learning using local coordinate coding. In Advances in Neural Information Processing Systems 22, pp. 2223Г2231, 2009.
Shuchang Zhou, Taihong Xiao, Yi Yang, Dieqiao Feng, Qinyao He, and Weiran He. Genegan: Learning object transfiguration and attribute subspace from unpaired data. CoRR, abs/1705.04932, 2017.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. CoRR, abs/1703.10593, 2017.
10

Under review as a conference paper at ICLR 2018

APPENDIX

To prove Theorem 1, we need the following lemma.
Lemma 1. A set S = {s1, . . . , sm} has m different elements, from which elements are being selected equally likely with replacement. The expected number of trials needed to collect a subset R = {s1, . . . , sn} of n(1  n  m) elements is

mи 1 + 1 +иии+ 1 .

12

n

Proof. Let T be the time to collect all n elements in the subset R, and let ti be the time to collect the i-th new elements after i - 1 elements in R have been collected. Observe that the probability
of collecting a new element is pi = (n - (i - 1))/m. Therefore, ti is a geometrically distributed random variable with expectation 1/pi. By the linearity of expectations, we have:

E(T ) = E(t1) + E(t2) + и и и + E(tn)

11

1

= + +иии+

p1 p2

pn

=

m n

+

m n-

1

+

иии

+

m 1

= mи 1 + 1 +иии+ 1 .

12

n

Proof. (of Theorem 1)
We first consider the case of randomly selecting pairs. All possible image pairs are actually in the product space X О X , whose cardinality is m2. If we take the order of two images in a pair into consideration, the number of possible pairs is m2. Recall that the useful pair denotes a pair of image of different labels. Therefore, the number of all useful pairs is i=j mimj. By Lemma 1, the expected number of iterations for randomly selecting pairs to select all useful pairs at least once is

E1 = m2

1+ 1 +иии+ 2

1 i=j mimj

= m2 1 + 1 + и и и + 2

1

2n i=1

(mi

j=i mj )

= m2 1 + 1 + и и и + 2

1

2n i=1

mi

(m

-

mi

)

= m2

11

1+ +иии+ 2 m2 -

2n i=1

mi2

.

(17)

Now we consider the case of iterative training. We always select a pair of images of different labels each time. Suppose we are selecting images with opposite labels at the s-th position. Let Is denote the indices of all labels with the s-th element 1, and Js denote the indices of all labels with the s-th element 0, where |Is| = |Js| = 2n-1. Then we consider the subproblem by neglecting the first position in data labels, the number of all possible pairs is 2 iIs,jJs mimj (regarding of order),
11

Under review as a conference paper at ICLR 2018

and the number of useful pairs is

(mik1 + mjk1 )(mik2 + mjk2 )
k1 =k2

2n-1

= (mik1 + mjk1 )(mik2 + mjk2 )
k1=1 k2=k1

2n-1
= (mik1 + mjk1 )(m - mik1 - mjk1 )
k1 =1

2n-1

= m2 -

(mik1 + mjk1 )2.

(18)

k1 =1

Therefore, the expectation to select all useful pairs at least once regardless of the s-th element in the

label is

1

E\s = 2

mimj

iIs ,j Js

1+ +иии+ 2 m2 -

1

(m2n-1
k1 =1

ik1

+ mjk1 )2

(19)

Since we rotate the subscript s from 1 to n, the expected number of iterations for iterative training

to select all useful pairs at least once is

E2



n

и

max
s=1,...,n

E\s

1

= 2n и max

mimj

s=1,...,n

iIs ,j Js

1+ +иии+ 2 m2 -

1

(m2n-1
k1 =1

ik1

+ mjk1 )2

.

(20)

Proof. (of Theorem 2) We firstly show that

2n-1

2n-1

2n

(mik1 + mjk1 )2 

(m2ik1 + mj2k1 ) =

mi2

k1 =1

k1 =1

i=1

According to the result of Theorem 1 and the Definition 1 of balancedness, we have

1

E2 = 2n и max

mimj

s

iIs ,j Js

1+ +иии+ 2 m2 -

1

(m2n-1
k1 =1

ik1

+ mjk1 )2

11

 2n и max
s

mimj

iIs ,j Js

1+ +иии+ 2 m2 -

2n i=1

mi2



11

= 2n и max
s

mi
iIs

 mj
jJs

1+ +иии+ 2 m2 -

2n i=1

m2i

= 2n и max sm m s s + 1 s + 1

11

1+ +иии+ 2 m2 -

2n i=1

m2i

=

max
s

2ns (s + 1)2

и m2

11

1+ +иии+ 2 m2 -

2n i=1

mi2

 E1.

Specifically, if n  2,

2ns (s + 1)2



4s (s + 1)2



1.

The inequality holds true forever.

(21)
(22) (23)

12

Under review as a conference paper at ICLR 2018

(a) Bangs and Eyeglasses

(b) Bangs and Eyeglasses

(c) Bangs and Smiling

(d) Bangs and Smiling

(e) Male and Smiling

(f) Male and Smiling

(g) Male and Smiling

(h) Male and Wearing Hat 13

Under review as a conference paper at ICLR 2018

(i) Male and Wearing Hat

(j) Male and Wearing Hat

(k) Smiling and Wearing Hat

(l) Smiling and Wearing Hat

(m) Smiling and Wearing Hat

(n) Smiling and Wearing Hat

(o) Wearing Hat and Mustache

(p) Wearing Hat and Mustache

Figure 5: More experimental results of DNA-GAN.

14

