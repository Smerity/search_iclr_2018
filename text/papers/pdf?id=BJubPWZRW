Under review as a conference paper at ICLR 2018
CROSS-VIEW TRAINING FOR SEMI-SUPERVISED LEARNING
Anonymous authors Paper under double-blind review
ABSTRACT
We present Cross-View Training (CVT), a simple but effective method for deep semi-supervised learning. On labeled examples, the model is trained with standard cross-entropy loss. On an unlabeled example, the model first performs inference (acting as a "teacher") to produce soft targets. The model then learns from these soft targets (acting as a "student"). We deviate from prior work by adding multiple auxiliary student softmax layers to the model. The input to each auxilliary student layer is a sub-network of the full model that has a restricted view of the input (e.g., only seeing one region of an image). The students can learn from the teacher (the full model) because the teacher sees more of each example. Concurrently, the students improve the quality of the representations used by the teacher as they learn to make predictions with limited data. We propose variants of our method for CNN image classifiers and BiLSTM sequence taggers. When combined with Virtual Adversarial Training, CVT improves upon the current state-of-the-art on semi-supervised CIFAR-10 and semi-supervised SVHN. We also apply CVT to train semi-supervised sequence taggers on four natural language processing tasks using hundreds of millions of sentences of unlabeled data. The resulting models improve upon or are competitive with the current state-of-the-art on every task.
1 INTRODUCTION
Deep learning classifiers work best when trained on large amounts of labeled data. However, acquiring labels can be costly, motivating the need for effective semi-supervised learning techniques that leverage unlabeled examples during training. Many semi-supervised learning algorithms rely on some form of self-labeling. In these approaches, the model acts as both a "teacher" that makes predictions about unlabeled examples and a "student" that is trained on the predictions. As the teacher and the student have the same parameters, these methods require an additional mechanism for the student to benefit from the teacher's outputs.
One approach that has enjoyed recent success is adding noise to the student's input (Bachman et al., 2014; Sajjadi et al., 2016). The loss between the teacher and the student becomes a consistency cost that penalizes the difference between the model's predictions with and without noise added to the example. This loss trains the model to give consistent predictions to nearby data points, encouraging smoothness in the model's output distribution with respect to the input. In order for the student to learn effectively from the teacher, there needs to be a sufficient difference between the two. However, simply increasing the amount of noise can result in unrealistic data points sent to the student. Instead, recent work proposes adversarially choosing the input perturbation (Miyato et al., 2016), or improving and reducing the variance of the teacher by tracking an exponential moving average of its predictions (Laine & Aila, 2017) or its weights (Tarvainen & Valpola, 2017).
We propose Cross-View-Training (CVT), a complementary approach to these methods. Instead of only training the full model as a student, CVT adds auxiliary softmax layers to the model and also trains them as students. The input to each student layer is a sub-network of the full model that sees a restricted view of the input example, an idea reminiscent of co-training (Blum & Mitchell, 1998). The full model is still used as the teacher. Unlike when using a large amount of input noise, CVT does not unrealistically alter examples during training. However, the student layers can still learn from the teacher because the teacher has a better, unrestricted view of the input. Meanwhile, the student layers improve the model's representations (and therefore the teacher) as they learn to make
1

Under review as a conference paper at ICLR 2018
accurate predictions with a limited view of the input. Our method can be easily combined with adding noise to the students (which improves results), but works well even when no noise is added.
We propose variants of our method for Convolutional Neural Network (CNN) image classifiers and Bidirectional Long Short-Term Memory (BiLSTM) sequence taggers. For CNNs, each auxiliary softmax layer sees a region of the input image. For sequence taggers, the auxiliary layers see the input sequence with some context removed. For example, one auxiliary layer is trained to make predictions without seeing any tokens to the right of the current one.
We first evaluate Cross-View Training on semi-supervised CIFAR-10 and semi-supervised SVHN. When combined with Virtual Adversarial Training (Miyato et al., 2017), CVT improves upon the current state-of-the-art on both datasets. We also train semi-supervised sequence taggers on four tasks from natural language processing: Combinatory Categorical Grammar Supertagging, Named Entity Recognition, Text Chunking, and Part-of-Speech Tagging. We use the 1 billion word language modeling benchmark (Chelba et al., 2014) as a source of unlabeled data. CVT works substantially better than purely supervised training, resulting in models that improve upon or are competitive with the current state-of-the-art on every task.
2 RELATED WORK
Semi-supervised learning in general has been widely studied (Chapelle et al., 2006). Early approaches to deep semi-supervised learning pre-train neural models on unlabeled data, which has been successful for applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and natural language processing (Dai & Le, 2015; Ramachandran et al., 2017). More recent work incorporates generative models based on autoencoders (Kingma et al., 2014; Rasmus et al., 2015) or Generative Adversarial Networks (Springenberg, 2015; Salimans et al., 2016) into the training.
Self-Training. One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965; Fralick, 1967). Initially, a classifier is trained on labeled data only. In each subsequent round of training, the classifier, acting as a "teacher," labels some of the unlabeled data and adds it to the training set. Then, acting as a "student," it is retrained on the new training set. The new examples added each round act as noisy "pseudo labels" (Lee, 2013) that the model can learn from. Many recent approaches train the student with soft targets from the teacher's output distribution rather than a hard label, making the procedure more akin to knowledge distillation (Hinton et al., 2015).
Consistency Training and Distributional Smoothing. Recent works add noise to the student's input (Bachman et al., 2014; Sajjadi et al., 2016). This trains the model to give consistent predictions to nearby data points, encouraging distributional smoothness in the model. Inspired by the success of adversarial training (Goodfellow et al., 2015), Miyato et al. (2016) extend this idea by adversarially selecting the perturbation to the input. Other approaches focus on improving the targets provided by the teacher (Laine & Aila, 2017; Tarvainen & Valpola, 2017). Our method is complimentary to these previous approaches, and can be combined with them effectively.
Co-Training. Co-Training (Blum & Mitchell, 1998; Nigam & Ghani, 2000) trains two models with disjoint views of the input. On unlabeled data, each one acts as a "teacher" for the other model. In contrast, our approach trains a single unified model where auxiliary prediction layers see different, but not necessarily independent views of the input.
Auxiliary Prediction Layers. Another way of leveraging unlabeled data is through the addition of auxiliary "self-supervised" losses. These approaches train auxiliary prediction layers on tasks where performance can be measured without human-provided labels. Previous work has jointly trained image classifiers with tasks like relative position and colorization (Doersch & Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017). Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input.
Data Augmentation. Data augmentation, such as random translations or crops of input images, bears some similarity to our method in that it also exposes the model to different views of input ex-
2

Under review as a conference paper at ICLR 2018

Learning on a Labeled Example

Model

y loss

Learning on an Unlabeled Example

Model acting as the teacher

+noise (optional)

Model acting as the student

y^
y^ y~ y~view1 y~view2 y~view3

loss

Receptive Fields of Auxilliary Softmax Layers:
view 1
view 2
view 3

Figure 1: An overview of Cross-View Training. The model is trained with standard supervised
learning on labeled examples. On unlabeled examples, auxiliary softmax layers with different views of the input are trained to agree with the primary softmax1layer. Although the model takes on
different roles (i.e., as the teacher or the student), only one set of parameters is trained.

amples. Data augmentation has become a common practice for both supervised and semi-supervised training of image classifiers (Simard et al., 2003; Krizhevsky et al., 2012).

3 CROSS-VIEW TRAINING

We first provide a general description of Cross-View Training. We then present specific constructions for auxiliary prediction layers that work well for image classification and sequence tagging.

3.1 METHOD

We use Dl = {(x1, y1), (x2, y2), ..., (xN , yN )} to represent a labeled dataset and Dul = {x1, x2, ..., xM } to represent an unlabeled dataset. We use p(y|xi) to denote the output distribu-
tion over classes produced by a model with parameters  on input xi. Our approach uses a standard
cross-entropy loss over the labeled data:

Lsup()

=

1 |Dl|

CE(yi, p(y|xi))
xi ,yi Dl

On unlabeled data, a popular approach is to add a consistency cost encouraging distributional
smoothness in the model. First, the model produces soft targets for the current example: y^i = p(y|xi). The model is then trained to minimize the consistency cost

Lconsistency()

=

1 |Dul|

E
xi Dul

[D(y^i, p(y|xi

+ ))]

where D is a distance function (we use KL divergence) and  is a perturbation to the input that can
be chosen randomly or adversarially. As is common in prior work, we hold the teacher's prediction y^i fixed during training (i.e., we don't back-propagate through it) so the student learns to imitate the teacher, but not vice versa.

Cross-View Training adds k additional prediction layers p1, ..., pk to the model. Each layer pj takes as input an intermediate representation hj(xi) produced by the model. It outputs a distribution over labels with a softmax layer (a linear transformation followed by a softmax activation function) applied to this representation: pj (y|xi) = SML(hj(xi)) = softmax(Wjhj(xi) + bj). At test time, only the main prediction layer p is used. Each hj is chosen such that it only uses a part of each input xi; the particular choice can depend on the task and model architecture. We propose variants for CNN image classifiers and BiLSTM sequence taggers in sections 3.2 and 3.3. We add the

3

Under review as a conference paper at ICLR 2018

   Softmax Layers for Image Classification

xi CNN

Mean Pooling

Predict

y^i

Softmax Layers for Sequence Tagging

17.1, 2.0, 1.0 17.1, 2.0, 1.0 17.1, 4.0, 1.0 17.1, 4.0, 1.0
Figure 2: Softmax layers for image classifiers and sequence taggers. Solid red arrows represent primary softmax layers and dashed blue arrows represent auxiliary ones. To simplify the diagram, the CNN only produces four feature vectors, the BiLSTM only has a single layer, and we only show auxiliary layers for the BiLSTM's second time step. For CNNs, auxiliary layers take a single feature vector as input. For BiLSTMs, auxiliary layers are attached to the forward and backward LSTMs.

1

distances between the output distributions of the teacher and auxiliary students to the consistency loss, resulting in a cross-view consistency (CVC) loss:



LCVC()

=

1 |Dul|

xi Dul

E

D(y^i, p(y|xi

+

))

+

1 k

k
D(y^i, pj(y|xi
j=1

+

))

We combine the supervised and CVC losses into the total loss, L = Lsup + 2LCVC, and minimize it with stochastic gradient descent. At each step, Lsup is computed over a minibatch of labeled examples and LCVC is computed over a minibatch of unlabeled examples. 1 and 2 are hyperparameters controlling the strength of the auxiliary prediction layers and the strength of the unsupervised loss. For all experiments we set 1 = 2 = 1 unless indicated otherwise. See Figure 1 for an illustration of the training procedure.
Although adding noise or an adversarial perturbation to the input generally improves results, LCVC can be trained without this enhancment (i.e., setting  = 0). In this case, the first term inside the expectation disappears (the student will exactly match the teacher, so the distance is zero). In contrast, Lconsistency requires a nonzero  to make the student and teacher output different distributions.
In most neural networks, a single softmax layer is computationally cheap compared to the portion of the model building up representations (such as a CNN or RNN). Therefore our method contributes little overhead to training time over consistency training.

3.2 IMAGE RECOGNITION MODELS
Our image recognition models are based on Convolutional Neural Networks, which produce a set of features H(xi)  Rn×n×d from an image xi. The first two dimensions of H index into the spatial coordinates of feature vectors and d is the size of the feature vectors. For shallower CNNs, a particular feature vector corresponds to a region of the input image. For example, H0,0 would be a d-dimensional vector of features extracted from the upper left corner. For deeper CNNs, a particular feature vector would be extracted from the whole image, but still only use a "region" of the representations from an earlier layer. The CNNs in our experiment are all in the first category.
The primary prediction layer for our CNNs take as input the mean of H over the first two dimensions, which results in a d-dimensional vector that is fed into a softmax layer: p(y|xi) = SML(global average pool(H)).
We add n2 auxiliary softmax layers to the top of the CNN. The jth layer takes a single feature vector as input, as shown in the left of Figure 2: pj (y|xi) = SML(H j/n ,j mod n). We also experimented with adding auxiliary softmaxes to the outputs of earlier layers in the CNN, but found this did not improve performance.

4

Under review as a conference paper at ICLR 2018

3.3 SEQUENCE TAGGING MODELS

In sequence tagging, each example (xi, yi) consists of T tokens x1i , ..., xTi and T corresponding labels yi1, ..., yiT . We assume an L-layer bidirectional RNN sequence tagging model, which has become standard for many sequence tagging tasks (Graves & Schmidhuber, 2005; Graves et al.,

2013). Each layer runs an RNN such as an LSTM (Hochreiter & Schmidhuber, 1997) in the forward direction (taking xti as input at each step t) and the backward direction (taking xiT -t+1 as input at each step) and concatenates the results. A softmax layer on top of the outputs of the last BiRNN layer, hL(xi) = [hL1 (xi), ..., hTL(xi)], makes the predictions: p(yt|xi) = SML(htL(xi)).
The auxiliary softmax layers take -h 1(xi) and h-1(xi), the outputs of the forward and backward
RNNs in the first BiRNN layer, as inputs. We add the following four softmax layers to the model

(see the right of Figure 2):

pfwd(yt|xi) = SML(-h 1t (xi)) pfuture(yt|xi) = SML(-h t1-1(xi))

pbwd(yt|xi) = SML(h-t1(xi)) ppast(yt|xi) = SML(h-t1+1(xi))

The "forward" and "backward" prediction layers use the RNN's current output to predict the current token. The "future" and "past" layers use the RNN's previous output (or, equivalently, they predict the label for the next token). The forward layer makes each prediction without seeing the right context of the current token. The future layer makes each prediction without the right context or the current token itself. Therefore it works like a neural language model that, instead of predicting which token comes next in the sequence, predicts which class of token comes next in the sequence.

4 EXPERIMENTS
We evaluate our method on two semi-supervised image recognition benchmarks and four sequence labeling tasks from Natural Language Processing (NLP).
4.1 IMAGE RECOGNITION
Data. We experiment with semi-supervised image classifiers on two datasets: CIFAR-10 (Krizhevsky & Hinton, 2009) and Street View House Numbers (SVHN) (Netzer et al., 2011). We follow previous work and make the datasets semi-supervised by only using the provided labels for a subset of the examples in the training set; the rest are treated as unlabeled examples.
Model. We use the convolutional neural network from Miyato et al. (2017), adapting their TensorFlow implementation1. Their model, based on Springenberg et al. (2014), contains 9 convolutional layers and 2 max pooling layers. See Appendix D of Miyato et al.'s paper for details.
We add 36 auxiliary softmax layers to the 6 × 6 collection of feature vectors produced by the CNN. Each auxilliary layer sees a patch of the image ranging in size from 21 × 21 pixels (the corner) to 29 × 29 pixels (the center) of the 32 × 32 pixel images. We optimize L with each minibatch consisting of 32 labeled and 128 unlabeled examples.
Miyato et al. use Virtual Adversarial Training (VAT), minimizing Lconsistency with the input perturbation  chosen adversarially. We train our cross-view models (which instead use LCVC) both with and without this adversarial noise. We report results with and without using data augmentation (random translations for SVHN and random translations and horizontal flipping for CIFAR-10) in Table 1.
Results. CVT works well as semi-supervised learning method without any noise being added to the student. When adversarial noise is added, it improves significantly over VAT, resulting in state-ofthe-art results on both tasks. The improvement is less when data augmentation is applied, perhaps because random translations of the input expose the model to different "views" in a similar manner as with CVT. We believe the gains on SVHN are smaller than CIFAR-10 because the digits in SVHN occur in the center of the image, so the auxiliary softmaxes seeing the sides and corner do not learn as effectively. We also note that incorporating auxiliary softmax layers into the supervised loss Lsup
1https://github.com/takerum/vat_tf

5

Under review as a conference paper at ICLR 2018

Method

SVHN

SVHN+

1000 labels

CIFAR-10 CIFAR-10+ 4000 labels

GANa
Stochastic Transformationsb  modelc Temporal Ensemblec
Mean Teacherd Complement GANe
VATf

­
­ 5.43 ± 0.25 ­
­ 4.25 ± 0.03
4.28

8.11 ± 1.3
­ 4.82 ± 0.17 4.42 ± 0.16 3.95 ± 0.19 ­
3.86

­
­ 16.55 ± 0.29 ­
­ 14.41 ± 0.30
13.15

18.63 ± 2.32 11.29 ± 0.24 12.36 ± 0.31 12.16 ± 0.24 12.31 ± 0.28 ­
10.55

Supervised VAT* CVT, no noise CVT, adversarial noise

10.68 ± 0.51 4.11 ± 0.13 4.48 ± 0.09 3.79 ± 0.08

10.10 ± 0.48 3.83 ± 0.16 4.37 ± 0.12 3.70 ± 0.15

23.61 ± 0.60 13.29 ± 0.33 14.63 ± 0.20 12.01 ± 0.11

19.61 ± 0.56 10.90 ± 0.31 12.44 ± 0.27 10.11 ± 0.15

aSalimans et al. (2016) bSajjadi et al. (2016) cLaine & Aila (2017) dTarvainen & Valpola (2017) eDai et al. (2017) fMiyato et al. (2017)
*We found Miyato et al.'s implementation produces slightly different results than the ones they
report in their paper.

Table 1: Error rates on semi-supervised learning benchmarks. We report means and standard deviations from 5 runs. + after a dataset means data augmentation was applied.

Average A(cRtiavtaiotioonf CoVf TF/eVaAtuTr)e Vectors
1.35 1.24 1.29 1.31 1.23 1.31 1.13 0.88 0.86 0.86 0.87 1.11 1.16 0.86 0.82 0.82 0.83 1.09 1.15 0.87 0.82 0.82 0.83 1.07 1.02 0.87 0.88 0.88 0.85 0.98 1.28 1.22 1.31 1.32 1.25 1.30

CVTLAauyxeirlliAacrycuPrraecdieicstion
0.68 0.73 0.75 0.75 0.73 0.67 0.74 0.78 0.81 0.81 0.78 0.74 0.77 0.81 0.83 0.83 0.81 0.76 0.76 0.80 0.82 0.82 0.80 0.76 0.73 0.78 0.79 0.79 0.77 0.73 0.68 0.72 0.74 0.74 0.72 0.68

VATLAauyxeirlliAacrycuPrraecdieicstion
0.39 0.45 0.49 0.49 0.46 0.39 0.47 0.57 0.61 0.61 0.57 0.48 0.52 0.62 0.66 0.66 0.62 0.52 0.53 0.63 0.67 0.66 0.63 0.53 0.51 0.58 0.62 0.62 0.58 0.50 0.43 0.49 0.51 0.51 0.48 0.43

Figure 3: Left: Ratio between the average activation of feature vectors from final layer of the CVT CNNs divided by the average from the VAT CNNs. Each square in a grid represents a single feature vector. Brighter means the feature vectors from the CVT model are more activated. Center, Right: Accuracy of prediction layers taking a single feature vector as input. The CVT model makes more use of the outside of the image and produces better representations for those regions
does not improve results (see Appendix B). This indicates that the benefit of CVT comes from the improved self-training mechanism, not the additional losses regularizing the model.
Model Analysis. To understand why CVT produces better results, we compare the behavior of the VAT and CVT (with adversarial noise) models trained on CIFAR-10. First, we record the average value of each feature vector produced by the CNNs when they run over the test set. As shown in the left of Figure 3, the CVT model has higher activation strengths for the feature vectors corresponding to the edges of the image. We hypothesize that the VAT model fits to the data while primarily using the center of the image, where the most discriminative information is contained. This results in less effective feature vectors for the outside regions. In contrast, the model with CVT must learn meaningful representations for the edge regions in order to train the corresponding auxiliary softmax layers. As these feature vectors are more useful, their magnitude become larger so they contribute more to the final representation produced by the global average pool.
To compare to discriminatory power of the feature vectors, we freeze the weights of the CNNs and add auxiliary softmax layers that are trained from scratch. We then measure the accuracies of the added layers (see the center and right of Figure 3). Unsurprisingly, the VAT model, which only learns representations that will be useful after the average pool, has much lower accuracies from individual feature vectors. The difference is particularly striking in the sides and corners, where CVT accuracies are around 50% higher (they are about 25% higher in the center). This finding
6

Under review as a conference paper at ICLR 2018
further indicates that CVT is improving the model's representations, particularly for the outside parts of images.
4.2 SEQUENCE TAGGING
Data. Although the widely-used benchmarks in the previous section provide validation of our approach, they are small datasets that are artificially made to be semi-supervised. In this section, we show CVT is successful on well-studied tasks where semi-supervised learning is rarely applied. In particular, we train semi-supervised sequence tagging models on the following NLP tasks:
· Combinatory Category Grammar (CCG) Supertagging: Labeling words with CCG supertags: lexical categories that encode information about the predicate-argument structure of the sentence. CCG is widely used in syntactic and semantic parsing. We use data from CCGBank (Hockenmaier & Steedman, 2007) and report word-level accuracy.
· Text Chunking: Dividing a sentence into syntactically correlated parts (e.g., a noun phrase followed by a verb phrase). We use the CoNLLL-2000 shared task data (Tjong Kim Sang & Buchholz, 2000) and report the F1 score over predicted chunks.
· Named Entity Recognition (NER): Identifying and classifying named entities (organizations, places, etc.) in a sentence. We use the CoNLL-2003 dataset (Tjong Kim Sang & De Meulder, 2003) and report entity-level F1 score.
· Part-of-Speech (POS) Tagging: Labeling words with their syntactic categories (e.g., determiner, adjective, etc.). We use the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993) and report word-level accuracy.
We use the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) as a pool of unlabeled sentences for semi-supervised learning.
Model. We use a CNN-BiLSTM sequence tagging model (Chiu & Nichols, 2016; Ma & Hovy, 2016). The model first represents each word as the sum of a word embedding and the output of a character-level CNN. This sequence of word representations is then fed through two BiLSTM layers and a softmax layer to produce predictions. See Appendix A for details about the model.
We set 1 to 4 so each auxiliary softmax layer has the same weight as the main softmax layer. Although we experimented with constraining the word embeddings to unit length and adding random or adversarial perturbations to them during training, we found it did not improve performance. This is perhaps because, unlike with RGB values in an image, words are discrete, so adding noise to their representations is less meaningful. Instead, we add dropout to the student but not the teacher. Recent work (Rei, 2017; Liu et al., 2017) has shown that jointly training a neural language model with sequence taggers improves results. We report accuracies with and without this enhancement (training the language model on the unlabeled data) in Table 2.
Results. CVT significantly improves over the supervised baseline on all tasks, both with and without the auxiliary language modeling objective. We report a new state-of-the-art on CCG-supertagging and results competitive with the current state-of-the-art on the other tasks. Of the prior results listed in the Table 2, only TagLM from Peters et al. (2017) is semi-supervised. However, their approach relies on pre-training rather than self-training: their model incorporates representation produced by an enormous separately-trained language model with 8192 hidden units. Our models use 1024 hidden units in their largest LSTMs, so they are many times faster to run.
Although there has been a large body of work successfully applying consistency-cost-based learning to vision tasks, we find it does not provide the same gains for NLP. In fact, training a model with the consistency loss Lconsistency did not improve over the baseline. This result is perhaps due to the lack of benefit from adding noise when the input consists of discrete tokens as discussed earlier. CVT, on the other hand, works well as a semi-supervised learning method for NLP.
Importance of Auxiliary Prediction Layers. To determine which of the auxiliary prediction layers are most valuable, we do a brief ablation study by training models without the pfwd/pbwd or pfuture/ ppast auxiliary softmax layers. We find that both kinds of layers improve performance, but
7

Under review as a conference paper at ICLR 2018

Method

CCG

Chunk

NER

POS

C2W + LSTMa
LSTM-CNN-CRFb Tri-Trained LSTMc
Shortcut LSTMd TagLMe
LM-LSTM-CNN-CRFf

­ ­ 94.7 95.08 ­ ­

­
­ ­
­ 96.37 ± 0.05 95.96 ± 0.08

­
91.21 ­
­ 91.93 ± 0.19 91.71 ± 0.10

97.78
97.55 ­
97.53 ­ 97.53 ± 0.03

Baseline (supervised)
Baseline + LM
Consistency CVT, no pfuture or ppast CVT, no pfwd or pbwd CVT
CVT + LM

94.98 ± 0.05 95.08 ± 0.05 95.05 ± 0.03
95.34 ± 0.04
95.43 ± 0.07 95.49 ± 0.04 95.49 ± 0.03

95.03 ± 0.02 95.64 ± 0.07 94.82 ± 0.05
95.63 ± 0.08
95.88 ± 0.09 96.10 ± 0.12 96.15 ± 0.09

90.86 ± 0.03 91.35 ± 0.07 90.84 ± 0.14
91.21 ± 0.07
91.62 ± 0.09 91.66 ± 0.10 92.08 ± 0.10

97.65 ± 0.02 97.70 ± 0.01 97.61 ± 0.02
97.67 ± 0.02
97.76 ± 0.01 97.76 ± 0.01 97.76 ± 0.01

aLing et al. (2015) bMa & Hovy (2016) cLewis et al. (2016) dWu et al. (2017) ePeters et al. (2017) fLiu et al. (2017)

Table 2: Results on sequence tagging tasks. We report the means and standard deviation of 5 runs.
"Baseline" trains with Lsup, "Consistency" trains with" Lsup + Lconsistency, and "CVT" trains with Lsup + LCVC. +LM means language modeling is added as an auxiliary task on the unlabeled data.

Accuracy

95.5 95.0 94.5
128 256
92 91
128 256

CCG
96

Accuracy F1

512 768 Model Size
NER

1024

512 768 Model Size

1024

Baseline (supervised)

95 128 256
97.7 97.6 97.5
128 256
Baseline + LM

Chunk
512 768 Model Size
POS
512 768 Model Size CVT + LM

1024 1024

F1

Figure 4: Accuracy vs. size of the LSTMs in the first layer; the second layer is half the size of the first one. Points and error bars correspond to means and standard deviations over 5 runs.

the "future" and "past" layers are more beneficial than the "forward" and "backward" ones, perhaps because theses provide a more distinct and challenging view of the input.
Training Larger NLP Models. Most sequence taggers in prior use work small LSTMs (hidden state sizes of around 200 units) because larger models yield little to no gains in performance (Reimers & Gurevych, 2017). We found this to be true for our own supervised models, and, to a lesser extent, our models when only using language modeling as the auxiliary task. In contrast, when using CVT accuracy scales much better with model size (see Figure 4). This result suggests the appropriate semi-supervised learning methods may enable the development of larger, more sophisticated models for natural language processing tasks with limited amounts of labeled data.
5 CONCLUSION
We propose Cross-View Training, a new method for semi-supervised learning. Our approach allows models to effectively leverage their own predictions on unlabeled data. We report excellent results on semi-supervised image recognition benchmarks and four sequence tagging tasks from natural language processing. We see the development of CVT architectures for other tasks and theoretical analysis of CVT as potential areas of future work.
8

Under review as a conference paper at ICLR 2018
REFERENCES
Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In NIPS, 2014.
Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In COLT. ACM, 1998.
Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 2006.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. In INTERSPEECH, 2014.
Jason PC Chiu and Eric Nichols. Named entity recognition with bidirectional lstm-cnns. TACL, 2016.
Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In NIPS, 2015.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan Salakhutdinov. Good semisupervised learning that requires a bad gan. In NIPS, 2017.
Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. arXiv preprint arXiv:1708.07860, 2017.
S Fralick. Learning to recognize patterns without a teacher. IEEE Transactions on Information Theory, 13(1):57­64, 1967.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In ICLR, 2015.
Alex Graves and Ju¨rgen Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5):602­610, 2005.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In ICASSP. IEEE, 2013.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.
Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735­1780, 1997.
Julia Hockenmaier and Mark Steedman. Ccgbank: a corpus of ccg derivations and dependency structures extracted from the penn treebank. Computational Linguistics, 33(3):355­396, 2007.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In ICLR, 2017.
Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What is the best multi-stage architecture for object recognition? In IEEE Conference on Computer Vision. IEEE, 2009.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In NIPS, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
9

Under review as a conference paper at ICLR 2018
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017.
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. Neural architectures for named entity recognition. In ACL, 2016.
Yann LeCun, Koray Kavukcuoglu, and Cle´ment Farabet. Convolutional networks and applications in vision. In ISCAS. IEEE, 2010.
Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML, 2013.
Mike Lewis, Kenton Lee, and Luke Zettlemoyer. Lstm ccg parsing. In HLT-NAACL, 2016.
Wang Ling, Tiago Lu´is, Lu´is Marujo, Ramo´n Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso. Finding function in form: Compositional character models for open vocabulary word representation. In EMNLP, 2015.
Liyuan Liu, Jingbo Shang, Frank Xu, Xiang Ren, Huan Gui, Jian Peng, and Jiawei Han. Empower sequence labeling with task-aware neural language model. arXiv preprint arXiv:1709.04109, 2017.
Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional lstm-cnns-crf. In ACL, 2016.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313­330, 1993.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing with virtual adversarial training. In ICLR, 2016.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, 2011.
Kamal Nigam and Rayid Ghani. Analyzing the effectiveness and applicability of co-training. In International conference on information and knowledge management. ACM, 2000.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In EMNLP, 2014.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural networks by penalizing confident output distributions. In ICLR, 2017.
Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. Semi-supervised sequence tagging with bidirectional language models. In ACL, 2017.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computational Mathematics and Mathematical Physics, 4(5):1­17, 1964.
Prajit Ramachandran, Peter J Liu, and Quoc V Le. Unsupervised pretraining for sequence to sequence learning. In EMNLP, 2017.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semisupervised learning with ladder networks. In NIPS, 2015.
Marek Rei. Semi-supervised multitask learning for sequence labeling. In ACL, 2017.
Nils Reimers and Iryna Gurevych. Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging. In EMNLP, 2017.
10

Under review as a conference paper at ICLR 2018
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In NIPS, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NIPS, 2016.
H Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions on Information Theory, 11(3):363­371, 1965.
Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to visual document analysis. In ICDAR, 2003.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. In ICLR, 2015.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In ICML, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.
Antti Tarvainen and Harri Valpola. Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint arXiv:1703.01780, 2017.
Erik F Tjong Kim Sang and Sabine Buchholz. Introduction to the conll-2000 shared task: Chunking. In CoNLL, 2000.
Erik F Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In HLT-NAACL, 2003.
Huijia Wu, Jiajun Zhang, and Chengqing Zong. Shortcut sequence tagging. arXiv preprint arXiv:1701.00576, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.
A SEQUENCE TAGGING MODEL DETAILS
Our sequence-labeling model is a two layer CNN-BiLSTM (Chiu & Nichols, 2016; Ma & Hovy, 2016; Lample et al., 2016). The model produces a representation for each word in the input sentence as follows. First, the characters in the word are embedded, resulting in a sequence of vectors. Then a 1-dimensional convolution over these vectors followed by a max-pooling operation produces a character-level representation of the word. Lastly, this character-based representation is added to a word vector from an embedding matrix. The resulting sequence of word embeddings is then fed through two BiLSTM layers and a softmax layer to produce an output distribution over labels for each token. We apply dropout (Hinton et al., 2012) to the word embeddings and outputs of each BiLSTM. We apply label smoothing (Szegedy et al., 2016; Pereyra et al., 2017) to the target labels. We use an exponential-moving-average (EMA) of the model weights during training as the final model; we found this to slightly improve accuracy and significantly reduce the variance in accuracy between models trained with different random initializations. For Chunking and Named Entity Recognition, we use a BIOES tagging scheme. The model is trained using SGD with momentum (Polyak, 1964; Sutskever et al., 2013). Word embeddings are initialized with GloVe vectors (Pennington et al., 2014). During training, we alternate minimizing Lsup over a minibatch of supervised examples and minimizing LCVC over a minibatch of unlabeled examples. The full set of model hyperparameters are listed below.
11

Under review as a conference paper at ICLR 2018

Parameter
Word Embeddings Initializiation Character Embedding Size Character CNN Filter Widths Character CNN Num Filters LSTM sizes Language model vocabulary size Dropout Label Smoothing EMA coefficient Learning rate Momentum Batch size Stopping criteria

Value
300d GloVe2 100 [2, 3, 4] 300 (100 per filter width) 1024 for the first layer, 512 for the second one 10,000 (only applies to the +LM models) 0.5 for labeled examples, 0.8 for unlabeled examples 0.1 for labeled examples, none for unlabeled examples 0.998 0.5/(1 + 0.005t0.5) (t is number of SGD updates so far) 0.9 64 150,000 updates

B CROSS-VIEW AUXILIARY LOSSES FOR SUPERVISED LEARNING

In initial experiments, we explored whether cross-view losses could benefit purely supervised classifiers. To do this, we trained models with the following objective:



Lsup-cv

=

1 |Dl|

CE(yi, p(y|xi)) +
xi ,yi Dl

1 k

k
CE(yi, pj(y|xi))
j=1

See Section 3.1 for an explanation of the notation. We hoped that adding auxiliary softmax layers with different views of the input would act as a regularizer on the model. However, we found little to no benefit from this approach. For sequence tagging, results improved slightly on CCG and POS but degraded on NER and Chunking. For image recognition, we augmented WideResNet (Zagoruyko & Komodakis, 2016) with auxiliary softmax layers and evaluated it on CIFAR-10 and CIFAR-100. On both datasets, the augmented model performed slightly worse (by 0.2% on CIFAR-10 and 0.9% on CIFAR-100).

We also experimented with using Lsup-cv instead of of Lsup on semi-supervised CIFAR-10 and CIFAR-10+. Surprisingly, it (slightly) decreased performance for all of the methods we experi-
mented with: supervised training, VAT, CVT, and CVT with adversarial noise. We note we only tried these experiments with 1 = 1, but this value of 1 did work well for the semi-supervised setting. These negative results suggest that the gains are from CVT are from the improved self-training
mechanism, not the additional prediction layers regularizing the model.

12

