Under review as a conference paper at ICLR 2018
A PAC-BAYESIAN APPROACH TO SPECTRALLY-NORMALIZED MARGIN BOUNDS FOR NEURAL NETWORKS
Anonymous authors Paper under double-blind review
ABSTRACT
We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The key ingredient is a bound on the changes in the output of a network with respect to perturbation of its weights, thereby bounding the sharpness of the network. We combine this perturbation bound with the PAC-Bayes analysis to derive the generalization bound.
1 INTRODUCTION
Learning with deep neural networks has enjoyed great success across a wide variety of tasks. Even though learning neural networks is a hard problem, even for one hidden layer Blum & Rivest (1993), simple optimization methods such as stochastic gradient descent (SGD) have been able to minimize the training error. More surprisingly, solutions found this way also have small test error, even though the networks used have more parameters than the number of training samples, and have the capacity to easily fit random labels (Zhang et al., 2017).
Harvey et al. (2017) provided a generalization error bound by showing that the VC dimension of d-layer networks is depth times the number parameters improving over earlier bounds by Bartlett et al. (1999). Such VC bounds which depend on the number of parameters of the networks, cannot explain the generalization behavior in the over parametrized settings, where the number of samples is much smaller than the number of parameters.
For linear classifiers we know that the generalization behavior depends on the norm and margin of the classifier and not on the actual number of parameters. Hence, a generalization bound for neural networks that only depends on the norms of its layers, and not the actual number of parameters, can explain the good generalization behavior of the over parametrized networks.
Bartlett & Mendelson (2002) showed generalization bounds for feedforward networks in terms of unit-wise 1 norm with exponential dependence on depth. In a more recent work, Neyshabur et al. (2015a) provided generalization bounds for general class of group norms including Frobenius norm with same exponential dependence on the depth and showed that the exponential dependence is unavoidable in the worst case.
Bartlett et al. (2017) showed a margin based generalization bound that depends on spectral norm and 1 norm of the layers of the networks. They show this bound using a complex covering number argument. This bound does not depend directly on the number of parameters of the network but depends on the norms of its layers. However the 1 term has high dependence on the number of hidden units if the weights are dense, which is typically the case for modern deep learning architectures.
Keskar et al. (2016) suggested a sharpness based measure to predict the difference in generalization behavior of networks trained with different batch size SGD. However sharpness is not a scale invariant measure and cannot predict the generalization behavior (Neyshabur et al., 2017). Instead sharpness when combined with the norms of the network can predict the generalization behavior according to the PAC-Bayes framework (McAllester, 1999). Dziugaite & Roy (2017) numerically evaluated a generalization error bound from the PAC-Bayes framework showing that it can predict the difference in generalization behavior of networks trained on true vs random labels.
1

Under review as a conference paper at ICLR 2018

In this paper we present and prove a margin based generalization bound for feedforward neural networks, that depends on the product of the spectral norm of the weights in each layer, as well as the Frobenius norm of the weights.
Our generalization bound shares much similarity with a margin based generalization bound recently presented by Bartlett et al. (2017). Both bounds depend similarly on the product of the spectral norms of each layer, multiplied by a factor that is additive across layers. In addition, Bartlett et al. (2017) bound depends on the elementwise 1-norm of the weights in each layer, while our bound depends on the Frobenius (elementwise 2) norm of the weights in each layer, with an additional multiplicative dependence on the "width". The two bounds are thus not directly comparable, and as we discuss in Section 3, each one dominates in a different regime, roughly depending on the sparsity of the weights. We also discuss in what regimes each of these bounds could dominate a VC-bound based on the overall number of weights.
More importantly, our proof technique is entirely different, and arguably simpler, than that of Bartlett et al. (2017). We derive our bound using PAC-Bayes analysis, and more specifically a generic PACBayes margin bound (Lemma 1). The main ingredient is a perturbation bound (Lemma 2), bounding the changes in the output of a network when the weights are perturbed, thereby its sharpness, in terms of the product of the spectral norm of the layers. This is an entirely different analysis approach from the covering number analysis of Bartlett et al. (2017). We hope our analysis can give more direct intuition into the different ingredients in the bound and will allow modifying the analysis, e.g. by using different prior and perturbation distributions in the PAC-Bayes bound, to obtain tighter bounds, perhaps with dependence on different layer-wise norms.
We note that prior bounds in terms of elementwise or unit-wise norms (such as the Frobenius norm and elementwise 1 norms of layers), without a spectral norm dependence, all have a multiplicative dependence across layers or exponential dependence on depth Bartlett & Mendelson (2002); Neyshabur et al. (2015b), or are for constant depth networks Bartlett (1998). Here only the spectral norm is multiplied across layers, and thus if the spectral norms are close to one, the exponential dependence on depth can be avoided.

1.1 PRELIMINARIES

Consider the classification task with input domain XB,n = x  Rn | |

n i=1

xi2



B2

and output

domain Rk where the output of the model is a score for each class and the class with the maximum

score will be selected as the predicted label. Let fw(x) : XB,n  Rk be the function computed

by a d layer feed-forward network for the classification task with parameters w = vec {Wi}id=1 ,

fw(x) = Wd (Wd-1 (....(W1x))), here  is the ReLU activation function. Let fwi (x) denote

the output of layer i before activation and h be an upper bound on the number of output units in each

layer. fwi (x)

We can then define fully = Wi(fwi-1(x)). Let .

connected feedforward networks recursively: F , . 1 and . 2 denote the Frobenius norm,

fw1 the

(x) = W1x and element-wise 1

norm and the spectral norm respectively. We further denote the p norm of a vector by |.|p.

Margin Loss. For any distribution D and margin  > 0, we define the expected margin loss as follows:

L (fw) = P(x,y)D fw(x)[y]   + max fw(x)[j]
j=y

(1)

Let L(fw) be the empirical estimate of the above expected margin loss. Since setting  = 0
corresponds to the classification loss, we will use L0(fw) and L0(fw) to refer to the expected risk and the training error. The loss L defined this way is bounded between 0 and 1.

1.2 PAC-BAYESIAN FRAMEWORK
The PAC-Bayesian framework (McAllester, 1998; 1999) provides generalization guarantees for randomized predictors, drawn form a learned distribution Q (as opposed to a learned single predictor) that depends on the training data. In particular, let fw be any predictor (not necessarily a neural network) learned from the training data and parametrized by w. We consider the distribution Q over predictors of the form fw+u, where u is a random variable whose distribution may also depend on

2

Under review as a conference paper at ICLR 2018

the training data. Given a "prior" distribution P over the set of predictors that is independent of the training data, the PAC-Bayes theorem states that with probability at least 1 -  over the draw of the training data, the expected error of fw+u can be bounded as follows (McAllester, 2003):

Eu[L0(fw+u)]  Eu[L0(fw+u)] + 2

2

KL (w + u

P

)

+

ln

2m 

.

m-1

(2)

To get a bound on the expected risk L0(fw) for a single predictor fw, we need to relate the expected perturbed loss, Eu[L0(fw+u)] in the above equation with L0(fw). Toward this we use the following lemma that gives a margin-based generalization bound derived from the PAC-Bayesian bound (2):

Lemma 1. Let fw(x) : X  Rk be any predictor (not necessarily a neural network) with parameters

w, and P be any distribution on the parameters that is independent of the training data. Then, for

any ,  > 0, with probability  1 -  over the training set of size m, for any w, and any random

perturbation u s.t. Pu

maxxX

|fw+u(x)

-

fw(x)|

<

 4



1 2

,

we

have:

L0(fw)  L (fw) + 4

KL

(w

+u P) m-1

+

ln

6m 

.

In the above expression the KL is evaluated for a fixed w and only u is random, i.e. the distribution of w + u is the distribution of u shifted by w. The lemma is analogous to similar analysis of Langford & Shawe-Taylor (2003) and McAllester (2003) obtaining PAC-Bayes margin bounds for linear predictors, and the proof, presented in Section 4, is essentially the same. As we state the lemma, it is not specific to linear separators, nor neural networks, and holds generally for any real-valued predictor.
We next show how to utilize the above general PAC-Bayes bound to prove generalization guarantees for feedforward networks based on the spectral norm of its layers.

2 GENERALIZATION BOUND

In this section we present our generalization bound for feedfoward networks with ReLU activations, derived using the PAC-Bayesian framework. Langford & Caruana (2001), and more recently Dziugaite & Roy (2017) and Neyshabur et al. (2017), used PAC-Bayes bounds to analyze generalization behavior in neural networks, evaluating the KL-divergence, "perturbation error" L[fw+u] - L[fw], or the entire bound numerically. Here, we use the PAC-Bayes framework as a tool to analytically derive a margin-based bound in terms of norms of the weights. As we saw in Lemma 1, the key to doing so is bounding the change in the output of the network when the weights are perturbed. In the following lemma, we bound this change in terms of the spectral norm of the layers:

Lemma 2 (Perturbation Bound). For any B, d > 0, let fw : XB,n  Rk be a d-layer network. Then

for any w, and x  XB,n, and any perturbation u = vec

{Ui}id=1

such that

Ui

2

1 d

Wi 2,

the change in the output of the network can be bounded as follows:

|fw+u(x) - fw(x)|2  eB

d
Wi 2
i=1

d Ui 2 . i=1 Wi 2

This lemma characterizes the change in the output of a network with respect to perturbation of its weights, thereby bounding the sharpness of the network as defined in Keskar et al. (2016).

The proof of this lemma is presented in Section 4. Next we use the above perturbation bound and the PAC-Bayes result (Lemma 1) to derive the following generalization guarantee.
Theorem 1 (Generalization Bound). For any B, d, h > 0, let fw : XB,n  Rk be a d-layer feedforward network with ReLU activations. Then, for any ,  > 0, with probability  1 -  over a training set of size m, for any w, we have:



L0 (fw )



L (fw)

+

O

 



B2d2h ln(dh)di=1

Wi

2 2

2m



d i=1

Wi Wi

2 F 2 2

+

ln

dm 

 

.



3

Under review as a conference paper at ICLR 2018

Proof. The proof involves mainly two steps. In the first step we calculate what is the maximum allowed perturbation of parameters to satisfy a given margin condition , using Lemma 2. In the second step we calculate the KL term in the PAC-Bayes bound in Lemma 1, for this value of the perturbation.

Let  =

d i=1

1/d
Wi 2 and consider a network with the normalized weights Wi =

 Wi

Wi.
2

Due to the homogeneity of the ReLU, we have that for feedforward networks with ReLU activations

fw = fw, and so the (empirical and expected) loss (including margin loss) is the same for w and w.

We can also verify that

d i=1

Wi 2

=

d i=1

Wi
2

and

=Wi F
Wi 2

W~ i W~ i

F , and so the excess
2

error in the Theorem statement is also invariant to this transformation. It is therefore sufficient to

prove the Theorem only for the normalized weights w~ , and hence we assume w.l.o.g. that the spectral

norm is equal across layers, i.e. for any layer i, Wi 2 = .

Choose the distribution of the prior P to be N (0, 2I), and consider the random perturbation

u  N (0, 2I), with the same , which we will set later according to . More precisely, since the

prior cannot depend on the learned predictor w or its norm, we will set  based on an approximation

~. For each value of ~ on a pre-determined grid, we will compute the PAC-Bayes bound, establishing

the

generalization

guarantee

for

all

w

for

which

|

-

~|



1 d

,

and

ensuring

that

each

relevant

value

of  is covered by some ~ on the grid. We will then take a union bound over all ~ on the grid. For now,

we

will

consider

a

fixed

~

and

the

w

for

which

|

- ~|



1 d



,

and

hence

1 e



d-1



~d-1



ed-1.

Since u  N (0, 2I), we get the following bound for the spectral norm of Ui Tropp (2012):

PUiN(0,2I) [ Ui 2 > t]  2he-t2/2h2 .

Taking a

union

bond

over

the

layers,

we get

that,

with probability



1 2

,

the

spectral

norm of the

perturbation Ui in each layer is bounded by  2h ln(4dh). Plugging this spectral norm bound into

Lemma

2

we

have

that

with

probability

at

least

1 2

,

max |fw+u(x) - fw(x)|  eBd

xXB,n

i

= eBd-1

Ui 2 
Ui 2  e2dB~d-1
i

2h ln(4dh)



 ,

4

(3)

where

we

choose



=


42dB~d-1 h ln(4hd)

to

get

the

last

inequality.

Hence,

the

perturbation

u

with

the above value of  satisfies the assumptions of the Lemma 1.

We now calculate the KL-term in Lemma 1 with the chosen distributions for P and u, for the above value of .

KL(w + u||P ) 

|w|2 22



O

B

2

d2h

ln(dh)

id=1 

Wi
2

2 2

d

i=1

Wi

2 F

Wi

2 2

.

Hence,

for

any

~,

with

probability



1

-



and

for

all

w

such

that,

|

-

~|



1 d



,

we

have:



L0 (fw )



L (fw)

+

O

 



B2d2h ln(dh)di=1

Wi

2 2

2m



d i=1

Wi Wi

2 F 2 2

+

ln

m 

 

.



(4)

Finally we need to take a union bound over different choices of ~. Let us see how many choices of ~

we

need

to

ensure

we

always

have

~

in

the

grid

s.t.

|~ -

|



1 d



.

We

only

need

to

consider

values

of  in the range

 2B

1/d   

 m
2B

1/d
. For  outside this range the theorem statement holds

trivially:

Recall

that

the

LHS

of

the

theorem

statement,

L0(fw)

is

always

bounded

by

1. If


d

<

 2B

,

then

for

any

x,

|fw(x)|



dB



/2 and

therefore

L

=

1.

Alternately,

if

d

>

m 2B

,

then

the

4

Under review as a conference paper at ICLR 2018

second term in equation 2 is greater than one. Hence, we only need to consider values of  in the

range

discussed

above.

Since

we

need

~

to

satisfy

|~

-

|



1 d





1 d

 2B

1/d, the size of the cover

we

need

to

consider

is

bounded

by

dm

1 2d

.

Taking

a

union

bound

over

the

choices

of

~

in

this

cover

and using the bound in equation (4) gives us the theorem statement.

3 COMPARISON TO EXISTING GENERALIZATION BOUNDS

In this section we will compare the bound of Theorem 1 with a similar spectral norm based margin bound recently obtained by Bartlett et al. (2017), as well as examine whether and when these bounds can improve over VC-based generalization guarantees.

The VC-dimension of fully connected feedforward neural networks with ReLU activation with d layers and h units per layer is ~ (d2h2) (Harvey et al. (2017)), yielding a generalization guarantee of

the form:

L0(fw)  L^0(fw) + O~

d2h2 m

(5)

where here and throughout this section we ignore logarithmic factors that depend on - the failure probability , the sample size m, the depth d and the number of units h.
Bartlett et al. (2017) showed a generalization bound for neural networks based on the spectral norm of its layers, using a different proof approach based on covering number arguments. For feedforward depth-d networks with ReLU activations, and when inputs are in XB,n, i.e. are of norm bounded by |x|2  B, their generalization guarantee, ignoring logarithmic factors, ensures that, with high probability, for any w,



L0 (fw )



L (fw)

+

O~

 



B 2 di=1

Wi

2 2

d i=1

2m


Wi 1 2/3 3

Wi 2

 

.



(6)

Comparing our Theorem 1 and the citetbartlett2017spectrally bound (6), the factor

O

1 2

B

2

id=1

Wi

2 2

appears in both bounds. The main difference is in the multiplicative factors,

d2h

d i=1

Wi

/2
F Wi

2 2

in

Theorem

1

compared

to

d i=1

/Wi 1 Wi 2

2/3

3
in (6). To get a sense

of how these two bounds compare, we will consider the case where the norms of the weight matrices

are uniform across layers--this is a reasonable situation as we already saw that the bounds are

invariant to re-balancing the norm between the layers. But for the sake of comparison, we further

assume that not only is the spectral norm equal across layers (this we can assume w.l.o.g.) but also

the Frobenius Wi F and element-wise 1 norm Wi 1 are uniform across layers (we acknowledge that this setting is somewhat favorable to our bound compared to (6)). In this case the numerator in

the generalization bound of Theorem 1 scales as:

O

d3h

Wi

2 F

Wi

2 2

,

(7)

while numerator in (6) scales as:

O

d3

Wi

2 1

Wi

2 2

.



(8)

Comparing between the bounds thus boils down to comparing h Wi F with Wi 1. Recalling that Wi is an h × h matrix, we have that Wi F  Wi 1  h Wi F . When the weights are

fairly dense and are of uniform magnitude, the second inequality will be tight, and we will have

h Wi F Wi 1, and Theorem 1 will dominate. When the weights are sparse with roughly a

constant number of significant weights per unit (i.e. weight matrix with sparsity (h)), the bounds

will be similar. Bartlett et al. (2017) bound will dominate when the weights are extremely sparse,

with much fewer significant weights than units, i.e. when most units do not have any incoming or

outgoing weights of significant magnitude.

5

Under review as a conference paper at ICLR 2018

It is also insightful to ask in what regime each bound could potentially improve over the VC-bound

(5) and thus provide a non-trivial guarantee. To this end, we consider the most "optimistic" scenario

where

B2 2

id=1

Wi

2 2

=

(1)

(it

certainly

cannot

be

lower

than

one

if

we

have

a

non-trivial

margin

loss). As before, we also take the norms of the weight matrices to be uniform across layers, yielding

the multiplicative factors in (7) and (8), which we must compare to the VC-dimension ~ (d2h2). We

get that the bound of Theorem 1 is smaller than the VC bound if

Wi F = o h/d Wi 2 .

(9)

 We always have Wi F  h Wi 2, and this is tight only for orthogonal matrices, where all eigenvalues are equal. Satisfying (9), and thus having Theorem 1 potentially improving over the

VC-bound, thus only requires fairly mild eigenvalue concentration (i.e. having multiple units be

similar to each other), reduced rank or row-level sparsity in the weight matrices. Note that we cannot

expect to improve over the VC-bound for unstructured "random" weight matrices--we can only

expect norm-based guarantees to improve over the VC bound if there is some specific degenerate

structure in the weights, and as we indeed see is the case here.

A similar comparison with Bartlett et al. (2017) bound (6) and its multiplicative factor (8), yields the

following condition for improving over the VC bound:

 Wi 1 = o (h/ d) Wi 2 .

(10)

Since Wi 1 can be as large as h1.5 Wi 2, in some sense more structure is required here in order to satisfy (10), such as elementwise sparsity combined with low-rank row structure. As discussed above, Theorem 1 and Bartlett et al. (2017) bound can each be better in different regimes. Also in terms of comparison to the VC-bound, it is possible for either one to improve over the VC bound while the other doesn't (i.e. for either (9) or (10) to be satisfied without the other one being satisfied), depending on the sparsity structure in the weights.

4 PROOFS OF LEMMAS

In this section we present the proofs of Lemmas 1 and 2.

Proof of Lemma 1. Let w = w + u. Let Sw be the set of perturbations with the following property:

Sw 

w

max |fw
xXB,n

(x) - fw(x)|

<

 4

.

Let q be the probability density function over the parameters w . We construct a new distribution Q~ over predictors fw~ where w~ is restricted to Sw with the probability density function:

q~(w~ ) = 1 q(w~ ) w~  Sw

Z0

otherwise.

Here Z

is a normalizing constant and by the lemma assumption Z

=

P [w

 Sw]



1 2

.

By the

definition of Q~, we have:



max
xXB,n

|fw~ (x)

-

fw

(x)|

<

. 4

Therefore,

the

perturbation

can

change

the

margin

between

two

output

units

of

fw

by

at

most

 2

;

i.e.

for any perturbed parameters w~ drawn from Q~:

max
i,j[k],xXB,n

|(|fw~ (x)[i]

-

fw~ (x)[j]|)

-

(|fw(x)[i]

-

fw(x)[j]|)|

<

 2

Since the above bound holds for any x in the domain XB,n, we can get the following a.s.:

L0(fw)



L

 2

(fw~ )

L

 2

(fw~ )



L (fw)

6

Under review as a conference paper at ICLR 2018

Now using the above inequalities together with the equation (2), with probability 1 -  over the training set we have:

L0(fw)  Ew~

L

 2

(fw~

)

 Ew~

L

 2

(fw~

)

+2

2(KL (w~

P

)

+

ln

2m 

)

m-1

 L(fw) + 2

2(KL (w~

P

)

+

ln

2m 

)

m-1

 L(fw) + 4

KL

(w P ) m-

+ 1

ln

6m 

,

The last inequality follows from the following calculation.
Let Swc denote the complement set of Sw and q~c denote the density function q restricted to Swc and normalized. Then,
KL(q||p) = ZKL(q~||p) + (1 - Z)KL(q~c||p) - H(Z),

where H(Z) = -Z ln Z - (1 - Z) ln(1 - Z)  1 is the binary entropy function. Since KL is always positive, we get,

KL(q~||p) = 1 [KL(q||p) + H(Z)) - (1 - Z)KL(q~c||p)]  2(KL(q||p) + 1). Z

Proof of Lemma 2. Let i = fwi +u(x) - fwi (x) 2. We will prove using induction that for any i  0:

i 

1i 1+
d

i
Wj 2
j=1

i
|x|2
j=1

Uj 2 . Wj 2

The above inequality together with

1

+

1 d

d  e proves the lemma statement. The induction base

clearly holds since 0 = |x - x|2 = 0. For any i  1, we have the following:

i+1 = (Wi+1 + Ui+1) i(fwi +u(x)) - Wi+1i(fwi (x)) 2 = (Wi+1 + Ui+1) i(fwi +u(x)) - i(fwi (x)) + Ui+1i(fwi (x)) 2  ( Wi+1 2 + Ui+1 2) i(fwi +u(x)) - i(fwi (x)) 2 + Ui+1 2 i(fwi (x)) 2  ( Wi+1 2 + Ui+1 2) fwi +u(x) - fwi (x) 2 + Ui+1 2 fwi (x) 2 = i ( Wi+1 2 + Ui+1 2) + Ui+1 2 fwi (x) 2 ,

where the last inequality is by the Lipschitz property of the activation function and using (0) = 0.

The 2 norm of outputs of layer i is bounded by |x|2 ji=1 Wj 2 and by the lemma assumption we

have

Ui+1

2



1 d

Wi+1 2. Therefore, using the induction step, we get the following bound:

i+1  i

1 1+
d

i

Wi+1 2 + Ui+1 2 |x|2

Wj 2

j=1



1 1+
d

i+1

i+1
Wj 2
j=1

i
|x|2
j=1

Uj 2 + Wj 2

Ui+1 Wi+1

2 2

i+1
|x|2
j=1

Wi

2



1 1+
d

i+1

i+1
Wj 2
j=1

i+1
|x|2
j=1

Uj 2 . Wj 2

7

Under review as a conference paper at ICLR 2018
5 CONCLUSION
In this paper, we presented new perturbation bounds for neural networks thereby giving a bound on its sharpness. We also discussed how PAC-Bayes framework can be used to derive generalization bounds based on the sharpness of a model class. Applying this to the feedforward networks, we showed that a tighter generalization bound can be achieved based on the spectral norm and Frobenius norm of the layers. The simplicity of the proof compared to that of covering number arguments in Bartlett et al. (2017) suggest that the PAC-Bayes framework might be an important tool in analyzing the generalization behavior of neural networks.
REFERENCES
Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1706.08498, 2017.
Peter L Bartlett. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE transactions on Information Theory, 44(2):525­536, 1998.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463­482, 2002.
Peter L Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise polynomial networks. In Advances in Neural Information Processing Systems, pp. 190­196, 1999.
Avrim L Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In Machine learning: From theory to applications, pp. 9­28. Springer, 1993.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. arXiv preprint arXiv:1703.11008, 2017.
Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension bounds for piecewise linear neural networks. arXiv preprint arXiv:1703.02930, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
John Langford and Rich Caruana. (not) bounding the true error. In Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, pp. 809­816. MIT Press, 2001.
John Langford and John Shawe-Taylor. Pac-bayes & margins. In Advances in neural information processing systems, pp. 439­446, 2003.
David McAllester. Simplified pac-bayesian margin bounds. Lecture notes in computer science, pp. 203­215, 2003.
David A McAllester. Some PAC-Bayesian theorems. In Proceedings of the eleventh annual conference on Computational learning theory, pp. 230­234. ACM, 1998.
David A McAllester. PAC-Bayesian model averaging. In Proceedings of the twelfth annual conference on Computational learning theory, pp. 164­170. ACM, 1999.
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro. Path-SGD: Path-normalized optimization in deep neural networks. In Advanced in Neural Information Processsing Systems (NIPS), 2015a.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In Proceeding of the 28th Conference on Learning Theory (COLT), 2015b.
8

Under review as a conference paper at ICLR 2018 Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring general-
ization in deep learning. arXiv preprint arXiv:1706.08947, 2017. Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational
mathematics, 12(4):389­434, 2012. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning Representations, 2017.
9

